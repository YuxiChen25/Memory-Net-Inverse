{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.DAMP_Network.DAMP import DAMP\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02895980746050676\n",
      "Average test loss: 0.018552980792191292\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015055556972821554\n",
      "Average test loss: 0.03334287686480416\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014060406862033739\n",
      "Average test loss: 0.020443564309014214\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013609955104688803\n",
      "Average test loss: 0.018710224603613216\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012931285546057754\n",
      "Average test loss: 0.018182370210687318\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012864478117889828\n",
      "Average test loss: 0.027331066283914778\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012601749579939578\n",
      "Average test loss: 0.014087731497155295\n",
      "Epoch 8/300\n",
      "Average training loss: 0.012114678683380287\n",
      "Average test loss: 0.01685232718785604\n",
      "Epoch 9/300\n",
      "Average training loss: 0.012103050229450067\n",
      "Average test loss: 0.015599329979055457\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011680418831606707\n",
      "Average test loss: 0.015654376731978524\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011507688006593122\n",
      "Average test loss: 0.22059065658516355\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011203930775324503\n",
      "Average test loss: 0.046084362301561564\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01090041312823693\n",
      "Average test loss: 0.04412705411513646\n",
      "Epoch 14/300\n",
      "Average training loss: 0.010711154229111142\n",
      "Average test loss: 0.04804559840758642\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010672417816188601\n",
      "Average test loss: 0.04240786629584101\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01026136837982469\n",
      "Average test loss: 2.2415086743036907\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010084168296721246\n",
      "Average test loss: 0.9261865721013811\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010110909691287412\n",
      "Average test loss: 0.024455462033549945\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01014491171307034\n",
      "Average test loss: 0.2712148871190018\n",
      "Epoch 20/300\n",
      "Average training loss: 0.011076885578533013\n",
      "Average test loss: 0.05578032236960199\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009893563570247756\n",
      "Average test loss: 0.1772527040110694\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009669055960244602\n",
      "Average test loss: 0.020077155162890752\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009502131146689257\n",
      "Average test loss: 0.05664371677570873\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009788249121771918\n",
      "Average test loss: 0.026736505364378292\n",
      "Epoch 25/300\n",
      "Average training loss: 0.009468306203683217\n",
      "Average test loss: 1.253296330663893\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009403629642393854\n",
      "Average test loss: 0.02871631958749559\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009238850995484325\n",
      "Average test loss: 0.01992690163354079\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009322703565160434\n",
      "Average test loss: 0.08060630995697446\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009201151418189208\n",
      "Average test loss: 0.02331596479813258\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009037826144033008\n",
      "Average test loss: 0.020717996440000004\n",
      "Epoch 31/300\n",
      "Average training loss: 0.009130316713617907\n",
      "Average test loss: 0.016079448488023545\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008836251150402758\n",
      "Average test loss: 0.014545120689604018\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008732799329691463\n",
      "Average test loss: 0.02284678715467453\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008747181216875712\n",
      "Average test loss: 0.04287286192178726\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008648651612301668\n",
      "Average test loss: 0.024865566339757707\n",
      "Epoch 36/300\n",
      "Average training loss: 0.008558210518625048\n",
      "Average test loss: 0.029143252068095735\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00860097947385576\n",
      "Average test loss: 0.03154621057377921\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00857494228416019\n",
      "Average test loss: 0.047331525067488354\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008332529513372315\n",
      "Average test loss: 0.2129417642156283\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008258658509287569\n",
      "Average test loss: 0.03748163053724501\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008215722063349353\n",
      "Average test loss: 0.028939422534571754\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008147616801990403\n",
      "Average test loss: 0.033495557285017435\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008068905296425025\n",
      "Average test loss: 0.02650099491245217\n",
      "Epoch 44/300\n",
      "Average training loss: 0.008085956277118788\n",
      "Average test loss: 0.026662583235237333\n",
      "Epoch 45/300\n",
      "Average training loss: 0.008116404605408509\n",
      "Average test loss: 0.014789829282297029\n",
      "Epoch 46/300\n",
      "Average training loss: 0.008078045113633076\n",
      "Average test loss: 0.045173994335863325\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00803351214610868\n",
      "Average test loss: 0.019352684028446674\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00797506541510423\n",
      "Average test loss: 0.02218627401192983\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00785072520830565\n",
      "Average test loss: 0.036682554711898165\n",
      "Epoch 50/300\n",
      "Average training loss: 0.007913697188099226\n",
      "Average test loss: 0.02176316121386157\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007723968150715033\n",
      "Average test loss: 0.033706850505537456\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007705370203488403\n",
      "Average test loss: 0.03460952899853389\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007715127639472485\n",
      "Average test loss: 0.0325755145351092\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007579527303162548\n",
      "Average test loss: 0.02857733816901843\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007626269770165285\n",
      "Average test loss: 0.02722387076417605\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0075976821788483195\n",
      "Average test loss: 0.022633859576450453\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007737309890902704\n",
      "Average test loss: 0.01570916716588868\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007505581988228692\n",
      "Average test loss: 0.026084896715150938\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007453336843599875\n",
      "Average test loss: 0.022119696239630383\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008099621948682599\n",
      "Average test loss: 0.025706973421904776\n",
      "Epoch 61/300\n",
      "Average training loss: 0.007769509825441573\n",
      "Average test loss: 0.01376794268604782\n",
      "Epoch 62/300\n",
      "Average training loss: 0.007564886488434341\n",
      "Average test loss: 0.017414800680345958\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007452887879891528\n",
      "Average test loss: 0.01967768921620316\n",
      "Epoch 64/300\n",
      "Average training loss: 0.007454618603818947\n",
      "Average test loss: 0.01711623234881295\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007361341615104013\n",
      "Average test loss: 0.04397588626543681\n",
      "Epoch 66/300\n",
      "Average training loss: 0.007363092941542466\n",
      "Average test loss: 0.015495419568485684\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007244005855586794\n",
      "Average test loss: 0.013472532153129577\n",
      "Epoch 68/300\n",
      "Average training loss: 0.00728021909089552\n",
      "Average test loss: 0.014763772751722071\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007264225763993131\n",
      "Average test loss: 0.06330256789757145\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007270395949068997\n",
      "Average test loss: 0.016732857518725924\n",
      "Epoch 71/300\n",
      "Average training loss: 0.00713725366940101\n",
      "Average test loss: 0.01253297753052579\n",
      "Epoch 72/300\n",
      "Average training loss: 0.007098431369082795\n",
      "Average test loss: 0.01626806065440178\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007111648852626482\n",
      "Average test loss: 0.012260806471937233\n",
      "Epoch 74/300\n",
      "Average training loss: 0.007048652056604624\n",
      "Average test loss: 0.02069441865964068\n",
      "Epoch 75/300\n",
      "Average training loss: 0.007028581420166625\n",
      "Average test loss: 0.019673511309756173\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0075090066318710645\n",
      "Average test loss: 4.45496964138084\n",
      "Epoch 77/300\n",
      "Average training loss: 0.00728953850766023\n",
      "Average test loss: 0.04505511240826713\n",
      "Epoch 78/300\n",
      "Average training loss: 0.007024622785548369\n",
      "Average test loss: 0.011524905353784561\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006968157390339507\n",
      "Average test loss: 0.014947611772351795\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006934312627133396\n",
      "Average test loss: 0.016664509612652992\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007055515199071831\n",
      "Average test loss: 0.01505524145066738\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007034289093067248\n",
      "Average test loss: 0.015480314555267494\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007071572960664829\n",
      "Average test loss: 0.021658820273147687\n",
      "Epoch 84/300\n",
      "Average training loss: 0.006985573449896441\n",
      "Average test loss: 0.014839405417442322\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006894571479409933\n",
      "Average test loss: 0.011239813261561923\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006895573334975375\n",
      "Average test loss: 0.018530656362573304\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00739403782867723\n",
      "Average test loss: 0.014807928760846457\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007304508731183079\n",
      "Average test loss: 0.014341450755794844\n",
      "Epoch 89/300\n",
      "Average training loss: 0.00700186621853047\n",
      "Average test loss: 0.019727597115768325\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006977270685136318\n",
      "Average test loss: 0.016749431206120387\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007480635170307425\n",
      "Average test loss: 0.15973896266354454\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007276862997147772\n",
      "Average test loss: 0.015399135040740171\n",
      "Epoch 93/300\n",
      "Average training loss: 0.006989367773549424\n",
      "Average test loss: 0.0195398612279031\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006903924422545566\n",
      "Average test loss: 0.014535351073576345\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00686139177903533\n",
      "Average test loss: 447.7900858561198\n",
      "Epoch 96/300\n",
      "Average training loss: 0.006877464215788576\n",
      "Average test loss: 0.018308915646539792\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0067243365434308845\n",
      "Average test loss: 0.02197507976161109\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006738845526344246\n",
      "Average test loss: 0.010104802947905328\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006691578440368175\n",
      "Average test loss: 0.015491498649120332\n",
      "Epoch 100/300\n",
      "Average training loss: 0.006686849871029457\n",
      "Average test loss: 0.011298118549088637\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0066354499546190104\n",
      "Average test loss: 0.011253133584227826\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006689749302549495\n",
      "Average test loss: 0.01137995111114449\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00668633876906501\n",
      "Average test loss: 0.02662171540160974\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006622457596162955\n",
      "Average test loss: 0.014127819476028284\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006541214894917276\n",
      "Average test loss: 0.016307294994592665\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0065969939008355145\n",
      "Average test loss: 0.014630124760170778\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006602606300678518\n",
      "Average test loss: 0.012985588431358337\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006586813609633181\n",
      "Average test loss: 0.013467283644609982\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006554398052808311\n",
      "Average test loss: 0.014896973576810624\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006530805901520782\n",
      "Average test loss: 0.026378432114919026\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00650514175163375\n",
      "Average test loss: 0.010097421208189593\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006481949679553509\n",
      "Average test loss: 0.01597711502512296\n",
      "Epoch 113/300\n",
      "Average training loss: 0.006641617543995381\n",
      "Average test loss: 3.6262691362433963\n",
      "Epoch 114/300\n",
      "Average training loss: 0.006648878335538838\n",
      "Average test loss: 0.01229334953510099\n",
      "Epoch 115/300\n",
      "Average training loss: 0.006472631596028805\n",
      "Average test loss: 0.03262861691249742\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006409981860054864\n",
      "Average test loss: 0.01156641497876909\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006678066569070021\n",
      "Average test loss: 0.023717418101098803\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006544116353823079\n",
      "Average test loss: 0.014113538802497917\n",
      "Epoch 119/300\n",
      "Average training loss: 0.00642733030517896\n",
      "Average test loss: 0.13703752003775702\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006430277979208364\n",
      "Average test loss: 0.016490781204568015\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006366996077199777\n",
      "Average test loss: 0.013243021809392505\n",
      "Epoch 122/300\n",
      "Average training loss: 0.006353532125552495\n",
      "Average test loss: 0.054779050562116835\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006359271105378866\n",
      "Average test loss: 0.020741417328516643\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006346215375181701\n",
      "Average test loss: 0.016656790476706294\n",
      "Epoch 125/300\n",
      "Average training loss: 0.006456092721058263\n",
      "Average test loss: 0.011549920690556367\n",
      "Epoch 126/300\n",
      "Average training loss: 0.006299998496141698\n",
      "Average test loss: 0.019138325019015207\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006433033960974879\n",
      "Average test loss: 0.012517443348136214\n",
      "Epoch 128/300\n",
      "Average training loss: 0.006273086994886399\n",
      "Average test loss: 0.015821367050210634\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0063553853606184325\n",
      "Average test loss: 0.012854265092147722\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006312408238649368\n",
      "Average test loss: 0.024739650461408826\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0062580249069465535\n",
      "Average test loss: 0.018727806021769842\n",
      "Epoch 132/300\n",
      "Average training loss: 0.006251264938877689\n",
      "Average test loss: 0.014414340144230259\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006242171989960803\n",
      "Average test loss: 0.012823292190829913\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0069541450010405645\n",
      "Average test loss: 0.014630554637147321\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006577681598150068\n",
      "Average test loss: 0.09274307696024577\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006412385130094157\n",
      "Average test loss: 0.018106805032326114\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00638248623162508\n",
      "Average test loss: 0.1239189431005054\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006353338253994783\n",
      "Average test loss: 0.2834791107111507\n",
      "Epoch 139/300\n",
      "Average training loss: 0.006349854649768935\n",
      "Average test loss: 0.03958162126938502\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006290917773627573\n",
      "Average test loss: 0.05016830393340853\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006274624232616689\n",
      "Average test loss: 1.1460413354237875\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006241680875006649\n",
      "Average test loss: 0.09268170846833124\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006215678833838966\n",
      "Average test loss: 0.022527110598153537\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007612646340909931\n",
      "Average test loss: 0.02263557977312141\n",
      "Epoch 145/300\n",
      "Average training loss: 0.006707285152541266\n",
      "Average test loss: 0.013427526651157273\n",
      "Epoch 146/300\n",
      "Average training loss: 0.006316974031842417\n",
      "Average test loss: 0.027272203150722714\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006229047960291306\n",
      "Average test loss: 0.041445344309012096\n",
      "Epoch 148/300\n",
      "Average training loss: 0.006162791384590997\n",
      "Average test loss: 0.10043596417374082\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006136068892975648\n",
      "Average test loss: 0.06267757329013612\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006155625881420241\n",
      "Average test loss: 0.014796795763075351\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0061029057726264\n",
      "Average test loss: 0.017000315431091522\n",
      "Epoch 152/300\n",
      "Average training loss: 0.006136707091910972\n",
      "Average test loss: 0.01455634481128719\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006364882585489087\n",
      "Average test loss: 0.01282751096288363\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006131803824255864\n",
      "Average test loss: 0.018830304303103024\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006232441395107243\n",
      "Average test loss: 0.023528498674432435\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00613267877118455\n",
      "Average test loss: 0.03343293546968036\n",
      "Epoch 157/300\n",
      "Average training loss: 0.006063435045795308\n",
      "Average test loss: 0.4487624943786197\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006053880968027644\n",
      "Average test loss: 0.02474546319908566\n",
      "Epoch 159/300\n",
      "Average training loss: 0.006037568762484524\n",
      "Average test loss: 0.012862703566749891\n",
      "Epoch 160/300\n",
      "Average training loss: 0.006030763213833173\n",
      "Average test loss: 0.22217732304996915\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006020924554102951\n",
      "Average test loss: 0.5548435371981727\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0060243860301044255\n",
      "Average test loss: 0.019945469897654323\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006027612796260251\n",
      "Average test loss: 0.020883808400895862\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006087581476403607\n",
      "Average test loss: 0.01081736214665903\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0059659753702580925\n",
      "Average test loss: 0.012221152879297733\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005998876061704424\n",
      "Average test loss: 0.6898107858763801\n",
      "Epoch 167/300\n",
      "Average training loss: 0.006034701728572448\n",
      "Average test loss: 0.02195127047681146\n",
      "Epoch 168/300\n",
      "Average training loss: 0.005965139260722532\n",
      "Average test loss: 0.02083608943141169\n",
      "Epoch 169/300\n",
      "Average training loss: 0.005940958055357139\n",
      "Average test loss: 0.09066323473718431\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007140849638316366\n",
      "Average test loss: 0.018579787975384128\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0064130919861296815\n",
      "Average test loss: 2.006183130456342\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006129657024724616\n",
      "Average test loss: 0.014003223992056316\n",
      "Epoch 173/300\n",
      "Average training loss: 0.005994544019301732\n",
      "Average test loss: 0.01584031608866321\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006064619977027178\n",
      "Average test loss: 0.013546242636111048\n",
      "Epoch 175/300\n",
      "Average training loss: 0.005959616364290317\n",
      "Average test loss: 0.049452731284830305\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005925820959524976\n",
      "Average test loss: 0.047243039935827257\n",
      "Epoch 177/300\n",
      "Average training loss: 0.005931385094506873\n",
      "Average test loss: 0.013274949611061149\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005925175084422032\n",
      "Average test loss: 0.014716419758068191\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0059049626278380556\n",
      "Average test loss: 0.01804278890126281\n",
      "Epoch 180/300\n",
      "Average training loss: 0.005868542503979471\n",
      "Average test loss: 0.014614736006491715\n",
      "Epoch 181/300\n",
      "Average training loss: 0.005872620691855748\n",
      "Average test loss: 0.05030842263168759\n",
      "Epoch 182/300\n",
      "Average training loss: 0.005882889924777879\n",
      "Average test loss: 0.14035167404678134\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006006374827689595\n",
      "Average test loss: 0.014470307694541083\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0058846023869183325\n",
      "Average test loss: 7.75027157107989\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005833594344970253\n",
      "Average test loss: 0.06525648454493946\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00593743390796913\n",
      "Average test loss: 0.04256572947071658\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005926920000463724\n",
      "Average test loss: 0.014979096293449403\n",
      "Epoch 188/300\n",
      "Average training loss: 0.005871093895286322\n",
      "Average test loss: 0.016454108698500527\n",
      "Epoch 189/300\n",
      "Average training loss: 0.005811308725840516\n",
      "Average test loss: 0.018300262072020106\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005822683377398385\n",
      "Average test loss: 0.42750755937894186\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005802973246408834\n",
      "Average test loss: 0.7959765272604095\n",
      "Epoch 192/300\n",
      "Average training loss: 0.005843475381119383\n",
      "Average test loss: 51.07964050181707\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00609173056938582\n",
      "Average test loss: 33.079561523755395\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006036322054142753\n",
      "Average test loss: 0.010855128228664399\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005810862670342127\n",
      "Average test loss: 0.025022162705659868\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005790846925228834\n",
      "Average test loss: 0.012070825513866212\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005751783532400926\n",
      "Average test loss: 0.03120825741853979\n",
      "Epoch 198/300\n",
      "Average training loss: 0.005757055465545919\n",
      "Average test loss: 0.5839147652122709\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005819533534348011\n",
      "Average test loss: 0.19650299515989092\n",
      "Epoch 200/300\n",
      "Average training loss: 0.005836351682742436\n",
      "Average test loss: 0.03547523049182362\n",
      "Epoch 201/300\n",
      "Average training loss: 0.005754693773471647\n",
      "Average test loss: 0.019251609512501294\n",
      "Epoch 202/300\n",
      "Average training loss: 0.005741460647227035\n",
      "Average test loss: 0.02289622640609741\n",
      "Epoch 203/300\n",
      "Average training loss: 0.005747967063138883\n",
      "Average test loss: 0.034401374253961774\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005755504808906052\n",
      "Average test loss: 0.05059386161963145\n",
      "Epoch 205/300\n",
      "Average training loss: 0.005708407106084956\n",
      "Average test loss: 0.017331567404998673\n",
      "Epoch 206/300\n",
      "Average training loss: 0.005713800696863069\n",
      "Average test loss: 0.023256034238470925\n",
      "Epoch 207/300\n",
      "Average training loss: 0.005689325179076857\n",
      "Average test loss: 0.0183931765175528\n",
      "Epoch 208/300\n",
      "Average training loss: 0.005741888922949632\n",
      "Average test loss: 0.27797374237908257\n",
      "Epoch 209/300\n",
      "Average training loss: 0.005702684611909919\n",
      "Average test loss: 0.023744482012258636\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005684822961688042\n",
      "Average test loss: 0.02325130439798037\n",
      "Epoch 211/300\n",
      "Average training loss: 0.005846845946792099\n",
      "Average test loss: 0.011366592569483651\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0057686668710990084\n",
      "Average test loss: 0.013780422855582503\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00568743232679036\n",
      "Average test loss: 0.021233030753003226\n",
      "Epoch 214/300\n",
      "Average training loss: 0.005664920661598444\n",
      "Average test loss: 0.021416562027401394\n",
      "Epoch 215/300\n",
      "Average training loss: 0.005629528265860346\n",
      "Average test loss: 0.026127943449550203\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005980627581063244\n",
      "Average test loss: 0.05737668006287681\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005721588043702973\n",
      "Average test loss: 0.03736770073572795\n",
      "Epoch 218/300\n",
      "Average training loss: 0.005745808805442519\n",
      "Average test loss: 0.03221858628590902\n",
      "Epoch 219/300\n",
      "Average training loss: 0.005622066330992513\n",
      "Average test loss: 0.02475942476756043\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0056110996380448344\n",
      "Average test loss: 0.02304027778903643\n",
      "Epoch 221/300\n",
      "Average training loss: 0.005606501134733359\n",
      "Average test loss: 0.19593631376160517\n",
      "Epoch 222/300\n",
      "Average training loss: 0.005615431071983443\n",
      "Average test loss: 0.030995688539412286\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005625695816758606\n",
      "Average test loss: 0.01893366489145491\n",
      "Epoch 224/300\n",
      "Average training loss: 0.005588023371994495\n",
      "Average test loss: 0.022322830031315487\n",
      "Epoch 225/300\n",
      "Average training loss: 0.005633584015071392\n",
      "Average test loss: 0.024827040361033545\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00567846542596817\n",
      "Average test loss: 0.025072135753101774\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0055688777193427085\n",
      "Average test loss: 0.02490691286491023\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0055573951150808066\n",
      "Average test loss: 0.04699788346555498\n",
      "Epoch 229/300\n",
      "Average training loss: 0.005706368434346384\n",
      "Average test loss: 5.53014496172799\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006326422625945674\n",
      "Average test loss: 0.021360589078731007\n",
      "Epoch 231/300\n",
      "Average training loss: 0.005627628587186337\n",
      "Average test loss: 0.019827092698878713\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006044419666959179\n",
      "Average test loss: 0.018681374546554354\n",
      "Epoch 233/300\n",
      "Average training loss: 0.005988464059929053\n",
      "Average test loss: 0.02327737658388085\n",
      "Epoch 234/300\n",
      "Average training loss: 0.005720806515051259\n",
      "Average test loss: 0.046022705280118516\n",
      "Epoch 235/300\n",
      "Average training loss: 0.005641764514562156\n",
      "Average test loss: 0.014367497743003898\n",
      "Epoch 236/300\n",
      "Average training loss: 0.005606476301534308\n",
      "Average test loss: 0.47655331881841023\n",
      "Epoch 237/300\n",
      "Average training loss: 0.005602337465931972\n",
      "Average test loss: 0.023106302875611516\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005570459517753787\n",
      "Average test loss: 0.03655871860517396\n",
      "Epoch 239/300\n",
      "Average training loss: 0.005549389124330547\n",
      "Average test loss: 0.03559800336427159\n",
      "Epoch 240/300\n",
      "Average training loss: 0.005554527074512508\n",
      "Average test loss: 0.025651852059695456\n",
      "Epoch 241/300\n",
      "Average training loss: 0.005541326384991408\n",
      "Average test loss: 0.026784628725714153\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0055504820011556145\n",
      "Average test loss: 0.025385005083349017\n",
      "Epoch 243/300\n",
      "Average training loss: 0.005522919733491209\n",
      "Average test loss: 0.029394052457478313\n",
      "Epoch 244/300\n",
      "Average training loss: 0.005504910811367962\n",
      "Average test loss: 0.037736120940910445\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005502876127759616\n",
      "Average test loss: 0.029745181264148816\n",
      "Epoch 246/300\n",
      "Average training loss: 0.005521317369821999\n",
      "Average test loss: 0.024414992584122552\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005533216727690565\n",
      "Average test loss: 0.029535920408036972\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005524620504015022\n",
      "Average test loss: 0.028697660725977684\n",
      "Epoch 249/300\n",
      "Average training loss: 0.005511686737752623\n",
      "Average test loss: 0.03014080175757408\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0057353299442264765\n",
      "Average test loss: 0.0295941880726152\n",
      "Epoch 251/300\n",
      "Average training loss: 0.005524944887393051\n",
      "Average test loss: 0.0392057627207703\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00548682053718302\n",
      "Average test loss: 0.019982377517554495\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005470540271658037\n",
      "Average test loss: 0.023184963655140663\n",
      "Epoch 254/300\n",
      "Average training loss: 0.005447916164166397\n",
      "Average test loss: 0.026945360496640206\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005499955763833391\n",
      "Average test loss: 0.026548625651333067\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005459905368586381\n",
      "Average test loss: 0.024425081342458726\n",
      "Epoch 257/300\n",
      "Average training loss: 0.005684108563595348\n",
      "Average test loss: 0.036856765230496726\n",
      "Epoch 258/300\n",
      "Average training loss: 0.005560988316105472\n",
      "Average test loss: 0.06217153091894256\n",
      "Epoch 259/300\n",
      "Average training loss: 0.005443324845698145\n",
      "Average test loss: 0.02474767420358128\n",
      "Epoch 260/300\n",
      "Average training loss: 0.005441184166818857\n",
      "Average test loss: 0.2965938442018297\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00543543386624919\n",
      "Average test loss: 0.02275057783557309\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005428007716106044\n",
      "Average test loss: 0.023017559066414832\n",
      "Epoch 263/300\n",
      "Average training loss: 0.00543322325622042\n",
      "Average test loss: 0.19824909395972887\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005446245288269387\n",
      "Average test loss: 0.03011763002971808\n",
      "Epoch 265/300\n",
      "Average training loss: 0.005453120153397321\n",
      "Average test loss: 8.13354908964369\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005638860263344314\n",
      "Average test loss: 0.026582057532336976\n",
      "Epoch 267/300\n",
      "Average training loss: 0.005452015373855829\n",
      "Average test loss: 0.026365848037931655\n",
      "Epoch 268/300\n",
      "Average training loss: 0.005391060904496246\n",
      "Average test loss: 0.04376057802637418\n",
      "Epoch 269/300\n",
      "Average training loss: 0.005392963422669305\n",
      "Average test loss: 0.03627571726838748\n",
      "Epoch 270/300\n",
      "Average training loss: 0.005400471195785536\n",
      "Average test loss: 0.028935661137104034\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005407892429580291\n",
      "Average test loss: 0.03267465128004551\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005392458252608776\n",
      "Average test loss: 0.031171346906158658\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0054158615991473195\n",
      "Average test loss: 0.053672783897982704\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005395739475058185\n",
      "Average test loss: 0.03928921534617742\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006951215479522943\n",
      "Average test loss: 0.0581183234055837\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005862830869439575\n",
      "Average test loss: 0.1182739234831598\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005571436889055702\n",
      "Average test loss: 0.020216140599714386\n",
      "Epoch 278/300\n",
      "Average training loss: 0.005469057220965624\n",
      "Average test loss: 0.14060874160130818\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005394842396386795\n",
      "Average test loss: 0.07255539952218533\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005393171964834134\n",
      "Average test loss: 7.816798319498698\n",
      "Epoch 281/300\n",
      "Average training loss: 0.005928336263116863\n",
      "Average test loss: 0.024846983949343364\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005428542182263401\n",
      "Average test loss: 0.1725597541862064\n",
      "Epoch 283/300\n",
      "Average training loss: 0.005373494601084126\n",
      "Average test loss: 0.018996081637011632\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0053694288312560986\n",
      "Average test loss: 0.02945863557524151\n",
      "Epoch 285/300\n",
      "Average training loss: 0.005382034178409312\n",
      "Average test loss: 0.03668365151352353\n",
      "Epoch 286/300\n",
      "Average training loss: 0.00535769921541214\n",
      "Average test loss: 0.037957200225856566\n",
      "Epoch 287/300\n",
      "Average training loss: 0.005344464710603158\n",
      "Average test loss: 0.030887602229913075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.005352501500397921\n",
      "Average test loss: 0.02624559782942136\n",
      "Epoch 289/300\n",
      "Average training loss: 0.005358327637943957\n",
      "Average test loss: 0.025920692203773393\n",
      "Epoch 290/300\n",
      "Average training loss: 0.006276041825612386\n",
      "Average test loss: 0.027941223189234733\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0057460663409696685\n",
      "Average test loss: 0.026037451485792795\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005479142176194323\n",
      "Average test loss: 0.026166044862733946\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005430161634874013\n",
      "Average test loss: 0.027295519961251152\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00534339428279135\n",
      "Average test loss: 0.028611297070980073\n",
      "Epoch 295/300\n",
      "Average training loss: 0.005328306720488602\n",
      "Average test loss: 0.030768296144074864\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00532085490723451\n",
      "Average test loss: 0.032494235399696564\n",
      "Epoch 297/300\n",
      "Average training loss: 0.005336000199947092\n",
      "Average test loss: 0.05945014257563485\n",
      "Epoch 298/300\n",
      "Average training loss: 0.005312041632003254\n",
      "Average test loss: 0.03634516127076414\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0053044162715474765\n",
      "Average test loss: 0.05381387449966537\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0054090543616977\n",
      "Average test loss: 0.6307664904991785\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.025115870528750948\n",
      "Average test loss: 0.015564801194601588\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011151160563031833\n",
      "Average test loss: 0.016815158322453498\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009568833282010423\n",
      "Average test loss: 0.01017674744874239\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008593739088210794\n",
      "Average test loss: 0.010308275565505027\n",
      "Epoch 5/300\n",
      "Average training loss: 0.007966026350027985\n",
      "Average test loss: 0.008987115832666556\n",
      "Epoch 6/300\n",
      "Average training loss: 0.007332036575095521\n",
      "Average test loss: 0.010117616155909167\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006739517450746563\n",
      "Average test loss: 0.007660894761896795\n",
      "Epoch 8/300\n",
      "Average training loss: 0.006391462189041906\n",
      "Average test loss: 0.007756243669324451\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006042911477386952\n",
      "Average test loss: 0.0076379542251427965\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005876935573501719\n",
      "Average test loss: 0.008260209248297744\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005677390086981985\n",
      "Average test loss: 0.0076478352596362435\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005456204402777884\n",
      "Average test loss: 0.00807621666126781\n",
      "Epoch 13/300\n",
      "Average training loss: 0.00526440862690409\n",
      "Average test loss: 0.0075648938177360426\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005264045466151502\n",
      "Average test loss: 0.008679326271845235\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005095196275661389\n",
      "Average test loss: 0.008221016507181857\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00498261669692066\n",
      "Average test loss: 0.006311816062364313\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004959663007822301\n",
      "Average test loss: 0.007024375437034501\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00483857225233482\n",
      "Average test loss: 0.0067499831815560655\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004799617728425397\n",
      "Average test loss: 0.006338793461935388\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0047354615595605635\n",
      "Average test loss: 0.007753010260148181\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00462807207264834\n",
      "Average test loss: 0.006851990699354145\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004642924067667789\n",
      "Average test loss: 0.009842399298316902\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004579461825804578\n",
      "Average test loss: 0.008098121122767529\n",
      "Epoch 24/300\n",
      "Average training loss: 0.00458788051456213\n",
      "Average test loss: 0.007711404327717092\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0045095527490807905\n",
      "Average test loss: 0.008621982770661512\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0044745508419970674\n",
      "Average test loss: 0.008172425794104736\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004483722842815849\n",
      "Average test loss: 0.008108254838320944\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004399933936281336\n",
      "Average test loss: 0.007366076962401469\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004331806694467862\n",
      "Average test loss: 0.007268502253211207\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004341072553147873\n",
      "Average test loss: 0.007299938778910372\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004342693546786905\n",
      "Average test loss: 0.006127384728855557\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004245075970888138\n",
      "Average test loss: 0.007863931711349222\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004268959432012505\n",
      "Average test loss: 0.007462943894995583\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004241102232494288\n",
      "Average test loss: 0.008539159850113921\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0042598296374910406\n",
      "Average test loss: 0.007772508385280768\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004177231968277031\n",
      "Average test loss: 0.0069155156070159545\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0041386640303664735\n",
      "Average test loss: 0.007351366584085756\n",
      "Epoch 38/300\n",
      "Average training loss: 0.004158835018260611\n",
      "Average test loss: 0.008369301103055476\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00411671566321618\n",
      "Average test loss: 0.007960542836950885\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004084005053879486\n",
      "Average test loss: 0.009066002292765512\n",
      "Epoch 41/300\n",
      "Average training loss: 0.004115192845463753\n",
      "Average test loss: 0.010133380515707864\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0040626972719199125\n",
      "Average test loss: 0.01185946980615457\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004054603705596593\n",
      "Average test loss: 0.009235483536289798\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004060984100732539\n",
      "Average test loss: 0.0086139860500892\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0039978788637866575\n",
      "Average test loss: 0.012073631488614612\n",
      "Epoch 46/300\n",
      "Average training loss: 0.003982513573227657\n",
      "Average test loss: 0.013634765518208344\n",
      "Epoch 47/300\n",
      "Average training loss: 0.003993946691974997\n",
      "Average test loss: 0.008796486695607503\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003959490469139483\n",
      "Average test loss: 0.008482458878722456\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0039302880470123555\n",
      "Average test loss: 0.01162501904202832\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003941681511493193\n",
      "Average test loss: 0.012888469985789722\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003938347244842185\n",
      "Average test loss: 0.012657357840074433\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0038896275589035616\n",
      "Average test loss: 0.010403767789403598\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0038980306084785196\n",
      "Average test loss: 0.00836740044504404\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0038977431843264234\n",
      "Average test loss: 0.011321403398281998\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0038797786624895202\n",
      "Average test loss: 0.009615270183318191\n",
      "Epoch 56/300\n",
      "Average training loss: 0.003829947966668341\n",
      "Average test loss: 0.011048553611669275\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0038210178582618634\n",
      "Average test loss: 0.013167868797977765\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003883905056036181\n",
      "Average test loss: 0.01598913024779823\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0038263243929379516\n",
      "Average test loss: 0.011828411985602643\n",
      "Epoch 60/300\n",
      "Average training loss: 0.003786534079660972\n",
      "Average test loss: 0.009909532258907955\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0037810722316304842\n",
      "Average test loss: 0.010120948545634747\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003768092623601357\n",
      "Average test loss: 0.014935857571661472\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003756969433070885\n",
      "Average test loss: 0.011765262971321742\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0037501583738873403\n",
      "Average test loss: 0.011522630829777983\n",
      "Epoch 65/300\n",
      "Average training loss: 0.003746618089783523\n",
      "Average test loss: 0.016912864446640016\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0037495429085360635\n",
      "Average test loss: 0.01042833197530773\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0037204909798585705\n",
      "Average test loss: 0.011494672681722376\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0037602826530734696\n",
      "Average test loss: 0.013224626271261109\n",
      "Epoch 69/300\n",
      "Average training loss: 0.003689897080883384\n",
      "Average test loss: 0.017648788407444955\n",
      "Epoch 70/300\n",
      "Average training loss: 0.00369739706421064\n",
      "Average test loss: 0.015709369384580188\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0036799102996786437\n",
      "Average test loss: 0.01440528124405278\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003709666086774733\n",
      "Average test loss: 0.01286938996364673\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0036438204124569894\n",
      "Average test loss: 0.015903268723024264\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0036653359161896836\n",
      "Average test loss: 0.01787338371078173\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0036495574480957457\n",
      "Average test loss: 0.015067318867478106\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0036172574629179306\n",
      "Average test loss: 0.011037239325543244\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0036720418762415646\n",
      "Average test loss: 0.012523106423517068\n",
      "Epoch 78/300\n",
      "Average training loss: 0.003614206312845151\n",
      "Average test loss: 0.011073095346490543\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0036319873539937867\n",
      "Average test loss: 0.024066789031028747\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0036031096573505136\n",
      "Average test loss: 0.01685065149433083\n",
      "Epoch 81/300\n",
      "Average training loss: 0.003600275092033876\n",
      "Average test loss: 0.016111455387539334\n",
      "Epoch 82/300\n",
      "Average training loss: 0.003590900751658612\n",
      "Average test loss: 0.018356744213236704\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003577228567045596\n",
      "Average test loss: 0.015362684126529429\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003565269193508559\n",
      "Average test loss: 0.01722400120728546\n",
      "Epoch 85/300\n",
      "Average training loss: 0.003563266754150391\n",
      "Average test loss: 0.021596028900808757\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003552879541491469\n",
      "Average test loss: 0.013358264518280824\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0035723670033944977\n",
      "Average test loss: 0.011966314284337892\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0035734661834107505\n",
      "Average test loss: 0.019714143354031776\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0035440468250049484\n",
      "Average test loss: 0.020783826323019134\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0035281873171528182\n",
      "Average test loss: 0.02394228270981047\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003526798787423306\n",
      "Average test loss: 0.017838785646690263\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0035139006183793147\n",
      "Average test loss: 0.021047582048508855\n",
      "Epoch 93/300\n",
      "Average training loss: 0.003515700513083074\n",
      "Average test loss: 0.01376346444669697\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0035025448449369934\n",
      "Average test loss: 0.01488112437642283\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0035028152654154434\n",
      "Average test loss: 0.014670632518827916\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0034936282258066866\n",
      "Average test loss: 0.02319565488729212\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0034893667319168646\n",
      "Average test loss: 0.023318201679322456\n",
      "Epoch 98/300\n",
      "Average training loss: 0.003460246150692304\n",
      "Average test loss: 0.02402648536198669\n",
      "Epoch 99/300\n",
      "Average training loss: 0.003482929996939169\n",
      "Average test loss: 0.014295048948791291\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0034709685359978013\n",
      "Average test loss: 0.033814074171913996\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0034859868575715354\n",
      "Average test loss: 0.026462408286001948\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0034651824033094776\n",
      "Average test loss: 0.033391447734501624\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0034464325105978385\n",
      "Average test loss: 0.02412277555796835\n",
      "Epoch 104/300\n",
      "Average training loss: 0.00348899265875419\n",
      "Average test loss: 0.03350789618492127\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0034308311632937857\n",
      "Average test loss: 0.024144648571809132\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0034547320310440327\n",
      "Average test loss: 0.03146024268534448\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0034374448152052033\n",
      "Average test loss: 0.01932692900300026\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0034425638794071145\n",
      "Average test loss: 0.024878796183400685\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0034350508861243725\n",
      "Average test loss: 0.035713656031423145\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003402841678924031\n",
      "Average test loss: 0.020949680543608136\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0034086921858704753\n",
      "Average test loss: 0.02608426133460469\n",
      "Epoch 112/300\n",
      "Average training loss: 0.003388005770328972\n",
      "Average test loss: 0.029410665220684477\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0034121219366788865\n",
      "Average test loss: 0.024351056042644714\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0034201986381991044\n",
      "Average test loss: 0.021298764717247752\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0034012055016226237\n",
      "Average test loss: 0.03611887572871314\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0033842348595046336\n",
      "Average test loss: 0.037165634946690666\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0033728307891223164\n",
      "Average test loss: 0.034052531007263395\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0033849972039461137\n",
      "Average test loss: 0.02557763997382588\n",
      "Epoch 119/300\n",
      "Average training loss: 0.003416373293639885\n",
      "Average test loss: 0.02917892340819041\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0033627327082471716\n",
      "Average test loss: 0.01951324756277932\n",
      "Epoch 121/300\n",
      "Average training loss: 0.003345094063422746\n",
      "Average test loss: 0.03681063208315107\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0033629059094107813\n",
      "Average test loss: 0.020908156462841564\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00335189594162835\n",
      "Average test loss: 0.027790213492181567\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0033441141852074197\n",
      "Average test loss: 0.03458408607376946\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0033491953865935407\n",
      "Average test loss: 0.030512139601839912\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0033366836830973625\n",
      "Average test loss: 0.032633769853247535\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003325299790956908\n",
      "Average test loss: 0.028170386852489578\n",
      "Epoch 128/300\n",
      "Average training loss: 0.00335549181037479\n",
      "Average test loss: 0.017960260863105457\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00332928965261413\n",
      "Average test loss: 0.02394012709458669\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0033259780050979722\n",
      "Average test loss: 0.029326584342453214\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0033175880822042623\n",
      "Average test loss: 0.035709629383352065\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003339759351892604\n",
      "Average test loss: 0.032075191805760066\n",
      "Epoch 133/300\n",
      "Average training loss: 0.003312067283524407\n",
      "Average test loss: 0.041402977731492784\n",
      "Epoch 134/300\n",
      "Average training loss: 0.003292293914904197\n",
      "Average test loss: 0.03555046992500623\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0033060737198425663\n",
      "Average test loss: 0.03364437171154552\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0033048150268279843\n",
      "Average test loss: 0.031183409455749725\n",
      "Epoch 137/300\n",
      "Average training loss: 0.003297324179361264\n",
      "Average test loss: 0.029685426079564623\n",
      "Epoch 138/300\n",
      "Average training loss: 0.003293472998879022\n",
      "Average test loss: 0.02593134277065595\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003292324184957478\n",
      "Average test loss: 0.03501848836408721\n",
      "Epoch 140/300\n",
      "Average training loss: 0.003288562432759338\n",
      "Average test loss: 0.03315593524773915\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0032853347160336044\n",
      "Average test loss: 0.0354145075612598\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0032806769178973304\n",
      "Average test loss: 0.018611052345898417\n",
      "Epoch 143/300\n",
      "Average training loss: 0.003287502648722794\n",
      "Average test loss: 0.034239066554440395\n",
      "Epoch 144/300\n",
      "Average training loss: 0.003263523600374659\n",
      "Average test loss: 0.028495728654993904\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0032836565182854734\n",
      "Average test loss: 0.025828352289067373\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0032745270907051033\n",
      "Average test loss: 0.03478685775730345\n",
      "Epoch 147/300\n",
      "Average training loss: 0.003246223370855053\n",
      "Average test loss: 0.022033268570899964\n",
      "Epoch 148/300\n",
      "Average training loss: 0.003253268230292532\n",
      "Average test loss: 0.018937036145064565\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0032510129660367967\n",
      "Average test loss: 0.028224027938312954\n",
      "Epoch 150/300\n",
      "Average training loss: 0.003249632477553354\n",
      "Average test loss: 0.027524150563610926\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0032583201695233583\n",
      "Average test loss: 0.030762834426429537\n",
      "Epoch 152/300\n",
      "Average training loss: 0.003240284990519285\n",
      "Average test loss: 0.03157161121732659\n",
      "Epoch 153/300\n",
      "Average training loss: 0.003235677232551906\n",
      "Average test loss: 0.023735784540573755\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0032323822103854684\n",
      "Average test loss: 0.030724787602821987\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00323232633765373\n",
      "Average test loss: 0.03368649032877551\n",
      "Epoch 156/300\n",
      "Average training loss: 0.003226216280212005\n",
      "Average test loss: 0.029037999686267642\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0032519980989810494\n",
      "Average test loss: 0.029144242435693742\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0032287066814800105\n",
      "Average test loss: 0.039053439968162115\n",
      "Epoch 159/300\n",
      "Average training loss: 0.003225366061553359\n",
      "Average test loss: 0.03444894342621167\n",
      "Epoch 160/300\n",
      "Average training loss: 0.003227836845856574\n",
      "Average test loss: 0.040686416149139405\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0032153874341812397\n",
      "Average test loss: 0.024008507617645794\n",
      "Epoch 162/300\n",
      "Average training loss: 0.003213725130384167\n",
      "Average test loss: 0.0240835596786605\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0032256407170659965\n",
      "Average test loss: 0.019343041823969946\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0032067138399514885\n",
      "Average test loss: 0.035467390494214164\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0032065583705488177\n",
      "Average test loss: 0.031145244012276332\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0032050772549377547\n",
      "Average test loss: 0.0222017049541076\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0031907425901542105\n",
      "Average test loss: 0.027076938801341585\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0032075570633427965\n",
      "Average test loss: 0.03063873056901826\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0031907375851232146\n",
      "Average test loss: 0.02218321692943573\n",
      "Epoch 170/300\n",
      "Average training loss: 0.003184823173719148\n",
      "Average test loss: 0.03308627108070585\n",
      "Epoch 171/300\n",
      "Average training loss: 0.003198575022526913\n",
      "Average test loss: 0.02407954497469796\n",
      "Epoch 172/300\n",
      "Average training loss: 0.003197131268887056\n",
      "Average test loss: 0.027530104527870813\n",
      "Epoch 173/300\n",
      "Average training loss: 0.003198744255842434\n",
      "Average test loss: 0.022196989064415296\n",
      "Epoch 174/300\n",
      "Average training loss: 0.003176771983400815\n",
      "Average test loss: 0.03473919428553846\n",
      "Epoch 175/300\n",
      "Average training loss: 0.003166079535666439\n",
      "Average test loss: 0.030949668761756685\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0031833399670819443\n",
      "Average test loss: 0.03929130995935864\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0031541629636453257\n",
      "Average test loss: 0.03611269558138318\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0031604236101524697\n",
      "Average test loss: 0.019857673063874245\n",
      "Epoch 179/300\n",
      "Average training loss: 0.003177520534230603\n",
      "Average test loss: 0.02755663671427303\n",
      "Epoch 180/300\n",
      "Average training loss: 0.003169548251769609\n",
      "Average test loss: 0.0254526686668396\n",
      "Epoch 181/300\n",
      "Average training loss: 0.003174191052921944\n",
      "Average test loss: 0.023802503074208895\n",
      "Epoch 182/300\n",
      "Average training loss: 0.00316250655013654\n",
      "Average test loss: 0.02179447259671158\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0031521466293682656\n",
      "Average test loss: 0.03088494473364618\n",
      "Epoch 184/300\n",
      "Average training loss: 0.003144422097959452\n",
      "Average test loss: 0.028634043186903\n",
      "Epoch 185/300\n",
      "Average training loss: 0.003147389544174075\n",
      "Average test loss: 0.023189412757754328\n",
      "Epoch 186/300\n",
      "Average training loss: 0.003160291795308391\n",
      "Average test loss: 0.024594587188627986\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0031472191731962895\n",
      "Average test loss: 0.02601958434946007\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0031414754684600567\n",
      "Average test loss: 0.021897080274091825\n",
      "Epoch 189/300\n",
      "Average training loss: 0.003152515700707833\n",
      "Average test loss: 0.02200325828956233\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0031437590647902755\n",
      "Average test loss: 0.01717714269955953\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0031403428146408664\n",
      "Average test loss: 0.019265438301695716\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0031251204609870913\n",
      "Average test loss: 0.02560157358646393\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0031342628732737568\n",
      "Average test loss: 0.024707991724212966\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0031509554050862787\n",
      "Average test loss: 0.035283049626482856\n",
      "Epoch 195/300\n",
      "Average training loss: 0.003127261275425553\n",
      "Average test loss: 0.04691996689968639\n",
      "Epoch 196/300\n",
      "Average training loss: 0.003126801691742407\n",
      "Average test loss: 0.023504068341520096\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0031108819028983514\n",
      "Average test loss: 0.02483896763291624\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0031198846021046243\n",
      "Average test loss: 0.02806857344839308\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0031063878330298595\n",
      "Average test loss: 0.02610052627656195\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0031022355107383597\n",
      "Average test loss: 0.024436080530285835\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0031253704379002253\n",
      "Average test loss: 0.018624735138482517\n",
      "Epoch 202/300\n",
      "Average training loss: 0.003112349515987767\n",
      "Average test loss: 0.03235519252883064\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0031089006875538163\n",
      "Average test loss: 0.028013585726420084\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0030942786377337246\n",
      "Average test loss: 0.04539170105258624\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0031066693409035604\n",
      "Average test loss: 0.025810086662570637\n",
      "Epoch 206/300\n",
      "Average training loss: 0.003141171759408381\n",
      "Average test loss: 0.025726233161158032\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0030942966335763536\n",
      "Average test loss: 0.020882311013009813\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003103049368908008\n",
      "Average test loss: 0.03217502196298705\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0030757939794825184\n",
      "Average test loss: 0.016904772102832793\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0030855756130897336\n",
      "Average test loss: 0.02215630636612574\n",
      "Epoch 211/300\n",
      "Average training loss: 0.003110110660807954\n",
      "Average test loss: 0.018330749968687693\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0030732953120023014\n",
      "Average test loss: 0.02711586708161566\n",
      "Epoch 213/300\n",
      "Average training loss: 0.003082121300821503\n",
      "Average test loss: 0.01774294144743019\n",
      "Epoch 214/300\n",
      "Average training loss: 0.003089391331912743\n",
      "Average test loss: 0.05123780680033896\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0030873799779348905\n",
      "Average test loss: 0.03743373684419526\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00308700198887123\n",
      "Average test loss: 0.034884775006108815\n",
      "Epoch 217/300\n",
      "Average training loss: 0.003082539315438933\n",
      "Average test loss: 0.024674662373132174\n",
      "Epoch 218/300\n",
      "Average training loss: 0.003078724310836858\n",
      "Average test loss: 0.01570614391565323\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0030696733206924463\n",
      "Average test loss: 0.02596832878722085\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0030646555044998723\n",
      "Average test loss: 0.026557395665182008\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0030630505794866218\n",
      "Average test loss: 0.03279859784576628\n",
      "Epoch 222/300\n",
      "Average training loss: 0.003066798559286528\n",
      "Average test loss: 0.03007212268312772\n",
      "Epoch 223/300\n",
      "Average training loss: 0.003069159463047981\n",
      "Average test loss: 0.019398413257466423\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0030569747580836215\n",
      "Average test loss: 0.024334962285227246\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0030677872283591165\n",
      "Average test loss: 0.01801288737191094\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0030436177365481855\n",
      "Average test loss: 0.01878144545190864\n",
      "Epoch 227/300\n",
      "Average training loss: 0.003415605964139104\n",
      "Average test loss: 0.03234378237194485\n",
      "Epoch 228/300\n",
      "Average training loss: 0.003041181872909268\n",
      "Average test loss: 0.02664772621459431\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0030489750819073782\n",
      "Average test loss: 0.026250117965870432\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003026039955723617\n",
      "Average test loss: 0.033406056149138344\n",
      "Epoch 231/300\n",
      "Average training loss: 0.003045655332505703\n",
      "Average test loss: 0.03527271330687735\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003056708761594362\n",
      "Average test loss: 0.0362532148261865\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0030464048058622415\n",
      "Average test loss: 0.029743658300903107\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0030478248776247105\n",
      "Average test loss: 0.023283613529470233\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0030491760166154965\n",
      "Average test loss: 0.01958161972794268\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0030410683140572573\n",
      "Average test loss: 0.021960813585254882\n",
      "Epoch 237/300\n",
      "Average training loss: 0.003058888599690464\n",
      "Average test loss: 0.023318849727511404\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0030462292902585534\n",
      "Average test loss: 0.026408927308188543\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0030394844218260713\n",
      "Average test loss: 0.020710584706730314\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0030377352341181703\n",
      "Average test loss: 0.023119577664468025\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00303377203270793\n",
      "Average test loss: 0.028721077760060627\n",
      "Epoch 242/300\n",
      "Average training loss: 0.003046432214271691\n",
      "Average test loss: 0.026718329108423658\n",
      "Epoch 243/300\n",
      "Average training loss: 0.003019579728651378\n",
      "Average test loss: 0.023874348304337926\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003032157483407193\n",
      "Average test loss: 0.019414082831806605\n",
      "Epoch 245/300\n",
      "Average training loss: 0.003033236905725466\n",
      "Average test loss: 0.024739660274651316\n",
      "Epoch 246/300\n",
      "Average training loss: 0.003040550916766127\n",
      "Average test loss: 0.024044498398900032\n",
      "Epoch 247/300\n",
      "Average training loss: 0.003020191539078951\n",
      "Average test loss: 0.022761291321780945\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0030306653992997275\n",
      "Average test loss: 0.018435101487570337\n",
      "Epoch 249/300\n",
      "Average training loss: 0.00304182915099793\n",
      "Average test loss: 0.021407903355028894\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003013111989117331\n",
      "Average test loss: 0.028546979380978478\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0030303957586487133\n",
      "Average test loss: 0.016077442817389966\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003018985276420911\n",
      "Average test loss: 0.02197556434240606\n",
      "Epoch 253/300\n",
      "Average training loss: 0.003022495566556851\n",
      "Average test loss: 0.019667342273725404\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0029999058050000004\n",
      "Average test loss: 0.02929219141602516\n",
      "Epoch 255/300\n",
      "Average training loss: 0.003005990547645423\n",
      "Average test loss: 0.01890130045844449\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0030159871230522792\n",
      "Average test loss: 0.016472998877366384\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003026187910594874\n",
      "Average test loss: 0.019706658449437885\n",
      "Epoch 258/300\n",
      "Average training loss: 0.003010515707027581\n",
      "Average test loss: 0.01807514804932806\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0030097886510193346\n",
      "Average test loss: 0.03202512746883763\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002995920705712504\n",
      "Average test loss: 0.03671332523226738\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003011853554803464\n",
      "Average test loss: 0.017891443954573736\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0029932045632352433\n",
      "Average test loss: 0.016733220865329107\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0030003921476503214\n",
      "Average test loss: 0.04388192772865295\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0030033895580304994\n",
      "Average test loss: 0.022069766180382835\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0029920105325678984\n",
      "Average test loss: 0.012712441244886982\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0030023042226417196\n",
      "Average test loss: 0.01824966719912158\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0030000485045214493\n",
      "Average test loss: 0.024204636010858746\n",
      "Epoch 268/300\n",
      "Average training loss: 0.003005036644016703\n",
      "Average test loss: 0.023037849126590623\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0029774412071953216\n",
      "Average test loss: 0.015779872669114008\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0029841540731075735\n",
      "Average test loss: 0.017205900056494607\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0030176179320034052\n",
      "Average test loss: 0.02087903347114722\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0029860248969246946\n",
      "Average test loss: 0.02018200974828667\n",
      "Epoch 273/300\n",
      "Average training loss: 0.002975512083619833\n",
      "Average test loss: 0.026245364343126614\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0029715550146583052\n",
      "Average test loss: 0.03250537869003084\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0029761732411053444\n",
      "Average test loss: 0.0180146248307493\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002985638034219543\n",
      "Average test loss: 0.020374429273936484\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0029930921494960784\n",
      "Average test loss: 0.02518582855165005\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0029814156943725216\n",
      "Average test loss: 0.02804926300048828\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002973917530849576\n",
      "Average test loss: 0.02444273898833328\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0029897784118851026\n",
      "Average test loss: 0.04083167888389693\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0029640061859455375\n",
      "Average test loss: 0.027873219089375602\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002982860084830059\n",
      "Average test loss: 0.03212235162655513\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00295963341390921\n",
      "Average test loss: 0.017429353576567438\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0029496418115579418\n",
      "Average test loss: 0.013118342158695062\n",
      "Epoch 285/300\n",
      "Average training loss: 0.002973255533311102\n",
      "Average test loss: 0.01749984932276938\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0029693122025993136\n",
      "Average test loss: 0.017683454992042647\n",
      "Epoch 287/300\n",
      "Average training loss: 0.002963780602026317\n",
      "Average test loss: 0.03339384445879195\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0029587642420083283\n",
      "Average test loss: 0.028843150428599782\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0029779795708341733\n",
      "Average test loss: 0.015668889666597047\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0029451064717852405\n",
      "Average test loss: 0.06853298898537953\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0029787877773245174\n",
      "Average test loss: 0.020330595955252648\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002969748313021329\n",
      "Average test loss: 0.02148569815688663\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002951666498763694\n",
      "Average test loss: 0.022476140355070433\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0029600629075947735\n",
      "Average test loss: 0.03044542538291878\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0029450766764995123\n",
      "Average test loss: 0.023238760529292954\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002981713509187102\n",
      "Average test loss: 0.028799219821890196\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002939628142863512\n",
      "Average test loss: 0.021513344299462108\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0029565583256383737\n",
      "Average test loss: 0.02190372199813525\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002956975511999594\n",
      "Average test loss: 0.027968858091367614\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0029338681352221304\n",
      "Average test loss: 0.01508519628478421\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.021300881626705327\n",
      "Average test loss: 0.013493572121693028\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008902634889715247\n",
      "Average test loss: 0.008872588824894693\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007547651889009608\n",
      "Average test loss: 0.007331647193266286\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006733389346549908\n",
      "Average test loss: 0.00810783590707514\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005905691483161515\n",
      "Average test loss: 0.005837923473782009\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0053126523080799315\n",
      "Average test loss: 0.006945438221924834\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004949803172714181\n",
      "Average test loss: 0.005348588832136658\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004612790584357249\n",
      "Average test loss: 0.006695626281201839\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0043340253656109174\n",
      "Average test loss: 0.006620592185192638\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004225438287274705\n",
      "Average test loss: 0.007001603743682305\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004029354673292902\n",
      "Average test loss: 0.01414818286564615\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003972854566656881\n",
      "Average test loss: 0.005389851701756318\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0038304799811707604\n",
      "Average test loss: 0.0056130750026139944\n",
      "Epoch 14/300\n",
      "Average training loss: 0.003843360590851969\n",
      "Average test loss: 0.005648586811704768\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003684348240494728\n",
      "Average test loss: 0.006951038240558571\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0035868532955646514\n",
      "Average test loss: 0.005741815048787329\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0035676232561883\n",
      "Average test loss: 0.009351481036179596\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003449061260248224\n",
      "Average test loss: 0.006026319326212008\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0034627364689691197\n",
      "Average test loss: 0.015281038522720338\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0034090456310659645\n",
      "Average test loss: 0.004906199127021763\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003366729218719734\n",
      "Average test loss: 0.004783373136487272\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003281511276339491\n",
      "Average test loss: 0.004817107218007247\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003257740256273084\n",
      "Average test loss: 0.00436573124387198\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0032156621393644147\n",
      "Average test loss: 0.005178112108674315\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0034038673088782364\n",
      "Average test loss: 0.004683878677172793\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0032371107670995923\n",
      "Average test loss: 0.004888218997667233\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0031225435547530653\n",
      "Average test loss: 0.004555331198705567\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0030999456561274\n",
      "Average test loss: 0.005019161110950841\n",
      "Epoch 29/300\n",
      "Average training loss: 0.003107457060366869\n",
      "Average test loss: 0.004851157614340385\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0030669575025224025\n",
      "Average test loss: 0.004722697560571962\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0030195633669694265\n",
      "Average test loss: 0.004654809723297755\n",
      "Epoch 32/300\n",
      "Average training loss: 0.003056458786957794\n",
      "Average test loss: 0.004992147249480088\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0030283577338688903\n",
      "Average test loss: 0.004338817666595181\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002994704217960437\n",
      "Average test loss: 0.004695357650104496\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0029488026777075398\n",
      "Average test loss: 0.004727128036941091\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0029935724273737934\n",
      "Average test loss: 0.004262572336941958\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0029280574133412705\n",
      "Average test loss: 0.0047293216863440145\n",
      "Epoch 38/300\n",
      "Average training loss: 0.002888927771192458\n",
      "Average test loss: 0.004562085304202305\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0029412889670994546\n",
      "Average test loss: 0.004309531927108765\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002864648209988243\n",
      "Average test loss: 0.0043684330760604805\n",
      "Epoch 41/300\n",
      "Average training loss: 0.002875481608427233\n",
      "Average test loss: 0.004402720306896501\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0028464121526728075\n",
      "Average test loss: 0.0044471279041220745\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002848059843397803\n",
      "Average test loss: 0.011558409816688962\n",
      "Epoch 44/300\n",
      "Average training loss: 0.002858571314149433\n",
      "Average test loss: 0.004040649450694521\n",
      "Epoch 45/300\n",
      "Average training loss: 0.002811496290067832\n",
      "Average test loss: 0.006166254881769419\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0027866562344133856\n",
      "Average test loss: 0.004635937515232298\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0027939490949114164\n",
      "Average test loss: 0.0051673891929288705\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00275147683525251\n",
      "Average test loss: 0.004457061820560031\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0027895019153753916\n",
      "Average test loss: 0.004788166425708267\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002778301091864705\n",
      "Average test loss: 0.004405696211175786\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002749277405751248\n",
      "Average test loss: 0.004641382053908375\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0027552108100305003\n",
      "Average test loss: 0.004605725379453765\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0027129292740590044\n",
      "Average test loss: 0.0047741878574921025\n",
      "Epoch 54/300\n",
      "Average training loss: 0.002760645301805602\n",
      "Average test loss: 0.004341465736842818\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002698992160252399\n",
      "Average test loss: 0.005013337173395686\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0026878233908986054\n",
      "Average test loss: 0.004801490880135033\n",
      "Epoch 57/300\n",
      "Average training loss: 0.002695625559737285\n",
      "Average test loss: 0.00492342792575558\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0027128893731989795\n",
      "Average test loss: 0.0044808531138632035\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0026616539710925687\n",
      "Average test loss: 0.004570501863128609\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0026479364234126275\n",
      "Average test loss: 0.004638419937756326\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0026552446130663158\n",
      "Average test loss: 0.004335786159046822\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002641217105504539\n",
      "Average test loss: 0.006333075616922644\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0026466868368701804\n",
      "Average test loss: 0.004438426771511635\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0026668524897346894\n",
      "Average test loss: 0.004517778933048248\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0026414443980902432\n",
      "Average test loss: 0.004659870904766851\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0026139204427599906\n",
      "Average test loss: 0.004571079379568498\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0025923234758277736\n",
      "Average test loss: 0.004726492726554473\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0026035623192373247\n",
      "Average test loss: 0.004958657703051965\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002628039470770293\n",
      "Average test loss: 0.005605126550628079\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0025971803354720275\n",
      "Average test loss: 0.0043260205305284925\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0025913598634716537\n",
      "Average test loss: 0.0055717417986856565\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0025658202537645895\n",
      "Average test loss: 0.004616088367584679\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0025634737664626704\n",
      "Average test loss: 0.0056706239547994405\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00254560343362391\n",
      "Average test loss: 0.004415394856077101\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0025721984193142916\n",
      "Average test loss: 0.0053030261947876875\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0025690115665396055\n",
      "Average test loss: 0.004796629449145662\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0025762079570235477\n",
      "Average test loss: 0.0065088401262958845\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0025408400442037318\n",
      "Average test loss: 0.0050086783634291756\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0025242678545829324\n",
      "Average test loss: 0.004517643107722203\n",
      "Epoch 80/300\n",
      "Average training loss: 0.00252523508005672\n",
      "Average test loss: 0.004697260418699847\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002517393328870336\n",
      "Average test loss: 0.004472385169731246\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0025480891302641895\n",
      "Average test loss: 0.006748138964176178\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0025523166845863063\n",
      "Average test loss: 0.006735108348230521\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0024911458202534252\n",
      "Average test loss: 0.005197685852646828\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0025244795280612176\n",
      "Average test loss: 0.00523489935696125\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0024943958004522656\n",
      "Average test loss: 0.004802104696631432\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0024922079408748283\n",
      "Average test loss: 0.00471072585384051\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002474901249425279\n",
      "Average test loss: 0.004758343099719948\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0024583519190135928\n",
      "Average test loss: 0.005363233148223824\n",
      "Epoch 90/300\n",
      "Average training loss: 0.002485509817178051\n",
      "Average test loss: 0.004875891018658877\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0024740528497431014\n",
      "Average test loss: 0.006010754389895333\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0024553045485582615\n",
      "Average test loss: 0.004985878227899472\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0024782512088616687\n",
      "Average test loss: 0.005056417093094852\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0024482105188071727\n",
      "Average test loss: 0.005495503656152222\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0024463601948486436\n",
      "Average test loss: 0.005068374562180704\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0024598570217688877\n",
      "Average test loss: 0.004603608463373449\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0024429918525533546\n",
      "Average test loss: 0.005958451004491912\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0024212269499484036\n",
      "Average test loss: 0.00705090787799822\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0024280290111071533\n",
      "Average test loss: 0.005073835816648271\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0024360074293282297\n",
      "Average test loss: 0.005300762433144781\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002429642649781373\n",
      "Average test loss: 0.005485333785414695\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0024243536582216623\n",
      "Average test loss: 0.004841546387722095\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0024107729964372186\n",
      "Average test loss: 0.006467727361453904\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0024403656211992105\n",
      "Average test loss: 0.005177661853531996\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0024144434802648093\n",
      "Average test loss: 0.005368647996750143\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002400708326811178\n",
      "Average test loss: 0.0046611418231493895\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0024097256385203865\n",
      "Average training loss: 0.0024062325788868797\n",
      "Average test loss: 0.0050251048583951265\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002379177951357431\n",
      "Average test loss: 0.005091470683614413\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002413558107490341\n",
      "Average test loss: 0.0072288344850142796\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0023984568694399465\n",
      "Average test loss: 0.005183108097563187\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0023779811213413874\n",
      "Average test loss: 0.010050285229252444\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002392007189181944\n",
      "Average test loss: 0.005556149085776674\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0023665870312187406\n",
      "Average test loss: 0.005515661985509925\n",
      "Epoch 115/300\n",
      "Average training loss: 0.002359068577384783\n",
      "Average test loss: 0.006064023025333882\n",
      "Epoch 116/300\n",
      "Average training loss: 0.002383234028807945\n",
      "Average test loss: 0.005426584544281165\n",
      "Epoch 117/300\n",
      "Average training loss: 0.002374467873531911\n",
      "Average test loss: 0.0058470153402951025\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0023561326664768988\n",
      "Average test loss: 0.004815746408369806\n",
      "Epoch 119/300\n",
      "Average training loss: 0.00236459519683073\n",
      "Average test loss: 0.004892242932899131\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002373145129531622\n",
      "Average test loss: 0.005039753578603268\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0023471306474465463\n",
      "Average test loss: 0.005786478103035026\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0023732002930094797\n",
      "Average test loss: 0.004497214012882776\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0028933915659371348\n",
      "Average test loss: 0.004659528263327148\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0025387192326080466\n",
      "Average test loss: 0.0063741166939338045\n",
      "Epoch 125/300\n",
      "Average training loss: 0.002436009088324176\n",
      "Average test loss: 0.004623452382783095\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0023856202707522446\n",
      "Average test loss: 0.005237414867099789\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0023443094204283425\n",
      "Average test loss: 0.00679775385103292\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0023774903828485145\n",
      "Average test loss: 0.0054877875645955405\n",
      "Epoch 129/300\n",
      "Average training loss: 0.002340269495215681\n",
      "Average test loss: 0.005079177789183127\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002338727570242352\n",
      "Average test loss: 0.0049071887851589256\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0023223881769097515\n",
      "Average test loss: 0.00492344696736998\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002318664683442977\n",
      "Average test loss: 0.004566433105617762\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0023337577066073817\n",
      "Average test loss: 0.008028214427332083\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0023111691835025947\n",
      "Average test loss: 0.004636102271783683\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0023326470332427156\n",
      "Average test loss: 0.005531725199686156\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0023479345206999114\n",
      "Average test loss: 0.013569265002177821\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0023208024673577812\n",
      "Average test loss: 0.004977967653009626\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0022969912740712366\n",
      "Average test loss: 0.005569992211957773\n",
      "Epoch 139/300\n",
      "Average training loss: 0.002337970033701923\n",
      "Average test loss: 0.004619803438170088\n",
      "Epoch 140/300\n",
      "Average training loss: 0.002329854525418745\n",
      "Average test loss: 0.0053786079076429206\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0022940479359693\n",
      "Average test loss: 0.00786899372521374\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0023019104831748538\n",
      "Average test loss: 0.006255104245824946\n",
      "Epoch 143/300\n",
      "Average training loss: 0.002298104244594773\n",
      "Average test loss: 0.005446200704409016\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0023092756427617535\n",
      "Average test loss: 0.00953724013434516\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0023116798951394028\n",
      "Average test loss: 0.006119488298479053\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0022831437529789076\n",
      "Average test loss: 0.005364156714744038\n",
      "Epoch 147/300\n",
      "Average training loss: 0.002291476385874881\n",
      "Average test loss: 0.006459735059489807\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0022863692874088883\n",
      "Average test loss: 0.00587357378544079\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002302108292053971\n",
      "Average test loss: 0.005815364527619547\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002271066894237366\n",
      "Average test loss: 0.004732440806925297\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0022876216289069917\n",
      "Average test loss: 0.0049885694901976325\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0023116004549794725\n",
      "Average test loss: 0.0054089050458537204\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002266361644077632\n",
      "Average test loss: 0.005645429434047805\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002258308689213461\n",
      "Average test loss: 0.005836568708635039\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0022577360513516597\n",
      "Average test loss: 0.0059418285033769076\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0022846386968675587\n",
      "Average test loss: 0.007267147739314371\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0022878365771224103\n",
      "Average test loss: 0.0044845074067513145\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0022594942189753057\n",
      "Average test loss: 0.00544240097163452\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0022496631224122312\n",
      "Average test loss: 0.005402751424660285\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0022967833711041346\n",
      "Average test loss: 0.0061199690869285\n",
      "Epoch 161/300\n",
      "Average training loss: 0.002244302352786892\n",
      "Average test loss: 0.005701216531710492\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0022517632792393365\n",
      "Average test loss: 0.005777793634268972\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0022481608873026237\n",
      "Average test loss: 0.005287012702474991\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00225487431531979\n",
      "Average test loss: 0.0055721621583733295\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00225248457180957\n",
      "Average test loss: 0.0061673676110804085\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0022685956745925875\n",
      "Average test loss: 0.005386517466770278\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002229639566193024\n",
      "Average test loss: 0.005287754556371106\n",
      "Epoch 168/300\n",
      "Average training loss: 0.002242488068114552\n",
      "Average test loss: 0.005271308346341054\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0022432457483890986\n",
      "Average test loss: 0.0046791319489065146\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0022341969669279127\n",
      "Average test loss: 0.005603705782857206\n",
      "Epoch 171/300\n",
      "Average training loss: 0.002227029599042402\n",
      "Average test loss: 0.007466162929932276\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002235050730407238\n",
      "Average test loss: 0.005272527284092374\n",
      "Epoch 173/300\n",
      "Average training loss: 0.002234850990999904\n",
      "Average test loss: 0.004973774707979626\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002227509495284822\n",
      "Average test loss: 0.008838596536053551\n",
      "Epoch 175/300\n",
      "Average training loss: 0.002221251884682311\n",
      "Average test loss: 0.008466079551312658\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0022192544494238167\n",
      "Average test loss: 0.007771179585407177\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0022512891051462954\n",
      "Average test loss: 0.005384620258791579\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0022132933143940237\n",
      "Average test loss: 0.005966953585131301\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0022252457067370415\n",
      "Average test loss: 0.007390713097320663\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0022223419735415114\n",
      "Average test loss: 0.0061398501234749955\n",
      "Epoch 181/300\n",
      "Average training loss: 0.002216009577943219\n",
      "Average test loss: 0.005727519173175096\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0022521032835874292\n",
      "Average test loss: 0.004858673689266046\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0022146323327389026\n",
      "Average test loss: 0.007257125537428591\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002204050195092956\n",
      "Average test loss: 0.007912892774989208\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0022034392512092988\n",
      "Average test loss: 0.006665525529947546\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0022116747682707177\n",
      "Average test loss: 0.007983129142887062\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0022038822221673196\n",
      "Average test loss: 0.007234427252577411\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002196317940329512\n",
      "Average test loss: 0.0046877590190205305\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00220158714056015\n",
      "Average test loss: 0.010774248568548096\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0022100602032409772\n",
      "Average test loss: 0.005533378195431498\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002191323255188763\n",
      "Average test loss: 0.005647789239469502\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0022370572812441323\n",
      "Average test loss: 0.007187813967880275\n",
      "Epoch 193/300\n",
      "Average training loss: 0.002185937075358298\n",
      "Average test loss: 0.010813640599449476\n",
      "Epoch 194/300\n",
      "Average training loss: 0.002184550487022433\n",
      "Average test loss: 0.0068791457129021485\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002213358630115787\n",
      "Average test loss: 0.006709760674585899\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0021827224360571966\n",
      "Average test loss: 0.0066717179152700635\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0021958042033430603\n",
      "Average test loss: 0.007338410122527016\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002178868445257346\n",
      "Average test loss: 0.005237126852903101\n",
      "Epoch 199/300\n",
      "Average training loss: 0.002161127557667593\n",
      "Average test loss: 0.009067997607092062\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0021862168764281605\n",
      "Average test loss: 0.005244783466888799\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002193361239300834\n",
      "Average test loss: 0.007932229826847713\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0021801781588130526\n",
      "Average test loss: 0.006837848137650225\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0021915603809886507\n",
      "Average test loss: 0.006326891773276859\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0021780566894966697\n",
      "Average test loss: 0.005822194782810079\n",
      "Epoch 205/300\n",
      "Average training loss: 0.002187508259796434\n",
      "Average test loss: 0.007896212239232328\n",
      "Epoch 206/300\n",
      "Average training loss: 0.002160389872060882\n",
      "Average test loss: 0.009176590284539593\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0021787847386052213\n",
      "Average test loss: 0.008891975555982855\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0021681953875554933\n",
      "Average test loss: 0.005359804899742206\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0021752050925667086\n",
      "Average test loss: 0.00844545191526413\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00216756250243634\n",
      "Average test loss: 0.006677878576848242\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0021693084061973626\n",
      "Average test loss: 0.007215145971212122\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0021670883796695204\n",
      "Average test loss: 0.005078950576898124\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0021671266870366204\n",
      "Average test loss: 0.009834587585594919\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002168138182618552\n",
      "Average test loss: 0.0065072737447917466\n",
      "Epoch 215/300\n",
      "Average training loss: 0.002161516221964525\n",
      "Average test loss: 0.0055870557075573335\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0021641028840094804\n",
      "Average test loss: 0.007639952475825946\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0021470534466207026\n",
      "Average test loss: 0.006593581553962496\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0021759894895884728\n",
      "Average test loss: 0.005370984406107002\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0021539778891536926\n",
      "Average test loss: 0.006210266358322568\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002165062785976463\n",
      "Average test loss: 0.006474996167338556\n",
      "Epoch 221/300\n",
      "Average training loss: 0.00215984881710675\n",
      "Average test loss: 0.0073355434197518565\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0021521711252215836\n",
      "Average test loss: 0.005538005104081498\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0021640370259475377\n",
      "Average test loss: 0.005514629031221072\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0021329247067785927\n",
      "Average test loss: 0.007269528744949235\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002152419709600508\n",
      "Average test loss: 0.008450462205542459\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0021718695395522647\n",
      "Average test loss: 0.007934746072110203\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002140188155592316\n",
      "Average test loss: 0.006661720499810246\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0021315189811090627\n",
      "Average test loss: 0.0060835816384189655\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0021269265185627673\n",
      "Average test loss: 0.00746930532488558\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0021550165890819495\n",
      "Average test loss: 0.008365935193167792\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0021490530436858533\n",
      "Average test loss: 0.005885634988960293\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0021301899012178185\n",
      "Average test loss: 0.006353671100818448\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0021323065949189995\n",
      "Average test loss: 0.005681706796503729\n",
      "Epoch 234/300\n",
      "Average training loss: 0.00214768345374614\n",
      "Average test loss: 0.005371343526161379\n",
      "Epoch 235/300\n",
      "Average training loss: 0.002141244045769175\n",
      "Average test loss: 0.007051641767223676\n",
      "Epoch 236/300\n",
      "Average training loss: 0.002121330791256494\n",
      "Average test loss: 0.00591453269827697\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0021470591417617267\n",
      "Average test loss: 0.007794345841639572\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002119715395797458\n",
      "Average test loss: 0.0069025959314571485\n",
      "Epoch 239/300\n",
      "Average training loss: 0.002160506601134936\n",
      "Average test loss: 0.005865215651277038\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0021247274757673342\n",
      "Average test loss: 0.0075900489803817535\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00213139592224939\n",
      "Average test loss: 0.006285449415859249\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0021200122671822708\n",
      "Average test loss: 0.006745108794834879\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002119780905958679\n",
      "Average test loss: 0.0071377932404478395\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002131100082149108\n",
      "Average test loss: 0.00818537956310643\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0021343304368977747\n",
      "Average test loss: 0.0056922139736513295\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0021206853362835116\n",
      "Average test loss: 0.00794836657659875\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0021220441394382054\n",
      "Average test loss: 0.0071641256171796055\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002118926882122954\n",
      "Average test loss: 0.0075834505351053344\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0021163104181695317\n",
      "Average test loss: 0.006439032712744342\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0021043913836280505\n",
      "Average test loss: 0.0071646805351807015\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0021332968773527276\n",
      "Average test loss: 0.005617924833877219\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002117642909288406\n",
      "Average test loss: 0.009942659659518136\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0021110243865599234\n",
      "Average test loss: 0.008138834786083963\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0021017734326629176\n",
      "Average test loss: 0.0052504463949137265\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00209740125367211\n",
      "Average test loss: 0.005850071154948738\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002125256374167899\n",
      "Average test loss: 0.006386683194587628\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0021133075752812957\n",
      "Average test loss: 0.00755264848884609\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0020991703402251007\n",
      "Average test loss: 0.009491568104260498\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00211045204102993\n",
      "Average test loss: 0.0077561760768294335\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0021113493589477407\n",
      "Average test loss: 0.00760805079092582\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0021107137023160857\n",
      "Average test loss: 0.00675528706320458\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0021029792175524765\n",
      "Average test loss: 0.007508227818955978\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0021130964493172037\n",
      "Average test loss: 0.00917870215078195\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0020985614074808027\n",
      "Average test loss: 0.007906757892419895\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00209964827220473\n",
      "Average test loss: 0.008132623559070958\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0021023524801971182\n",
      "Average test loss: 0.010218445562654072\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0021094806370221906\n",
      "Average test loss: 0.0057158678579661585\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0020965642204715147\n",
      "Average test loss: 0.011045444577932358\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002095351096553107\n",
      "Average test loss: 0.01072967014875677\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0021064486984784404\n",
      "Average test loss: 0.008622514376209842\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0020847872147957483\n",
      "Average test loss: 0.01343539213637511\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0020950732116277017\n",
      "Average test loss: 0.006110449968940682\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0020968296165681547\n",
      "Average test loss: 0.006207039913369549\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002078934690915048\n",
      "Average test loss: 0.009907042760815886\n",
      "Epoch 275/300\n",
      "Average training loss: 0.002083154772925708\n",
      "Average test loss: 0.00794492200182544\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0020827489437328446\n",
      "Average test loss: 0.010346498105261061\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0020871033668518066\n",
      "Average test loss: 0.010889770765271452\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0021214903402659627\n",
      "Average test loss: 0.009341190399395094\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002094458200658361\n",
      "Average test loss: 0.008620640587061643\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002070199782649676\n",
      "Average test loss: 0.006942453050365051\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00209584675460226\n",
      "Average test loss: 0.008293891903426913\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0020790263533385263\n",
      "Average test loss: 0.008289415595432123\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002076236698983444\n",
      "Average test loss: 0.007592312077267302\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002078335251659155\n",
      "Average test loss: 0.012360795228845544\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0020717916548666027\n",
      "Average test loss: 0.00952271214624246\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0020918648131191733\n",
      "Average test loss: 0.009724820241332054\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0020858647529449726\n",
      "Average test loss: 0.007040887818568283\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002077915882795221\n",
      "Average test loss: 0.009096487069295512\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0020861027220057116\n",
      "Average test loss: 0.008172057532601886\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002073184858697156\n",
      "Average test loss: 0.009422753853930368\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0020860405549820928\n",
      "Average test loss: 0.006977691647907098\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002077803237363696\n",
      "Average test loss: 0.006497977403302988\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0020670278251378074\n",
      "Average test loss: 0.007283845760756069\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0020757483512991006\n",
      "Average test loss: 0.005903395964039697\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0020826341894765693\n",
      "Average test loss: 0.009425738994032144\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0020799036311606564\n",
      "Average test loss: 0.011013252498375046\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0020780912321060898\n",
      "Average test loss: 0.009282790111170875\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0020643163474483622\n",
      "Average test loss: 0.010824654657807615\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002068897430888481\n",
      "Average test loss: 0.007290762414120966\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0020677146494595543\n",
      "Average test loss: 0.01094639498160945\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.020456422488722537\n",
      "Average test loss: 0.03438270013199912\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008364454217255115\n",
      "Average test loss: 0.43967499763435786\n",
      "Epoch 3/300\n",
      "Average training loss: 0.006898217017451922\n",
      "Average test loss: 0.01157413950893614\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00578212855466538\n",
      "Average test loss: 0.006879505029155148\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005273555304855108\n",
      "Average test loss: 0.004703669067472219\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004683032558196121\n",
      "Average test loss: 0.009492690517670578\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0042920258765419324\n",
      "Average test loss: 0.10470970879991849\n",
      "Epoch 8/300\n",
      "Average training loss: 0.003997974936539928\n",
      "Average test loss: 5.967730486323436\n",
      "Epoch 9/300\n",
      "Average training loss: 0.003811979414688216\n",
      "Average test loss: 0.06749215522077348\n",
      "Epoch 10/300\n",
      "Average training loss: 0.00360599738255971\n",
      "Average test loss: 0.08332584297160307\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0034840314930511844\n",
      "Average test loss: 5.553179059065051\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0033821500514944394\n",
      "Average test loss: 0.02025634912070301\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0032467798122929204\n",
      "Average test loss: 0.015573046777811315\n",
      "Epoch 14/300\n",
      "Average training loss: 0.003432391935131616\n",
      "Average test loss: 0.009047386818048026\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0031719517304251593\n",
      "Average test loss: 1.4073127650154962\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0030531204634656507\n",
      "Average test loss: 0.0071912914644926785\n",
      "Epoch 17/300\n",
      "Average training loss: 0.003083670992611183\n",
      "Average test loss: 0.004507782910433081\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0029876442555752066\n",
      "Average test loss: 0.01598602435613672\n",
      "Epoch 19/300\n",
      "Average training loss: 0.002875585522916582\n",
      "Average test loss: 0.0056196618196037084\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0028853089306503533\n",
      "Average test loss: 0.03955716994073656\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00280601139449411\n",
      "Average test loss: 0.005008948009047243\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0028354566782299016\n",
      "Average test loss: 0.009235550848560202\n",
      "Epoch 23/300\n",
      "Average training loss: 0.002721421499012245\n",
      "Average test loss: 2.387539589536687\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0027720156301640803\n",
      "Average test loss: 0.02329506505322125\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0027243578150454493\n",
      "Average test loss: 0.07993630856772264\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002656559399639567\n",
      "Average test loss: 0.05944880186393857\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0025743907276127072\n",
      "Average test loss: 0.03067668924894598\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0025706395194348363\n",
      "Average test loss: 0.009497300610774093\n",
      "Epoch 29/300\n",
      "Average training loss: 0.002595567746915751\n",
      "Average test loss: 0.04724033380879296\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0025498041390544838\n",
      "Average test loss: 0.008025524667981598\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002587360443133447\n",
      "Average test loss: 0.029123991284105512\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002510981484212809\n",
      "Average test loss: 5.804115241835515\n",
      "Epoch 33/300\n",
      "Average training loss: 0.002482269136235118\n",
      "Average test loss: 5.783931504070758\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002463423683721986\n",
      "Average test loss: 0.01997564136940572\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002454680080422097\n",
      "Average test loss: 0.09666624536116918\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0024646426677289936\n",
      "Average test loss: 0.006946878293322192\n",
      "Epoch 37/300\n",
      "Average training loss: 0.002469368332169122\n",
      "Average test loss: 0.010215296659204695\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0023859891961846087\n",
      "Average test loss: 0.11241400809089343\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0023971798432370025\n",
      "Average test loss: 0.9702392291757795\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0023372839409857987\n",
      "Average test loss: 0.20747715013681187\n",
      "Epoch 41/300\n",
      "Average training loss: 0.002396418979805377\n",
      "Average test loss: 0.09325797301530837\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0024042138486272758\n",
      "Average test loss: 0.03021932800817821\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002339641759172082\n",
      "Average test loss: 0.09783790404515134\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0023022170417631668\n",
      "Average test loss: 0.012571552617682351\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0023416806283510392\n",
      "Average test loss: 0.010997669719159603\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0023406650278096396\n",
      "Average test loss: 0.015741801982124645\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0022728081298992037\n",
      "Average test loss: 0.0508541681336032\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0023169133173715738\n",
      "Average test loss: 0.47902072597626183\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0022414753899599113\n",
      "Average test loss: 4.413689398990737\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0022935045166975923\n",
      "Average test loss: 0.02604364553259479\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0023007729498462546\n",
      "Average test loss: 0.02451773836132553\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0022848832845273944\n",
      "Average test loss: 0.02002340147892634\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0022590913594596916\n",
      "Average test loss: 0.029757047654853926\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0022163356937881973\n",
      "Average test loss: 0.013728874468141132\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0022385996064792075\n",
      "Average test loss: 0.010270142248935169\n",
      "Epoch 56/300\n",
      "Average training loss: 0.002244322600464026\n",
      "Average test loss: 0.05643864154815674\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0022178880923117202\n",
      "Average test loss: 0.007773205267058479\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0022063379531933203\n",
      "Average test loss: 0.007045533216661877\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0022240568161424664\n",
      "Average test loss: 0.11713504509131113\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0021947689878029954\n",
      "Average test loss: 1.6841062527894974\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0022010181554489665\n",
      "Average test loss: 0.012518997514413462\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0022313113663759497\n",
      "Average test loss: 0.011671833006872072\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002142324496888452\n",
      "Average test loss: 0.012538574068910546\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002142349114227626\n",
      "Average test loss: 0.011883367574877209\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002176398098675741\n",
      "Average test loss: 0.09278527338057757\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002170604406235119\n",
      "Average test loss: 0.03761528848608335\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0021441811031351486\n",
      "Average test loss: 0.28255781756507026\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0021494931088139615\n",
      "Average test loss: 0.030090320120255153\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0021484516592075427\n",
      "Average test loss: 0.02088520355688201\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002128800545922584\n",
      "Average test loss: 0.49481513715783754\n",
      "Epoch 71/300\n",
      "Average training loss: 0.002153403335561355\n",
      "Average test loss: 0.02305362888177236\n",
      "Epoch 72/300\n",
      "Average training loss: 0.002134282681470116\n",
      "Average test loss: 0.09282673777888219\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00208956752003481\n",
      "Average test loss: 1.3125540088150236\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002111065478891962\n",
      "Average test loss: 0.22837955017719003\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0020977751465721265\n",
      "Average test loss: 0.3846716725544797\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002112916097251905\n",
      "Average test loss: 0.0075137192631761236\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0020998564083129167\n",
      "Average test loss: 0.16249998713367514\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0021126168609286347\n",
      "Average test loss: 0.061234200063678954\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0020658616239411965\n",
      "Average test loss: 0.014676794373326831\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0021052741201387512\n",
      "Average test loss: 5.1462995028396445\n",
      "Epoch 81/300\n",
      "Average training loss: 0.002071916225469775\n",
      "Average test loss: 0.04443402143816153\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0020713049178529117\n",
      "Average test loss: 0.014957414517800013\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0020581485451095635\n",
      "Average test loss: 1.6067268639074432\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0020948212014304268\n",
      "Average test loss: 0.11208852514624595\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002116471969626016\n",
      "Average test loss: 0.12362822311785486\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00205102306459513\n",
      "Average test loss: 39.40718587154812\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0020447403306348456\n",
      "Average test loss: 0.01903973948293262\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002064594976293544\n",
      "Average test loss: 0.010891149721211857\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0020430194459234675\n",
      "Average test loss: 11.204827266646756\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0020281792307893435\n",
      "Average test loss: 1.1090693542220527\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0020074189292887848\n",
      "Average test loss: 0.6242252854787641\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002073233975821899\n",
      "Average test loss: 0.03312351996368832\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0020524659398943185\n",
      "Average test loss: 0.05342227736446593\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0020224615588991177\n",
      "Average test loss: 0.01961752645340231\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002018513749870989\n",
      "Average test loss: 9.72389564356539\n",
      "Epoch 96/300\n",
      "Average training loss: 0.002017304789274931\n",
      "Average test loss: 0.2670001271300846\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0020327361948374247\n",
      "Average test loss: 0.009758060578670767\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0020060612064682773\n",
      "Average test loss: 0.16244154784911208\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002010522562700013\n",
      "Average test loss: 0.018675902523928218\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0019840092518263394\n",
      "Average test loss: 3209.5496292255984\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002018885593654381\n",
      "Average test loss: 0.16073439505861864\n",
      "Epoch 102/300\n",
      "Average training loss: 0.001997119144329594\n",
      "Average test loss: 829.3235924241808\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002148491354452239\n",
      "Average test loss: 0.14214916130196717\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0019678892888542678\n",
      "Average test loss: 1.140047282901075\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0019699831129983067\n",
      "Average test loss: 0.30567672833800313\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0019746313081640335\n",
      "Average test loss: 0.01237410404947069\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002015119847945041\n",
      "Average test loss: 0.2658942756520377\n",
      "Epoch 108/300\n",
      "Average training loss: 0.00197326646765901\n",
      "Average test loss: 0.042828098956081605\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0019772002823236915\n",
      "Average test loss: 0.01194518190705114\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002010174624207947\n",
      "Average test loss: 0.049599233425325816\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0019554961536907486\n",
      "Average test loss: 0.05083812333891789\n",
      "Epoch 112/300\n",
      "Average training loss: 0.001961522074933681\n",
      "Average test loss: 0.031713944089081555\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0019729145355522632\n",
      "Average test loss: 0.013135157003170915\n",
      "Epoch 114/300\n",
      "Average training loss: 0.001965572334619032\n",
      "Average test loss: 0.019320436244209607\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0020038022166118026\n",
      "Average test loss: 0.04193680245263709\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0019524515571279657\n",
      "Average test loss: 0.011771058935258124\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0020247059759373465\n",
      "Average test loss: 0.19088203478935692\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0020408443649195963\n",
      "Average test loss: 0.032834026114808186\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0019459308315482404\n",
      "Average test loss: 0.47562391144699523\n",
      "Epoch 120/300\n",
      "Average training loss: 0.001943475934262905\n",
      "Average test loss: 0.5777652873479657\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0019314081683341\n",
      "Average test loss: 0.02102406322914693\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0019435391346406606\n",
      "Average test loss: 5.319949146197902\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0019619772717770602\n",
      "Average test loss: 0.02835595924986733\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0019420530806399054\n",
      "Average test loss: 0.0158666895031929\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0019377369804100858\n",
      "Average test loss: 0.010021526041958067\n",
      "Epoch 126/300\n",
      "Average training loss: 0.001916639532893896\n",
      "Average test loss: 0.035053032210303675\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0019342718830125199\n",
      "Average test loss: 0.009617136251595285\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019042340689856144\n",
      "Average test loss: 0.23795845667852294\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0019523433835970032\n",
      "Average test loss: 0.016256330288118785\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019146664353708427\n",
      "Average test loss: 0.2632163660857413\n",
      "Epoch 131/300\n",
      "Average training loss: 0.001969687825586233\n",
      "Average test loss: 0.011276567712426185\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0019093575808737013\n",
      "Average test loss: 0.32303794399234986\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0019305899179437095\n",
      "Average test loss: 0.2471778359313806\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0019036224572401908\n",
      "Average test loss: 0.03936568817993005\n",
      "Epoch 135/300\n",
      "Average training loss: 0.001909112961548898\n",
      "Average test loss: 1.1835022003807956\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0018892093130076924\n",
      "Average test loss: 0.01722614172432158\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0019240870560622877\n",
      "Average test loss: 0.0581843083732658\n",
      "Epoch 138/300\n",
      "Average training loss: 0.00189120760301335\n",
      "Average test loss: 0.01335768179429902\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0018758181103815635\n",
      "Average test loss: 0.01722111251619127\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0019543623950125442\n",
      "Average test loss: 0.48468092859453626\n",
      "Epoch 141/300\n",
      "Average training loss: 0.001891076428298321\n",
      "Average test loss: 0.13679792850133446\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0019212982381383579\n",
      "Average test loss: 0.01313870013008515\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0019262207759958175\n",
      "Average test loss: 0.027824210264616542\n",
      "Epoch 144/300\n",
      "Average training loss: 0.001903943080558545\n",
      "Average test loss: 0.025840732645657328\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0018709244813977016\n",
      "Average test loss: 0.17007270600315597\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0018765762590078844\n",
      "Average test loss: 0.03423811122402549\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0019040062156402402\n",
      "Average test loss: 0.009909493169022931\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0019007454502085845\n",
      "Average test loss: 0.04684442982243167\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0018679413423976965\n",
      "Average test loss: 0.0803384090297752\n",
      "Epoch 150/300\n",
      "Average training loss: 0.001869881758880284\n",
      "Average test loss: 0.28508657732274795\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0018971987886147366\n",
      "Average test loss: 0.3747680265241199\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018637572914982836\n",
      "Average test loss: 0.017598987713456155\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0018736166748114758\n",
      "Average test loss: 2.097008766171833\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0018777563535711831\n",
      "Average test loss: 0.026414459188779195\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0018712372742593288\n",
      "Average test loss: 1.2053970555414757\n",
      "Epoch 156/300\n",
      "Average training loss: 0.001867553518153727\n",
      "Average test loss: 4.120867207252317\n",
      "Epoch 157/300\n",
      "Average training loss: 0.001977800071860353\n",
      "Average test loss: 0.03671198288930787\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0020017854051871432\n",
      "Average test loss: 0.014392771288752555\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018817079899211724\n",
      "Average test loss: 0.013391253449850612\n",
      "Epoch 160/300\n",
      "Average training loss: 0.001852056665966908\n",
      "Average test loss: 0.007369530163705349\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0018597499359813002\n",
      "Average test loss: 0.025042128652334215\n",
      "Epoch 162/300\n",
      "Average training loss: 0.001859818611604472\n",
      "Average test loss: 0.024108994969063335\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0018499448608814014\n",
      "Average test loss: 0.022706632380684218\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0018931550749888024\n",
      "Average test loss: 0.49349991368254026\n",
      "Epoch 165/300\n",
      "Average training loss: 0.001822363270032737\n",
      "Average test loss: 0.01733153940902816\n",
      "Epoch 166/300\n",
      "Average training loss: 0.00183712279310243\n",
      "Average test loss: 0.6882375030741096\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0018370185228478578\n",
      "Average test loss: 0.7953693963545891\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0018495129000188576\n",
      "Average test loss: 0.02415366790526443\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0018220394097475542\n",
      "Average test loss: 0.030908118123809496\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0018300840860853592\n",
      "Average test loss: 0.17493426527248487\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0018235667292028665\n",
      "Average test loss: 0.22059975069099003\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0018342256121751336\n",
      "Average test loss: 0.1729465858116746\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0018272175364610222\n",
      "Average test loss: 0.3186585660709275\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0018383204682419698\n",
      "Average test loss: 0.02912418055203226\n",
      "Epoch 175/300\n",
      "Average training loss: 0.001828466219206651\n",
      "Average test loss: 0.010673640447358291\n",
      "Epoch 176/300\n",
      "Average training loss: 0.001824647136963904\n",
      "Average test loss: 0.02073580559922589\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018630607994273305\n",
      "Average test loss: 0.012088917343980736\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0018176767173119718\n",
      "Average test loss: 0.03292128315899107\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0018057764968317416\n",
      "Average test loss: 3.967750090784497\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0018209903562027548\n",
      "Average test loss: 0.019895215211643114\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0018144945985534124\n",
      "Average test loss: 0.01694654518034723\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0018171506083259979\n",
      "Average test loss: 0.04161286497116089\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0018100202242947287\n",
      "Average test loss: 1.2044911680585808\n",
      "Epoch 184/300\n",
      "Average training loss: 0.001820298123587337\n",
      "Average test loss: 0.009670476179983881\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0018016138340656955\n",
      "Average test loss: 0.025393526332245932\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0018062858744007017\n",
      "Average test loss: 0.017307107700241936\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0018130781451861063\n",
      "Average test loss: 1.1874454577432738\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0018436277581171856\n",
      "Average test loss: 0.11854379952616162\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018043283408300745\n",
      "Average test loss: 0.015846470235122576\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00180871249600831\n",
      "Average test loss: 0.9187674877295892\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0017883806344535615\n",
      "Average test loss: 0.3749811863766776\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0018052917540901238\n",
      "Average test loss: 0.04011720677547985\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0017822118225610918\n",
      "Average test loss: 0.027521942880418565\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0017894128672778606\n",
      "Average test loss: 0.014798310577869416\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0017950093212227027\n",
      "Average test loss: 0.02189187124537097\n",
      "Epoch 196/300\n",
      "Average training loss: 0.001796926236607962\n",
      "Average test loss: 3.3870854467120437\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0018287394343771868\n",
      "Average test loss: 0.017262202102277014\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0017730154238848222\n",
      "Average test loss: 0.048016195903221766\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0017845984373448625\n",
      "Average test loss: 1.2515241195592615\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0017633743644174602\n",
      "Average test loss: 0.08176654458211528\n",
      "Epoch 201/300\n",
      "Average training loss: 0.001787655155484875\n",
      "Average test loss: 8.553005691276656\n",
      "Epoch 202/300\n",
      "Average training loss: 0.001795893286044399\n",
      "Average test loss: 0.017030096034208932\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0017701298130883111\n",
      "Average test loss: 0.011680137141711183\n",
      "Epoch 204/300\n",
      "Average training loss: 0.001781679676224788\n",
      "Average test loss: 0.01641155439366897\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0017903365028194254\n",
      "Average test loss: 0.016952453556987976\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0017694403072819114\n",
      "Average test loss: 0.02665223333405124\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0017562119658622477\n",
      "Average test loss: 0.01232213741292556\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0017794636121640603\n",
      "Average test loss: 0.008961136860152086\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0017466425797384646\n",
      "Average test loss: 0.0180662908355395\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0017708339242057667\n",
      "Average test loss: 0.14207190205984646\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0017925053483082189\n",
      "Average test loss: 1.8931820452610653\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0017464381219405268\n",
      "Average test loss: 0.012997485641804007\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0017683915611770417\n",
      "Average test loss: 0.018103814840316774\n",
      "Epoch 214/300\n",
      "Average training loss: 0.001766311782412231\n",
      "Average test loss: 0.13182862176828913\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0017972596554706494\n",
      "Average test loss: 0.00870112167381578\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0017583224347068203\n",
      "Average test loss: 0.015586032410462696\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0017536125652905968\n",
      "Average test loss: 0.01859724681576093\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0017465708707976673\n",
      "Average test loss: 0.02985580244080888\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0017891384713972609\n",
      "Average test loss: 0.007997394054714176\n",
      "Epoch 220/300\n",
      "Average training loss: 0.001753568003575007\n",
      "Average test loss: 0.019635608553886415\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0017454321456866133\n",
      "Average test loss: 0.15334853618964553\n",
      "Epoch 222/300\n",
      "Average training loss: 0.001737153516461452\n",
      "Average test loss: 0.015958249972098403\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0017493131347000599\n",
      "Average test loss: 0.009900548414637646\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0017450131528700392\n",
      "Average test loss: 0.09091592292570405\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0017550083309825924\n",
      "Average test loss: 0.012990532646576564\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0017622315528699093\n",
      "Average test loss: 0.026079004883766174\n",
      "Epoch 227/300\n",
      "Average training loss: 0.001723171112438043\n",
      "Average test loss: 0.018652324005961417\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0017361516690709525\n",
      "Average test loss: 0.01395566766626305\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0017384276764674318\n",
      "Average test loss: 0.013212226221958796\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0017672373321321276\n",
      "Average test loss: 0.020586898773908614\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0017226551473140716\n",
      "Average test loss: 0.015611731830570432\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0017558631263673305\n",
      "Average test loss: 0.018011318946050275\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0017308891436292066\n",
      "Average test loss: 0.008101365615510277\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0017286724586867623\n",
      "Average test loss: 0.6839984718643957\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0017317151385876867\n",
      "Average test loss: 0.18988409886757532\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0017374726809147331\n",
      "Average test loss: 0.018863787637816536\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0017202508630644944\n",
      "Average test loss: 1.7604531832842363\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0017193528898060322\n",
      "Average test loss: 0.01470743547545539\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001751289489885999\n",
      "Average test loss: 0.006854693141662412\n",
      "Epoch 240/300\n",
      "Average training loss: 0.001700340126330654\n",
      "Average test loss: 0.0360102185871866\n",
      "Epoch 241/300\n",
      "Average training loss: 0.001714860510494974\n",
      "Average test loss: 0.033263682943251396\n",
      "Epoch 242/300\n",
      "Average training loss: 0.001715438051149249\n",
      "Average test loss: 0.01014140066007773\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0017322530070733692\n",
      "Average test loss: 0.012406349228488074\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0017114209853526618\n",
      "Average test loss: 0.01590177562998401\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0017496020512448418\n",
      "Average test loss: 0.1454126608305507\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0017308055623951886\n",
      "Average test loss: 4.6105856322315\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0017042125520399876\n",
      "Average test loss: 0.08839279732066724\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0016955091402762466\n",
      "Average test loss: 1.135303332382606\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0017062008806193868\n",
      "Average test loss: 0.009746014667054017\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0017110600675352746\n",
      "Average test loss: 0.011785550992521975\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0019291990849499901\n",
      "Average test loss: 0.010996488102608257\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0016980363970829381\n",
      "Average test loss: 0.008681204499055942\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0016965975242977341\n",
      "Average test loss: 1.4006069333487088\n",
      "Epoch 254/300\n",
      "Average training loss: 0.001682852195782794\n",
      "Average test loss: 0.029759982041186756\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0017136323704487748\n",
      "Average test loss: 0.01370324902733167\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0017020141305401921\n",
      "Average test loss: 0.008033582620322704\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0017105134952192505\n",
      "Average test loss: 0.013041850484907627\n",
      "Epoch 258/300\n",
      "Average test loss: 0.116332248551978\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0016816477538603876\n",
      "Average test loss: 0.010430923118359513\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0017081935506107078\n",
      "Average test loss: 9.334525569399197\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0016877304007195764\n",
      "Average test loss: 0.018662335193405547\n",
      "Epoch 265/300\n",
      "Average training loss: 0.001702768115637203\n",
      "Average test loss: 0.022379241440031263\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0016887584135143294\n",
      "Average test loss: 0.11650159032311705\n",
      "Epoch 267/300\n",
      "Average training loss: 0.001692346709056033\n",
      "Average test loss: 0.058174461007118226\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0016789034652627177\n",
      "Average test loss: 0.006851675715297461\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0017123252511438397\n",
      "Average test loss: 0.010139369927346707\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0016861121468245983\n",
      "Average test loss: 224.5555076408386\n",
      "Epoch 271/300\n",
      "Average training loss: 0.001703419065516856\n",
      "Average test loss: 0.009813122967878977\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0016867652100821336\n",
      "Average test loss: 2.0831796543167695\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0016961181920228732\n",
      "Average test loss: 0.01214308376196358\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0016939538496856888\n",
      "Average test loss: 0.031413261362248\n",
      "Epoch 275/300\n",
      "Average training loss: 0.001684515239774353\n",
      "Average test loss: 0.02175585177540779\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0016686178707621163\n",
      "Average test loss: 0.013824840307235717\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0016667241318565275\n",
      "Average test loss: 0.012098269989920987\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0017247621453263693\n",
      "Average test loss: 0.051157159181104764\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0016611763230628437\n",
      "Average test loss: 0.6035189700027307\n",
      "Epoch 280/300\n",
      "Average training loss: 0.001672471958419515\n",
      "Average test loss: 0.009777692178885142\n",
      "Epoch 281/300\n",
      "Average training loss: 0.001660875945041577\n",
      "Average test loss: 0.33611502453436454\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0016850790425928103\n",
      "Average test loss: 0.01605633167260223\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0016792318277681867\n",
      "Average test loss: 0.011551190166837639\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0016729425689619448\n",
      "Average test loss: 0.013077233883241812\n",
      "Epoch 285/300\n",
      "Average training loss: 0.001662507459314333\n",
      "Average test loss: 0.6442875183030136\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0016632775471856196\n",
      "Average test loss: 0.02610534231695864\n",
      "Epoch 287/300\n",
      "Average training loss: 0.001686407755025559\n",
      "Average test loss: 216.47458352491591\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0018642642282777361\n",
      "Average test loss: 0.391174361858103\n",
      "Epoch 289/300\n",
      "Average training loss: 0.001650437510675854\n",
      "Average test loss: 0.009825647373994191\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0016714412116756042\n",
      "Average test loss: 0.011909293279051781\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0016563903450344999\n",
      "Average test loss: 0.009824149197174442\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00165945666987035\n",
      "Average test loss: 59.475636845125095\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0016754548974956076\n",
      "Average test loss: 0.05592271431783835\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0016771921034281453\n",
      "Average test loss: 12.256489104277557\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0016571960223631726\n",
      "Average test loss: 0.03573512997892168\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0016495946801991926\n",
      "Average test loss: 0.012356896158721712\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0019034405407599277\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DAMP_No_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: -12.33\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 10.83\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 21.10\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 16.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.00\n",
      "Average PSNR for Projection Layer 0 across 2500 images: -9.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 12.46\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 19.78\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 18.94\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.07\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.025585786362489066\n",
      "Average test loss: 0.01886017457313008\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01473247568640444\n",
      "Average test loss: 0.7783492579294575\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013620455945531526\n",
      "Average test loss: 0.20301939954111972\n",
      "Epoch 4/300\n",
      "Average training loss: 0.012823423234124979\n",
      "Average test loss: 1.755157343480322\n",
      "Epoch 5/300\n",
      "Average training loss: 0.011734123920400937\n",
      "Average test loss: 4.194762672040198\n",
      "Epoch 6/300\n",
      "Average training loss: 0.011423991880483098\n",
      "Average test loss: 0.01581843978001012\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011122138099537956\n",
      "Average test loss: 0.9355832248942719\n",
      "Epoch 8/300\n",
      "Average training loss: 0.010512172136041853\n",
      "Average test loss: 0.03599745808872912\n",
      "Epoch 9/300\n",
      "Average training loss: 0.010966295165320238\n",
      "Average test loss: 0.7961201444698704\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0101829907198747\n",
      "Average test loss: 26.34540851364533\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010995702072978019\n",
      "Average test loss: 1.2171577167510987\n",
      "Epoch 12/300\n",
      "Average training loss: 0.009970933193133937\n",
      "Average test loss: 0.04845284214284685\n",
      "Epoch 13/300\n",
      "Average training loss: 0.009688347446421781\n",
      "Average test loss: 73.93806065538195\n",
      "Epoch 15/300\n",
      "Average training loss: 0.009575619124703937\n",
      "Average test loss: 20.248952650924522\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009067222913934124\n",
      "Average test loss: 21.137322827994822\n",
      "Epoch 17/300\n",
      "Average training loss: 0.008838931996375322\n",
      "Average test loss: 6435.841013480246\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008128091475201977\n",
      "Average test loss: 0.02068992465734482\n",
      "Epoch 21/300\n",
      "Average training loss: 0.007919981473435958\n",
      "Average test loss: 2115.330579338319\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007986634077711238\n",
      "Average test loss: 0.6999018066914545\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007691136646601889\n",
      "Average test loss: 12.190040890620814\n",
      "Epoch 24/300\n",
      "Average training loss: 0.007599999349978235\n",
      "Average test loss: 1.62210452817546\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007418584353807899\n",
      "Average test loss: 995.4307056659133\n",
      "Epoch 26/300\n",
      "Average training loss: 0.007274004292984804\n",
      "Average test loss: 21723.11593084549\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00747959383080403\n",
      "Average test loss: 27.574990575790405\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007684797329207261\n",
      "Average test loss: 1.5053231977787283\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0073545841135912475\n",
      "Average test loss: 10.268254553980297\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00711355768268307\n",
      "Average test loss: 6667.721706713151\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0070878712290691\n",
      "Average test loss: 0.6602645823475387\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007044000602016846\n",
      "Average test loss: 0.07526210627290937\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006969114712956879\n",
      "Average test loss: 2.459091405563884\n",
      "Epoch 35/300\n",
      "Average training loss: 0.006838114231410954\n",
      "Average test loss: 82.70437532019697\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006858521378702588\n",
      "Average test loss: 8.76666427335143\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006771845523681905\n",
      "Average test loss: 1101.7240208886133\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006850991013149421\n",
      "Average test loss: 15494.218793012142\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00682748712433709\n",
      "Average test loss: 16.64539048335453\n",
      "Epoch 40/300\n",
      "Average training loss: 0.006776629532376925\n",
      "Average test loss: 7.8031009547048145\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006676229542742173\n",
      "Average test loss: 0.011447583674556679\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006690364917947186\n",
      "Average test loss: 735.5772022063202\n",
      "Epoch 43/300\n",
      "Average training loss: 0.00788676628222068\n",
      "Average test loss: 14.689810612145397\n",
      "Epoch 45/300\n",
      "Average training loss: 0.006889451204902596\n",
      "Average test loss: 0.42672500385675166\n",
      "Epoch 46/300\n",
      "Average training loss: 0.006637884614782201\n",
      "Average test loss: 229.95265702633725\n",
      "Epoch 47/300\n",
      "Average training loss: 0.006490964534795946\n",
      "Average test loss: 0.10710849192572965\n",
      "Epoch 48/300\n",
      "Average training loss: 0.006439164313591188\n",
      "Average test loss: 21760.368460321875\n",
      "Epoch 49/300\n",
      "Average training loss: 0.006358457109166517\n",
      "Average test loss: 6642.143739479193\n",
      "Epoch 50/300\n",
      "Average training loss: 0.006383665486342377\n",
      "Average test loss: 657.2934580311312\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006285062266306745\n",
      "Average test loss: 495.25799766333233\n",
      "Epoch 52/300\n",
      "Average training loss: 0.006258508219487137\n",
      "Average test loss: 1024109436.6372517\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00623200955407487\n",
      "Average test loss: 3090139929308154.0\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006534636558757888\n",
      "Average test loss: 71.33185676406738\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00622027790091104\n",
      "Average test loss: 766.9986000119977\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006114967421938976\n",
      "Average test loss: 5.696845837333136\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0062194503756860895\n",
      "Average test loss: 1033.0055736265117\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00635931219243341\n",
      "Average test loss: 4814.502059248057\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006703642971813679\n",
      "Average test loss: 6308.731327582478\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006529303546167082\n",
      "Average test loss: 5548.4174978298615\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006462221655166811\n",
      "Average test loss: 309.36857135384946\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006493199397292402\n",
      "Average test loss: 54048.68042202688\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007404271680447791\n",
      "Average test loss: 0.021441095432473554\n",
      "Epoch 66/300\n",
      "Average training loss: 0.006703219078895119\n",
      "Average test loss: 0.01815348207950592\n",
      "Epoch 67/300\n",
      "Average training loss: 0.006671167182839579\n",
      "Average test loss: 24.501168068205317\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006558345374961694\n",
      "Average test loss: 0.04288777328862084\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006514156911108229\n",
      "Average test loss: 100.84410131425328\n",
      "Epoch 70/300\n",
      "Average training loss: 0.006685072443137566\n",
      "Average test loss: 7.695522016372945\n",
      "Epoch 71/300\n",
      "Average training loss: 0.006670177783403132\n",
      "Average test loss: 2.0632454504768054\n",
      "Epoch 72/300\n",
      "Average training loss: 0.006332084935158491\n",
      "Average test loss: 22.475008437847098\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0063147545167141494\n",
      "Average test loss: 0.011473150952822633\n",
      "Epoch 75/300\n",
      "Average training loss: 0.006359423650221692\n",
      "Average test loss: 0.010450909473001957\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0062319115735590454\n",
      "Average test loss: 0.017004239534338314\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0063445410745011435\n",
      "Average test loss: 0.11394846358729734\n",
      "Epoch 78/300\n",
      "Average training loss: 0.006364925519873698\n",
      "Average test loss: 0.012785502212742965\n",
      "Epoch 79/300\n",
      "Average training loss: 0.006289643699096309\n",
      "Average test loss: 0.011748190061085753\n",
      "Epoch 80/300\n",
      "Average training loss: 0.006221587686902947\n",
      "Average test loss: 0.01361835419966115\n",
      "Epoch 81/300\n",
      "Average training loss: 0.006316461715433333\n",
      "Average test loss: 0.015988390611277686\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0064546341229644085\n",
      "Average test loss: 0.013242221711410417\n",
      "Epoch 83/300\n",
      "Average training loss: 0.006222677442348666\n",
      "Average test loss: 0.013194502812292841\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0061750059351325035\n",
      "Average test loss: 0.009934988175829251\n",
      "Epoch 85/300\n",
      "Average training loss: 0.006162860419187281\n",
      "Average test loss: 0.01704995607998636\n",
      "Epoch 86/300\n",
      "Average training loss: 0.006218938587440385\n",
      "Average test loss: 0.011545602958235476\n",
      "Epoch 87/300\n",
      "Average training loss: 0.006031344265987476\n",
      "Average test loss: 0.011000862557854917\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0061969361826777454\n",
      "Average test loss: 0.05535349262091849\n",
      "Epoch 90/300\n",
      "Average training loss: 0.006042486755384339\n",
      "Average test loss: 32.980576096417174\n",
      "Epoch 91/300\n",
      "Average training loss: 0.006085587471723557\n",
      "Average test loss: 0.18033411434292793\n",
      "Epoch 92/300\n",
      "Average training loss: 0.006167223011040025\n",
      "Average test loss: 2.0353724399399424\n",
      "Epoch 93/300\n",
      "Average training loss: 0.006030950978812244\n",
      "Average test loss: 6.647060356122752\n",
      "Epoch 94/300\n",
      "Average training loss: 0.006274181954976585\n",
      "Average test loss: 5.669913225305577\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0059831801214151915\n",
      "Average test loss: 300.46771609003014\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0060252980387045275\n",
      "Average test loss: 0.01649821796019872\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005996750795178943\n",
      "Average test loss: 0.012133758650057846\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006026588160544634\n",
      "Average test loss: 0.07609507768187258\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005924291777527994\n",
      "Average test loss: 278.8561510459118\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005905821125540468\n",
      "Average test loss: 14.49352203070455\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005892521238161458\n",
      "Average test loss: 702.6536955546604\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005900511147247421\n",
      "Average test loss: 0.10814813795768552\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005894209094759491\n",
      "Average test loss: 0.032384073886606426\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005885606601420376\n",
      "Average test loss: 64.54839674382242\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005877977177086804\n",
      "Average test loss: 0.07025369760062959\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005836278967973259\n",
      "Average test loss: 0.014252632838156488\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0058314109593629836\n",
      "Average test loss: 1.992634475370248\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005798921779212024\n",
      "Average test loss: 0.11232301886710856\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005917449346019162\n",
      "Average test loss: 4.333725232885944\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0057985510706073705\n",
      "Average training loss: 0.0059008687196506395\n",
      "Average test loss: 18.784354426178666\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005791234429097838\n",
      "Average test loss: 2.1404292142275305\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005768647061040004\n",
      "Average test loss: 1.2299969961345196\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005750403148846494\n",
      "Average test loss: 0.8679686347676648\n",
      "Epoch 116/300\n",
      "Average training loss: 0.005776170344816314\n",
      "Average test loss: 0.013210988915628857\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005680275583018859\n",
      "Average test loss: 0.013825240393479665\n",
      "Epoch 118/300\n",
      "Average training loss: 0.005802249645607339\n",
      "Average test loss: 0.013158579071362813\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005661702597720755\n",
      "Average test loss: 0.039623082070714896\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005856484611415201\n",
      "Average test loss: 0.45766952379379006\n",
      "Epoch 121/300\n",
      "Average training loss: 0.00593578998827272\n",
      "Average test loss: 3.281776276528835\n",
      "Epoch 122/300\n",
      "Average training loss: 0.00583491935291224\n",
      "Average test loss: 1.8804272026088502\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005720128662470314\n",
      "Average test loss: 19.623867484894063\n",
      "Epoch 124/300\n",
      "Average training loss: 0.005687532684869236\n",
      "Average test loss: 0.019009904283616277\n",
      "Epoch 125/300\n",
      "Average training loss: 0.005723775511814488\n",
      "Average test loss: 0.8567949214511448\n",
      "Epoch 126/300\n",
      "Average training loss: 0.005687348896016678\n",
      "Average test loss: 2.0968159917452267\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00604705509212282\n",
      "Average test loss: 4.227754990332657\n",
      "Epoch 128/300\n",
      "Average training loss: 0.005773639618522591\n",
      "Average test loss: 0.21671502227253384\n",
      "Epoch 129/300\n",
      "Average training loss: 0.005741818201210763\n",
      "Average test loss: 0.016554974931809636\n",
      "Epoch 130/300\n",
      "Average training loss: 0.005743562361018526\n",
      "Average test loss: 777.8457997977337\n",
      "Epoch 131/300\n",
      "Average training loss: 0.00565666138794687\n",
      "Average test loss: 43.55694654966891\n",
      "Epoch 132/300\n",
      "Average training loss: 0.005633616481804185\n",
      "Average test loss: 164.57074355861297\n",
      "Epoch 133/300\n",
      "Average training loss: 0.005646577491528458\n",
      "Average test loss: 2.330201735673679\n",
      "Epoch 134/300\n",
      "Average training loss: 0.005542631287955575\n",
      "Average test loss: 3.008582392995556\n",
      "Epoch 136/300\n",
      "Average training loss: 0.005555268292625745\n",
      "Average test loss: 3.1291527025227746\n",
      "Epoch 137/300\n",
      "Average training loss: 0.005534001486168967\n",
      "Average test loss: 59.79423591909309\n",
      "Epoch 138/300\n",
      "Average training loss: 0.005572355514599218\n",
      "Average test loss: 4525.168322299896\n",
      "Epoch 139/300\n",
      "Average training loss: 0.005558036368340254\n",
      "Average test loss: 42.737659304671816\n",
      "Epoch 140/300\n",
      "Average training loss: 0.005575920678675175\n",
      "Average test loss: 39.61926199064983\n",
      "Epoch 141/300\n",
      "Average training loss: 0.005774747443695863\n",
      "Average test loss: 0.01776147801015112\n",
      "Epoch 142/300\n",
      "Average training loss: 0.005769625964264075\n",
      "Average test loss: 0.04366015268862247\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00564542150290476\n",
      "Average test loss: 0.019981085856755575\n",
      "Epoch 144/300\n",
      "Average training loss: 0.005650967203908497\n",
      "Average test loss: 0.01915023365120093\n",
      "Epoch 145/300\n",
      "Average training loss: 0.005755631123565965\n",
      "Average test loss: 0.01610546500153012\n",
      "Epoch 146/300\n",
      "Average training loss: 0.005590519265168243\n",
      "Average test loss: 0.023625944597853554\n",
      "Epoch 148/300\n",
      "Average training loss: 0.005654683990197049\n",
      "Average test loss: 0.023079747643735674\n",
      "Epoch 149/300\n",
      "Average training loss: 0.005528488614906867\n",
      "Average test loss: 0.01305753155052662\n",
      "Epoch 150/300\n",
      "Average training loss: 0.005509500804874632\n",
      "Average test loss: 0.017559079877204366\n",
      "Epoch 151/300\n",
      "Average training loss: 0.005558330787138807\n",
      "Average test loss: 0.014037628889083862\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00580191291620334\n",
      "Average test loss: 0.023194471031427383\n",
      "Epoch 153/300\n",
      "Average training loss: 0.005663905400368902\n",
      "Average test loss: 0.016533265438344744\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005538775887754229\n",
      "Average test loss: 0.04789808855288558\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005640975162386894\n",
      "Average test loss: 0.012956428767906295\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0057212856954170594\n",
      "Average test loss: 0.01790657422112094\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0056096278031667075\n",
      "Average test loss: 0.021011520395676296\n",
      "Epoch 158/300\n",
      "Average training loss: 0.005495172548211283\n",
      "Average test loss: 645.2864038342138\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0055086377445194455\n",
      "Average test loss: 0.03869936141702864\n",
      "Epoch 160/300\n",
      "Average training loss: 0.005454440573023425\n",
      "Average test loss: 1.4911278680778213\n",
      "Epoch 162/300\n",
      "Average training loss: 0.005605439133528206\n",
      "Average test loss: 0.23657835198773278\n",
      "Epoch 163/300\n",
      "Average training loss: 0.005558288949231307\n",
      "Average test loss: 0.07259399311741194\n",
      "Epoch 164/300\n",
      "Average training loss: 0.005549006145447493\n",
      "Average test loss: 0.016118318680259917\n",
      "Epoch 165/300\n",
      "Average training loss: 0.005482548533214463\n",
      "Average test loss: 0.12094974202579922\n",
      "Epoch 166/300\n",
      "Average training loss: 0.005548784948471519\n",
      "Average test loss: 12.396933921913305\n",
      "Epoch 167/300\n",
      "Average training loss: 0.005637562423116631\n",
      "Average test loss: 2.1783130631513066\n",
      "Epoch 168/300\n",
      "Average training loss: 0.005635898091726833\n",
      "Average test loss: 9.10179570971595\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0056023152718941375\n",
      "Average test loss: 8.779812333345413\n",
      "Epoch 170/300\n",
      "Average training loss: 0.005653459031548765\n",
      "Average test loss: 1.2455846071657208\n",
      "Epoch 171/300\n",
      "Average training loss: 0.005529524363163445\n",
      "Average test loss: 12.32739098795255\n",
      "Epoch 172/300\n",
      "Average training loss: 0.005556362015919553\n",
      "Average test loss: 0.878349824918641\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0054913161595662435\n",
      "Average test loss: 146.0496240632799\n",
      "Epoch 174/300\n",
      "Average training loss: 0.005458106874591775\n",
      "Average test loss: 6336.700243996234\n",
      "Epoch 176/300\n",
      "Average training loss: 0.005503342414481772\n",
      "Average test loss: 3.576402494620946\n",
      "Epoch 177/300\n",
      "Average training loss: 0.005553063751508792\n",
      "Average test loss: 0.939624330100086\n",
      "Epoch 178/300\n",
      "Average training loss: 0.005408331404543585\n",
      "Average test loss: 1.4448556903600693\n",
      "Epoch 179/300\n",
      "Average training loss: 0.006855751116656595\n",
      "Average test loss: 4389643.415166667\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0058344434342450565\n",
      "Average test loss: 4.622511629273494\n",
      "Epoch 181/300\n",
      "Average training loss: 0.005672560755991273\n",
      "Average test loss: 849.2719199147697\n",
      "Epoch 182/300\n",
      "Average training loss: 0.005413232651021746\n",
      "Average test loss: 0.429637329392963\n",
      "Epoch 183/300\n",
      "Average training loss: 0.005411346486459176\n",
      "Average test loss: 0.025360383437739477\n",
      "Epoch 184/300\n",
      "Average training loss: 0.005427351919313272\n",
      "Average test loss: 77.85400486454037\n",
      "Epoch 185/300\n",
      "Average training loss: 0.005388241653227144\n",
      "Average test loss: 0.05325003743502829\n",
      "Epoch 186/300\n",
      "Average training loss: 0.005371801325430472\n",
      "Average test loss: 0.49639313922822476\n",
      "Epoch 187/300\n",
      "Average training loss: 0.005367193712542454\n",
      "Average test loss: 0.04500716166529391\n",
      "Epoch 189/300\n",
      "Average training loss: 0.005478647914611631\n",
      "Average test loss: 0.02549636529551612\n",
      "Epoch 190/300\n",
      "Average training loss: 0.005360171972877449\n",
      "Average test loss: 0.12178140941262246\n",
      "Epoch 191/300\n",
      "Average training loss: 0.005436204647438394\n",
      "Average test loss: 126.61172635511558\n",
      "Epoch 192/300\n",
      "Average training loss: 0.005701247157735957\n",
      "Average test loss: 9.250084808799956\n",
      "Epoch 193/300\n",
      "Average training loss: 0.005744139230499665\n",
      "Average test loss: 755.0634705885798\n",
      "Epoch 194/300\n",
      "Average training loss: 0.005674418351302544\n",
      "Average test loss: 1919.6152905137803\n",
      "Epoch 195/300\n",
      "Average training loss: 0.005809461872610781\n",
      "Average test loss: 0.016766325563192366\n",
      "Epoch 196/300\n",
      "Average training loss: 0.005867412757956319\n",
      "Average test loss: 1.438566493143638\n",
      "Epoch 197/300\n",
      "Average training loss: 0.005775488807509343\n",
      "Average test loss: 0.08248816998137368\n",
      "Epoch 198/300\n",
      "Average training loss: 0.005690999863876237\n",
      "Average test loss: 1.1587673404216767\n",
      "Epoch 199/300\n",
      "Average training loss: 0.005628229485203823\n",
      "Average test loss: 0.4887343316078186\n",
      "Epoch 202/300\n",
      "Average training loss: 0.005486481299830807\n",
      "Average test loss: 0.048593739696674874\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006015900798141956\n",
      "Average test loss: 0.18912036006814903\n",
      "Epoch 204/300\n",
      "Average training loss: 0.005839077348096503\n",
      "Average test loss: 39.136657644033434\n",
      "Epoch 205/300\n",
      "Average training loss: 0.005550366361935934\n",
      "Average test loss: 0.532878821629617\n",
      "Epoch 206/300\n",
      "Average training loss: 0.005459767193843921\n",
      "Average test loss: 0.33418971133894393\n",
      "Epoch 207/300\n",
      "Average training loss: 0.005657668713066313\n",
      "Average test loss: 0.1225175127585729\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0054302802222470446\n",
      "Average test loss: 0.0457670866664913\n",
      "Epoch 209/300\n",
      "Average training loss: 0.005333879297806157\n",
      "Average test loss: 0.5475737463235855\n",
      "Epoch 210/300\n",
      "Average training loss: 0.005258276123139593\n",
      "Average test loss: 562.5358733723958\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00576332457198037\n",
      "Average test loss: 0.015803810861375597\n",
      "Epoch 212/300\n",
      "Average training loss: 0.005647642170803414\n",
      "Average test loss: 8.062072353627947\n",
      "Epoch 213/300\n",
      "Average training loss: 0.005501367807388306\n",
      "Average test loss: 0.1935815374536647\n",
      "Epoch 214/300\n",
      "Average training loss: 0.005336714945733547\n",
      "Average test loss: 0.8964725038343005\n",
      "Epoch 216/300\n",
      "Average training loss: 0.005365456116282277\n",
      "Average test loss: 2.6154608793275225\n",
      "Epoch 217/300\n",
      "Average training loss: 0.005286035551379124\n",
      "Average test loss: 1.2947668101522658\n",
      "Epoch 218/300\n",
      "Average training loss: 0.005303383376035425\n",
      "Average test loss: 9.54232632528411\n",
      "Epoch 219/300\n",
      "Average training loss: 0.005397309859179788\n",
      "Average test loss: 6.688244379520416\n",
      "Epoch 220/300\n",
      "Average training loss: 0.005677961371839046\n",
      "Average test loss: 44244.60037152778\n",
      "Epoch 221/300\n",
      "Average training loss: 0.005680813900298542\n",
      "Average test loss: 138.1899677254624\n",
      "Epoch 222/300\n",
      "Average training loss: 0.005646800295346313\n",
      "Average test loss: 0.20475063412884872\n",
      "Epoch 223/300\n",
      "Average training loss: 0.005549627320220073\n",
      "Average test loss: 2394.0379336825345\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0055839656918413106\n",
      "Average test loss: 163.6917570122613\n",
      "Epoch 225/300\n",
      "Average training loss: 0.005652079501085811\n",
      "Average test loss: 32.887266696506074\n",
      "Epoch 226/300\n",
      "Average training loss: 0.005631055812040965\n",
      "Average test loss: 53758.69916145833\n",
      "Epoch 227/300\n",
      "Average training loss: 0.005508064238561525\n",
      "Average test loss: 1.391472755855984\n",
      "Epoch 230/300\n",
      "Average training loss: 0.005462928647382392\n",
      "Average test loss: 0.19939643239312702\n",
      "Epoch 231/300\n",
      "Average training loss: 0.005413037803437975\n",
      "Average test loss: 0.02048172290954325\n",
      "Epoch 232/300\n",
      "Average training loss: 0.005407481404642264\n",
      "Average test loss: 25.750372302235828\n",
      "Epoch 233/300\n",
      "Average training loss: 0.005417751879327827\n",
      "Average test loss: 12.131665497938792\n",
      "Epoch 234/300\n",
      "Average training loss: 0.005451706799781985\n",
      "Average test loss: 0.14248043128185803\n",
      "Epoch 235/300\n",
      "Average training loss: 0.005388237095955345\n",
      "Average test loss: 21.274106341858705\n",
      "Epoch 236/300\n",
      "Average training loss: 0.005306982655078172\n",
      "Average test loss: 2.916800373778575\n",
      "Epoch 237/300\n",
      "Average training loss: 0.005327161429242955\n",
      "Average test loss: 2.9014239112006295\n",
      "Epoch 238/300\n",
      "Average training loss: 0.005258747627751695\n",
      "Average test loss: 1.2674505251480472\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0052884805959959825\n",
      "Average test loss: 295.065243039204\n",
      "Epoch 240/300\n",
      "Average training loss: 0.005254790510568354\n",
      "Average test loss: 6.837295693119367\n",
      "Epoch 241/300\n",
      "Average training loss: 0.005401869112004836\n",
      "Average training loss: 0.005412143412563536\n",
      "Average test loss: 0.5611635171307457\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0052990914682547255\n",
      "Average test loss: 0.01988981446954939\n",
      "Epoch 245/300\n",
      "Average training loss: 0.005267916740642653\n",
      "Average test loss: 85953.95364405015\n",
      "Epoch 246/300\n",
      "Average training loss: 0.005254469126462937\n",
      "Average test loss: 0.40151073102818596\n",
      "Epoch 247/300\n",
      "Average training loss: 0.005444979581567976\n",
      "Average test loss: 37.63389429333475\n",
      "Epoch 248/300\n",
      "Average training loss: 0.005588904436263773\n",
      "Average test loss: 342.608487102492\n",
      "Epoch 249/300\n",
      "Average training loss: 0.005466909777786997\n",
      "Average test loss: 61199.43639398871\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005532239636199342\n",
      "Average test loss: 753.9937239583334\n",
      "Epoch 251/300\n",
      "Average training loss: 0.005340735646171702\n",
      "Average test loss: 12.097304825607273\n",
      "Epoch 252/300\n",
      "Average training loss: 0.005245658861266242\n",
      "Average test loss: 0.04840783928996987\n",
      "Epoch 253/300\n",
      "Average training loss: 0.005192547716200352\n",
      "Average test loss: 0.2401053005407254\n",
      "Epoch 254/300\n",
      "Average training loss: 0.005433197944528526\n",
      "Average test loss: 0.05540011149975989\n",
      "Epoch 255/300\n",
      "Average training loss: 0.005416621771537595\n",
      "Average test loss: 0.03138347546259562\n",
      "Epoch 256/300\n",
      "Average training loss: 0.005434670651952426\n",
      "Average test loss: 0.7867299441099167\n",
      "Epoch 257/300\n",
      "Average training loss: 0.005300744753744867\n",
      "Average training loss: 0.005614322560114993\n",
      "Average test loss: 0.11824381269183423\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00539342578417725\n",
      "Average test loss: 0.03792882250249386\n",
      "Epoch 261/300\n",
      "Average training loss: 0.005645984495679537\n",
      "Average test loss: 1.075205992139048\n",
      "Epoch 262/300\n",
      "Average training loss: 0.005520488674028053\n",
      "Average test loss: 0.038968945792979666\n",
      "Epoch 263/300\n",
      "Average training loss: 0.005540570245848761\n",
      "Average test loss: 0.0278762729085154\n",
      "Epoch 264/300\n",
      "Average training loss: 0.005461118169542816\n",
      "Average test loss: 0.030132981810304854\n",
      "Epoch 265/300\n",
      "Average training loss: 0.005319930196636253\n",
      "Average test loss: 15.139670026272535\n",
      "Epoch 266/300\n",
      "Average training loss: 0.005361094871742858\n",
      "Average test loss: 0.028645933757225673\n",
      "Epoch 267/300\n",
      "Average training loss: 0.005300481433669726\n",
      "Average test loss: 0.18621381476190355\n",
      "Epoch 268/300\n",
      "Average training loss: 0.005278819841022293\n",
      "Average test loss: 1.4567368955959876\n",
      "Epoch 269/300\n",
      "Average training loss: 0.005455285285909971\n",
      "Average test loss: 0.01802068575554424\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00539786864982711\n",
      "Average test loss: 17.447303145395384\n",
      "Epoch 271/300\n",
      "Average training loss: 0.005624431098914809\n",
      "Average test loss: 0.011933349565499358\n",
      "Epoch 272/300\n",
      "Average training loss: 0.005377871332069238\n",
      "Average test loss: 0.7317042742967605\n",
      "Epoch 274/300\n",
      "Average training loss: 0.005352447910855214\n",
      "Average test loss: 1.7658502469890647\n",
      "Epoch 275/300\n",
      "Average training loss: 0.005376323201590114\n",
      "Average test loss: 29.220377403287426\n",
      "Epoch 276/300\n",
      "Average training loss: 0.005345758465015226\n",
      "Average test loss: 0.32776192782322566\n",
      "Epoch 277/300\n",
      "Average training loss: 0.005449239074769947\n",
      "Average test loss: 2.7596708884504104\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0054619859308004375\n",
      "Average test loss: 0.26440735752880573\n",
      "Epoch 279/300\n",
      "Average training loss: 0.005360113336808151\n",
      "Average test loss: 0.2623138506412506\n",
      "Epoch 280/300\n",
      "Average training loss: 0.005451712112459871\n",
      "Average test loss: 0.05318911084863875\n",
      "Epoch 281/300\n",
      "Average training loss: 0.005855849424584045\n",
      "Average test loss: 0.0312895162511203\n",
      "Epoch 282/300\n",
      "Average training loss: 0.005727366161843141\n",
      "Average test loss: 0.01793840400543478\n",
      "Epoch 283/300\n",
      "Average training loss: 0.005753017993022998\n",
      "Average test loss: 52.9110524541537\n",
      "Epoch 284/300\n",
      "Average training loss: 0.005666605791283978\n",
      "Average test loss: 0.7950846196346812\n",
      "Epoch 285/300\n",
      "Average training loss: 0.005515288279702266\n",
      "Average test loss: 0.222277711947759\n",
      "Epoch 286/300\n",
      "Average training loss: 0.005620680272579193\n",
      "Average test loss: 0.07543775768578052\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0055731660313904285\n",
      "Average test loss: 0.531523752729098\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0056295530891252886\n",
      "Average test loss: 0.05130956807898151\n",
      "Epoch 290/300\n",
      "Average training loss: 0.005700760922084252\n",
      "Average test loss: 0.7845524904686544\n",
      "Epoch 291/300\n",
      "Average training loss: 0.005654177302287684\n",
      "Average test loss: 0.4820297521882587\n",
      "Epoch 292/300\n",
      "Average training loss: 0.005713847238777412\n",
      "Average test loss: 0.05405655273463991\n",
      "Epoch 293/300\n",
      "Average training loss: 0.005561786137935188\n",
      "Average test loss: 113.38529064941406\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00610950839974814\n",
      "Average test loss: 0.02407202973216772\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006081589718245798\n",
      "Average test loss: 0.03349599835276604\n",
      "Epoch 296/300\n",
      "Average training loss: 0.006030604851328664\n",
      "Average test loss: 0.1430997718109025\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006007311234457626\n",
      "Average test loss: 0.039840263017349775\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00599497830743591\n",
      "Average test loss: 0.03592466072241465\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006115471023652288\n",
      "Average test loss: 0.013766385502285428\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02341167076759868\n",
      "Average test loss: 0.016288299755917654\n",
      "Epoch 2/300\n",
      "Average training loss: 0.011556571034093698\n",
      "Average test loss: 0.037097548921075134\n",
      "Epoch 3/300\n",
      "Average training loss: 0.010151670970022678\n",
      "Average test loss: 2.1272374831570517\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008798587530023522\n",
      "Average test loss: 1.5932291409100094\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008270699008471437\n",
      "Average test loss: 32.14923781659537\n",
      "Epoch 6/300\n",
      "Average training loss: 0.007742802547911803\n",
      "Average test loss: 84.17157920490371\n",
      "Epoch 7/300\n",
      "Average training loss: 0.00743668615569671\n",
      "Average test loss: 21410.331596420656\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007043031400276555\n",
      "Average test loss: 10974.291042734869\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006880484196460909\n",
      "Average test loss: 3242.082317828864\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006459384503049983\n",
      "Average test loss: 42709.18945908569\n",
      "Epoch 11/300\n",
      "Average training loss: 0.006132250590870777\n",
      "Average test loss: 115.06081456832588\n",
      "Epoch 12/300\n",
      "Average training loss: 0.006166103798482153\n",
      "Average test loss: 315.4313237665892\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005770291805267334\n",
      "Average test loss: 26563.199102791423\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005549888092610571\n",
      "Average test loss: 2661.6244906820853\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005418535923378335\n",
      "Average test loss: 0.0068995766941871905\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00539136632407705\n",
      "Average test loss: 428.4547401158412\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005268863730546501\n",
      "Average test loss: 29744.16661049802\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005032729162524144\n",
      "Average test loss: 1707817997.8386452\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0050693309418857095\n",
      "Average test loss: 105478.30547204625\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00497418814814753\n",
      "Average test loss: 1072566.4769822676\n",
      "Epoch 22/300\n",
      "Average training loss: 0.005084600656810734\n",
      "Average test loss: 541.0636703634436\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004971686855786376\n",
      "Average test loss: 49597.01047896527\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0055755697157647876\n",
      "Average test loss: 2.166810931003756\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0050320351479782\n",
      "Average test loss: 1968007392526.8416\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004880568897558583\n",
      "Average test loss: 78.58876928956641\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0048757041634784806\n",
      "Average test loss: 633.1802516138107\n",
      "Epoch 28/300\n",
      "Average training loss: 0.00499747160780761\n",
      "Average test loss: 132211.1062713547\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0056242005013757285\n",
      "Average test loss: 1.6523870761013693\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004757899949120151\n",
      "Average test loss: 609.9053848800394\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0048996457867324355\n",
      "Average test loss: 230.114642209409\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004652590766963031\n",
      "Average test loss: 756.186535110289\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0046158050994078315\n",
      "Average test loss: 4.990038322707017\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004480081304907799\n",
      "Average test loss: 0.0072759051600264175\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004625140385909213\n",
      "Average test loss: 15191.967883479609\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00449660524725914\n",
      "Average test loss: 0.008680859435763624\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004472478028800753\n",
      "Average test loss: 0.012397393989066283\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004449722213877572\n",
      "Average test loss: 0.005492400159438451\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0042251920133001275\n",
      "Average test loss: 205.98444420555856\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006030647754669189\n",
      "Average test loss: 4.28103062054846\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004844115402135584\n",
      "Average test loss: 418.79857548014326\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0046910644136369225\n",
      "Average test loss: 138.42957874651088\n",
      "Epoch 46/300\n",
      "Average training loss: 0.004970706918173366\n",
      "Average test loss: 340.24695389041307\n",
      "Epoch 47/300\n",
      "Average training loss: 0.004777258682996034\n",
      "Average test loss: 0.22809696851546565\n",
      "Epoch 48/300\n",
      "Average training loss: 0.004547947799166043\n",
      "Average test loss: 143.73705205901464\n",
      "Epoch 49/300\n",
      "Average training loss: 0.004637727238651779\n",
      "Average test loss: 6.166849019319647\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004555281678007709\n",
      "Average test loss: 2.279518502213061\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005019268017676141\n",
      "Average test loss: 129479.17915744991\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004821584727201197\n",
      "Average test loss: 0.4724355054406656\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004882732991543081\n",
      "Average test loss: 104.64215802486737\n",
      "Epoch 54/300\n",
      "Average training loss: 0.004943885818537739\n",
      "Average test loss: 0.0487987090655499\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005566862033473121\n",
      "Average test loss: 0.014692797797835536\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005490952701618274\n",
      "Average test loss: 1.4627284584757354\n",
      "Epoch 57/300\n",
      "Average training loss: 0.004995772743390666\n",
      "Average test loss: 63.61268126365046\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0049538479124506315\n",
      "Average test loss: 456.5814034556217\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005017445986055666\n",
      "Average test loss: 83.69466089109\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0047728955058587924\n",
      "Average test loss: 1574152.653398457\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00544560136190719\n",
      "Average test loss: 1.5461994808779822\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0055312974895868035\n",
      "Average test loss: 114.10502426905103\n",
      "Epoch 63/300\n",
      "Average training loss: 0.004809956492235264\n",
      "Average test loss: 0.013007784025536644\n",
      "Epoch 64/300\n",
      "Average training loss: 0.004789231253166993\n",
      "Average test loss: 0.6874607365843323\n",
      "Epoch 65/300\n",
      "Average training loss: 0.00504240825234188\n",
      "Average test loss: 4.140963069279989\n",
      "Epoch 66/300\n",
      "Average training loss: 0.004724781888640589\n",
      "Average test loss: 0.007232512460814582\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0047246943617032635\n",
      "Average test loss: 10.346590945223967\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004661262258059449\n",
      "Average test loss: 0.6556695706397295\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004673489027139213\n",
      "Average test loss: 0.011462475306457944\n",
      "Epoch 71/300\n",
      "Average training loss: 0.006795232740127378\n",
      "Average test loss: 0.2415570527886351\n",
      "Epoch 72/300\n",
      "Average training loss: 0.00504588318326407\n",
      "Average test loss: 1.159123221601877\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005072870912651221\n",
      "Average test loss: 0.18818778873814476\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0050074048555559586\n",
      "Average test loss: 0.016075257331132888\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004925715281317631\n",
      "Average test loss: 233376.88940225093\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00478739291553696\n",
      "Average test loss: 176.84310047405958\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004670184017469486\n",
      "Average test loss: 0.027794973958697584\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004961568032287889\n",
      "Average test loss: 0.04883578167690171\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004586566384881735\n",
      "Average test loss: 1.372533424006568\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004518050057192644\n",
      "Average test loss: 50.15555997394522\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004386215325444936\n",
      "Average test loss: 0.11864094034996298\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0043862888668146395\n",
      "Average test loss: 4.084940842466222\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004269888172960944\n",
      "Average test loss: 0.00830591290940841\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0042619773757954435\n",
      "Average test loss: 180.71770520276493\n",
      "Epoch 85/300\n",
      "Average training loss: 0.004349832418478198\n",
      "Average test loss: 9.711484697540602\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0041771758877568775\n",
      "Average test loss: 30651.32131261326\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004363145851219693\n",
      "Average test loss: 434.030918718689\n",
      "Epoch 88/300\n",
      "Average training loss: 0.004300500614775551\n",
      "Average test loss: 203.76831445034014\n",
      "Epoch 89/300\n",
      "Average training loss: 0.004209520923387673\n",
      "Average test loss: 29.80412742737805\n",
      "Epoch 90/300\n",
      "Average training loss: 0.00410469481555952\n",
      "Average test loss: 32.917509064025346\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0040935436238845185\n",
      "Average test loss: 0.7844673999547959\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004108874113609394\n",
      "Average test loss: 1.3638965042084457\n",
      "Epoch 93/300\n",
      "Average training loss: 0.004156325347721577\n",
      "Average test loss: 242.89686211133335\n",
      "Epoch 94/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DAMP_No_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DAMP_No_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
