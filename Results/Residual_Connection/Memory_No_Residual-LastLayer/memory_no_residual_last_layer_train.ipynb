{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee6282c-4511-472e-98d8-f8d738471540",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 1\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02761332994202773\n",
      "Average test loss: 0.021945374604728486\n",
      "Epoch 2/300\n",
      "Average training loss: 0.013560625482764509\n",
      "Average test loss: 0.0189071390380462\n",
      "Epoch 3/300\n",
      "Average training loss: 0.012296039634280734\n",
      "Average test loss: 0.014657438702053494\n",
      "Epoch 4/300\n",
      "Average training loss: 0.011641302271849579\n",
      "Average test loss: 0.010732660875552231\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01142002314163579\n",
      "Average test loss: 0.010337873821457228\n",
      "Epoch 6/300\n",
      "Average training loss: 0.010388842720952298\n",
      "Average test loss: 0.009597411429716481\n",
      "Epoch 7/300\n",
      "Average training loss: 0.009984493181109428\n",
      "Average test loss: 0.010364861845970155\n",
      "Epoch 8/300\n",
      "Average training loss: 0.009544904560678535\n",
      "Average test loss: 0.01804316251642174\n",
      "Epoch 9/300\n",
      "Average training loss: 0.009168251859645048\n",
      "Average test loss: 0.009680710665881633\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008917994691679875\n",
      "Average test loss: 0.008320807923873266\n",
      "Epoch 11/300\n",
      "Average training loss: 0.00869410521702634\n",
      "Average test loss: 0.008342537467678388\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00853806729035245\n",
      "Average test loss: 0.009833661725123724\n",
      "Epoch 13/300\n",
      "Average training loss: 0.008354393335680168\n",
      "Average test loss: 0.010680281578666634\n",
      "Epoch 14/300\n",
      "Average training loss: 0.008221188608970907\n",
      "Average test loss: 0.010266470978657405\n",
      "Epoch 15/300\n",
      "Average training loss: 0.008235540875958071\n",
      "Average test loss: 0.008027303471333451\n",
      "Epoch 16/300\n",
      "Average training loss: 0.007984089967277315\n",
      "Average test loss: 0.008201781375540628\n",
      "Epoch 17/300\n",
      "Average training loss: 0.00838071932643652\n",
      "Average test loss: 0.008667001970940166\n",
      "Epoch 18/300\n",
      "Average training loss: 0.008002444592614968\n",
      "Average test loss: 0.008549659439259106\n",
      "Epoch 19/300\n",
      "Average training loss: 0.007706348381108708\n",
      "Average test loss: 0.00823842592123482\n",
      "Epoch 20/300\n",
      "Average training loss: 0.007702042062249449\n",
      "Average test loss: 0.007903669975284073\n",
      "Epoch 21/300\n",
      "Average training loss: 0.007571427119274934\n",
      "Average test loss: 0.007502062471376525\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007512870044344001\n",
      "Average test loss: 0.009236611053347587\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007542816860808267\n",
      "Average test loss: 0.007438235406660372\n",
      "Epoch 24/300\n",
      "Average training loss: 0.007328356618682544\n",
      "Average test loss: 0.007774227844758166\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007309751504825221\n",
      "Average test loss: 0.007483654013110532\n",
      "Epoch 26/300\n",
      "Average training loss: 0.007254908100598388\n",
      "Average test loss: 0.007497102153797944\n",
      "Epoch 27/300\n",
      "Average training loss: 0.007333824229323202\n",
      "Average test loss: 0.007813693183577722\n",
      "Epoch 28/300\n",
      "Average training loss: 0.007150946889900499\n",
      "Average test loss: 0.0071272913329303265\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0070769082667926945\n",
      "Average test loss: 0.007573415428813961\n",
      "Epoch 30/300\n",
      "Average training loss: 0.007166726388037205\n",
      "Average test loss: 0.0076291821098162065\n",
      "Epoch 31/300\n",
      "Average training loss: 0.007004711850235859\n",
      "Average test loss: 0.00804830303043127\n",
      "Epoch 32/300\n",
      "Average training loss: 0.00709125089024504\n",
      "Average test loss: 0.007241569242543645\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0069150131800108485\n",
      "Average test loss: 0.0076393552741242775\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006926271791259448\n",
      "Average test loss: 0.008632765820042954\n",
      "Epoch 35/300\n",
      "Average training loss: 0.006888086704744233\n",
      "Average test loss: 0.007357343823131588\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0068280335358447495\n",
      "Average test loss: 0.007661637990011109\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006752349922640456\n",
      "Average test loss: 0.00689448493719101\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00676270012350546\n",
      "Average test loss: 0.009710615096820726\n",
      "Epoch 39/300\n",
      "Average training loss: 0.006695035818964243\n",
      "Average test loss: 0.008618528910809093\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0067300487955411275\n",
      "Average test loss: 0.011573574512369103\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006666486514525281\n",
      "Average test loss: 0.008934362801826662\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006604985850966639\n",
      "Average test loss: 0.0075298226111465034\n",
      "Epoch 43/300\n",
      "Average training loss: 0.006599785231881671\n",
      "Average test loss: 0.007938136176102692\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0065466177455253075\n",
      "Average test loss: 0.007502337733904521\n",
      "Epoch 45/300\n",
      "Average training loss: 0.006596581019047234\n",
      "Average test loss: 0.009340210201011764\n",
      "Epoch 46/300\n",
      "Average training loss: 0.006526251159608364\n",
      "Average test loss: 0.006831412468519476\n",
      "Epoch 47/300\n",
      "Average training loss: 0.006493197089682023\n",
      "Average test loss: 0.00707757576306661\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00650283295288682\n",
      "Average test loss: 0.0068570998021297985\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0063878034216662246\n",
      "Average test loss: 0.009508841740588346\n",
      "Epoch 50/300\n",
      "Average training loss: 0.006510902346008354\n",
      "Average test loss: 0.007371467273268434\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006432068330132299\n",
      "Average test loss: 0.006862036812222666\n",
      "Epoch 52/300\n",
      "Average training loss: 0.006313396961738666\n",
      "Average test loss: 0.006755046342809995\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0063329577648805245\n",
      "Average test loss: 0.0070951926989687815\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0063013288130362825\n",
      "Average test loss: 0.007005833595577214\n",
      "Epoch 55/300\n",
      "Average training loss: 0.006285494666132662\n",
      "Average test loss: 0.006885558277368546\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006270383076949252\n",
      "Average test loss: 0.00755183943402436\n",
      "Epoch 57/300\n",
      "Average training loss: 0.006195877309060759\n",
      "Average test loss: 0.00728868272072739\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0062324287446422705\n",
      "Average test loss: 0.007399530519627862\n",
      "Epoch 59/300\n",
      "Average training loss: 0.006545914923151334\n",
      "Average test loss: 0.006946752109875281\n",
      "Epoch 60/300\n",
      "Average training loss: 0.006179348116119703\n",
      "Average test loss: 0.0075092917101250755\n",
      "Epoch 61/300\n",
      "Average training loss: 0.006221156684474813\n",
      "Average test loss: 0.006771982524544\n",
      "Epoch 62/300\n",
      "Average training loss: 0.006141160896668832\n",
      "Average test loss: 0.006590236183669832\n",
      "Epoch 63/300\n",
      "Average training loss: 0.006074581852803628\n",
      "Average test loss: 0.006581464031918181\n",
      "Epoch 64/300\n",
      "Average training loss: 0.006084831116100153\n",
      "Average test loss: 0.006619396254006359\n",
      "Epoch 65/300\n",
      "Average training loss: 0.006009794426461061\n",
      "Average test loss: 0.006697802352408568\n",
      "Epoch 66/300\n",
      "Average training loss: 0.006086100368864007\n",
      "Average test loss: 0.006877821369303597\n",
      "Epoch 67/300\n",
      "Average training loss: 0.006018098978118764\n",
      "Average test loss: 0.007664248327828116\n",
      "Epoch 68/300\n",
      "Average training loss: 0.006105650816940599\n",
      "Average test loss: 0.006653807080454296\n",
      "Epoch 69/300\n",
      "Average training loss: 0.006000574818501869\n",
      "Average test loss: 0.0068085127075513206\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005987997202409639\n",
      "Average test loss: 0.0065490050868441665\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005960049627969663\n",
      "Average test loss: 0.012673700233300528\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005969261282847987\n",
      "Average test loss: 0.0069273221207161745\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005940585070807073\n",
      "Average test loss: 0.006975964590907097\n",
      "Epoch 74/300\n",
      "Average training loss: 0.005867044840008021\n",
      "Average test loss: 0.028839448266559178\n",
      "Epoch 75/300\n",
      "Average training loss: 0.005947467086629735\n",
      "Average test loss: 0.0065314012674821745\n",
      "Epoch 76/300\n",
      "Average training loss: 0.005858968594008022\n",
      "Average test loss: 0.010787985038426188\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005836790654187401\n",
      "Average test loss: 0.006579475882566637\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005867584205336041\n",
      "Average test loss: 0.006769307118323114\n",
      "Epoch 79/300\n",
      "Average training loss: 0.005856919678135051\n",
      "Average test loss: 0.006837642867945962\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005796798363741901\n",
      "Average test loss: 0.006872685305360291\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00580050797180997\n",
      "Average test loss: 0.006862508566843139\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005763136104163197\n",
      "Average test loss: 0.006746754840016365\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0057733001547555125\n",
      "Average test loss: 0.006804976514644093\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005744601178086466\n",
      "Average test loss: 0.006580255049798224\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005748892600751585\n",
      "Average test loss: 0.006979167087210549\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005683290796147452\n",
      "Average test loss: 0.0067451916577087505\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0057168368167347374\n",
      "Average test loss: 0.007395803060382605\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0056850379614366425\n",
      "Average test loss: 0.0070524305920634\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0056708784277240435\n",
      "Average test loss: 0.006483150966051552\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005624611640555991\n",
      "Average test loss: 0.006590985058082474\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0057073989825116266\n",
      "Average test loss: 0.006762778859999445\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005627603082193269\n",
      "Average test loss: 0.006607071768906381\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005668238538834784\n",
      "Average test loss: 0.0064400944809118905\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005597493696957827\n",
      "Average test loss: 0.006564939786990484\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005624724860613545\n",
      "Average test loss: 0.006576179952257209\n",
      "Epoch 96/300\n",
      "Average training loss: 0.005564038349522485\n",
      "Average test loss: 0.006509077743523651\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005598071690648794\n",
      "Average test loss: 0.006755124450557762\n",
      "Epoch 98/300\n",
      "Average training loss: 0.005590951228721274\n",
      "Average test loss: 0.007164827744166056\n",
      "Epoch 99/300\n",
      "Average training loss: 0.005555223104440504\n",
      "Average test loss: 0.007045564700331953\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005541850671999984\n",
      "Average test loss: 0.00706710754831632\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005484709703259998\n",
      "Average test loss: 0.006626285211907492\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005647957478132513\n",
      "Average test loss: 0.006925461119247808\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005481991569201152\n",
      "Average test loss: 0.007021802133984036\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005436857464826769\n",
      "Average test loss: 0.0065394926774832935\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005491416439414024\n",
      "Average test loss: 0.008508941646251414\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005466752428561449\n",
      "Average test loss: 0.006631111831300788\n",
      "Epoch 107/300\n",
      "Average training loss: 0.005417256678971979\n",
      "Average test loss: 0.0067871855385601525\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005438586300445927\n",
      "Average test loss: 0.006905021489494376\n",
      "Epoch 109/300\n",
      "Average training loss: 0.005435760263767507\n",
      "Average test loss: 0.007584639057930973\n",
      "Epoch 110/300\n",
      "Average training loss: 0.005392105124476883\n",
      "Average test loss: 0.006673234697017405\n",
      "Epoch 111/300\n",
      "Average training loss: 0.005369631422062715\n",
      "Average test loss: 0.007204119722048441\n",
      "Epoch 112/300\n",
      "Average training loss: 0.005388187667561902\n",
      "Average test loss: 0.006699882835563686\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0053954023665024175\n",
      "Average test loss: 0.04282721189988984\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005359872268926766\n",
      "Average test loss: 0.006654556704478131\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005326144840982225\n",
      "Average test loss: 0.006979799334373739\n",
      "Epoch 116/300\n",
      "Average training loss: 0.005359821638299359\n",
      "Average test loss: 0.011650642308096091\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005329803093440003\n",
      "Average test loss: 0.008434664574348265\n",
      "Epoch 118/300\n",
      "Average training loss: 0.005419119702031215\n",
      "Average test loss: 0.006718708564009931\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005293385855853558\n",
      "Average test loss: 0.006692420151498583\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005271665128568808\n",
      "Average test loss: 0.006718782403816779\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005289091582099597\n",
      "Average test loss: 0.006676947602381309\n",
      "Epoch 122/300\n",
      "Average training loss: 0.005254107681827412\n",
      "Average test loss: 0.00712355366597573\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005327543843123648\n",
      "Average test loss: 0.006552156464093261\n",
      "Epoch 124/300\n",
      "Average training loss: 0.005258598719620042\n",
      "Average test loss: 0.008034218684666687\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0053279220627413856\n",
      "Average test loss: 0.007247271082467503\n",
      "Epoch 126/300\n",
      "Average training loss: 0.005199254692014721\n",
      "Average test loss: 0.006626867051339812\n",
      "Epoch 127/300\n",
      "Average training loss: 0.005242278779339459\n",
      "Average test loss: 0.006690642629232671\n",
      "Epoch 128/300\n",
      "Average training loss: 0.005250704594784313\n",
      "Average test loss: 0.008658486803372702\n",
      "Epoch 129/300\n",
      "Average training loss: 0.005160956610822016\n",
      "Average test loss: 0.41988820009761385\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00545655788315667\n",
      "Average test loss: 0.007290366609891256\n",
      "Epoch 131/300\n",
      "Average training loss: 0.005283448886540201\n",
      "Average test loss: 0.0074756639574964845\n",
      "Epoch 132/300\n",
      "Average training loss: 0.005163728132843971\n",
      "Average test loss: 0.007623071406450537\n",
      "Epoch 133/300\n",
      "Average training loss: 0.005135236937138769\n",
      "Average test loss: 0.00653086692136195\n",
      "Epoch 134/300\n",
      "Average training loss: 0.005155300909032425\n",
      "Average test loss: 0.006710679060882992\n",
      "Epoch 135/300\n",
      "Average training loss: 0.005094068416290813\n",
      "Average test loss: 0.006751456724272834\n",
      "Epoch 136/300\n",
      "Average training loss: 0.00512578039823307\n",
      "Average test loss: 0.006678789705451992\n",
      "Epoch 137/300\n",
      "Average training loss: 0.00510448830988672\n",
      "Average test loss: 0.006650810213552581\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0051008123680949214\n",
      "Average test loss: 0.0065565070571998755\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0051808486460811565\n",
      "Average test loss: 0.007409381214529276\n",
      "Epoch 140/300\n",
      "Average training loss: 0.005097886385189162\n",
      "Average test loss: 0.006720758575946092\n",
      "Epoch 141/300\n",
      "Average training loss: 0.005086716189152665\n",
      "Average test loss: 0.006751595420969857\n",
      "Epoch 142/300\n",
      "Average training loss: 0.005083784988770882\n",
      "Average test loss: 0.006452284528149499\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0050438754496475065\n",
      "Average test loss: 0.0068735012701816025\n",
      "Epoch 144/300\n",
      "Average training loss: 0.005069993362658553\n",
      "Average test loss: 0.006675905151499642\n",
      "Epoch 145/300\n",
      "Average training loss: 0.005091739406188329\n",
      "Average test loss: 0.006825390873683824\n",
      "Epoch 146/300\n",
      "Average training loss: 0.005046120387812455\n",
      "Average test loss: 0.00659249667574962\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004994623584051927\n",
      "Average test loss: 0.006740895232392682\n",
      "Epoch 148/300\n",
      "Average training loss: 0.005025605443037219\n",
      "Average test loss: 0.006648907424675094\n",
      "Epoch 149/300\n",
      "Average training loss: 0.004999922398477793\n",
      "Average test loss: 0.006735595340530078\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004974077111730973\n",
      "Average test loss: 0.006669901758432389\n",
      "Epoch 151/300\n",
      "Average training loss: 0.005034093046767844\n",
      "Average test loss: 0.008285355977714062\n",
      "Epoch 152/300\n",
      "Average training loss: 0.005222264202932517\n",
      "Average test loss: 0.006481732826679945\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0049276130559543765\n",
      "Average test loss: 0.006890344931019677\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004940076327572266\n",
      "Average test loss: 0.007094576275183095\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004966673062493404\n",
      "Average test loss: 0.006651665567109982\n",
      "Epoch 156/300\n",
      "Average training loss: 0.004964645089581609\n",
      "Average test loss: 0.006605360002981292\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004925524841994047\n",
      "Average test loss: 0.00828467187202639\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004884136559234725\n",
      "Average test loss: 0.006642439633607864\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004878794263220495\n",
      "Average test loss: 0.02383614816599422\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004941910222586658\n",
      "Average test loss: 0.006578895688884788\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004891035299748182\n",
      "Average test loss: 0.007031367902126577\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004887737176898453\n",
      "Average test loss: 0.006646631693674458\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00484220370484723\n",
      "Average test loss: 0.006493472385737631\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004858483600119749\n",
      "Average test loss: 0.01381408746706115\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00488296663802531\n",
      "Average test loss: 0.006578969398306476\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004829773235652182\n",
      "Average test loss: 0.006595800877445274\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00491066821747356\n",
      "Average test loss: 0.006736197616491053\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004864230327722099\n",
      "Average test loss: 0.006599425871339109\n",
      "Epoch 169/300\n",
      "Average training loss: 0.004765580037815703\n",
      "Average test loss: 0.0068743058571385015\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00477994439833694\n",
      "Average test loss: 0.006556912302350004\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004770596807615625\n",
      "Average test loss: 0.006767766950445043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004826103452593088\n",
      "Average test loss: 0.006890946376654837\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004780022562998865\n",
      "Average test loss: 0.006729154299116797\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004766388918790552\n",
      "Average test loss: 0.006644076816737652\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004772404229475392\n",
      "Average test loss: 0.007936016560428673\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00475397576039864\n",
      "Average test loss: 0.006632731668651104\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004749956963376867\n",
      "Average test loss: 0.006730397871798939\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004759960544192129\n",
      "Average test loss: 0.006828707419749763\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004732408765289519\n",
      "Average test loss: 0.009484447432061036\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004756694085482094\n",
      "Average test loss: 0.006603823738793532\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00473795281474789\n",
      "Average test loss: 0.006591502442128128\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004691872898903158\n",
      "Average test loss: 0.006815859918793042\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004719830709613032\n",
      "Average test loss: 0.006723676603701379\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0046980156728790865\n",
      "Average test loss: 0.006885327472454972\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004714920935738418\n",
      "Average test loss: 0.0066994801234039995\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004678887986060646\n",
      "Average test loss: 0.00997834971629911\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004698584837218126\n",
      "Average test loss: 0.02761129769682884\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004710365455183718\n",
      "Average test loss: 0.006732724589606126\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004672861293993062\n",
      "Average test loss: 0.0066402454781863424\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00466958515263266\n",
      "Average test loss: 0.006751208394351933\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004649116698652506\n",
      "Average test loss: 0.006925351704988215\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004642653505007426\n",
      "Average test loss: 0.006798335519101885\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004652555604775747\n",
      "Average test loss: 0.006710584091229571\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0046171457703328795\n",
      "Average test loss: 0.008149238417545954\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004629817483739721\n",
      "Average test loss: 0.0065999567500419085\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004631229439129432\n",
      "Average test loss: 0.006746232249670558\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0045965408641431066\n",
      "Average test loss: 0.006698374418748749\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004641702778637409\n",
      "Average test loss: 0.006983700637188223\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004585929300636053\n",
      "Average test loss: 0.00656572086405423\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004623561400920153\n",
      "Average test loss: 0.006772323642753893\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004581926479521725\n",
      "Average test loss: 0.006839017402794626\n",
      "Epoch 202/300\n",
      "Average training loss: 0.004610913174433841\n",
      "Average test loss: 0.007431648275090588\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004590720986740457\n",
      "Average test loss: 0.007156701090021266\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004598498886658086\n",
      "Average test loss: 0.006736184052295155\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004589299705707365\n",
      "Average test loss: 0.0066689936137861675\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004571597180846665\n",
      "Average test loss: 0.006662345870087544\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00456597792605559\n",
      "Average test loss: 0.006653428491618898\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004560231460465325\n",
      "Average test loss: 0.006757321931835678\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004564342165572776\n",
      "Average test loss: 0.006999880993117889\n",
      "Epoch 210/300\n",
      "Average training loss: 0.004543902886824476\n",
      "Average test loss: 0.006780497481425603\n",
      "Epoch 211/300\n",
      "Average training loss: 0.004539923587607013\n",
      "Average test loss: 0.0068463175019456285\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004517938706609938\n",
      "Average test loss: 0.01132925748328368\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004550148310346736\n",
      "Average test loss: 0.006713365266306533\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004560274532892638\n",
      "Average test loss: 0.007103590950369835\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00450333636543817\n",
      "Average test loss: 0.0066985837138361405\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004509322541662388\n",
      "Average test loss: 0.006649123112774558\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0045174088813364505\n",
      "Average test loss: 0.006678276502009895\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004543608430359098\n",
      "Average test loss: 0.006789861601673894\n",
      "Epoch 219/300\n",
      "Average training loss: 0.004525534611195326\n",
      "Average test loss: 0.0067596398182213305\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004487288008547492\n",
      "Average test loss: 0.007445830166339875\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004506817724555731\n",
      "Average test loss: 0.006714972140474452\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004499059601376454\n",
      "Average test loss: 0.008913089162773556\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004489510647745596\n",
      "Average test loss: 0.006756779832144578\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004503832405640019\n",
      "Average test loss: 0.006714571372916301\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0045072464048862455\n",
      "Average test loss: 0.006934021226233906\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004472730814375811\n",
      "Average test loss: 0.006814122653669781\n",
      "Epoch 227/300\n",
      "Average training loss: 0.004462866294094258\n",
      "Average test loss: 0.006705927812390857\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004478212337940931\n",
      "Average test loss: 0.008710624063180553\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004482450238118569\n",
      "Average test loss: 0.006818036014421118\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00445617572342356\n",
      "Average test loss: 0.00800503475881285\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004461098935041163\n",
      "Average test loss: 0.00671844795677397\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004438680461918314\n",
      "Average test loss: 0.006646010605825318\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004465920667681429\n",
      "Average test loss: 0.007118265149080091\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004443081676546071\n",
      "Average test loss: 0.006955912770496475\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004478286364840137\n",
      "Average test loss: 0.006776175335463551\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004435509371674723\n",
      "Average test loss: 0.006750968798581097\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004429008325354921\n",
      "Average test loss: 0.007029685385525226\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004471254371520546\n",
      "Average test loss: 0.007074805184370941\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004418189868330955\n",
      "Average test loss: 0.0068378658265703255\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004476539424724049\n",
      "Average test loss: 0.0069841863587498665\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00439514762080378\n",
      "Average test loss: 0.006696804439855947\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004538825454811255\n",
      "Average test loss: 0.012767007493310504\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004467920944508579\n",
      "Average test loss: 0.007952330711401171\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0044117615562346245\n",
      "Average test loss: 0.0068985045821302465\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004392838400685125\n",
      "Average test loss: 0.00804780414990253\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00437575882466303\n",
      "Average test loss: 0.0072474670463966\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004399727553129196\n",
      "Average test loss: 0.00706901969263951\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004389408293076687\n",
      "Average test loss: 0.006726969405594799\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004421671267184946\n",
      "Average test loss: 0.00664461622097426\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004371586829216944\n",
      "Average test loss: 0.006769058363305198\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0043809746456229025\n",
      "Average test loss: 0.006821046049810118\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004366515988277065\n",
      "Average test loss: 0.006883655911518468\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004390230993015899\n",
      "Average test loss: 0.0067534079949061075\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0043767216118673485\n",
      "Average test loss: 0.006638971976935863\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004366780646352304\n",
      "Average test loss: 0.006751670916875204\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004379729274453389\n",
      "Average test loss: 0.006778210534817642\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004368177285624875\n",
      "Average test loss: 0.006735616042382187\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0043740304571886855\n",
      "Average test loss: 0.00709083653986454\n",
      "Epoch 259/300\n",
      "Average training loss: 0.00436391813473569\n",
      "Average test loss: 0.007115153338346216\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004335216704756021\n",
      "Average test loss: 0.0067191429576940005\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0043385354922049575\n",
      "Average test loss: 0.006686205367247264\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00435023787120978\n",
      "Average test loss: 0.006734901337573926\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004367721144523886\n",
      "Average test loss: 0.007009176626801491\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004347069255179829\n",
      "Average test loss: 0.006822001129388809\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004353518094453546\n",
      "Average test loss: 0.006899432840446631\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004310057759698894\n",
      "Average test loss: 0.00665910699011551\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004335958821285102\n",
      "Average test loss: 0.006747090278400315\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004342710153096252\n",
      "Average test loss: 0.006752613817652067\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004321030276517073\n",
      "Average test loss: 0.007076907768845558\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004345405676298672\n",
      "Average test loss: 0.006707247767183516\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004321733098891047\n",
      "Average test loss: 0.0068567762300372126\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004334730495181348\n",
      "Average test loss: 0.01766502051552137\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0043899760200745526\n",
      "Average test loss: 0.006831418912857771\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004292932028157843\n",
      "Average test loss: 0.0067267868878940745\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004305725586911042\n",
      "Average test loss: 0.006643609519633982\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0042999249193817375\n",
      "Average test loss: 0.006727230856815974\n",
      "Epoch 277/300\n",
      "Average training loss: 0.004304835279782613\n",
      "Average test loss: 0.007249971280081404\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0043070923110677135\n",
      "Average test loss: 0.006860394097450707\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0043176998918255174\n",
      "Average test loss: 0.0066945321808258695\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004296697815673219\n",
      "Average test loss: 0.006687012778802051\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004293843756947253\n",
      "Average test loss: 0.007110517917407883\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004287959240790871\n",
      "Average test loss: 0.006818561919033527\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004291117566534214\n",
      "Average test loss: 0.006854505574537648\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004291955770717727\n",
      "Average test loss: 0.006687562253740099\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004308354741583268\n",
      "Average test loss: 0.006923018213775423\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004278133489605453\n",
      "Average test loss: 0.010432357830305895\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0042872346908681925\n",
      "Average test loss: 0.007445388922674788\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004270933667818705\n",
      "Average test loss: 0.006798106760200527\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0042768082324829365\n",
      "Average test loss: 0.0069542876482009885\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004273553139219681\n",
      "Average test loss: 0.00705680348061853\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004253504175692797\n",
      "Average test loss: 0.0067872088592913415\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004259664877421326\n",
      "Average test loss: 0.007125130634340975\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004254442156189018\n",
      "Average test loss: 0.006868646010342571\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004256635896033711\n",
      "Average test loss: 0.006835625172075298\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004255926569302877\n",
      "Average test loss: 0.01098514232950078\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004270310031871001\n",
      "Average test loss: 0.006779931696752707\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004331631806989511\n",
      "Average test loss: 0.0068220168517695535\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004315812620644768\n",
      "Average test loss: 0.006925022894309627\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0042327227406203745\n",
      "Average test loss: 0.007189629636704922\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004223494337250789\n",
      "Average test loss: 0.0068632926466978255\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.025712626829743384\n",
      "Average test loss: 0.0258960527115398\n",
      "Epoch 2/300\n",
      "Average training loss: 0.010862986670600044\n",
      "Average test loss: 0.011288297711975044\n",
      "Epoch 3/300\n",
      "Average training loss: 0.009651721512277922\n",
      "Average test loss: 0.008564601799680127\n",
      "Epoch 4/300\n",
      "Average training loss: 0.008861524730920792\n",
      "Average test loss: 0.011634769394993782\n",
      "Epoch 5/300\n",
      "Average training loss: 0.008616538364026281\n",
      "Average test loss: 0.011645090435114172\n",
      "Epoch 6/300\n",
      "Average training loss: 0.009006356291472912\n",
      "Average test loss: 0.01252638978097174\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008021945951713455\n",
      "Average test loss: 0.00864232358833154\n",
      "Epoch 8/300\n",
      "Average training loss: 0.007658606155465046\n",
      "Average test loss: 0.008213901358346144\n",
      "Epoch 9/300\n",
      "Average training loss: 0.007056703851868709\n",
      "Average test loss: 0.011215295688973533\n",
      "Epoch 10/300\n",
      "Average training loss: 0.006617715923322572\n",
      "Average test loss: 0.0060787651042143506\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0062379209568931\n",
      "Average test loss: 0.0060093971751630304\n",
      "Epoch 12/300\n",
      "Average training loss: 0.00608610852724976\n",
      "Average test loss: 0.005761537618521187\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0059298559481071106\n",
      "Average test loss: 0.008686749961641099\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005721468300455146\n",
      "Average test loss: 0.007882792673177189\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005658996595276727\n",
      "Average test loss: 0.03736957731511858\n",
      "Epoch 16/300\n",
      "Average training loss: 0.005450608787851201\n",
      "Average test loss: 0.0058589489170246655\n",
      "Epoch 17/300\n",
      "Average training loss: 0.005375200509197182\n",
      "Average test loss: 0.005532012866603004\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0054080590825113985\n",
      "Average test loss: 0.006724858040610949\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005222277533262968\n",
      "Average test loss: 0.005209033285992013\n",
      "Epoch 20/300\n",
      "Average training loss: 0.005097929511840145\n",
      "Average test loss: 0.006399157820476426\n",
      "Epoch 21/300\n",
      "Average training loss: 0.005053350020200014\n",
      "Average test loss: 0.0048983379180232685\n",
      "Epoch 22/300\n",
      "Average training loss: 0.005014375416768922\n",
      "Average test loss: 0.00810264934764968\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004928855656749672\n",
      "Average test loss: 0.0053008287342058286\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004892295882105827\n",
      "Average test loss: 0.005262028698292043\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004903896717147695\n",
      "Average test loss: 0.011526011333697373\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004865070578952631\n",
      "Average test loss: 0.0077719295769929884\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004697843737900257\n",
      "Average test loss: 0.004919376389847861\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004678237529678477\n",
      "Average test loss: 0.0054425954479310245\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004685138343729907\n",
      "Average test loss: 0.00467790365508861\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00456378393392596\n",
      "Average test loss: 0.004694588293631872\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004822669651773241\n",
      "Average test loss: 0.006330304058475627\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0045326361676885025\n",
      "Average test loss: 0.00476871710187859\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004443571814646324\n",
      "Average test loss: 0.005237488779342837\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004418419189337227\n",
      "Average test loss: 0.004489638481289148\n",
      "Epoch 35/300\n",
      "Average training loss: 0.00438361392584112\n",
      "Average test loss: 0.0048833130246235265\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004382568544397752\n",
      "Average test loss: 0.008723955959909492\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004323047543979354\n",
      "Average test loss: 0.004802218302256531\n",
      "Epoch 38/300\n",
      "Average training loss: 0.004273196195976602\n",
      "Average test loss: 0.004547194425430562\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004330056266652213\n",
      "Average test loss: 0.004720897394749853\n",
      "Epoch 40/300\n",
      "Average training loss: 0.004261899221895469\n",
      "Average test loss: 0.004815604679700401\n",
      "Epoch 41/300\n",
      "Average training loss: 0.004194124992522928\n",
      "Average test loss: 0.00574107626784179\n",
      "Epoch 42/300\n",
      "Average training loss: 0.004222569564978282\n",
      "Average test loss: 0.004426055191498664\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004152899447000689\n",
      "Average test loss: 0.00439380113987459\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004125419211263458\n",
      "Average test loss: 0.004749730915659004\n",
      "Epoch 45/300\n",
      "Average training loss: 0.004118480009337266\n",
      "Average test loss: 0.00497338358230061\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0041042920644912455\n",
      "Average test loss: 0.004306337919500139\n",
      "Epoch 47/300\n",
      "Average training loss: 0.004053017529969414\n",
      "Average test loss: 0.004332289015046425\n",
      "Epoch 48/300\n",
      "Average training loss: 0.004054128168357743\n",
      "Average test loss: 0.004414225546022256\n",
      "Epoch 49/300\n",
      "Average training loss: 0.004023691272776988\n",
      "Average test loss: 0.004130942905942599\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004015852739413579\n",
      "Average test loss: 0.004459827597770426\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0040149939987394545\n",
      "Average test loss: 0.004226880578117238\n",
      "Epoch 52/300\n",
      "Average training loss: 0.004025614984126555\n",
      "Average test loss: 0.004410974289808008\n",
      "Epoch 53/300\n",
      "Average training loss: 0.004003745925923188\n",
      "Average test loss: 0.005262319174077776\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003906863229142295\n",
      "Average test loss: 0.004566567210687531\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003952347030656205\n",
      "Average test loss: 0.00421409822234677\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0039005102399322722\n",
      "Average test loss: 0.0042935383551650575\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00391457732891043\n",
      "Average test loss: 0.006095082679556476\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003912074954973327\n",
      "Average test loss: 0.004465589096148808\n",
      "Epoch 59/300\n",
      "Average training loss: 0.004273596386735638\n",
      "Average test loss: 0.004411444809701707\n",
      "Epoch 60/300\n",
      "Average training loss: 0.00391137295609547\n",
      "Average test loss: 0.004276684576231572\n",
      "Epoch 61/300\n",
      "Average training loss: 0.003972550431266427\n",
      "Average test loss: 0.0041617663767602705\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003843326325010922\n",
      "Average test loss: 0.004341846724972129\n",
      "Epoch 63/300\n",
      "Average training loss: 0.003839215468822254\n",
      "Average test loss: 0.005316949647333887\n",
      "Epoch 64/300\n",
      "Average training loss: 0.003830148957669735\n",
      "Average test loss: 0.00414041025750339\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0038093014533321064\n",
      "Average test loss: 0.005994702897965908\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0038012290688024626\n",
      "Average test loss: 0.004198676355183124\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0037741685898767577\n",
      "Average test loss: 0.004596864170498319\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0037875378475421004\n",
      "Average test loss: 0.004640747094526887\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0037842022820065418\n",
      "Average test loss: 0.004078462616437011\n",
      "Epoch 70/300\n",
      "Average training loss: 0.003778245141522752\n",
      "Average test loss: 0.015455225875808134\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0037433702473839126\n",
      "Average test loss: 0.004198165818014079\n",
      "Epoch 72/300\n",
      "Average training loss: 0.003754202475771308\n",
      "Average test loss: 0.00503539495749606\n",
      "Epoch 73/300\n",
      "Average training loss: 0.003732725942093465\n",
      "Average test loss: 0.004335445671031873\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0036726398604611556\n",
      "Average test loss: 0.004688780137648185\n",
      "Epoch 75/300\n",
      "Average training loss: 0.003682946819605099\n",
      "Average test loss: 0.00406827097841435\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0036984120706717175\n",
      "Average test loss: 0.00394432884040806\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0037058535238934888\n",
      "Average test loss: 0.004133629651326272\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0036606824344231023\n",
      "Average test loss: 0.004035861721883217\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0036571894528137313\n",
      "Average test loss: 0.004257827795214123\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0036339701337532865\n",
      "Average test loss: 0.004083969133181704\n",
      "Epoch 81/300\n",
      "Average training loss: 0.003754419876676467\n",
      "Average test loss: 0.004036378568659226\n",
      "Epoch 82/300\n",
      "Average training loss: 0.003658869043820434\n",
      "Average test loss: 0.0040018125478592185\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0036144124757912423\n",
      "Average test loss: 0.004342438602613078\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0036152716285238664\n",
      "Average test loss: 0.004059005507164531\n",
      "Epoch 85/300\n",
      "Average training loss: 0.003584282787516713\n",
      "Average test loss: 0.004222285885777739\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0035885770914869178\n",
      "Average test loss: 0.004050611445059379\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0036266454646570814\n",
      "Average test loss: 0.004393966443008847\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0036239317781809302\n",
      "Average test loss: 0.004052675243053172\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0035956523993776906\n",
      "Average test loss: 0.003978194243378109\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003561454871462451\n",
      "Average test loss: 0.008645481229656273\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0035538361732744507\n",
      "Average test loss: 0.004101320899609062\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0035329974469625286\n",
      "Average test loss: 0.004430239332218965\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0036430604172249634\n",
      "Average test loss: 0.0039351751088268225\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0035360467380119694\n",
      "Average test loss: 0.003910350245320135\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0034858235178722276\n",
      "Average test loss: 0.007646899275481701\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0036130543905827732\n",
      "Average test loss: 0.003932977330767446\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0035622434599532024\n",
      "Average test loss: 0.004326882748140229\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0034904000676340527\n",
      "Average test loss: 0.004020440680699216\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0034815961728907295\n",
      "Average test loss: 0.004314453896135092\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0034734712317585946\n",
      "Average test loss: 0.004141629458094637\n",
      "Epoch 101/300\n",
      "Average training loss: 0.003449786537223392\n",
      "Average test loss: 0.004039596876336469\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0034710831112331813\n",
      "Average test loss: 0.004601836948759026\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0034592942365755637\n",
      "Average test loss: 0.07566136310829057\n",
      "Epoch 104/300\n",
      "Average training loss: 0.003515866849778427\n",
      "Average test loss: 0.00400272184320622\n",
      "Epoch 105/300\n",
      "Average training loss: 0.003477409273179041\n",
      "Average test loss: 0.004019739277660847\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0034360823625077803\n",
      "Average test loss: 0.004075909064047867\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003434230420428018\n",
      "Average test loss: 0.003872625414902965\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0034310857901970544\n",
      "Average test loss: 0.004019420401917564\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0034442372644941013\n",
      "Average test loss: 0.004436011598755916\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0034283104559613598\n",
      "Average test loss: 0.003908636214625504\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0034965926841315294\n",
      "Average test loss: 0.004243852181360126\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0034435753774725727\n",
      "Average test loss: 0.00399266597835554\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0034041636068787838\n",
      "Average test loss: 0.004430796292093065\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0034402022142377164\n",
      "Average test loss: 0.004086160403572851\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0033648738873501617\n",
      "Average test loss: 0.003974663501605392\n",
      "Epoch 116/300\n",
      "Average training loss: 0.003397213500820928\n",
      "Average test loss: 0.0039014385189447137\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003360333105342256\n",
      "Average test loss: 0.0039456342711216875\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0033770523671474723\n",
      "Average test loss: 0.003907433591783047\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0034720631951673164\n",
      "Average test loss: 0.003920453049449457\n",
      "Epoch 120/300\n",
      "Average training loss: 0.003366253107993139\n",
      "Average test loss: 0.003872040569368336\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0033919567863146464\n",
      "Average test loss: 0.005012450664821598\n",
      "Epoch 122/300\n",
      "Average training loss: 0.003357232357478804\n",
      "Average test loss: 0.003961143700612916\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0033896256652143266\n",
      "Average test loss: 0.004274470538314846\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0033144355908864076\n",
      "Average test loss: 0.0038824332474420467\n",
      "Epoch 125/300\n",
      "Average training loss: 0.003338261044273774\n",
      "Average test loss: 0.0041542483278446726\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00333638003282249\n",
      "Average test loss: 0.004033841135187281\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0033365995592127243\n",
      "Average test loss: 0.004054258485221201\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003340848303089539\n",
      "Average test loss: 0.004382900255835718\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0033562926405833826\n",
      "Average test loss: 0.003975893942432271\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003315944878384471\n",
      "Average test loss: 0.005011700144244565\n",
      "Epoch 131/300\n",
      "Average training loss: 0.003322321461306678\n",
      "Average test loss: 0.004146332128387359\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003309995578394996\n",
      "Average test loss: 0.0042835847282161315\n",
      "Epoch 133/300\n",
      "Average training loss: 0.003306229436977042\n",
      "Average test loss: 0.004259126080407036\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0032721364365683662\n",
      "Average test loss: 0.004085822472141848\n",
      "Epoch 135/300\n",
      "Average training loss: 0.00329190888421403\n",
      "Average test loss: 0.0043611942699386015\n",
      "Epoch 136/300\n",
      "Average training loss: 0.003261637651051084\n",
      "Average test loss: 0.0039999713773528735\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0032970118928286763\n",
      "Average test loss: 0.00432252099364996\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0032799092773348093\n",
      "Average test loss: 0.003940501132152147\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0032395918803910413\n",
      "Average test loss: 0.003853978729289439\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0032635326977405285\n",
      "Average test loss: 0.0044498956389725204\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0032781659654445116\n",
      "Average test loss: 0.004322090570297506\n",
      "Epoch 142/300\n",
      "Average training loss: 0.003230261676841312\n",
      "Average test loss: 0.003936033205853568\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0032371079563680624\n",
      "Average test loss: 0.004118602223694324\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0032539173424657847\n",
      "Average test loss: 0.004056940546466245\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003239606251940131\n",
      "Average test loss: 0.003948449206435018\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0033292433967192967\n",
      "Average test loss: 0.003955580701430639\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0032070513148274686\n",
      "Average test loss: 0.003986148333383931\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0033122697996182575\n",
      "Average test loss: 0.004130177726762162\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0032492489769226974\n",
      "Average test loss: 0.0040142524581816465\n",
      "Epoch 150/300\n",
      "Average training loss: 0.003199927808302972\n",
      "Average test loss: 0.003994375655634536\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0032400793209671976\n",
      "Average test loss: 0.004233814844654666\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0032158516796512737\n",
      "Average test loss: 0.007884022334383593\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0032141253002401855\n",
      "Average test loss: 0.004499205213040113\n",
      "Epoch 154/300\n",
      "Average training loss: 0.003171417471849256\n",
      "Average test loss: 0.004304415156443914\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0032659094205333126\n",
      "Average test loss: 0.004475942755531934\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0032073222537421517\n",
      "Average test loss: 0.003932011355335514\n",
      "Epoch 157/300\n",
      "Average training loss: 0.003214634806331661\n",
      "Average test loss: 0.004946513136227925\n",
      "Epoch 158/300\n",
      "Average training loss: 0.003195877486425969\n",
      "Average test loss: 0.003888046695540349\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0031823628099842205\n",
      "Average test loss: 0.003971845907055669\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0031982163656502963\n",
      "Average test loss: 0.004685330378512541\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0031662905524588295\n",
      "Average test loss: 0.004057943684359392\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0031743944697082043\n",
      "Average test loss: 0.003877758488472965\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0031943787154224182\n",
      "Average test loss: 0.003931357896162404\n",
      "Epoch 164/300\n",
      "Average training loss: 0.003141800928860903\n",
      "Average test loss: 0.004019342344461216\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0031782441611091297\n",
      "Average test loss: 0.0042171162931869425\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0031352241415944365\n",
      "Average test loss: 0.0046615678417599865\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0032438044351422125\n",
      "Average test loss: 0.004005394648760557\n",
      "Epoch 168/300\n",
      "Average training loss: 0.003139868742475907\n",
      "Average test loss: 0.0038985017008251613\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0031176095352404647\n",
      "Average test loss: 0.00445179699609677\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0031443284325715568\n",
      "Average test loss: 0.004037726276036766\n",
      "Epoch 171/300\n",
      "Average training loss: 0.003120301393378112\n",
      "Average test loss: 0.020161275905039574\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0031429938160710863\n",
      "Average test loss: 0.003994451695225305\n",
      "Epoch 173/300\n",
      "Average training loss: 0.003123054043700298\n",
      "Average test loss: 0.00399769924622443\n",
      "Epoch 174/300\n",
      "Average training loss: 0.003149613831192255\n",
      "Average test loss: 0.003972761280834675\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0031298140848262444\n",
      "Average test loss: 0.0039987612329423425\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0031104512868656052\n",
      "Average test loss: 0.00409156180297335\n",
      "Epoch 177/300\n",
      "Average training loss: 0.003085799268550343\n",
      "Average test loss: 0.02203505661421352\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0032266890017522707\n",
      "Average test loss: 0.005184393033385277\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0031415064670145512\n",
      "Average test loss: 0.003881842858261532\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0031024014616592064\n",
      "Average test loss: 0.004827197064128187\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0031162577085196972\n",
      "Average test loss: 0.004138487709893121\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0031426516609887284\n",
      "Average test loss: 0.003984113418807586\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0030700507344057163\n",
      "Average test loss: 0.003942644935515192\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0030890614330354666\n",
      "Average test loss: 0.004015974996404516\n",
      "Epoch 185/300\n",
      "Average training loss: 0.003076119791302416\n",
      "Average test loss: 0.004343836670534478\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0030621168116728463\n",
      "Average test loss: 0.0040486409279207386\n",
      "Epoch 187/300\n",
      "Average training loss: 0.003119761521824532\n",
      "Average test loss: 0.0038937677649988067\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003057290747968687\n",
      "Average test loss: 0.04624543656905492\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0030982848970840375\n",
      "Average test loss: 0.004007646906707022\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0030910374594645368\n",
      "Average test loss: 0.0039177184963805805\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0032700254263149367\n",
      "Average test loss: 0.003930995997662346\n",
      "Epoch 192/300\n",
      "Average training loss: 0.003048709115220441\n",
      "Average test loss: 0.00412634085263643\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0030281981047656802\n",
      "Average test loss: 0.003935872250960933\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0033122998538116614\n",
      "Average test loss: 0.004181705105842815\n",
      "Epoch 195/300\n",
      "Average training loss: 0.00307637505295376\n",
      "Average test loss: 0.004007495166940821\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0030430828225281505\n",
      "Average test loss: 0.004250163279473781\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0030996247621046173\n",
      "Average test loss: 0.004348323904805713\n",
      "Epoch 198/300\n",
      "Average training loss: 0.003059252610637082\n",
      "Average test loss: 0.004769587999623683\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0030308933789945313\n",
      "Average test loss: 0.0050067714990841014\n",
      "Epoch 200/300\n",
      "Average training loss: 0.003074964448809624\n",
      "Average test loss: 0.003990004809366332\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0030546339160452286\n",
      "Average test loss: 0.006156213321205643\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0030727312237852147\n",
      "Average test loss: 0.003912777701185809\n",
      "Epoch 203/300\n",
      "Average training loss: 0.003080772581199805\n",
      "Average test loss: 0.004125243028418885\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0030463745343602367\n",
      "Average test loss: 0.004038428088857068\n",
      "Epoch 205/300\n",
      "Average training loss: 0.003184072497404284\n",
      "Average test loss: 0.003965478832729989\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00301408292994731\n",
      "Average test loss: 0.004258433346533113\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00300399134390884\n",
      "Average test loss: 0.004233117268938157\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003072765838354826\n",
      "Average test loss: 0.003931860940323936\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0030230436283681127\n",
      "Average test loss: 0.003983018685753147\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00298213120818966\n",
      "Average test loss: 0.004060193199871315\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00305067748338398\n",
      "Average test loss: 0.004123158868816164\n",
      "Epoch 212/300\n",
      "Average training loss: 0.003031036336181892\n",
      "Average test loss: 0.004082051563180155\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0030246951195100945\n",
      "Average test loss: 0.003969806008868747\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0030212824737860097\n",
      "Average test loss: 0.00413143773128589\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0029862235765904187\n",
      "Average test loss: 0.004172553480706281\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0031491200191279254\n",
      "Average test loss: 0.004005414124785198\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0030257405814611248\n",
      "Average test loss: 0.004432560228639179\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0030610193655722672\n",
      "Average test loss: 0.003902593498635623\n",
      "Epoch 219/300\n",
      "Average training loss: 0.003017100925453835\n",
      "Average test loss: 0.004025477972295549\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0030366454964710606\n",
      "Average test loss: 0.003920259715782272\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0029800870114316544\n",
      "Average test loss: 0.003851326509896252\n",
      "Epoch 222/300\n",
      "Average training loss: 0.002974573325158821\n",
      "Average test loss: 0.003934178991450204\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0029821962318900557\n",
      "Average test loss: 0.010478201806545257\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00299156169883079\n",
      "Average test loss: 0.004318733538811405\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0030377527069714336\n",
      "Average test loss: 0.004130958932555384\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002978825099559294\n",
      "Average test loss: 0.004026032784332832\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0029855830489347378\n",
      "Average test loss: 0.0038888090786834556\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0032800408653501007\n",
      "Average test loss: 0.00432447244413197\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0031507134495509996\n",
      "Average test loss: 0.004287151793017983\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003109573614059223\n",
      "Average test loss: 0.003973056673175759\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0029759267756922377\n",
      "Average test loss: 0.004119817496587833\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002944240379354192\n",
      "Average test loss: 0.004086115016291539\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0029598364236040247\n",
      "Average test loss: 0.007284377929237154\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0029996020005395017\n",
      "Average test loss: 0.0038858843544084163\n",
      "Epoch 235/300\n",
      "Average training loss: 0.002966285465285182\n",
      "Average test loss: 0.0038873670275012652\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0029346380862924786\n",
      "Average test loss: 0.006008352001094156\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0029694639357427757\n",
      "Average test loss: 0.003937299315093292\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002949949680103196\n",
      "Average test loss: 0.004063747964385483\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0029603588773558535\n",
      "Average test loss: 0.005521188822885354\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0029505347601241536\n",
      "Average test loss: 0.0045997239543745915\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0030310179518742695\n",
      "Average test loss: 0.004036468031091822\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002911910560189022\n",
      "Average test loss: 0.004064274352043867\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0029595003295689822\n",
      "Average test loss: 0.0039286043608768115\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0029183870177302094\n",
      "Average test loss: 0.005152393862191173\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0028924571294337512\n",
      "Average test loss: 0.003920805173201694\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0028885956648737193\n",
      "Average test loss: 0.003995516855683592\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0028613465070310568\n",
      "Average test loss: 0.003931445480013887\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002921912836117877\n",
      "Average test loss: 0.004130521134369903\n",
      "Epoch 249/300\n",
      "Average training loss: 0.002860849003824923\n",
      "Average test loss: 0.003982736107375887\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0028984064089341297\n",
      "Average test loss: 0.003939369398479661\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0028942428359554876\n",
      "Average test loss: 0.004268502388563421\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0028570457380264996\n",
      "Average test loss: 0.004875528932445579\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0028985482236991325\n",
      "Average test loss: 0.004534911641023225\n",
      "Epoch 254/300\n",
      "Average training loss: 0.002837132241990831\n",
      "Average test loss: 0.003950109601020813\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0028364218067791725\n",
      "Average test loss: 0.0038457520455121995\n",
      "Epoch 256/300\n",
      "Average training loss: 0.002844998984080222\n",
      "Average test loss: 0.0038100431826379563\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0029002643850528534\n",
      "Average test loss: 0.003963677408380641\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0028287579800105756\n",
      "Average test loss: 0.003939415813113252\n",
      "Epoch 259/300\n",
      "Average training loss: 0.002825509544255005\n",
      "Average test loss: 0.004068871209190952\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0028140571748630867\n",
      "Average test loss: 0.004215753936726186\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0028278301681081454\n",
      "Average test loss: 0.00386069093644619\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002821010527304477\n",
      "Average test loss: 0.003915814282165633\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0028118276567094857\n",
      "Average test loss: 0.003909651578507489\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0028069881649894846\n",
      "Average test loss: 0.0038892122445007165\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00282496314889027\n",
      "Average test loss: 0.00390250087570813\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0027912277663126586\n",
      "Average test loss: 0.003963509354740382\n",
      "Epoch 267/300\n",
      "Average training loss: 0.003399748022357623\n",
      "Average test loss: 0.004142863128127323\n",
      "Epoch 268/300\n",
      "Average training loss: 0.003326030243602064\n",
      "Average test loss: 0.0039263376372141974\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002972901934964789\n",
      "Average test loss: 0.0039279472385015755\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0028494338385967747\n",
      "Average test loss: 0.0041696749822133115\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0028075606444229683\n",
      "Average test loss: 0.004016514412644836\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0027865175660699605\n",
      "Average test loss: 0.004071970316684909\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00279163003526628\n",
      "Average test loss: 0.003903270739234156\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002769916187350949\n",
      "Average test loss: 0.00396557825907237\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0028136820188826984\n",
      "Average test loss: 0.0039093796478377445\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0027817796510126854\n",
      "Average test loss: 0.0038991207227938706\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0027762815172059667\n",
      "Average test loss: 0.003895368421243297\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0027631481072554987\n",
      "Average test loss: 0.003942310332838032\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0027747759121573633\n",
      "Average test loss: 0.0039548633036514125\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0027829685469882358\n",
      "Average test loss: 0.0038758998119996653\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0027869629404611057\n",
      "Average test loss: 0.003856915763268868\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0027699223048985003\n",
      "Average test loss: 0.004046934992902809\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002764252174645662\n",
      "Average test loss: 0.004313340297382739\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0027773491353210475\n",
      "Average test loss: 0.004039291354103221\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0027645639963448047\n",
      "Average test loss: 0.0042325773023896745\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002764969205069873\n",
      "Average test loss: 0.003996815777901146\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0033565370864752264\n",
      "Average test loss: 0.003857894990593195\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0028687790744006633\n",
      "Average test loss: 0.0039048768621351986\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0027698112301942374\n",
      "Average test loss: 0.0038731530076927608\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002757497871087657\n",
      "Average test loss: 0.0038876961800787184\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0027494667313165133\n",
      "Average test loss: 0.004009365204514729\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0027430314036707083\n",
      "Average test loss: 0.0040745364568299716\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002732995571568608\n",
      "Average test loss: 0.0038984347343858747\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0027478981928692925\n",
      "Average test loss: 0.003918539424737294\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0027734017488029268\n",
      "Average test loss: 0.0038568113520741462\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002747886054838697\n",
      "Average test loss: 0.0038824329264461996\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0027248841628639236\n",
      "Average test loss: 0.003950625967441334\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0027447365816268654\n",
      "Average test loss: 0.003907117693788475\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0027588676681949034\n",
      "Average test loss: 0.0066425582766532895\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0027377547011193304\n",
      "Average test loss: 0.0038769550195170773\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.025810742975109154\n",
      "Average test loss: 0.015154900999532806\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009651058482213154\n",
      "Average test loss: 0.007519782578779591\n",
      "Epoch 3/300\n",
      "Average training loss: 0.008001700864069991\n",
      "Average test loss: 0.010693322805066904\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006998671118170023\n",
      "Average test loss: 0.006898746100978719\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006270975276413891\n",
      "Average test loss: 0.005962625104520056\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005836818587448862\n",
      "Average test loss: 0.005423106991582447\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005299853556685978\n",
      "Average test loss: 0.005144155132480794\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005108154178907474\n",
      "Average test loss: 0.005596900626189179\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004768910213684042\n",
      "Average test loss: 0.004585764555881421\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004500507801771164\n",
      "Average test loss: 0.0041959611806604595\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004268016814357705\n",
      "Average test loss: 0.0043763464832057555\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004640452458212773\n",
      "Average test loss: 0.004208324239899715\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004082582918513152\n",
      "Average test loss: 0.004034431996858782\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004379291202045149\n",
      "Average test loss: 0.010251199377493726\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003998308736003107\n",
      "Average test loss: 0.004249861949433883\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0037975906723489365\n",
      "Average test loss: 0.0039862970078570975\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0036817713021818133\n",
      "Average test loss: 0.003529337607945005\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003595547438495689\n",
      "Average test loss: 0.004677334212180641\n",
      "Epoch 19/300\n",
      "Average training loss: 0.003684144429862499\n",
      "Average test loss: 0.003458449358327521\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0034414242185238334\n",
      "Average test loss: 0.003444288361610638\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0033859784120900765\n",
      "Average test loss: 0.003517886119377282\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003377662847025527\n",
      "Average test loss: 0.0035030471295532254\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003353301466960046\n",
      "Average test loss: 0.004030408188286755\n",
      "Epoch 24/300\n",
      "Average training loss: 0.003236125031279193\n",
      "Average test loss: 0.0034243467785418035\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0033730996373213\n",
      "Average test loss: 0.003252489464978377\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0031679365038871764\n",
      "Average test loss: 0.0036259384254614512\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0031673251742290126\n",
      "Average test loss: 0.0031864001107298665\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0031120773926377296\n",
      "Average test loss: 0.0032701294099291164\n",
      "Epoch 29/300\n",
      "Average training loss: 0.003187575292463104\n",
      "Average test loss: 0.003100584659104546\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0030042424810429414\n",
      "Average test loss: 0.0031371975675639177\n",
      "Epoch 31/300\n",
      "Average training loss: 0.00304262275269462\n",
      "Average test loss: 0.00387370482335488\n",
      "Epoch 32/300\n",
      "Average training loss: 0.00296883656622635\n",
      "Average test loss: 0.0030996906920853586\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0029780921927756734\n",
      "Average test loss: 0.0031973089205308094\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0029243756027685273\n",
      "Average test loss: 0.0036767902359780336\n",
      "Epoch 35/300\n",
      "Average training loss: 0.002907286942419079\n",
      "Average test loss: 0.0031088371583157114\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002859048051138719\n",
      "Average test loss: 0.002988086638144321\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00282223599548969\n",
      "Average test loss: 0.0029655945737742714\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0028971549905836584\n",
      "Average test loss: 0.002988056059823268\n",
      "Epoch 39/300\n",
      "Average training loss: 0.002807116920956307\n",
      "Average test loss: 0.003331550570204854\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0028569977256572908\n",
      "Average test loss: 0.002902861350629893\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0027634054478257895\n",
      "Average test loss: 0.0028566195116274886\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0027820724411350157\n",
      "Average test loss: 0.002862041716774305\n",
      "Epoch 43/300\n",
      "Average training loss: 0.002718810305930674\n",
      "Average test loss: 0.0029369510408076976\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0027210470044778455\n",
      "Average test loss: 0.002874329382967618\n",
      "Epoch 45/300\n",
      "Average training loss: 0.002706551087813245\n",
      "Average test loss: 0.011664050611356894\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0044527109567489885\n",
      "Average test loss: 0.00358030830282304\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0031481946079681316\n",
      "Average test loss: 0.0031519361399114133\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0028561376585728594\n",
      "Average test loss: 0.003149656310574048\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0027928459398034546\n",
      "Average test loss: 0.0047213452416989534\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002748767140114473\n",
      "Average test loss: 0.0030428916027562484\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002706489545603593\n",
      "Average test loss: 0.003039735784754157\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0027348384890291426\n",
      "Average test loss: 0.002884171422570944\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00265455566946831\n",
      "Average test loss: 0.0033797902398639253\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0026400939540730584\n",
      "Average test loss: 0.002842108881721894\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0026481080781668425\n",
      "Average test loss: 0.0028200923378268877\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0026107617538008426\n",
      "Average test loss: 0.0029193555855502686\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0026166935863180295\n",
      "Average test loss: 0.0029428142985949914\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0025927883353498247\n",
      "Average test loss: 0.0028422753378334973\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0026264132422705493\n",
      "Average test loss: 0.0029680650225943988\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0025586907021287416\n",
      "Average test loss: 0.002941625424557262\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002629675873865684\n",
      "Average test loss: 0.003097280876711011\n",
      "Epoch 62/300\n",
      "Average training loss: 0.002537686520566543\n",
      "Average test loss: 0.002811167254216141\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0025411361830515993\n",
      "Average test loss: 0.004991783352361785\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002556729700209366\n",
      "Average test loss: 0.00320959566347301\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0025665368626101148\n",
      "Average test loss: 0.0027486498260663617\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002496544354284803\n",
      "Average test loss: 0.0028981465813186433\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002515443125532733\n",
      "Average test loss: 0.002847648406608237\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0024925229928145806\n",
      "Average test loss: 0.002736525863202082\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002475008562828104\n",
      "Average test loss: 0.012368347993327512\n",
      "Epoch 70/300\n",
      "Average training loss: 0.002472430702091919\n",
      "Average test loss: 0.0028544554370972847\n",
      "Epoch 71/300\n",
      "Average training loss: 0.002502146543107099\n",
      "Average test loss: 0.004589354785780112\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0024771566713849705\n",
      "Average test loss: 0.031331241158975495\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00248739269355105\n",
      "Average test loss: 0.0028796414979216124\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0025303057961993747\n",
      "Average test loss: 0.00273718788040181\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0024187361143736376\n",
      "Average test loss: 0.0027777774414668483\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00243772827150921\n",
      "Average test loss: 0.002717285786444942\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0024095655375470718\n",
      "Average test loss: 0.0027936695333984165\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0024014769877410596\n",
      "Average test loss: 0.0033216963975379863\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002511785112735298\n",
      "Average test loss: 0.002770645057161649\n",
      "Epoch 80/300\n",
      "Average training loss: 0.002409991328708\n",
      "Average test loss: 0.0027596884080105357\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0024137085808648004\n",
      "Average test loss: 0.003039222861536675\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0023891165014356375\n",
      "Average test loss: 0.009292303832868736\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0023937280900362466\n",
      "Average test loss: 0.002736282335387336\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0023763688151828117\n",
      "Average test loss: 0.002814328136129512\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0023853567558237247\n",
      "Average test loss: 0.0028665424682613877\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002411680022254586\n",
      "Average test loss: 0.0030292128997130526\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0023409217887868485\n",
      "Average test loss: 0.0030592695567756892\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002365419413480494\n",
      "Average test loss: 0.0033554563168436287\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0023472215640876026\n",
      "Average test loss: 0.002724632397707966\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0023381337713864114\n",
      "Average test loss: 0.004157805761943261\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00232376887752778\n",
      "Average test loss: 0.0068154777367081905\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002367631543634666\n",
      "Average test loss: 0.004213146299123764\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0023545347679820327\n",
      "Average test loss: 0.002738840512000024\n",
      "Epoch 94/300\n",
      "Average training loss: 0.00231691693338669\n",
      "Average test loss: 0.002687824169587758\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00233719483224882\n",
      "Average test loss: 0.0028193915693296326\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0023355816333658166\n",
      "Average test loss: 0.0027451342874103123\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0023103370749288136\n",
      "Average test loss: 0.00729345615228845\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0025052772930098904\n",
      "Average test loss: 0.0027457158143321674\n",
      "Epoch 99/300\n",
      "Average training loss: 0.002342065030708909\n",
      "Average test loss: 0.0027917163417571123\n",
      "Epoch 100/300\n",
      "Average training loss: 0.002299495963897142\n",
      "Average test loss: 0.003443566815720664\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0023124508733550707\n",
      "Average test loss: 0.002755595496131314\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002284814016479585\n",
      "Average test loss: 0.009708750622036556\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0022859596189939314\n",
      "Average test loss: 0.0028622454651114015\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002289224421812428\n",
      "Average test loss: 0.002895950975517432\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0024104615485088694\n",
      "Average test loss: 0.002679413231089711\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002261768073257473\n",
      "Average test loss: 0.0032009809511817163\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0022523353974231415\n",
      "Average test loss: 0.0026908637999246516\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0022602183719476064\n",
      "Average test loss: 0.00278069025485052\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0022728681767152413\n",
      "Average test loss: 0.002652004338386986\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0023069633152335883\n",
      "Average test loss: 0.0037106205303635864\n",
      "Epoch 111/300\n",
      "Average training loss: 0.002236570032623907\n",
      "Average test loss: 0.002782827323509587\n",
      "Epoch 112/300\n",
      "Average training loss: 0.00230171237885952\n",
      "Average test loss: 0.0037066230194436178\n",
      "Epoch 113/300\n",
      "Average training loss: 0.002242616848502722\n",
      "Average test loss: 0.0027720462294916312\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0022258966099470852\n",
      "Average test loss: 0.0026999527379456493\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0022421702172400223\n",
      "Average test loss: 0.0028955376520752906\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0022294802077942426\n",
      "Average test loss: 0.002649761443750726\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0022511639679885574\n",
      "Average test loss: 0.0026898917100495762\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002222734013365375\n",
      "Average test loss: 0.0027008970501936143\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002235838415308131\n",
      "Average test loss: 0.0027504824919419154\n",
      "Epoch 120/300\n",
      "Average training loss: 0.002196209687118729\n",
      "Average test loss: 0.002784743803035882\n",
      "Epoch 121/300\n",
      "Average training loss: 0.002271082313731313\n",
      "Average test loss: 0.006706058825055758\n",
      "Epoch 122/300\n",
      "Average training loss: 0.002436893636671205\n",
      "Average test loss: 0.0027913087440861596\n",
      "Epoch 123/300\n",
      "Average training loss: 0.002190944583258695\n",
      "Average test loss: 0.005864464310722219\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0021858598506078126\n",
      "Average test loss: 0.003108828824841314\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0021964687017930877\n",
      "Average test loss: 0.002855419280110962\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0022292648748391203\n",
      "Average test loss: 0.002726329426177674\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0021951876424459948\n",
      "Average test loss: 0.0029770420257829956\n",
      "Epoch 128/300\n",
      "Average training loss: 0.002184828817844391\n",
      "Average test loss: 0.0028671611477103496\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0022085870233260925\n",
      "Average test loss: 0.002753973301500082\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002196261160903507\n",
      "Average test loss: 0.0027735546715557573\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0022020309602634775\n",
      "Average test loss: 0.003131730396093594\n",
      "Epoch 132/300\n",
      "Average training loss: 0.002201783998248478\n",
      "Average test loss: 0.0026786083098914886\n",
      "Epoch 133/300\n",
      "Average training loss: 0.002162300540962153\n",
      "Average test loss: 0.0076519672870635986\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0022641219848559963\n",
      "Average test loss: 0.002721876598066754\n",
      "Epoch 135/300\n",
      "Average training loss: 0.002170548556993405\n",
      "Average test loss: 0.0047079589859479005\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0021668999512783355\n",
      "Average test loss: 0.0028058884596038197\n",
      "Epoch 137/300\n",
      "Average training loss: 0.002150732413555185\n",
      "Average test loss: 0.005298479221761227\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002178465501508779\n",
      "Average test loss: 0.0030549066194022695\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0021864487681951786\n",
      "Average test loss: 0.0027336963206115697\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0021467775396174856\n",
      "Average test loss: 0.003011296125335826\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0022496100024630627\n",
      "Average test loss: 0.0027226059196723833\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0021291280357788006\n",
      "Average test loss: 0.0027437150176200602\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0021597056318488385\n",
      "Average test loss: 0.003097771201075779\n",
      "Epoch 144/300\n",
      "Average training loss: 0.002151675646710727\n",
      "Average test loss: 0.0030631598002380796\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002148547945026722\n",
      "Average test loss: 0.002753734971086184\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0021621333273748556\n",
      "Average test loss: 0.0027882480958683624\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0021410413100901578\n",
      "Average test loss: 1.0634211401542029\n",
      "Epoch 148/300\n",
      "Average training loss: 0.002183493487433427\n",
      "Average test loss: 0.008879706162131495\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002126688004574842\n",
      "Average test loss: 0.0032147998420728576\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0021221513328038982\n",
      "Average test loss: 0.005312056615534756\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0021540271993726492\n",
      "Average test loss: 0.0029534415192902087\n",
      "Epoch 152/300\n",
      "Average training loss: 0.002120793443897532\n",
      "Average test loss: 0.06723236880699794\n",
      "Epoch 153/300\n",
      "Average training loss: 0.002224113706085417\n",
      "Average test loss: 0.002948247462304102\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0021559905838221313\n",
      "Average test loss: 0.003938355277602871\n",
      "Epoch 155/300\n",
      "Average training loss: 0.002104180866231521\n",
      "Average test loss: 0.002994346050545573\n",
      "Epoch 156/300\n",
      "Average training loss: 0.002103919407973687\n",
      "Average test loss: 0.0027371113058179616\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0021201134071581894\n",
      "Average test loss: 0.002731606295125352\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0021027262129096522\n",
      "Average test loss: 0.002685988842509687\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0021061159748997954\n",
      "Average test loss: 0.00395056935855084\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0021140660788449978\n",
      "Average test loss: 0.0027961256497850023\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0024306836754290594\n",
      "Average test loss: 0.0033603096625043284\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0027152526564896105\n",
      "Average test loss: 0.0030329981876744162\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002236154491909676\n",
      "Average test loss: 0.0030017737255742154\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0021754385826902255\n",
      "Average test loss: 0.002736950165902575\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0021070889448747037\n",
      "Average test loss: 0.0028830290753394366\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002089783218689263\n",
      "Average test loss: 0.0027885249824159677\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0020861291212754115\n",
      "Average test loss: 0.0028568057753145695\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0021073118848726155\n",
      "Average test loss: 0.002680562713286943\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0023238638101352586\n",
      "Average test loss: 0.003040566577058699\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002128136735202538\n",
      "Average test loss: 0.0027902796607878473\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0020868633310827944\n",
      "Average test loss: 0.023646710852781933\n",
      "Epoch 172/300\n",
      "Average training loss: 0.00212977966066036\n",
      "Average test loss: 0.0027276965915742847\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0020738624894163673\n",
      "Average test loss: 0.0029410264080183372\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002083833962575429\n",
      "Average test loss: 0.0027766761620425517\n",
      "Epoch 175/300\n",
      "Average training loss: 0.002075031419802043\n",
      "Average test loss: 0.003078450938479768\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0020714375618845226\n",
      "Average test loss: 0.012861750496344434\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002091618610545993\n",
      "Average test loss: 0.002741005759686232\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0020913465987477038\n",
      "Average test loss: 0.00271251654273106\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0020883536434008016\n",
      "Average test loss: 0.00273677765391767\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002071706141448683\n",
      "Average test loss: 0.04152862311154604\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0020754247129791312\n",
      "Average test loss: 0.0027649132394128376\n",
      "Epoch 182/300\n",
      "Average training loss: 0.002071128437605997\n",
      "Average test loss: 0.07141882266352574\n",
      "Epoch 183/300\n",
      "Average training loss: 0.002066758126951754\n",
      "Average test loss: 0.0032944616687794527\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002076638252474368\n",
      "Average test loss: 0.0027458744450575777\n",
      "Epoch 185/300\n",
      "Average training loss: 0.002087473469372425\n",
      "Average test loss: 0.007043680804471175\n",
      "Epoch 186/300\n",
      "Average training loss: 0.002053308446167244\n",
      "Average test loss: 0.049813206482264734\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0020824375006680686\n",
      "Average test loss: 0.0038894146676692696\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002055212047364977\n",
      "Average test loss: 0.007621968637324041\n",
      "Epoch 189/300\n",
      "Average training loss: 0.002041994207849105\n",
      "Average test loss: 0.00278406390775409\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0020481796310179764\n",
      "Average test loss: 0.0027879538037296797\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002056013103781475\n",
      "Average test loss: 0.008057924104233583\n",
      "Epoch 192/300\n",
      "Average training loss: 0.002046927730863293\n",
      "Average test loss: 0.0027295150984492566\n",
      "Epoch 193/300\n",
      "Average training loss: 0.002058251590364509\n",
      "Average test loss: 0.0027250872488237088\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0020283564436766838\n",
      "Average test loss: 0.0028319670158541864\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002052908807475534\n",
      "Average test loss: 0.003151612056212293\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0021059406989564497\n",
      "Average test loss: 0.0030462269886500307\n",
      "Epoch 197/300\n",
      "Average training loss: 0.002050753002572391\n",
      "Average test loss: 0.002939618038220538\n",
      "Epoch 198/300\n",
      "Average training loss: 0.002019284122106102\n",
      "Average test loss: 0.00275897692112873\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0020440861443057658\n",
      "Average test loss: 0.004594480558402008\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0020338470701956085\n",
      "Average test loss: 0.002808257651204864\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002043480528725518\n",
      "Average test loss: 0.0028077108391250175\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0020547335715964438\n",
      "Average test loss: 0.0027832401297572585\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0020205008055393893\n",
      "Average test loss: 0.002780806764339407\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0020292243218670287\n",
      "Average test loss: 0.004847171294606394\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0020362456792758572\n",
      "Average test loss: 0.006628512634999222\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0020278479957746135\n",
      "Average test loss: 0.002788632055123647\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0020118482703756955\n",
      "Average test loss: 0.00287144423276186\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0020343892585693133\n",
      "Average test loss: 0.002829412825819519\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002002733554587596\n",
      "Average test loss: 0.003031144707153241\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0020220429657234087\n",
      "Average test loss: 0.0027247301834738918\n",
      "Epoch 211/300\n",
      "Average training loss: 0.001998301083842913\n",
      "Average test loss: 0.0031211729364262687\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0020327083597787552\n",
      "Average test loss: 0.002826318208128214\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0020006346499754322\n",
      "Average test loss: 0.02256438482304414\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0019983590905451112\n",
      "Average test loss: 0.004417303590724866\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0020332261993446284\n",
      "Average test loss: 0.0029578365166154173\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002007331166519887\n",
      "Average test loss: 0.006570909627195862\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0020022401426815326\n",
      "Average test loss: 0.003226896886403362\n",
      "Epoch 218/300\n",
      "Average training loss: 0.002427570145370232\n",
      "Average test loss: 0.002881121220274104\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0023150581483625705\n",
      "Average test loss: 0.002732689083657331\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0021244874228205945\n",
      "Average test loss: 0.00548425863766008\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0020483423268629446\n",
      "Average test loss: 0.002817752974314822\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0020345210853136247\n",
      "Average test loss: 0.004607138981421789\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0020214530697299374\n",
      "Average test loss: 0.002766818941053417\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0020302702547051014\n",
      "Average test loss: 0.002887344934874111\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0019990805957673325\n",
      "Average test loss: 0.0028022802112003167\n",
      "Epoch 226/300\n",
      "Average training loss: 0.001991304918088847\n",
      "Average test loss: 0.0031241519223484727\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0019738051563294398\n",
      "Average test loss: 0.016964014757010672\n",
      "Epoch 228/300\n",
      "Average training loss: 0.001997543036937714\n",
      "Average test loss: 0.00329372175803615\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0019863446189814973\n",
      "Average test loss: 0.0028710129087169967\n",
      "Epoch 230/300\n",
      "Average training loss: 0.002114473649404115\n",
      "Average test loss: 0.00284984231284923\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0021158173359516596\n",
      "Average test loss: 0.045458904936909676\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0019903440251946448\n",
      "Average test loss: 0.004389517318043444\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0019703870933088993\n",
      "Average test loss: 0.023597216222849157\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0019741834298604064\n",
      "Average test loss: 0.005429130408498976\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0019795777212000556\n",
      "Average test loss: 0.002746802030959063\n",
      "Epoch 236/300\n",
      "Average training loss: 0.001989283558705615\n",
      "Average test loss: 0.00575589852531751\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0019717834728459516\n",
      "Average test loss: 0.0033938643679850633\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0019972205890549555\n",
      "Average test loss: 0.04993891687484251\n",
      "Epoch 239/300\n",
      "Average training loss: 0.001985877576904992\n",
      "Average test loss: 0.004367583693936467\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0019574126369423335\n",
      "Average test loss: 0.013853766994343864\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0019684380329110557\n",
      "Average test loss: 0.0027650762599789433\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00198853287856198\n",
      "Average test loss: 0.0071469466719362475\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0020030451473883456\n",
      "Average test loss: 0.0028251698161992763\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0019625559418151774\n",
      "Average test loss: 0.003682388478062219\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0019873458701703283\n",
      "Average test loss: 0.002772908050980833\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0019642380606383087\n",
      "Average test loss: 0.0029025612879130577\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0019692994736962848\n",
      "Average test loss: 0.002730753427164422\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002400928739872244\n",
      "Average test loss: 0.003352114139124751\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0020646076912267343\n",
      "Average test loss: 0.0027787797815269893\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0019857493386500414\n",
      "Average test loss: 0.002752634040493932\n",
      "Epoch 251/300\n",
      "Average training loss: 0.001969822963596218\n",
      "Average test loss: 0.0027701248363074327\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0019530716749529043\n",
      "Average test loss: 0.008990839281015926\n",
      "Epoch 253/300\n",
      "Average training loss: 0.001949507529536883\n",
      "Average test loss: 0.14270825775629944\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0020476029589772226\n",
      "Average test loss: 0.004204627689388063\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0019431378985237744\n",
      "Average test loss: 0.013905247607992756\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0019514244051857127\n",
      "Average test loss: 0.009793672922584746\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0019508318098054992\n",
      "Average test loss: 0.002838916325527761\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0019492707651936345\n",
      "Average test loss: 0.0027403600791262254\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0020084827464694776\n",
      "Average test loss: 0.002810478478256199\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0019493320663977\n",
      "Average test loss: 0.005027039537620214\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0019523949153307412\n",
      "Average test loss: 0.0038734802835517458\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0019440245189600522\n",
      "Average test loss: 0.003111277452773518\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0019520427388035589\n",
      "Average test loss: 0.005566166082190143\n",
      "Epoch 264/300\n",
      "Average training loss: 0.001955890406233569\n",
      "Average test loss: 0.003391472490918305\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0020105833096636665\n",
      "Average test loss: 0.0034611754678189753\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0019570154041672746\n",
      "Average test loss: 0.0027443164284858437\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0019339096834883093\n",
      "Average test loss: 0.036885707351896495\n",
      "Epoch 268/300\n",
      "Average training loss: 0.001959338560907377\n",
      "Average test loss: 0.09144821224610011\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0019527049662752284\n",
      "Average test loss: 0.0028102588877081873\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0019780918616387577\n",
      "Average test loss: 0.0046501040595273175\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0019428935791883204\n",
      "Average test loss: 6.518459535168277\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0020402165197042957\n",
      "Average test loss: 0.002797487962577078\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0019359054567499292\n",
      "Average test loss: 0.002824393894937303\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0019414132903847429\n",
      "Average test loss: 0.06388509984645578\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0019488741042506363\n",
      "Average test loss: 0.0027588844702889524\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0019256260537852843\n",
      "Average test loss: 0.0028467533282107776\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0020201595403874914\n",
      "Average test loss: 0.0027549149685849746\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0019482255790175664\n",
      "Average test loss: 0.0030204175646520322\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0019206017481370105\n",
      "Average test loss: 0.01524976655592521\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0019154492289655738\n",
      "Average test loss: 0.002980032578524616\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0019575375505826538\n",
      "Average test loss: 0.002973811890722977\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0019186750202336246\n",
      "Average test loss: 0.0028423737328913475\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0019234332286028398\n",
      "Average test loss: 0.0028833856659217015\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0019258916618095504\n",
      "Average test loss: 0.0030419274701012507\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0019338610302656889\n",
      "Average test loss: 0.005936348734630479\n",
      "Epoch 286/300\n",
      "Average training loss: 0.001955608767353826\n",
      "Average test loss: 0.0027981204092502594\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0021915312185883523\n",
      "Average test loss: 0.0027660571328467793\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0020594819496489235\n",
      "Average test loss: 0.020019767643676863\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0019267804431211616\n",
      "Average test loss: 0.003384170793617765\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0019069223996872704\n",
      "Average test loss: 0.008781288393669658\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0019011598327714535\n",
      "Average test loss: 0.0028047537099983957\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0019045649150179493\n",
      "Average test loss: 0.0028822353320817153\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0019231215075900157\n",
      "Average test loss: 0.0028648461289703846\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0019089147277797263\n",
      "Average test loss: 0.0029949168215195336\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0019174430215110382\n",
      "Average test loss: 0.0031695470665064123\n",
      "Epoch 296/300\n",
      "Average training loss: 0.001935493729594681\n",
      "Average test loss: 0.00425875947996974\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0019127059172218044\n",
      "Average test loss: 0.0027902214700977006\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0019209246130453216\n",
      "Average test loss: 0.16210157755679555\n",
      "Epoch 299/300\n",
      "Average training loss: 0.001914115960088869\n",
      "Average test loss: 0.0029221129361540078\n",
      "Epoch 300/300\n",
      "Average training loss: 0.001924383374241491\n",
      "Average test loss: 0.007955656266874738\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.022841881663435034\n",
      "Average test loss: 0.02346676683591472\n",
      "Epoch 2/300\n",
      "Average training loss: 0.007486063246925672\n",
      "Average test loss: 0.4590691895319356\n",
      "Epoch 3/300\n",
      "Average training loss: 0.006517706207931042\n",
      "Average test loss: 0.09142662672077616\n",
      "Epoch 4/300\n",
      "Average training loss: 0.005608334768563509\n",
      "Average test loss: 0.005378292839974165\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005321188682069381\n",
      "Average test loss: 5390.938827690972\n",
      "Epoch 6/300\n",
      "Average training loss: 0.004877322220967876\n",
      "Average test loss: 4.144913344502449\n",
      "Epoch 7/300\n",
      "Average training loss: 0.004226110506802798\n",
      "Average test loss: 45.116423729949524\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0039728158596489165\n",
      "Average test loss: 0.3893030957993534\n",
      "Epoch 9/300\n",
      "Average training loss: 0.003769970092508528\n",
      "Average test loss: 0.11349115380603406\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003593029651376936\n",
      "Average test loss: 0.031771247550431224\n",
      "Epoch 11/300\n",
      "Average training loss: 0.003443887848613991\n",
      "Average test loss: 1.184558138474822\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0033480130657553672\n",
      "Average test loss: 167.7966957177023\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0032297050853570303\n",
      "Average test loss: 0.00312054631465839\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0031398140939159526\n",
      "Average test loss: 3.8960811122001875\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003089227605611086\n",
      "Average test loss: 0.1412581271827221\n",
      "Epoch 16/300\n",
      "Average training loss: 0.003014088754231731\n",
      "Average test loss: 5.976304160920282\n",
      "Epoch 17/300\n",
      "Average training loss: 0.002937214295897219\n",
      "Average test loss: 0.06971919166213936\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0031488678604364396\n",
      "Average test loss: 3.241396982582079\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0029051817583127153\n",
      "Average test loss: 7.298924411091333\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0028364467960264946\n",
      "Average test loss: 0.1465336592056685\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0032145009407152734\n",
      "Average test loss: 187.96224945189059\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0027866995222866537\n",
      "Average test loss: 11.360771890871433\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0026863679581632218\n",
      "Average test loss: 0.3185676066103495\n",
      "Epoch 24/300\n",
      "Average training loss: 0.002653355096363359\n",
      "Average test loss: 2.59432376708318\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0026242548152804373\n",
      "Average test loss: 3.3363396616776786\n",
      "Epoch 26/300\n",
      "Average training loss: 0.002601635025607215\n",
      "Average test loss: 0.11222443821798596\n",
      "Epoch 27/300\n",
      "Average training loss: 0.002643414712407523\n",
      "Average test loss: 35.40132961741669\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0026200849159310263\n",
      "Average test loss: 0.5131394545742207\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0026632392675512368\n",
      "Average test loss: 0.02284218238045772\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0025338685613953407\n",
      "Average test loss: 5.944588722209136\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0024811286711030538\n",
      "Average test loss: 0.06604981309103056\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002458361459689008\n",
      "Average test loss: 15.059117265046678\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0024016907142682207\n",
      "Average test loss: 0.0024132582741893\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0028639575835938255\n",
      "Average test loss: 0.18400005456453397\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0024491132685086794\n",
      "Average test loss: 4.881044856283814\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0023815258536487817\n",
      "Average test loss: 0.0051777066813988816\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0023686570752826\n",
      "Average test loss: 8.661035607932343\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0023996158548527293\n",
      "Average test loss: 0.002580338884765903\n",
      "Epoch 39/300\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_No_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_No_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_No_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
