{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.DAMP_Network.DAMP import DAMP\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.3808459279537201\n",
      "Average test loss: 0.018576968133449553\n",
      "Epoch 2/300\n",
      "Average training loss: 0.17236851516034868\n",
      "Average test loss: 0.016451407094796498\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12480808010366228\n",
      "Average test loss: 0.013107813086774615\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09946828444136514\n",
      "Average test loss: 0.009934333908061187\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08378559938404295\n",
      "Average test loss: 0.009116699056906832\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0749849911795722\n",
      "Average test loss: 0.022454297999540965\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07116832012931505\n",
      "Average test loss: 0.01015155575838354\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0669077684018347\n",
      "Average test loss: 0.011861263109578026\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06585690048668119\n",
      "Average test loss: 0.01307776451855898\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06286730029847887\n",
      "Average test loss: 0.01736633976466126\n",
      "Epoch 11/300\n",
      "Average training loss: 0.060643465850088334\n",
      "Average test loss: 0.010975852389302519\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05923654892047246\n",
      "Average test loss: 0.009475441207488378\n",
      "Epoch 13/300\n",
      "Average training loss: 0.057725324226750266\n",
      "Average test loss: 0.010445457823574544\n",
      "Epoch 14/300\n",
      "Average training loss: 0.055659894035922156\n",
      "Average test loss: 0.008379703010121982\n",
      "Epoch 15/300\n",
      "Average training loss: 0.055397901657554836\n",
      "Average test loss: 0.008530982909103235\n",
      "Epoch 16/300\n",
      "Average training loss: 0.053883686972988974\n",
      "Average test loss: 0.009127736284501023\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05319627103540633\n",
      "Average test loss: 0.00837096793949604\n",
      "Epoch 18/300\n",
      "Average training loss: 0.053056337783734\n",
      "Average test loss: 0.007939081172148386\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05252422578467263\n",
      "Average test loss: 0.008155455945266617\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05184200254413816\n",
      "Average test loss: 0.00919447291807996\n",
      "Epoch 21/300\n",
      "Average training loss: 0.050752254333761\n",
      "Average test loss: 0.008493011257714695\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05032638736234771\n",
      "Average test loss: 0.008622259554349714\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04933638991249932\n",
      "Average test loss: 0.009177311523093118\n",
      "Epoch 24/300\n",
      "Average training loss: 0.048782099849647945\n",
      "Average test loss: 0.0077384600155055525\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04834574143422975\n",
      "Average test loss: 0.009712872573898898\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0476948584748639\n",
      "Average test loss: 0.008212245913429393\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0477872552341885\n",
      "Average test loss: 0.010931779958307743\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04729345205095079\n",
      "Average test loss: 0.00877333216369152\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04654217209087478\n",
      "Average test loss: 0.007502263483073976\n",
      "Epoch 30/300\n",
      "Average training loss: 0.046183434665203096\n",
      "Average test loss: 0.007824148758004109\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04588430424862438\n",
      "Average test loss: 0.007732679348852899\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04562829077574942\n",
      "Average test loss: 0.00949375387860669\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04562743514445093\n",
      "Average test loss: 0.007656156010304888\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04522491790850957\n",
      "Average test loss: 0.007430914331641462\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04503417744901445\n",
      "Average test loss: 0.008383572140087683\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04462565661801232\n",
      "Average test loss: 0.007603307603134049\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04471424030926492\n",
      "Average test loss: 0.007561674233525991\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04422741739286317\n",
      "Average test loss: 0.007689308036118746\n",
      "Epoch 39/300\n",
      "Average training loss: 0.043905494911803136\n",
      "Average test loss: 0.007408070983572139\n",
      "Epoch 40/300\n",
      "Average training loss: 0.043998958670430716\n",
      "Average test loss: 0.007925950916277038\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04378754454520013\n",
      "Average test loss: 0.00708677458225025\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04353138600786527\n",
      "Average test loss: 0.008033720106300381\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04326237440771527\n",
      "Average test loss: 0.007620258633047342\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04347011441323492\n",
      "Average test loss: 0.007209984776874383\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04323020984729131\n",
      "Average test loss: 0.007499762608773179\n",
      "Epoch 46/300\n",
      "Average training loss: 0.043220096055004334\n",
      "Average test loss: 0.00697503111180332\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04294111693236563\n",
      "Average test loss: 0.00708909156670173\n",
      "Epoch 48/300\n",
      "Average training loss: 0.042690831328431766\n",
      "Average test loss: 0.008394554161363178\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04282870208885935\n",
      "Average test loss: 0.007151963589092095\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04247233060664601\n",
      "Average test loss: 0.007031811888433165\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04242455169227388\n",
      "Average test loss: 0.007497082029779752\n",
      "Epoch 52/300\n",
      "Average training loss: 0.042453061050838896\n",
      "Average test loss: 0.0075801124225060145\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04252792154086961\n",
      "Average test loss: 0.007905194348759121\n",
      "Epoch 54/300\n",
      "Average training loss: 0.041912138001786337\n",
      "Average test loss: 0.0069913273141202\n",
      "Epoch 55/300\n",
      "Average training loss: 0.042020470745033685\n",
      "Average test loss: 0.008323662622935242\n",
      "Epoch 56/300\n",
      "Average training loss: 0.042517131792174444\n",
      "Average test loss: 0.00725063073138396\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04172233717309104\n",
      "Average test loss: 0.007060781390716632\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04191793124212159\n",
      "Average test loss: 0.007035520469976796\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0418142582807276\n",
      "Average test loss: 0.007439026245226463\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04164321574568748\n",
      "Average test loss: 0.007179333066774739\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04157713214556376\n",
      "Average test loss: 0.007135034939895073\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04129057675268915\n",
      "Average test loss: 0.007464842146055566\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04131993166605632\n",
      "Average test loss: 0.007037280779331923\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04133636145624849\n",
      "Average test loss: 0.007187271499799358\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04112435310747888\n",
      "Average test loss: 0.007231175262067053\n",
      "Epoch 66/300\n",
      "Average training loss: 0.041490280005666944\n",
      "Average test loss: 0.007199428317447503\n",
      "Epoch 67/300\n",
      "Average training loss: 0.041124403019746146\n",
      "Average test loss: 0.007365150224417448\n",
      "Epoch 68/300\n",
      "Average training loss: 0.041052011931935944\n",
      "Average test loss: 0.007083273041993379\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04108086326056057\n",
      "Average test loss: 0.00717909437417984\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04117268988490105\n",
      "Average test loss: 0.007126482560402817\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04064224099781778\n",
      "Average test loss: 0.007367595461921559\n",
      "Epoch 72/300\n",
      "Average training loss: 0.040694808148675495\n",
      "Average test loss: 0.006922891625927554\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04080712917778227\n",
      "Average test loss: 0.006890285710493723\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04056756513648563\n",
      "Average test loss: 0.007140326320711109\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04075273346238666\n",
      "Average test loss: 0.006773111636439959\n",
      "Epoch 76/300\n",
      "Average training loss: 0.040525795655118095\n",
      "Average test loss: 0.0070246880418724485\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0405825215710534\n",
      "Average test loss: 0.0070990846662057775\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04023666720257865\n",
      "Average test loss: 0.006817216647168001\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04036145638757282\n",
      "Average test loss: 0.007131165051625835\n",
      "Epoch 80/300\n",
      "Average training loss: 0.040267786810795465\n",
      "Average test loss: 0.008720362819731235\n",
      "Epoch 81/300\n",
      "Average training loss: 0.040246506892972524\n",
      "Average test loss: 0.008175875801179145\n",
      "Epoch 82/300\n",
      "Average training loss: 0.040377902772691515\n",
      "Average test loss: 0.007028392033858431\n",
      "Epoch 83/300\n",
      "Average training loss: 0.040214967681301964\n",
      "Average test loss: 0.006985672423409091\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04005440627535184\n",
      "Average test loss: 0.006937560421311193\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04020375897155868\n",
      "Average test loss: 0.006982792558769385\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03986282417178154\n",
      "Average test loss: 0.007093220629211929\n",
      "Epoch 87/300\n",
      "Average training loss: 0.039988077521324154\n",
      "Average test loss: 0.01192306150496006\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04003637710213661\n",
      "Average test loss: 0.007244020437200864\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03988617307941119\n",
      "Average test loss: 0.007031987277170022\n",
      "Epoch 90/300\n",
      "Average training loss: 0.039957904577255246\n",
      "Average test loss: 0.006975410718470812\n",
      "Epoch 91/300\n",
      "Average training loss: 0.039820810976955626\n",
      "Average test loss: 0.006782855369978481\n",
      "Epoch 92/300\n",
      "Average training loss: 0.039719080564048555\n",
      "Average test loss: 0.007035948157310486\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03982459217309952\n",
      "Average test loss: 0.006733585269500812\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03983257768220372\n",
      "Average test loss: 0.006887231839199861\n",
      "Epoch 95/300\n",
      "Average training loss: 0.039711185134119455\n",
      "Average test loss: 0.0073688778235680525\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03953942774070634\n",
      "Average test loss: 0.007342175552414523\n",
      "Epoch 97/300\n",
      "Average training loss: 0.039527265118228065\n",
      "Average test loss: 0.007101703918228547\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03950453246633212\n",
      "Average test loss: 0.006939279902312491\n",
      "Epoch 99/300\n",
      "Average training loss: 0.039543205310901004\n",
      "Average test loss: 0.006921829344911708\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03950497585866186\n",
      "Average test loss: 0.007155550776256455\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03946713977389865\n",
      "Average test loss: 0.006735358175304201\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03941193237569597\n",
      "Average test loss: 0.006789235716892613\n",
      "Epoch 103/300\n",
      "Average training loss: 0.039451368083556496\n",
      "Average test loss: 0.00720682090603643\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03938860845565796\n",
      "Average test loss: 0.007217353095197015\n",
      "Epoch 105/300\n",
      "Average training loss: 0.039219588743315804\n",
      "Average test loss: 0.006939556465380721\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03925934791896078\n",
      "Average test loss: 0.007211457177168793\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03907530802488327\n",
      "Average test loss: 0.006955741777188248\n",
      "Epoch 108/300\n",
      "Average training loss: 0.039183462431033454\n",
      "Average test loss: 0.006994654811918736\n",
      "Epoch 109/300\n",
      "Average training loss: 0.039132402850521934\n",
      "Average test loss: 0.007186741845889224\n",
      "Epoch 110/300\n",
      "Average training loss: 0.039267933029267524\n",
      "Average test loss: 0.006752460888690419\n",
      "Epoch 111/300\n",
      "Average training loss: 0.038922446080380015\n",
      "Average test loss: 0.007958696621987555\n",
      "Epoch 112/300\n",
      "Average training loss: 0.039021348158518473\n",
      "Average test loss: 0.0068673595984776814\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03906883650355869\n",
      "Average test loss: 0.007107261391149627\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03897281903690762\n",
      "Average test loss: 0.007221545719438129\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03893005072739389\n",
      "Average test loss: 0.006907321232060591\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03900007085336579\n",
      "Average test loss: 0.0072097123836477595\n",
      "Epoch 117/300\n",
      "Average training loss: 0.038767758578062056\n",
      "Average test loss: 0.006635126743879583\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03883520884977447\n",
      "Average test loss: 0.006988004754607876\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03910693899293741\n",
      "Average test loss: 0.006885315080069833\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03876353739698728\n",
      "Average test loss: 0.00742689922451973\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03875719070103433\n",
      "Average test loss: 0.006766585578107172\n",
      "Epoch 122/300\n",
      "Average training loss: 0.038757576485474905\n",
      "Average test loss: 0.006777609343330065\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03871920038925277\n",
      "Average test loss: 0.007192727573215961\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03854417060481177\n",
      "Average test loss: 0.006911121136612362\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03870269699229134\n",
      "Average test loss: 0.006770952962338924\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0387224808037281\n",
      "Average test loss: 0.0068665394683678944\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03872858826650514\n",
      "Average test loss: 0.006872112927337488\n",
      "Epoch 128/300\n",
      "Average training loss: 0.038531945390833745\n",
      "Average test loss: 0.006639127963119083\n",
      "Epoch 129/300\n",
      "Average training loss: 0.038503441241052414\n",
      "Average test loss: 0.007001701051990191\n",
      "Epoch 130/300\n",
      "Average training loss: 0.038579316092862026\n",
      "Average test loss: 0.006729368762009674\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03846492287516594\n",
      "Average test loss: 0.006679082517822584\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03837897438142035\n",
      "Average test loss: 0.006906188595212168\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03846591542164485\n",
      "Average test loss: 0.007004695303738117\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03842256428466903\n",
      "Average test loss: 0.006907086338433954\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03835325792762968\n",
      "Average test loss: 0.006903394366304079\n",
      "Epoch 136/300\n",
      "Average training loss: 0.038424532582362496\n",
      "Average test loss: 0.006999117282529672\n",
      "Epoch 137/300\n",
      "Average training loss: 0.038243234743674595\n",
      "Average test loss: 0.006680661638577779\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0383228375878599\n",
      "Average test loss: 0.0071746110104852255\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03823568846782049\n",
      "Average test loss: 0.006983011867437098\n",
      "Epoch 140/300\n",
      "Average training loss: 0.038212269531355966\n",
      "Average test loss: 0.0069196510174208215\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03831217639313804\n",
      "Average test loss: 0.007331512882063786\n",
      "Epoch 142/300\n",
      "Average training loss: 0.038166909535725914\n",
      "Average test loss: 0.006600026117430793\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03810043697224723\n",
      "Average test loss: 0.006716325274772114\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03810822159383032\n",
      "Average test loss: 0.0068189850321246515\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03816253521707323\n",
      "Average test loss: 0.007044897582795885\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03825984861453374\n",
      "Average test loss: 0.0066722388544844255\n",
      "Epoch 147/300\n",
      "Average training loss: 0.038164237770769334\n",
      "Average test loss: 0.006612066148055924\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03800044882297516\n",
      "Average test loss: 0.006722318493657642\n",
      "Epoch 149/300\n",
      "Average training loss: 0.037993741081820596\n",
      "Average test loss: 0.006898569002747536\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03801823443836636\n",
      "Average test loss: 0.006777337023367485\n",
      "Epoch 151/300\n",
      "Average training loss: 0.038079006569253074\n",
      "Average test loss: 0.0067784640950461226\n",
      "Epoch 152/300\n",
      "Average training loss: 0.037941606567965615\n",
      "Average test loss: 0.006834482588701778\n",
      "Epoch 153/300\n",
      "Average training loss: 0.037944704946544436\n",
      "Average test loss: 0.006827167426132493\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03790835939182176\n",
      "Average test loss: 0.006681506445838346\n",
      "Epoch 155/300\n",
      "Average training loss: 0.037918938944737114\n",
      "Average test loss: 0.006924253829237488\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03793331407672829\n",
      "Average test loss: 0.006712842537297143\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03779067959388097\n",
      "Average test loss: 0.006964086089697149\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03788753194941415\n",
      "Average test loss: 0.006796053285813994\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03788476476735539\n",
      "Average test loss: 0.00685157501945893\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03775549136930042\n",
      "Average test loss: 0.0067957078359193275\n",
      "Epoch 161/300\n",
      "Average training loss: 0.037875213662783305\n",
      "Average test loss: 0.006757107772760921\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03774742643038432\n",
      "Average test loss: 0.006783518923239576\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03777079395784272\n",
      "Average test loss: 0.0066655073269373845\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03781372631920708\n",
      "Average test loss: 0.007212402498142587\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03766201322442955\n",
      "Average test loss: 0.006854040837536255\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03776331100530095\n",
      "Average test loss: 0.006714927995784415\n",
      "Epoch 167/300\n",
      "Average training loss: 0.037799765517314274\n",
      "Average test loss: 0.006774281216578351\n",
      "Epoch 168/300\n",
      "Average training loss: 0.037712895936436126\n",
      "Average test loss: 0.006719286546525028\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03778100911776225\n",
      "Average test loss: 0.006780376584165626\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03759256980154249\n",
      "Average test loss: 0.00702423472619719\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03765421273973253\n",
      "Average test loss: 0.007344008651872476\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03756340704030461\n",
      "Average test loss: 0.006968985617160797\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03751349975996547\n",
      "Average test loss: 0.006713254774610202\n",
      "Epoch 174/300\n",
      "Average training loss: 0.037546568267875245\n",
      "Average test loss: 0.006878315716154045\n",
      "Epoch 175/300\n",
      "Average training loss: 0.037575106647279526\n",
      "Average test loss: 0.0066334672085940835\n",
      "Epoch 176/300\n",
      "Average training loss: 0.037566598620679646\n",
      "Average test loss: 0.006709753749271234\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03752604417668449\n",
      "Average test loss: 0.006849913538744052\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03743096798327234\n",
      "Average test loss: 0.007086280423733923\n",
      "Epoch 179/300\n",
      "Average training loss: 0.037487774577405715\n",
      "Average test loss: 0.006716947455786997\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03747085010674265\n",
      "Average test loss: 0.007586469579488039\n",
      "Epoch 181/300\n",
      "Average training loss: 0.037456178896956974\n",
      "Average test loss: 0.006797201341638962\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03734884520371755\n",
      "Average test loss: 0.009216618101216024\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03739524939325121\n",
      "Average test loss: 0.006884615658885903\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03740837828649415\n",
      "Average test loss: 0.006684142798185348\n",
      "Epoch 185/300\n",
      "Average training loss: 0.037478348278337056\n",
      "Average test loss: 0.0068882801495492455\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03741058587034543\n",
      "Average test loss: 0.0066430126374794375\n",
      "Epoch 187/300\n",
      "Average training loss: 0.037302551693386504\n",
      "Average test loss: 0.007124080095854071\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03743567170202732\n",
      "Average test loss: 0.00702937632964717\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03743874258796374\n",
      "Average test loss: 0.006711866833269596\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0372494521273507\n",
      "Average test loss: 0.006619290629194843\n",
      "Epoch 191/300\n",
      "Average training loss: 0.037232251796457505\n",
      "Average test loss: 0.007223216385477119\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03726882856422001\n",
      "Average test loss: 0.0067555070796774495\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03746972742345598\n",
      "Average test loss: 0.006870195591615306\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03717909711764918\n",
      "Average test loss: 0.007196724080377155\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03733367235627439\n",
      "Average test loss: 0.0067096118202639955\n",
      "Epoch 196/300\n",
      "Average training loss: 0.037300645957390466\n",
      "Average test loss: 0.00701757725659344\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03711126998066902\n",
      "Average test loss: 0.006835416903098425\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03715560072660446\n",
      "Average test loss: 0.006783005014061928\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0371764036781258\n",
      "Average test loss: 0.006942657097346253\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0371566081278854\n",
      "Average test loss: 0.0070235065792997676\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03721971537007226\n",
      "Average test loss: 0.006652066982040803\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03726936877601676\n",
      "Average test loss: 0.007022983635465304\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03713256602154838\n",
      "Average test loss: 0.006788753887017568\n",
      "Epoch 204/300\n",
      "Average training loss: 0.037196781449847754\n",
      "Average test loss: 0.00741170014689366\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03705281148685349\n",
      "Average test loss: 0.0069650629833340645\n",
      "Epoch 206/300\n",
      "Average training loss: 0.037141992366976205\n",
      "Average test loss: 0.00667517701908946\n",
      "Epoch 207/300\n",
      "Average training loss: 0.037000024030605955\n",
      "Average test loss: 0.0070145729076531195\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03691789604889022\n",
      "Average test loss: 0.006653045456028647\n",
      "Epoch 209/300\n",
      "Average training loss: 0.036966738637950684\n",
      "Average test loss: 0.007107620770732561\n",
      "Epoch 210/300\n",
      "Average training loss: 0.036946492205063505\n",
      "Average test loss: 0.006850722754581107\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03704026047057576\n",
      "Average test loss: 0.00778144889117943\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0370002958410316\n",
      "Average test loss: 0.19625223363770378\n",
      "Epoch 213/300\n",
      "Average training loss: 0.036949120624197855\n",
      "Average test loss: 0.0066553102119101416\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0370352290670077\n",
      "Average test loss: 0.0067044187751081254\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03696195532215966\n",
      "Average test loss: 0.006775350327706999\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03689117541578081\n",
      "Average test loss: 0.006777742432223426\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03684617064727677\n",
      "Average test loss: 0.006930741901612944\n",
      "Epoch 218/300\n",
      "Average training loss: 0.036926882631248895\n",
      "Average test loss: 0.006737680617305968\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03692451696263419\n",
      "Average test loss: 0.006624626729637384\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03677636301517487\n",
      "Average test loss: 0.006597242045733664\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03682332277960247\n",
      "Average test loss: 0.006664445529381434\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03691913641326957\n",
      "Average test loss: 0.0068454470150172714\n",
      "Epoch 223/300\n",
      "Average training loss: 0.036828932722409564\n",
      "Average test loss: 0.006747094403538439\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03679679915640089\n",
      "Average test loss: 0.006766122084524896\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03681343917714225\n",
      "Average test loss: 0.0070055122445854875\n",
      "Epoch 226/300\n",
      "Average training loss: 0.036793893547521696\n",
      "Average test loss: 0.0067606887502802745\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0367426871392462\n",
      "Average test loss: 0.006713032343735298\n",
      "Epoch 228/300\n",
      "Average training loss: 0.036773958911498386\n",
      "Average test loss: 0.006652481456597646\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03660017893380589\n",
      "Average test loss: 0.007572575996319453\n",
      "Epoch 230/300\n",
      "Average training loss: 0.036664911733733284\n",
      "Average test loss: 0.00681317615798778\n",
      "Epoch 231/300\n",
      "Average training loss: 0.036776016303234633\n",
      "Average test loss: 0.006784766438934538\n",
      "Epoch 232/300\n",
      "Average training loss: 0.036662472403711746\n",
      "Average test loss: 0.006757411186893781\n",
      "Epoch 233/300\n",
      "Average training loss: 0.036596636315186816\n",
      "Average test loss: 0.006675173270205657\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0366558700700601\n",
      "Average test loss: 0.006845742404046986\n",
      "Epoch 235/300\n",
      "Average training loss: 0.036683755811717775\n",
      "Average test loss: 0.006974142287340429\n",
      "Epoch 236/300\n",
      "Average training loss: 0.036628987461328505\n",
      "Average test loss: 0.006850514780316088\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03654300326108933\n",
      "Average test loss: 0.006899775413589345\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03670166456699371\n",
      "Average test loss: 0.0067125100526544785\n",
      "Epoch 239/300\n",
      "Average training loss: 0.036575439443190895\n",
      "Average test loss: 0.006591967071923945\n",
      "Epoch 240/300\n",
      "Average training loss: 0.036583685752418306\n",
      "Average test loss: 0.006720568327026235\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03652228923638662\n",
      "Average test loss: 0.006686440894173251\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03659133550524712\n",
      "Average test loss: 0.006696736694624027\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03661585344539748\n",
      "Average test loss: 0.0073001571065849724\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03649712405933274\n",
      "Average test loss: 0.006628376641621192\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03653180023199982\n",
      "Average test loss: 0.006590049160437451\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03649421849184566\n",
      "Average test loss: 0.006607700552377436\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03652451477117009\n",
      "Average test loss: 0.006654341024243169\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03652863139907519\n",
      "Average test loss: 0.006714464224047131\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03652238271964921\n",
      "Average test loss: 0.006702730481823286\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03647997613747914\n",
      "Average test loss: 0.007146461226046085\n",
      "Epoch 251/300\n",
      "Average training loss: 0.036526577750841775\n",
      "Average test loss: 0.006915640101664596\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03645401556458738\n",
      "Average test loss: 0.007962428490320842\n",
      "Epoch 253/300\n",
      "Average training loss: 0.036370633204778034\n",
      "Average test loss: 0.006651385854101843\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03638606813549995\n",
      "Average test loss: 0.006788588934474521\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03642729190654225\n",
      "Average test loss: 0.006684574134647846\n",
      "Epoch 256/300\n",
      "Average training loss: 0.036363578144047\n",
      "Average test loss: 0.006739308981431855\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03632179755634732\n",
      "Average test loss: 0.006742912076827552\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0364830562306775\n",
      "Average test loss: 0.006964214927620358\n",
      "Epoch 259/300\n",
      "Average training loss: 0.036478381514549255\n",
      "Average test loss: 0.006722145639359951\n",
      "Epoch 260/300\n",
      "Average training loss: 0.036317679779397115\n",
      "Average test loss: 0.007416594562431177\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03630101620488697\n",
      "Average test loss: 0.006627317519651519\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03634316350354089\n",
      "Average test loss: 0.006656635985606247\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03634603488445282\n",
      "Average test loss: 0.006644291234927045\n",
      "Epoch 264/300\n",
      "Average training loss: 0.036251471092303596\n",
      "Average test loss: 0.006770459764533573\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03627677459186978\n",
      "Average test loss: 0.006768286362290383\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03636658634079827\n",
      "Average test loss: 0.006644782215770748\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0363663075549735\n",
      "Average test loss: 0.00683397899236944\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03626650009221501\n",
      "Average test loss: 0.006626636530790064\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03627467303143607\n",
      "Average test loss: 0.00667652543551392\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03628423853384124\n",
      "Average test loss: 0.0067961959677437945\n",
      "Epoch 271/300\n",
      "Average training loss: 0.036254084004296194\n",
      "Average test loss: 0.00701887602193488\n",
      "Epoch 272/300\n",
      "Average training loss: 0.036151178899738526\n",
      "Average test loss: 0.007956769144783417\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03628052261471748\n",
      "Average test loss: 0.0066625556734701\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0361797630223963\n",
      "Average test loss: 0.006781007922771904\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03620825069811609\n",
      "Average test loss: 0.04437982102897432\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03621596658064259\n",
      "Average test loss: 0.006799937689883842\n",
      "Epoch 277/300\n",
      "Average training loss: 0.036200114571385916\n",
      "Average test loss: 0.006886875709725751\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03620289810332987\n",
      "Average test loss: 0.0069311573029392295\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0361766508469979\n",
      "Average test loss: 0.006647157774617275\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03619473023878204\n",
      "Average test loss: 0.007016855173640781\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03614659495155017\n",
      "Average test loss: 0.006617004012895955\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03617022842168808\n",
      "Average test loss: 0.0065951977059659035\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03618335150844521\n",
      "Average test loss: 0.006591124678651492\n",
      "Epoch 284/300\n",
      "Average training loss: 0.036148853811952805\n",
      "Average test loss: 0.006585140786651108\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03614085078570578\n",
      "Average test loss: 0.006828532283918725\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03605735447340541\n",
      "Average test loss: 0.006760012358013126\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03613645906580819\n",
      "Average test loss: 0.008718547630641196\n",
      "Epoch 288/300\n",
      "Average training loss: 0.036113758547438514\n",
      "Average test loss: 0.006848712194297049\n",
      "Epoch 289/300\n",
      "Average training loss: 0.036194798625177806\n",
      "Average test loss: 0.007049642104241583\n",
      "Epoch 290/300\n",
      "Average training loss: 0.036106085151433945\n",
      "Average test loss: 0.006729599557816982\n",
      "Epoch 291/300\n",
      "Average training loss: 0.036113511006037394\n",
      "Average test loss: 0.006630588511625925\n",
      "Epoch 292/300\n",
      "Average training loss: 0.036093741678529315\n",
      "Average test loss: 0.006970726487123304\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03599384877085686\n",
      "Average test loss: 0.007943801037967204\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03595241432057487\n",
      "Average test loss: 0.0067520792041387825\n",
      "Epoch 295/300\n",
      "Average training loss: 0.036093230565388996\n",
      "Average test loss: 0.006615668036871487\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03601915987994936\n",
      "Average test loss: 0.0067659582669536275\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03594807922840118\n",
      "Average test loss: 0.006747916746056742\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03597362866666582\n",
      "Average test loss: 0.0066339819236761995\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03603991095225016\n",
      "Average test loss: 0.0066394292964703505\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03595783822569582\n",
      "Average test loss: 0.007050727483299044\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.17784330726332134\n",
      "Average test loss: 0.009395145004408228\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06406931006908417\n",
      "Average test loss: 0.008130046863522795\n",
      "Epoch 3/300\n",
      "Average training loss: 0.055640037328004835\n",
      "Average test loss: 0.007111818728347619\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0519248818092876\n",
      "Average test loss: 0.008005000204675728\n",
      "Epoch 5/300\n",
      "Average training loss: 0.048993335962295534\n",
      "Average test loss: 0.0061833968957265215\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04593210941553116\n",
      "Average test loss: 0.006025335494428873\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04496696324812041\n",
      "Average test loss: 0.005863724429160356\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04326291803850068\n",
      "Average test loss: 0.005704788217114077\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04221373391813702\n",
      "Average test loss: 0.005685165774491098\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04111879392464956\n",
      "Average test loss: 0.005852443059285482\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04016901227003998\n",
      "Average test loss: 0.005275226180752118\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03955367826753192\n",
      "Average test loss: 0.006234303211172422\n",
      "Epoch 13/300\n",
      "Average training loss: 0.038381477064556545\n",
      "Average test loss: 0.006281290851119492\n",
      "Epoch 14/300\n",
      "Average training loss: 0.038034831904702714\n",
      "Average test loss: 0.006479012612253428\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03734965333342552\n",
      "Average test loss: 0.005447584931221273\n",
      "Epoch 16/300\n",
      "Average training loss: 0.036116495665576724\n",
      "Average test loss: 0.005288566931254334\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0358619069258372\n",
      "Average test loss: 0.006307371587802966\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03500955096549458\n",
      "Average test loss: 0.0050749638113710615\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03470964275134934\n",
      "Average test loss: 0.004937239940795634\n",
      "Epoch 20/300\n",
      "Average training loss: 0.034482429024246\n",
      "Average test loss: 0.0075257694369388955\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03404456389943759\n",
      "Average test loss: 0.005487995276848475\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03367865295873748\n",
      "Average test loss: 0.0051331737492647436\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03330451211333275\n",
      "Average test loss: 0.004751263343832559\n",
      "Epoch 24/300\n",
      "Average training loss: 0.033343122793568505\n",
      "Average test loss: 0.004851103482147058\n",
      "Epoch 25/300\n",
      "Average training loss: 0.032970166908370124\n",
      "Average test loss: 0.006119441440121995\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03298604415853818\n",
      "Average test loss: 0.0063310633504556285\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0326396878576941\n",
      "Average test loss: 0.00602185071963403\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03223258309231864\n",
      "Average test loss: 0.0046827123223079576\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03235357586873902\n",
      "Average test loss: 0.00499016532757216\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03216308909654617\n",
      "Average test loss: 0.005706255258785354\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03197169899609354\n",
      "Average test loss: 0.004932379908031887\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03170211362176471\n",
      "Average test loss: 0.004750425289902422\n",
      "Epoch 33/300\n",
      "Average training loss: 0.031770799309015274\n",
      "Average test loss: 0.004880957904996143\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03143198596603341\n",
      "Average test loss: 0.004711879250490003\n",
      "Epoch 35/300\n",
      "Average training loss: 0.031684515002701016\n",
      "Average test loss: 0.0049803239119549595\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03126252220074336\n",
      "Average test loss: 0.0049356879368424415\n",
      "Epoch 37/300\n",
      "Average training loss: 0.031020901103814443\n",
      "Average test loss: 0.005838646787736151\n",
      "Epoch 38/300\n",
      "Average training loss: 0.030915181626876197\n",
      "Average test loss: 0.010164179096619288\n",
      "Epoch 39/300\n",
      "Average training loss: 0.030933850940730836\n",
      "Average test loss: 0.005358642357091109\n",
      "Epoch 40/300\n",
      "Average training loss: 0.030747925678888958\n",
      "Average test loss: 0.0048186092020736804\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03074620631833871\n",
      "Average test loss: 0.004486217726642887\n",
      "Epoch 42/300\n",
      "Average training loss: 0.030609189732207193\n",
      "Average test loss: 0.004585582947565449\n",
      "Epoch 43/300\n",
      "Average training loss: 0.030685343205928802\n",
      "Average test loss: 0.004797718156543043\n",
      "Epoch 44/300\n",
      "Average training loss: 0.030394654795527457\n",
      "Average test loss: 0.00508119015975131\n",
      "Epoch 45/300\n",
      "Average training loss: 0.030186739971240363\n",
      "Average test loss: 0.005145875489132272\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03024276978108618\n",
      "Average test loss: 0.004965963057759736\n",
      "Epoch 47/300\n",
      "Average training loss: 0.030084468967384762\n",
      "Average test loss: 0.004495185203436348\n",
      "Epoch 48/300\n",
      "Average training loss: 0.029952565646833844\n",
      "Average test loss: 0.004587920237746504\n",
      "Epoch 49/300\n",
      "Average training loss: 0.030021747138765122\n",
      "Average test loss: 0.004517912284367614\n",
      "Epoch 50/300\n",
      "Average training loss: 0.029922437745663853\n",
      "Average test loss: 0.004502237183766232\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02989039927886592\n",
      "Average test loss: 0.004730305485841301\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0298469042364094\n",
      "Average test loss: 0.004444949362840917\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02968360009789467\n",
      "Average test loss: 0.00518035918681158\n",
      "Epoch 54/300\n",
      "Average training loss: 0.029739196879996193\n",
      "Average test loss: 0.011209713950339291\n",
      "Epoch 55/300\n",
      "Average training loss: 0.029670915103620955\n",
      "Average test loss: 0.004755184141505096\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02945307221512\n",
      "Average test loss: 0.004313798001243009\n",
      "Epoch 57/300\n",
      "Average training loss: 0.029423066591223082\n",
      "Average test loss: 0.004383350408325592\n",
      "Epoch 58/300\n",
      "Average training loss: 0.029626693806714482\n",
      "Average test loss: 0.0043958766998516184\n",
      "Epoch 59/300\n",
      "Average training loss: 0.029470976889133454\n",
      "Average test loss: 0.004562368035730388\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02949539088540607\n",
      "Average test loss: 0.004673296224946777\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02934882386525472\n",
      "Average test loss: 0.004458976389633285\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02918319113055865\n",
      "Average test loss: 0.004682032665444745\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02932900194823742\n",
      "Average test loss: 0.004714951053261757\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02902304948369662\n",
      "Average test loss: 0.004355333009113868\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029145757292707762\n",
      "Average test loss: 0.005072191085252497\n",
      "Epoch 66/300\n",
      "Average training loss: 0.029115831774142054\n",
      "Average test loss: 0.004297134804642863\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02897339640226629\n",
      "Average test loss: 0.004240190302746164\n",
      "Epoch 68/300\n",
      "Average training loss: 0.029183512085013918\n",
      "Average test loss: 0.004262194224322836\n",
      "Epoch 69/300\n",
      "Average training loss: 0.028926487694183985\n",
      "Average test loss: 0.004453545439160532\n",
      "Epoch 70/300\n",
      "Average training loss: 0.028946695789694787\n",
      "Average test loss: 0.004350020447745919\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02894861830936538\n",
      "Average test loss: 0.004502023752778769\n",
      "Epoch 72/300\n",
      "Average training loss: 0.029044993109173245\n",
      "Average test loss: 0.0047574985892408425\n",
      "Epoch 73/300\n",
      "Average training loss: 0.028729593195848994\n",
      "Average test loss: 0.004693094268027279\n",
      "Epoch 74/300\n",
      "Average training loss: 0.028950453648964564\n",
      "Average test loss: 0.004246730068491565\n",
      "Epoch 75/300\n",
      "Average training loss: 0.028677799769573742\n",
      "Average test loss: 0.0042667842619121075\n",
      "Epoch 76/300\n",
      "Average training loss: 0.028625242890583144\n",
      "Average test loss: 0.004361866096034646\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02858877469599247\n",
      "Average test loss: 0.004251313866840469\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02854772086441517\n",
      "Average test loss: 0.004395685699250963\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02868535202741623\n",
      "Average test loss: 0.004543131266617113\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028729170272747677\n",
      "Average test loss: 0.0045067475376029805\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02854204357829359\n",
      "Average test loss: 0.004351974505517218\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02840340072909991\n",
      "Average test loss: 0.004208151528404818\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02831477920876609\n",
      "Average test loss: 0.004447850902875265\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028425551982389555\n",
      "Average test loss: 0.004341658846785625\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02858436460296313\n",
      "Average test loss: 0.0042699769184821185\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02830584220919344\n",
      "Average test loss: 0.004441326765964429\n",
      "Epoch 87/300\n",
      "Average training loss: 0.028307239822215505\n",
      "Average test loss: 0.004381800907767481\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02835255693561501\n",
      "Average test loss: 0.004772726751656996\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028235139768984584\n",
      "Average test loss: 0.004242328204628494\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02832116362452507\n",
      "Average test loss: 0.004256315430419312\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02835510456562042\n",
      "Average test loss: 0.004433201468446188\n",
      "Epoch 92/300\n",
      "Average training loss: 0.028244495171639655\n",
      "Average test loss: 0.0043192819634245505\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02807595087753402\n",
      "Average test loss: 0.004831785841120614\n",
      "Epoch 94/300\n",
      "Average training loss: 0.028021904175480207\n",
      "Average test loss: 0.004323019489232037\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02816730930738979\n",
      "Average test loss: 0.004627067897882727\n",
      "Epoch 96/300\n",
      "Average training loss: 0.028171780467033385\n",
      "Average test loss: 0.004503976979189449\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02804347204665343\n",
      "Average test loss: 0.00429843717151218\n",
      "Epoch 98/300\n",
      "Average training loss: 0.028060856194959745\n",
      "Average test loss: 0.004272792036334673\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028092127399312126\n",
      "Average test loss: 0.004308963847450084\n",
      "Epoch 100/300\n",
      "Average training loss: 0.028022906884551047\n",
      "Average test loss: 0.004247489557911952\n",
      "Epoch 101/300\n",
      "Average training loss: 0.027875105629364648\n",
      "Average test loss: 0.004488457470718357\n",
      "Epoch 102/300\n",
      "Average training loss: 0.027995550842748747\n",
      "Average test loss: 0.0042054004590544435\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02789458617899153\n",
      "Average test loss: 0.004268981819351514\n",
      "Epoch 104/300\n",
      "Average training loss: 0.027939768117335107\n",
      "Average test loss: 0.004204450386679835\n",
      "Epoch 105/300\n",
      "Average training loss: 0.027907885404096708\n",
      "Average test loss: 0.006974464912381437\n",
      "Epoch 106/300\n",
      "Average training loss: 0.027880270815557905\n",
      "Average test loss: 0.00432197656441066\n",
      "Epoch 107/300\n",
      "Average training loss: 0.027801776331331993\n",
      "Average test loss: 0.004229264151718881\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02777248490187857\n",
      "Average test loss: 0.00449335370088617\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02790238297316763\n",
      "Average test loss: 0.004181456503768762\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027742924920386738\n",
      "Average test loss: 0.005389049117763837\n",
      "Epoch 111/300\n",
      "Average training loss: 0.027704516059822507\n",
      "Average test loss: 0.007573396576361524\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027677310095893012\n",
      "Average test loss: 0.004382134774078925\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02773197194437186\n",
      "Average test loss: 0.004209257828278674\n",
      "Epoch 114/300\n",
      "Average training loss: 0.027789522647857667\n",
      "Average test loss: 0.004316891022026539\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02782934812704722\n",
      "Average test loss: 0.004367154296901491\n",
      "Epoch 116/300\n",
      "Average training loss: 0.027635615420010355\n",
      "Average test loss: 0.004133829561372598\n",
      "Epoch 117/300\n",
      "Average training loss: 0.027586719863944585\n",
      "Average test loss: 0.004373642765399482\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027621320916546715\n",
      "Average test loss: 0.004188556403749519\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02754402096238401\n",
      "Average test loss: 0.0042176583547972974\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02763259805076652\n",
      "Average test loss: 0.07968394226498074\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027537015830477077\n",
      "Average test loss: 0.004246382782028781\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027451684282885656\n",
      "Average test loss: 0.004197804135994779\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027559115969472463\n",
      "Average test loss: 0.004165943323738045\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027498556968238618\n",
      "Average test loss: 0.004130570845471488\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0275723002569543\n",
      "Average test loss: 0.004434047418336073\n",
      "Epoch 126/300\n",
      "Average training loss: 0.027477247428562907\n",
      "Average test loss: 0.004693687774654892\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02734910483823882\n",
      "Average test loss: 0.004204913742012447\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027449062395426962\n",
      "Average test loss: 0.004177345301542017\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02740378991762797\n",
      "Average test loss: 0.00427706650365144\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027527972721391254\n",
      "Average test loss: 0.009073021947509712\n",
      "Epoch 131/300\n",
      "Average training loss: 0.027353328852189912\n",
      "Average test loss: 0.0042320807251251405\n",
      "Epoch 132/300\n",
      "Average training loss: 0.027406722161504957\n",
      "Average test loss: 0.004312292129629188\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02732117341624366\n",
      "Average test loss: 0.004242047174730234\n",
      "Epoch 134/300\n",
      "Average training loss: 0.027284470294912655\n",
      "Average test loss: 0.004234774195071724\n",
      "Epoch 135/300\n",
      "Average training loss: 0.027356228937705358\n",
      "Average test loss: 0.004334331230156951\n",
      "Epoch 136/300\n",
      "Average training loss: 0.027378470326463383\n",
      "Average test loss: 0.004257578837581807\n",
      "Epoch 137/300\n",
      "Average training loss: 0.027258579775691034\n",
      "Average test loss: 0.004288980279531744\n",
      "Epoch 138/300\n",
      "Average training loss: 0.027217890007628334\n",
      "Average test loss: 0.005204804752022028\n",
      "Epoch 139/300\n",
      "Average training loss: 0.027266812082793978\n",
      "Average test loss: 0.004536648506712582\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02723203517165449\n",
      "Average test loss: 0.004117591469859083\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02717479978998502\n",
      "Average test loss: 0.004395961239106125\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027257711658875146\n",
      "Average test loss: 0.00473306954652071\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02718028950691223\n",
      "Average test loss: 0.004092835515323612\n",
      "Epoch 144/300\n",
      "Average training loss: 0.027131903259290588\n",
      "Average test loss: 0.0042068911130643556\n",
      "Epoch 145/300\n",
      "Average training loss: 0.027203051014078987\n",
      "Average test loss: 0.004300394418338935\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027224877854188284\n",
      "Average test loss: 0.004079189098957512\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02714424667921331\n",
      "Average test loss: 0.00434104396816757\n",
      "Epoch 148/300\n",
      "Average training loss: 0.027070024970504973\n",
      "Average test loss: 0.00437486337704791\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02714101417031553\n",
      "Average test loss: 0.0049510540374451215\n",
      "Epoch 150/300\n",
      "Average training loss: 0.027023467229472267\n",
      "Average test loss: 0.00415819644017352\n",
      "Epoch 151/300\n",
      "Average training loss: 0.027098433209790124\n",
      "Average test loss: 0.0042640774964044495\n",
      "Epoch 152/300\n",
      "Average training loss: 0.026982982544435395\n",
      "Average test loss: 0.00411540993137492\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02703734396563636\n",
      "Average test loss: 0.004125323626316256\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027029675357871586\n",
      "Average test loss: 0.004064062926918268\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02699572510851754\n",
      "Average test loss: 0.004273729593286084\n",
      "Epoch 156/300\n",
      "Average training loss: 0.026932560816407203\n",
      "Average test loss: 0.00439343453798857\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02709011967976888\n",
      "Average test loss: 0.004041729448984067\n",
      "Epoch 158/300\n",
      "Average training loss: 0.026947758532232708\n",
      "Average test loss: 0.0046055660881102085\n",
      "Epoch 159/300\n",
      "Average training loss: 0.026971457067463133\n",
      "Average test loss: 0.004168011298610105\n",
      "Epoch 160/300\n",
      "Average training loss: 0.026919125975834\n",
      "Average test loss: 0.004364589487098986\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02692394342025121\n",
      "Average test loss: 0.00412523209841715\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0269243010762665\n",
      "Average test loss: 0.004259306394805511\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026914233068625133\n",
      "Average test loss: 0.004296477684751153\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026889584390653504\n",
      "Average test loss: 0.0041666062385257744\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02685040415989028\n",
      "Average test loss: 0.004282783802391755\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026832155403163697\n",
      "Average test loss: 0.008610187657591369\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026832975829641025\n",
      "Average test loss: 0.004130481386971143\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026845110623372925\n",
      "Average test loss: 0.004095688306209114\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026784878932767445\n",
      "Average test loss: 0.004246361304902368\n",
      "Epoch 170/300\n",
      "Average training loss: 0.026790135963923403\n",
      "Average test loss: 0.004050175888256894\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026862645357847215\n",
      "Average test loss: 0.004122348761703405\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026796612946523562\n",
      "Average test loss: 0.004195594726130366\n",
      "Epoch 173/300\n",
      "Average training loss: 0.026797266552845637\n",
      "Average test loss: 0.0041440669376817015\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02683656778600481\n",
      "Average test loss: 0.004113665426149964\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02677827240029971\n",
      "Average test loss: 0.00414062221451766\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026736488569113943\n",
      "Average test loss: 0.00417959125381377\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026672915521595213\n",
      "Average test loss: 0.004137203243664569\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02675152360399564\n",
      "Average test loss: 0.004171429314547115\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02671787445412742\n",
      "Average test loss: 0.004145772400001685\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026731748807761403\n",
      "Average test loss: 0.00437050987366173\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026776717336641418\n",
      "Average test loss: 0.004132573129816188\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02668534580204222\n",
      "Average test loss: 0.004317271895706654\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02667769196960661\n",
      "Average test loss: 0.004180236745418774\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026721666188703645\n",
      "Average test loss: 0.0040757769793272015\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026614889580342506\n",
      "Average test loss: 0.004144938679618968\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026653979786568218\n",
      "Average test loss: 0.0049520042025380665\n",
      "Epoch 187/300\n",
      "Average training loss: 0.026665824186470774\n",
      "Average test loss: 0.004105132260463304\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02655633812977208\n",
      "Average test loss: 0.0042026425786316395\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026629075017240312\n",
      "Average test loss: 0.0041176681232949095\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026567280542519358\n",
      "Average test loss: 0.00494308373497592\n",
      "Epoch 191/300\n",
      "Average training loss: 0.026553050054444208\n",
      "Average test loss: 0.00486868772862686\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02664091479447153\n",
      "Average test loss: 0.004121411388946904\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02660506208240986\n",
      "Average test loss: 0.004169554162356589\n",
      "Epoch 194/300\n",
      "Average training loss: 0.026704381727510027\n",
      "Average test loss: 0.004915980033576488\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026514846925934157\n",
      "Average test loss: 0.0041484308255215485\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02656037219862143\n",
      "Average test loss: 0.004260576793303092\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0265444228640861\n",
      "Average test loss: 0.004246406928532653\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02647719062368075\n",
      "Average test loss: 0.0041187063898477285\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02647730069855849\n",
      "Average test loss: 0.004275260122285949\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026446195738183128\n",
      "Average test loss: 0.0041314298957586286\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02647205686900351\n",
      "Average test loss: 0.0043386350857714815\n",
      "Epoch 202/300\n",
      "Average training loss: 0.026530765023496414\n",
      "Average test loss: 0.004081511467281315\n",
      "Epoch 203/300\n",
      "Average training loss: 0.026489946552448802\n",
      "Average test loss: 0.004252571081742644\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026401044006148976\n",
      "Average test loss: 0.004269803885370493\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02667003126608001\n",
      "Average test loss: 0.004218670861588584\n",
      "Epoch 206/300\n",
      "Average training loss: 0.026534755150477093\n",
      "Average test loss: 0.004241664289186398\n",
      "Epoch 207/300\n",
      "Average training loss: 0.026454412708679834\n",
      "Average test loss: 0.0041435669921338555\n",
      "Epoch 208/300\n",
      "Average training loss: 0.026308132282561725\n",
      "Average test loss: 0.004099683119604985\n",
      "Epoch 209/300\n",
      "Average training loss: 0.026389593303203583\n",
      "Average test loss: 0.004218852791935205\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02643353895843029\n",
      "Average test loss: 0.004113875399033229\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02643727224154605\n",
      "Average test loss: 0.00411964636709955\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026439026892185212\n",
      "Average test loss: 0.004034334528777334\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026400987659891448\n",
      "Average test loss: 0.004671606267078055\n",
      "Epoch 214/300\n",
      "Average training loss: 0.026362903990679316\n",
      "Average test loss: 0.004447493046936061\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02636364279190699\n",
      "Average test loss: 0.004014144409447909\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026367435763279597\n",
      "Average test loss: 0.004018019950224294\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026344680496388013\n",
      "Average test loss: 0.004042837753891945\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026374295426739587\n",
      "Average test loss: 0.004795169813351498\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026350338384509085\n",
      "Average test loss: 0.004373065860321124\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026330890085962084\n",
      "Average test loss: 0.004203974447316594\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026311693226297696\n",
      "Average test loss: 0.004031896817187468\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026258479818701743\n",
      "Average test loss: 0.004086135155210892\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026461552970939214\n",
      "Average test loss: 0.004199067284870479\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026312023260527187\n",
      "Average test loss: 0.004072447390812967\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026286044422123168\n",
      "Average test loss: 0.004239292921705378\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026244320839643478\n",
      "Average test loss: 0.004150458296553956\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02626404711769687\n",
      "Average test loss: 0.004186875797394249\n",
      "Epoch 228/300\n",
      "Average training loss: 0.026316055928667387\n",
      "Average test loss: 0.0040343026295304295\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026234854471352365\n",
      "Average test loss: 0.004761931239730782\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026288534065087635\n",
      "Average test loss: 0.004078709447549449\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026294924274086952\n",
      "Average test loss: 0.004174711565590567\n",
      "Epoch 232/300\n",
      "Average training loss: 0.026245865901311238\n",
      "Average test loss: 0.004116194150721034\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02623345111641619\n",
      "Average test loss: 0.004053752733601464\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02616145470738411\n",
      "Average test loss: 0.0040792717043724325\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026254141435027124\n",
      "Average test loss: 0.004240651957069834\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02617222993241416\n",
      "Average test loss: 0.004015267305283083\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026214268467492526\n",
      "Average test loss: 0.004053071914447678\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026232561874720787\n",
      "Average test loss: 0.004385685720377498\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026102337857087453\n",
      "Average test loss: 0.004235750617252456\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026169996354315016\n",
      "Average test loss: 0.0042688394284082784\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026129571931229698\n",
      "Average test loss: 0.004043255074363616\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026165804300043317\n",
      "Average test loss: 0.00402865861935748\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02607583642668194\n",
      "Average test loss: 0.004182507072885831\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0262189798951149\n",
      "Average test loss: 0.0041407686935530765\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02613541936708821\n",
      "Average test loss: 0.004128499814826581\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02609204751584265\n",
      "Average test loss: 0.004139939122729831\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02612172614203559\n",
      "Average test loss: 0.00424487822709812\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026025238871574402\n",
      "Average test loss: 0.0041418675825827655\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026149089341362317\n",
      "Average test loss: 0.004101755566067166\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02602749234272374\n",
      "Average test loss: 0.0041819717598458135\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02617820717725489\n",
      "Average test loss: 0.004719900863038169\n",
      "Epoch 252/300\n",
      "Average training loss: 0.026048632378379503\n",
      "Average test loss: 0.004038677473035123\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02597986730022563\n",
      "Average test loss: 0.004157983486437135\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026028480908936925\n",
      "Average test loss: 0.0041658603035741384\n",
      "Epoch 255/300\n",
      "Average training loss: 0.026130927092499204\n",
      "Average test loss: 0.004203030662404166\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026008884698152542\n",
      "Average test loss: 0.004763514194223615\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02600949135257138\n",
      "Average test loss: 0.004005043547186587\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02602195216053062\n",
      "Average test loss: 0.004112566880881787\n",
      "Epoch 263/300\n",
      "Average training loss: 0.026079107658730612\n",
      "Average test loss: 0.004134053586257828\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02601161956290404\n",
      "Average test loss: 0.004496115735421578\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025926512685086993\n",
      "Average test loss: 0.010944064633713828\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02599557348423534\n",
      "Average test loss: 0.004118460059165954\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02612546712325679\n",
      "Average test loss: 0.00461633138358593\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025933565595083766\n",
      "Average test loss: 0.005157781566596693\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025923171920908823\n",
      "Average test loss: 0.004234864823934105\n",
      "Epoch 270/300\n",
      "Average training loss: 0.025951482089029417\n",
      "Average test loss: 0.004131078480846352\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02592075901064608\n",
      "Average test loss: 0.00407024876607789\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025929105492101777\n",
      "Average test loss: 0.004018805651822024\n",
      "Epoch 277/300\n",
      "Average training loss: 0.025959968101647166\n",
      "Average test loss: 0.00404402549440662\n",
      "Epoch 278/300\n",
      "Average training loss: 0.025868494333492385\n",
      "Average test loss: 0.0040572560547540585\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02589492879476812\n",
      "Average test loss: 0.004101163749479585\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025927128598093988\n",
      "Average test loss: 0.004189476349701484\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025884123090240692\n",
      "Average test loss: 0.004187168750084109\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02590887799196773\n",
      "Average test loss: 0.004023354574210114\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02581242143445545\n",
      "Average test loss: 0.0041268349070515895\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02584083340730932\n",
      "Average test loss: 0.0040947899133380915\n",
      "Epoch 285/300\n",
      "Average training loss: 0.025869059266315567\n",
      "Average test loss: 0.004090902161267069\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02584595121277703\n",
      "Average test loss: 0.004021308269351721\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025820360167158975\n",
      "Average test loss: 0.004171063139103353\n",
      "Epoch 288/300\n",
      "Average training loss: 0.025799036979675293\n",
      "Average test loss: 0.004106304261419508\n",
      "Epoch 289/300\n",
      "Average training loss: 0.025807755688826244\n",
      "Average test loss: 0.004119288371461961\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02582071557144324\n",
      "Average test loss: 0.009389837147461043\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02596575154032972\n",
      "Average test loss: 0.004129377126900686\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02582999119328128\n",
      "Average test loss: 0.004060702424496412\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025789447537726825\n",
      "Average test loss: 0.05971585338976648\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025798602918783824\n",
      "Average test loss: 0.00402033998651637\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025741180333826278\n",
      "Average test loss: 0.004016417840702666\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025930262610316276\n",
      "Average test loss: 0.004420659141821994\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025813721130291623\n",
      "Average test loss: 0.004062053841228286\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025791788225372633\n",
      "Average test loss: 0.004040872620418668\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025771101117134095\n",
      "Average test loss: 0.004360920187085867\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025773133289482858\n",
      "Average test loss: 0.004122907388955354\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15051465997431013\n",
      "Average test loss: 0.007258232888248232\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05294812334246106\n",
      "Average test loss: 0.006737710022264057\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0454716412127018\n",
      "Average test loss: 0.005822865939388673\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04222959380017387\n",
      "Average test loss: 0.007057685054424737\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04003631730212106\n",
      "Average test loss: 0.004905862069171336\n",
      "Epoch 6/300\n",
      "Average training loss: 0.036972341305679744\n",
      "Average test loss: 0.005543233885119359\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03131527343061235\n",
      "Average test loss: 0.004592955787562662\n",
      "Epoch 13/300\n",
      "Average training loss: 0.030620563950803543\n",
      "Average test loss: 0.0039894823015977935\n",
      "Epoch 14/300\n",
      "Average training loss: 0.029945690972937478\n",
      "Average test loss: 0.004288127288636234\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02935721014274491\n",
      "Average test loss: 0.004753465192599429\n",
      "Epoch 16/300\n",
      "Average training loss: 0.028692001569602225\n",
      "Average test loss: 0.0038601715717878605\n",
      "Epoch 17/300\n",
      "Average training loss: 0.028318284274803266\n",
      "Average test loss: 0.0036000934640566507\n",
      "Epoch 18/300\n",
      "Average training loss: 0.028029328347908125\n",
      "Average test loss: 0.0035039814996222656\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02783525043229262\n",
      "Average test loss: 0.014659989681508806\n",
      "Epoch 20/300\n",
      "Average training loss: 0.027284084550208514\n",
      "Average test loss: 0.0036541182941032782\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02710892544521226\n",
      "Average test loss: 0.003971102389196555\n",
      "Epoch 22/300\n",
      "Average training loss: 0.026871275736225976\n",
      "Average test loss: 0.0036510787751111717\n",
      "Epoch 23/300\n",
      "Average training loss: 0.026779317085941633\n",
      "Average test loss: 0.004037887403948439\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02655720932284991\n",
      "Average test loss: 0.0038982170931994914\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026051493207613626\n",
      "Average test loss: 0.003507714724375142\n",
      "Epoch 26/300\n",
      "Average training loss: 0.026171381337775124\n",
      "Average test loss: 0.0042264763348632385\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02585267549422052\n",
      "Average test loss: 0.0034341117522368827\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02512266174952189\n",
      "Average test loss: 0.0033993322749932605\n",
      "Epoch 32/300\n",
      "Average training loss: 0.025073409204681714\n",
      "Average test loss: 0.003514358912491136\n",
      "Epoch 33/300\n",
      "Average training loss: 0.025118394853340255\n",
      "Average test loss: 0.0035507144675486616\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025195955640739866\n",
      "Average test loss: 0.003780450262957149\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024832774463627073\n",
      "Average test loss: 0.00326436908211973\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024942494927181137\n",
      "Average test loss: 0.0034551431760191918\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024666595364610355\n",
      "Average test loss: 0.0034664258923795487\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024632253828975888\n",
      "Average test loss: 0.003425190457039409\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0245532018409835\n",
      "Average test loss: 0.003432905164443784\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024446555947264036\n",
      "Average test loss: 0.0041573251131922\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02447816541294257\n",
      "Average test loss: 0.003214282702033718\n",
      "Epoch 42/300\n",
      "Average training loss: 0.024273356404569416\n",
      "Average test loss: 0.0032111825947132373\n",
      "Epoch 43/300\n",
      "Average training loss: 0.024396048640211422\n",
      "Average test loss: 0.0031213219857050313\n",
      "Epoch 44/300\n",
      "Average training loss: 0.024249696317646237\n",
      "Average test loss: 0.0036178070058425268\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024216613012883397\n",
      "Average test loss: 0.003734273773514562\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02397599562174744\n",
      "Average test loss: 0.003161471935816937\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0240335276507669\n",
      "Average test loss: 0.0032962149443725745\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02382751420305835\n",
      "Average test loss: 0.0033000999879505897\n",
      "Epoch 49/300\n",
      "Average training loss: 0.023861086453000703\n",
      "Average test loss: 0.0037182380863361886\n",
      "Epoch 50/300\n",
      "Average training loss: 0.023854763163460625\n",
      "Average test loss: 0.003142986876683103\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02369529094050328\n",
      "Average test loss: 0.0032515294431812235\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02364654404587216\n",
      "Average test loss: 0.0031391162218319044\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023657403661145104\n",
      "Average test loss: 0.003117491895953814\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02360014868941572\n",
      "Average test loss: 0.0030899490223576624\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02343048463927375\n",
      "Average test loss: 0.0032077394930852786\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02345287758111954\n",
      "Average test loss: 0.003025381785713964\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023500118787089984\n",
      "Average test loss: 0.003084481062160598\n",
      "Epoch 58/300\n",
      "Average training loss: 0.023425729295445813\n",
      "Average test loss: 0.0032021560087386104\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02334039781159825\n",
      "Average test loss: 0.0032176770116719936\n",
      "Epoch 60/300\n",
      "Average training loss: 0.023296617266204623\n",
      "Average test loss: 0.0032281721252948046\n",
      "Epoch 61/300\n",
      "Average training loss: 0.023195594034261174\n",
      "Average test loss: 0.0033431077885131043\n",
      "Epoch 62/300\n",
      "Average training loss: 0.023065374986992942\n",
      "Average test loss: 0.0032984360853830974\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02323702075249619\n",
      "Average test loss: 0.0032193190296077065\n",
      "Epoch 64/300\n",
      "Average training loss: 0.023136887955996725\n",
      "Average test loss: 0.003229377571079466\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02314736077023877\n",
      "Average test loss: 0.004545657822448346\n",
      "Epoch 66/300\n",
      "Average training loss: 0.023024685124556223\n",
      "Average test loss: 0.0031507166247400973\n",
      "Epoch 67/300\n",
      "Average training loss: 0.023047058138582443\n",
      "Average test loss: 0.0031566332423438627\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0228810957380467\n",
      "Average test loss: 0.0032457736674696208\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022705619815323087\n",
      "Average test loss: 0.003141755344553126\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022869511786434385\n",
      "Average test loss: 0.0031667862050235272\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02270835027595361\n",
      "Average test loss: 0.0030405097930795615\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02273211994767189\n",
      "Average test loss: 0.003088502064761188\n",
      "Epoch 76/300\n",
      "Average training loss: 0.022715448562469747\n",
      "Average test loss: 0.0030578776095062494\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02279498368750016\n",
      "Average test loss: 0.0030517886510739722\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02273340657684538\n",
      "Average test loss: 0.004465392145017783\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02258899194498857\n",
      "Average test loss: 0.003277807626459334\n",
      "Epoch 80/300\n",
      "Average training loss: 0.022515983973940213\n",
      "Average test loss: 0.003090889505421122\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022518627115421824\n",
      "Average test loss: 0.0030848992189599408\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02264169792003102\n",
      "Average test loss: 0.003083108186307881\n",
      "Epoch 83/300\n",
      "Average training loss: 0.022630614411499765\n",
      "Average test loss: 0.003108071433587207\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02239830330842071\n",
      "Average test loss: 0.003517379546744956\n",
      "Epoch 85/300\n",
      "Average training loss: 0.022522933368881542\n",
      "Average test loss: 0.003184223936042852\n",
      "Epoch 86/300\n",
      "Average training loss: 0.022424358136124083\n",
      "Average test loss: 0.003239150948615538\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02233779942161507\n",
      "Average test loss: 0.00301728688677152\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02227236601213614\n",
      "Average test loss: 0.0029600555976438854\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022356528109974332\n",
      "Average test loss: 0.003115890204699503\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022250431271062958\n",
      "Average test loss: 0.0030582480964561304\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02224028850760725\n",
      "Average test loss: 0.0030390216608842213\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022232595346040197\n",
      "Average test loss: 0.002985137788578868\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022157381296157838\n",
      "Average test loss: 0.0030263204454547827\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02218521606591013\n",
      "Average test loss: 0.003150826436157028\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022159173727035523\n",
      "Average test loss: 0.003295464739203453\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022190434204207528\n",
      "Average test loss: 0.0030248400221268338\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02213466726243496\n",
      "Average test loss: 0.0032872778094477123\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022159386871589554\n",
      "Average test loss: 0.003009556840484341\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022048289078805183\n",
      "Average test loss: 0.0034449280475576717\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02207146994769573\n",
      "Average test loss: 0.018791035615735585\n",
      "Epoch 105/300\n",
      "Average training loss: 0.022027771226233905\n",
      "Average test loss: 0.00295074121053848\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0220058404488696\n",
      "Average test loss: 0.003079057248102294\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022014775953359073\n",
      "Average test loss: 0.003036570890910096\n",
      "Epoch 112/300\n",
      "Average training loss: 0.021876073453161452\n",
      "Average test loss: 0.007404147458573183\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021906079932219453\n",
      "Average test loss: 0.0029470096358822453\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021811147317290305\n",
      "Average test loss: 0.0030318490610354476\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02186778463754389\n",
      "Average test loss: 0.003012239455141955\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021888119457496538\n",
      "Average test loss: 0.005485397948573033\n",
      "Epoch 117/300\n",
      "Average training loss: 0.021778752653135194\n",
      "Average test loss: 0.0029444158644311956\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021816492247912617\n",
      "Average test loss: 0.004126609500290619\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02188034894400173\n",
      "Average test loss: 0.0029141244499219787\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021745579830474322\n",
      "Average test loss: 0.00310609686623017\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021751773041155602\n",
      "Average test loss: 0.002964057046920061\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0217550288654036\n",
      "Average test loss: 0.0029475481727470956\n",
      "Epoch 123/300\n",
      "Average training loss: 0.021761498416463534\n",
      "Average test loss: 0.0031735782213509083\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021696526990996466\n",
      "Average test loss: 0.002951019031306108\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02161893229931593\n",
      "Average test loss: 0.003460216093187531\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021649264704849985\n",
      "Average test loss: 0.0030017101948873865\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02160320055981477\n",
      "Average test loss: 0.00307396663311455\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02155258384346962\n",
      "Average test loss: 0.003139832099278768\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021630715418193076\n",
      "Average test loss: 0.0031499203633930948\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02154364119635688\n",
      "Average test loss: 0.007167218015011814\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021562384641832775\n",
      "Average test loss: 0.002984188584403859\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021580820941262777\n",
      "Average test loss: 0.0037598846734811864\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021557950334416494\n",
      "Average test loss: 0.0029178989571001794\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021526147403650813\n",
      "Average test loss: 0.002956704229944282\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021608436761630906\n",
      "Average test loss: 0.0036249138936400413\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021679443303081726\n",
      "Average test loss: 0.0030013746343966986\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02146915944914023\n",
      "Average test loss: 0.00338860126853817\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02155685961412059\n",
      "Average test loss: 0.0031946514542731974\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021426519965132076\n",
      "Average test loss: 0.008617692450682323\n",
      "Epoch 144/300\n",
      "Average training loss: 0.021429503044320477\n",
      "Average test loss: 0.002898468984083997\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021421093687415124\n",
      "Average test loss: 0.0031189384448031586\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021404741873343784\n",
      "Average test loss: 0.002921562400749988\n",
      "Epoch 147/300\n",
      "Average training loss: 0.021403968526257408\n",
      "Average test loss: 0.00310680072216524\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021334122787747118\n",
      "Average test loss: 0.0030107642021030188\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021336169119510386\n",
      "Average test loss: 0.0031253451181368695\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021343299218349987\n",
      "Average test loss: 0.0029418219315509\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021389988492760394\n",
      "Average test loss: 0.0029412072288493313\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02142280444006125\n",
      "Average test loss: 0.0029206313128686615\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021310173521439234\n",
      "Average test loss: 0.0028702347818762063\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02130976067483425\n",
      "Average test loss: 0.0030907817053505114\n",
      "Epoch 155/300\n",
      "Average training loss: 0.021292653451363247\n",
      "Average test loss: 0.0030649036591251693\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02138988834950659\n",
      "Average test loss: 0.0029105481430888175\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021332662601437832\n",
      "Average test loss: 0.003207081319971217\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02125875432540973\n",
      "Average test loss: 0.0029924102938837477\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021235518147548038\n",
      "Average test loss: 0.0034432413358655243\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021222744334075186\n",
      "Average test loss: 0.003068238822122415\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021285459849569532\n",
      "Average test loss: 0.002872816246209873\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021361700144078996\n",
      "Average test loss: 0.0030709832908792627\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021198105424642563\n",
      "Average test loss: 0.004996085297730234\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02118915866315365\n",
      "Average test loss: 0.003031639128509495\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021171597881449594\n",
      "Average test loss: 0.0031924905677636465\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02110385049879551\n",
      "Average test loss: 0.003261371799641185\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02114001476764679\n",
      "Average test loss: 0.002970766524473826\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02110446340507931\n",
      "Average test loss: 0.003042766306549311\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02111177636600203\n",
      "Average test loss: 0.0032514556613233354\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021181873712274764\n",
      "Average test loss: 0.003106782839530044\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021046460279160076\n",
      "Average test loss: 0.0028870218406534857\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021089318744010394\n",
      "Average test loss: 0.002865966268090738\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02112221810221672\n",
      "Average test loss: 0.0030348081058926054\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021169954838024245\n",
      "Average test loss: 0.013402250193059444\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02107669560611248\n",
      "Average test loss: 0.0028850214346829387\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02110793013870716\n",
      "Average test loss: 0.002933556548630198\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02106897948185603\n",
      "Average test loss: 0.002850412805668182\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021073850754234527\n",
      "Average test loss: 0.003122840531170368\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021060525765021643\n",
      "Average test loss: 0.002982047751545906\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021082578101091914\n",
      "Average test loss: 0.002937338351789448\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02096166308886475\n",
      "Average test loss: 0.0029752814510009356\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02095976743598779\n",
      "Average test loss: 0.002863706830682026\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021073696687817573\n",
      "Average test loss: 0.0028898455999377703\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02105455836488141\n",
      "Average test loss: 0.0037761414241459634\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0209632416350974\n",
      "Average test loss: 0.002963081317850285\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020916961853702863\n",
      "Average test loss: 0.0031110927342540688\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020984365277820164\n",
      "Average test loss: 0.0028662803700814643\n",
      "Epoch 192/300\n",
      "Average training loss: 0.020968552347686555\n",
      "Average test loss: 0.002872017902839515\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02094198375609186\n",
      "Average test loss: 0.0028710122058788934\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021016920079787574\n",
      "Average test loss: 0.002878390458515949\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020970016611946952\n",
      "Average test loss: 0.0029113325350400474\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020862941334644954\n",
      "Average test loss: 0.003019932174020343\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02089140155249172\n",
      "Average test loss: 0.0031686044817583426\n",
      "Epoch 201/300\n",
      "Average training loss: 0.020929486847586103\n",
      "Average test loss: 0.003043115675656332\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020899360355403688\n",
      "Average test loss: 0.0032652834956016807\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020935101464390756\n",
      "Average test loss: 0.002905509935484992\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02083926270239883\n",
      "Average test loss: 0.0029110528512133493\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02082529999481307\n",
      "Average test loss: 0.002946125177666545\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020775494280788634\n",
      "Average test loss: 0.002890399263136917\n",
      "Epoch 207/300\n",
      "Average training loss: 0.020870984580781723\n",
      "Average test loss: 0.0032193194342156253\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02076775055627028\n",
      "Average test loss: 0.002928542061191466\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02081777846068144\n",
      "Average test loss: 0.002899496799748805\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020831663250923158\n",
      "Average test loss: 0.0034452409317923915\n",
      "Epoch 211/300\n",
      "Average training loss: 0.020844321875108613\n",
      "Average test loss: 0.0032710141158766217\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020792986541986466\n",
      "Average test loss: 0.002975962485290236\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020761567797925738\n",
      "Average test loss: 0.002983659382081694\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020790395208530956\n",
      "Average test loss: 0.0030136763215478923\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02084683975742923\n",
      "Average test loss: 0.0028283616699692274\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020801638573408127\n",
      "Average test loss: 0.002936337008658383\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02075546076397101\n",
      "Average test loss: 0.0029771693324049313\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020842682962616283\n",
      "Average test loss: 0.003404621430569225\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020816732292373977\n",
      "Average test loss: 0.0030788905337038965\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0207365221372909\n",
      "Average test loss: 0.0029694857500079607\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02077085348467032\n",
      "Average test loss: 0.002907629978946514\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020741765292154418\n",
      "Average test loss: 0.00289123481263717\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0206914221164253\n",
      "Average test loss: 0.002934504684888654\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02070271409054597\n",
      "Average test loss: 0.002883673610165715\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020720950660606224\n",
      "Average test loss: 0.0028981671370565893\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02066798796918657\n",
      "Average test loss: 0.0029411680212037433\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020748177299896874\n",
      "Average test loss: 0.0028581653874781396\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020694120483265983\n",
      "Average test loss: 0.002826355083949036\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02065566067232026\n",
      "Average test loss: 0.002865574776919352\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020612075151668654\n",
      "Average test loss: 0.00288272619061172\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02065901937418514\n",
      "Average test loss: 0.003102839414237274\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0206362066153023\n",
      "Average test loss: 0.0028679943995343314\n",
      "Epoch 238/300\n",
      "Average training loss: 0.020612183888753254\n",
      "Average test loss: 0.003252008582982752\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02056649853206343\n",
      "Average test loss: 0.0028553244119717013\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02062635728551282\n",
      "Average test loss: 0.0028707981277257206\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020618838497334056\n",
      "Average test loss: 0.00282091793190274\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02058253519733747\n",
      "Average test loss: 0.0029057914306306176\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020643453436593216\n",
      "Average test loss: 0.002932981016321315\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020531725116074085\n",
      "Average test loss: 0.0030892907745308346\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020571069137917626\n",
      "Average test loss: 0.0028869369884745944\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020555636485417684\n",
      "Average test loss: 0.0032626532688736914\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020507147144940164\n",
      "Average test loss: 0.0031013200908071464\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020533486786815856\n",
      "Average test loss: 0.0028718227344668576\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020572463883294\n",
      "Average test loss: 0.0028445831560012366\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020473843433790738\n",
      "Average test loss: 0.01691419383055634\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02052512463927269\n",
      "Average test loss: 0.0028221724950191047\n",
      "Epoch 256/300\n",
      "Average training loss: 0.020562413224743472\n",
      "Average test loss: 0.0029901175063310396\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020515669592552715\n",
      "Average test loss: 0.0029555087393770617\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020457416171828904\n",
      "Average test loss: 0.0029941660492784445\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020480434070030848\n",
      "Average test loss: 0.005183912106272247\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02047785438431634\n",
      "Average test loss: 0.002892796503379941\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02048477401667171\n",
      "Average training loss: 0.020387888186507754\n",
      "Average test loss: 0.0028278879984799357\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020474358162118327\n",
      "Average test loss: 0.007036166109765569\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020439227011468674\n",
      "Average test loss: 0.0029408890689826676\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020419530015852717\n",
      "Average test loss: 0.0029083094004955555\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02041488402750757\n",
      "Average test loss: 0.008853031066556771\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020390190116233296\n",
      "Average test loss: 0.003173467488752471\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020407208729949264\n",
      "Average test loss: 0.0028483259357098075\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02033323627213637\n",
      "Average test loss: 0.0028216507012645406\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020409374016854498\n",
      "Average test loss: 0.002928380713901586\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020355405378672812\n",
      "Average test loss: 0.0029670830727037455\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02033761354453034\n",
      "Average test loss: 0.0028344815619703798\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020393340988291635\n",
      "Average test loss: 0.002861363691381282\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020297193197740447\n",
      "Average test loss: 0.0028283024333003496\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02035012544029289\n",
      "Average test loss: 0.0032420387766841385\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02037801088558303\n",
      "Average test loss: 0.0030300480843418174\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020290700821412933\n",
      "Average test loss: 0.002913415634797679\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020425227466556763\n",
      "Average test loss: 0.0032589341710425085\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020356656658980583\n",
      "Average test loss: 0.002998309079143736\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020337594792246817\n",
      "Average test loss: 0.002968164764965574\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020337109623683822\n",
      "Average test loss: 0.0028715059926940336\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020329767793416977\n",
      "Average test loss: 0.0028518332621703547\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02029912235173914\n",
      "Average test loss: 0.003179021661894189\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020307972797089152\n",
      "Average test loss: 0.002870732452099522\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020315583780407907\n",
      "Average test loss: 0.0029274898446682426\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02031356134927935\n",
      "Average test loss: 0.003962499040282435\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020312962386343215\n",
      "Average test loss: 0.0028507896904937094\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020241896861129337\n",
      "Average test loss: 0.0028596711316042477\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020307222733894983\n",
      "Average test loss: 0.0028476365386611887\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020293443322181703\n",
      "Average test loss: 0.002903324017715123\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020229131476746664\n",
      "Average test loss: 0.0028193687982857227\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02043165370821953\n",
      "Average test loss: 0.003006622863519523\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12502514650093186\n",
      "Average test loss: 0.004998784338434537\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04387881346874767\n",
      "Average test loss: 0.007108796623845895\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03020039599802759\n",
      "Average test loss: 0.008113217789265844\n",
      "Epoch 8/300\n",
      "Average training loss: 0.028670207871331108\n",
      "Average test loss: 0.004388960227784183\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0281057964000437\n",
      "Average test loss: 0.004247569595360093\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02717781037092209\n",
      "Average test loss: 0.003772304434950153\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02611618242661158\n",
      "Average test loss: 0.003219620746249954\n",
      "Epoch 12/300\n",
      "Average training loss: 0.025476494272549947\n",
      "Average test loss: 0.002898476636658112\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024845187980267736\n",
      "Average test loss: 0.0038858733690447277\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0246366421646542\n",
      "Average test loss: 0.003136598937627342\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023852399561140273\n",
      "Average test loss: 0.002629707551250855\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023526978025833766\n",
      "Average test loss: 0.0025853883802062936\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02319807326628102\n",
      "Average test loss: 0.0027118546068668364\n",
      "Epoch 18/300\n",
      "Average training loss: 0.023334409781628186\n",
      "Average test loss: 0.0032161790426406597\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022589208859536383\n",
      "Average test loss: 0.003140811872565084\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022387865900993346\n",
      "Average test loss: 0.002656428296946817\n",
      "Epoch 21/300\n",
      "Average training loss: 0.022314252465963365\n",
      "Average test loss: 0.002912704657142361\n",
      "Epoch 22/300\n",
      "Average training loss: 0.022051425046390957\n",
      "Average test loss: 0.002708949993054072\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021833689858516057\n",
      "Average test loss: 0.006457011570533117\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02186411720679866\n",
      "Average test loss: 0.002657967881816957\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021578935123152204\n",
      "Average test loss: 0.0038208301154275736\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02143817628920078\n",
      "Average test loss: 0.0026048685154981084\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021120061341259214\n",
      "Average test loss: 0.0024487261390313507\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02100145413643784\n",
      "Average test loss: 0.0024436012043928105\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021139359013901817\n",
      "Average test loss: 0.002480671716026134\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02035649385386043\n",
      "Average test loss: 0.0024773861409889326\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02040209299325943\n",
      "Average test loss: 0.005707026774684588\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020384869520862898\n",
      "Average test loss: 0.002634297149462832\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02017489200168186\n",
      "Average test loss: 0.0025182668165200286\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020057214496864213\n",
      "Average test loss: 0.0024340963976250754\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020168571368687684\n",
      "Average test loss: 0.00297286282479763\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01996047163175212\n",
      "Average test loss: 0.002519091233714587\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019999987716476122\n",
      "Average test loss: 0.0025005852147522898\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020001424552665816\n",
      "Average test loss: 0.0024665530156344176\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019760680625836056\n",
      "Average test loss: 0.0026220470430950325\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01979750033716361\n",
      "Average test loss: 0.0024984667994495894\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02002843222518762\n",
      "Average test loss: 0.002608903590693242\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019628031596541404\n",
      "Average test loss: 0.00232447565616005\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019588089944587814\n",
      "Average test loss: 0.0024424964878708125\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01967132448156675\n",
      "Average test loss: 0.002329460930493143\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0194553774015771\n",
      "Average test loss: 0.0023663675615357028\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0194522490054369\n",
      "Average test loss: 0.0034010660921533903\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019548826272288957\n",
      "Average test loss: 0.0022786555474417075\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019363324377271864\n",
      "Average test loss: 0.0024417479624971746\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01936998066968388\n",
      "Average test loss: 0.0023565288862834374\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019348341428571277\n",
      "Average test loss: 0.00245463376533654\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019180105141467518\n",
      "Average test loss: 0.0023095110601021184\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019360010594129562\n",
      "Average test loss: 0.0035129660103056168\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019524255582027965\n",
      "Average test loss: 0.0023929881087193885\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019021929408941003\n",
      "Average test loss: 0.002253955272440281\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019125744147433173\n",
      "Average test loss: 0.00230282361081077\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018918007330762016\n",
      "Average test loss: 0.0022320328038185834\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019157208917869463\n",
      "Average test loss: 0.002443724204475681\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01917049337923527\n",
      "Average test loss: 0.0022679495867341756\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018868365744749704\n",
      "Average test loss: 0.0021981791572438345\n",
      "Epoch 64/300\n",
      "Average training loss: 0.019075889413555463\n",
      "Average test loss: 0.002273168765836292\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018882166453533702\n",
      "Average test loss: 0.006567054833802912\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018830409397681554\n",
      "Average test loss: 0.0026834235679772164\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01874659217480156\n",
      "Average test loss: 0.0024172694958332513\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01874451995227072\n",
      "Average test loss: 0.00248114816347758\n",
      "Epoch 73/300\n",
      "Average training loss: 0.018707815347446335\n",
      "Average test loss: 0.009818235172165764\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018700724431210094\n",
      "Average test loss: 0.0023863566437115273\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018688451894455487\n",
      "Average test loss: 0.002496234116040998\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018596627339720725\n",
      "Average test loss: 0.0022133493170970017\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018617174333996244\n",
      "Average test loss: 0.002274609847408202\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018552743760248024\n",
      "Average test loss: 0.0027071561202820804\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018566378470924166\n",
      "Average test loss: 0.002898181366837687\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018617639932367536\n",
      "Average test loss: 0.0022085316559920707\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018536994069814683\n",
      "Average test loss: 0.002219414319636093\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018532092308004698\n",
      "Average test loss: 0.0022271806221041412\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018386959817674426\n",
      "Average test loss: 0.0022019225125097567\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01853701868156592\n",
      "Average test loss: 0.002345019161494242\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018616738410459625\n",
      "Average test loss: 0.0034054674744192098\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018426134816474385\n",
      "Average test loss: 0.0024554774570796226\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018395469112528696\n",
      "Average test loss: 0.002345231837282578\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0183917999068896\n",
      "Average test loss: 0.0022024721704009508\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018354151555233532\n",
      "Average test loss: 0.0022934020400668183\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01829933058553272\n",
      "Average test loss: 0.0022426954658908978\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01836007220380836\n",
      "Average test loss: 0.0038869958163963423\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018175536218616697\n",
      "Average test loss: 0.0027141306716948746\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018259160728918183\n",
      "Average test loss: 0.0021883510436034865\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018214036208887896\n",
      "Average test loss: 0.002203090548308359\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01818221158451504\n",
      "Average test loss: 0.002435825613637765\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018250753940807447\n",
      "Average test loss: 0.002151501393566529\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01825926332672437\n",
      "Average test loss: 0.002226412675033013\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01810390965226624\n",
      "Average test loss: 0.0023208213781730994\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018141452287634214\n",
      "Average test loss: 0.002234097647377186\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018078236595624023\n",
      "Average test loss: 0.002228711724695232\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01817086516486274\n",
      "Average test loss: 0.01920606835020913\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01819947985973623\n",
      "Average test loss: 0.0023969807935257754\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01800847090118461\n",
      "Average test loss: 0.002217362006712291\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018007832694384787\n",
      "Average test loss: 0.002945076362333364\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018018247720268037\n",
      "Average test loss: 0.0025034288592222665\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01819086168292496\n",
      "Average test loss: 0.0023274792701833778\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017965358962615332\n",
      "Average test loss: 0.002420244017822875\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01801463639901744\n",
      "Average test loss: 0.0023448510468006135\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017990049478080538\n",
      "Average test loss: 0.0022265952283309566\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017968961464034188\n",
      "Average test loss: 0.0022005436159670353\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017918208109007943\n",
      "Average test loss: 0.002174310508287615\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017842580650415687\n",
      "Average test loss: 0.002192137728755673\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01794055655433072\n",
      "Average test loss: 0.0021827891392426357\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01778063076734543\n",
      "Average test loss: 0.0022752518579363824\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01785810094575087\n",
      "Average test loss: 0.0021257295144928826\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01782176549401548\n",
      "Average test loss: 0.0021289606822861564\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017842914750178657\n",
      "Average test loss: 0.0023139127387354773\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017685079397426712\n",
      "Average test loss: 0.025196187143110567\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017775230845643416\n",
      "Average test loss: 0.0026349292536162667\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0177692225318816\n",
      "Average test loss: 0.002178771837097075\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017887586378388935\n",
      "Average test loss: 0.0021925089442067676\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01777775109062592\n",
      "Average test loss: 0.0021060898074259362\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01766732423669762\n",
      "Average test loss: 0.0021654066308918925\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017748524917496577\n",
      "Average test loss: 0.002134346341714263\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017662266203098825\n",
      "Average test loss: 0.00213168985893329\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017682346053421498\n",
      "Average test loss: 0.0021993154264572596\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017635736319753858\n",
      "Average test loss: 0.0023474404027478564\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01764314879477024\n",
      "Average test loss: 0.0020856155283335183\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017652007829811838\n",
      "Average test loss: 0.002198440011487239\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017697056432565052\n",
      "Average test loss: 3.121166191207038\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017773347132735783\n",
      "Average test loss: 0.0021415898678824306\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017530527568525738\n",
      "Average test loss: 0.0021353406322499116\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01755313168466091\n",
      "Average test loss: 0.0021102924758775368\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017636520240041944\n",
      "Average test loss: 0.002275319725068079\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017509712170395587\n",
      "Average test loss: 0.0021278802208188506\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01763578948378563\n",
      "Average test loss: 0.0020988350744462676\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017491564525498285\n",
      "Average test loss: 0.0021390418094686336\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0174710698136025\n",
      "Average test loss: 0.00222806718862719\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017452642114626035\n",
      "Average test loss: 0.00542522444224192\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01742907577670283\n",
      "Average test loss: 0.00221874072154363\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017492665017644564\n",
      "Average test loss: 0.0021776439437849655\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017402445695466465\n",
      "Average test loss: 0.0022194152360575066\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017432418133649562\n",
      "Average test loss: 0.0022128080991614197\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01738878580596712\n",
      "Average test loss: 0.0021425229352381495\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017416274544265534\n",
      "Average test loss: 0.002152015921556287\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017455352365970612\n",
      "Average test loss: 0.002211703649411599\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01741366201473607\n",
      "Average test loss: 0.0020643188421510987\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017502865238322153\n",
      "Average test loss: 0.002135594498159157\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017425308518939548\n",
      "Average test loss: 0.002113620090815756\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01733466474380758\n",
      "Average test loss: 0.0021699260347005393\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017323504014147652\n",
      "Average test loss: 0.002076784670352936\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017354725806249513\n",
      "Average test loss: 0.0021697132779906194\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01729530852370792\n",
      "Average test loss: 0.002184024006335272\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017345085781481532\n",
      "Average test loss: 0.0022137240119692353\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017303110318051443\n",
      "Average test loss: 0.002125107817351818\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017351015367441706\n",
      "Average test loss: 0.0020715184214835365\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017288348297278086\n",
      "Average test loss: 0.0029861923418939115\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01731185323993365\n",
      "Average test loss: 0.002177533449191186\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017259885254833435\n",
      "Average test loss: 0.002566384006084667\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01726522962997357\n",
      "Average test loss: 0.0020790115512079664\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017252933705018625\n",
      "Average test loss: 0.0021725058454192345\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0172567056061493\n",
      "Average test loss: 0.0021262435718543\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01727642086148262\n",
      "Average test loss: 0.002218859925555686\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017204451200862725\n",
      "Average test loss: 0.0021051266205807527\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01718876927263207\n",
      "Average test loss: 0.002241438230292665\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01724359759522809\n",
      "Average test loss: 0.002128043305542734\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017277291792962286\n",
      "Average test loss: 0.0021232417629410824\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017159987825486396\n",
      "Average test loss: 0.0022417457899492647\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017217290767365032\n",
      "Average test loss: 0.002346708520832989\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01717782999244001\n",
      "Average test loss: 0.0021495395195153025\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017150054042538006\n",
      "Average test loss: 0.002360926560022765\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017106596507959895\n",
      "Average test loss: 0.0025180070816228787\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017085848915908072\n",
      "Average test loss: 0.0021697089396831063\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01708116624504328\n",
      "Average test loss: 0.002070295419957903\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01713618844581975\n",
      "Average test loss: 0.002211624112394121\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017160698099268807\n",
      "Average test loss: 0.002112910661432478\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017077828541398048\n",
      "Average test loss: 0.002318092816819747\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017083787857658333\n",
      "Average test loss: 0.0021612539084421263\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017136006217863823\n",
      "Average test loss: 0.002258726852428582\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017126649679409134\n",
      "Average test loss: 0.0022261962588462567\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01706092655327585\n",
      "Average test loss: 0.0020603804319269127\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017012817783488166\n",
      "Average test loss: 0.0021276277471333743\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01700730494823721\n",
      "Average test loss: 0.0020889787326256433\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017044816819330056\n",
      "Average test loss: 0.00211311807235082\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017029589475856886\n",
      "Average test loss: 0.02258421471880542\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01706294362909264\n",
      "Average test loss: 0.0021455722368425795\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017030255797836517\n",
      "Average test loss: 0.002945353355258703\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01705999194085598\n",
      "Average test loss: 0.007621596033788389\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016999248527818256\n",
      "Average test loss: 0.0021601067886998257\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01698727219965723\n",
      "Average test loss: 0.002228103017434478\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01698083593696356\n",
      "Average test loss: 0.0022175803999933933\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016985667372743287\n",
      "Average test loss: 0.0029470827978932196\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01698101219203737\n",
      "Average test loss: 0.0020958428171773754\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017001441462172404\n",
      "Average test loss: 0.002101369743131929\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01691726348631912\n",
      "Average test loss: 0.002690814831190639\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016952339271704357\n",
      "Average test loss: 0.0020868472405191924\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016959091641836697\n",
      "Average test loss: 0.0022182678703425657\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016995888958374658\n",
      "Average test loss: 0.0020646325891009636\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0168869834591945\n",
      "Average test loss: 0.07809855113426845\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016962848568128214\n",
      "Average test loss: 0.0022035064006017315\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016905805200338363\n",
      "Average test loss: 0.0020754026511891023\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01697000593940417\n",
      "Average test loss: 0.0020772475952075585\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01693644349856509\n",
      "Average test loss: 0.0021638464064647756\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016888822545607883\n",
      "Average test loss: 0.0027950514995803435\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01692029199997584\n",
      "Average test loss: 0.0021156152602699067\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016892828486031956\n",
      "Average test loss: 0.0021001651018030115\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0169033929200636\n",
      "Average test loss: 0.002152183493392335\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016879579205479885\n",
      "Average test loss: 0.002127675555025538\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0169016356004609\n",
      "Average test loss: 0.0020784482835895486\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016917290376292336\n",
      "Average test loss: 0.0020211240303599173\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016884120033846962\n",
      "Average test loss: 0.0021490980413638883\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016819245899717013\n",
      "Average test loss: 0.0021107429731637237\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01681096743212806\n",
      "Average test loss: 0.0020514501730600995\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016801351444588768\n",
      "Average test loss: 0.002188964815085961\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016833650636176267\n",
      "Average test loss: 0.0021726959037284056\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0168269358906481\n",
      "Average test loss: 0.0022389616693059603\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016875095768107307\n",
      "Average test loss: 0.0021145066747234927\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01686846325546503\n",
      "Average test loss: 0.0020991641925647854\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016802270882659488\n",
      "Average test loss: 0.002147693814502822\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016819752408398524\n",
      "Average test loss: 0.0024651146435903178\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016775578381286727\n",
      "Average test loss: 0.0021067331238753266\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016729042973783283\n",
      "Average test loss: 0.0022483839819000826\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016832245197561053\n",
      "Average test loss: 0.0020823543502224815\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016822115348445046\n",
      "Average test loss: 0.002101888532232907\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016764151935776076\n",
      "Average test loss: 0.002051226654296948\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01674927183902926\n",
      "Average test loss: 0.0028732409906677076\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016722287820445166\n",
      "Average test loss: 0.002061966818032993\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016730301382640998\n",
      "Average test loss: 0.002051979332541426\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016684880904853344\n",
      "Average test loss: 0.0021106402058568264\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01675488040347894\n",
      "Average test loss: 0.0021108047305088903\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01682783482140965\n",
      "Average test loss: 0.0020857200378345117\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01669005076918337\n",
      "Average test loss: 0.0020690838671806786\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016662206307053565\n",
      "Average test loss: 0.002107535408706301\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016697973837455114\n",
      "Average test loss: 0.002114523967728019\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016666997166971367\n",
      "Average test loss: 0.0020594237039072644\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016746065143081877\n",
      "Average test loss: 0.0020469466015282604\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01667027451015181\n",
      "Average test loss: 0.002123703351761732\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0167153655356831\n",
      "Average test loss: 0.01571194024880727\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016640689506298965\n",
      "Average test loss: 0.0020491654726987082\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016640664781961176\n",
      "Average test loss: 0.002078241557917661\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016660461156732507\n",
      "Average test loss: 0.002070223132976227\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016693466591338316\n",
      "Average test loss: 0.0021246036027765106\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016624949167172113\n",
      "Average test loss: 0.0020411159385823542\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016643037438392638\n",
      "Average test loss: 0.0020361350525377525\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016675831796394452\n",
      "Average test loss: 0.0021530092388598455\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016703740063640806\n",
      "Average test loss: 0.00238598225131217\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016603406232264306\n",
      "Average test loss: 0.002072306919739478\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01661939671304491\n",
      "Average test loss: 0.0020934569870846137\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016616826485428546\n",
      "Average test loss: 0.0022805539559986854\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016617586844497257\n",
      "Average test loss: 0.002077916845679283\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016639651834964752\n",
      "Average test loss: 0.0020780250922673277\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016557659088737436\n",
      "Average test loss: 0.0020774976571814882\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016585956044495104\n",
      "Average test loss: 0.002081241821663247\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01658693509797255\n",
      "Average test loss: 0.002075169421008064\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016634809583425524\n",
      "Average test loss: 0.0020741820952130687\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016566825050446723\n",
      "Average test loss: 0.0020567137540007632\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01655607622365157\n",
      "Average test loss: 0.0020856461541520223\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01657590913689799\n",
      "Average test loss: 0.0020491073913872244\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01654502437843217\n",
      "Average test loss: 0.002027180930910011\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016596751754482587\n",
      "Average test loss: 0.0020576013785062563\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016456171720392175\n",
      "Average test loss: 0.0026438152988751728\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016565849936670728\n",
      "Average test loss: 0.0020543175456631513\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0165947195308076\n",
      "Average test loss: 0.8356320758925544\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01761622604976098\n",
      "Average test loss: 0.0020759484972804783\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01649923204547829\n",
      "Average test loss: 0.0021249869742120306\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01650319817993376\n",
      "Average test loss: 0.002092624034939541\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016506956295834648\n",
      "Average test loss: 0.0021524201691564587\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016466227607594596\n",
      "Average test loss: 0.003439260441602932\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016533859367171923\n",
      "Average test loss: 0.0022889190836706094\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016508432178861564\n",
      "Average test loss: 0.0020989535223278734\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016514672408501307\n",
      "Average test loss: 0.0020279968582714598\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016474122065636847\n",
      "Average test loss: 0.0021359448128690324\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016559269534216988\n",
      "Average test loss: 0.0020365337640460995\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016512293194731076\n",
      "Average test loss: 0.0021080113475521406\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01649696527255906\n",
      "Average test loss: 0.0021015568568060797\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016492068734433915\n",
      "Average test loss: 0.002083391456036932\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DAMP_No_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.67\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.51\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.09\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.18\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.01\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.71\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.1800055012173125\n",
      "Average test loss: 117979568.67336014\n",
      "Epoch 2/300\n",
      "Average training loss: 4.355925323910183\n",
      "Average test loss: 67812727106926.06\n",
      "Epoch 3/300\n",
      "Average training loss: 4.082700787438287\n",
      "Average test loss: 8.383418857965204\n",
      "Epoch 4/300\n",
      "Average training loss: 3.860702891031901\n",
      "Average test loss: 0.017107017568416066\n",
      "Epoch 5/300\n",
      "Average training loss: 3.708304023530748\n",
      "Average test loss: 1.6087866895927323\n",
      "Epoch 6/300\n",
      "Average training loss: 3.3464947444068063\n",
      "Average test loss: 0.09423196072379748\n",
      "Epoch 9/300\n",
      "Average training loss: 3.2364255089230007\n",
      "Average test loss: 44.8250340353118\n",
      "Epoch 10/300\n",
      "Average training loss: 3.124351438946194\n",
      "Average test loss: 0.018570915354622734\n",
      "Epoch 11/300\n",
      "Average training loss: 3.038920180214776\n",
      "Average test loss: 0.356279423918989\n",
      "Epoch 12/300\n",
      "Average training loss: 2.929962611304389\n",
      "Average test loss: 128349.52382419162\n",
      "Epoch 13/300\n",
      "Average training loss: 2.8030720433129206\n",
      "Average test loss: 0.05465845390740368\n",
      "Epoch 14/300\n",
      "Average training loss: 2.6982770387861463\n",
      "Average test loss: 53965.96028390164\n",
      "Epoch 15/300\n",
      "Average training loss: 2.7289700090620252\n",
      "Average test loss: 0.27514107298519874\n",
      "Epoch 16/300\n",
      "Average training loss: 2.595964491314358\n",
      "Average test loss: 1032.3431636987527\n",
      "Epoch 17/300\n",
      "Average training loss: 2.5327653819190132\n",
      "Average test loss: 12.39815077398883\n",
      "Epoch 18/300\n",
      "Average training loss: 2.4190788928137885\n",
      "Average test loss: 0.3271250887612502\n",
      "Epoch 19/300\n",
      "Average training loss: 2.334368986129761\n",
      "Average test loss: 18340.78261953366\n",
      "Epoch 20/300\n",
      "Average training loss: 2.2824871277279324\n",
      "Average test loss: 1.0918859048874843\n",
      "Epoch 22/300\n",
      "Average training loss: 2.2023830466800267\n",
      "Average test loss: 372.34695684569743\n",
      "Epoch 23/300\n",
      "Average training loss: 2.0870027372572157\n",
      "Average test loss: 0.12302134616176287\n",
      "Epoch 24/300\n",
      "Average training loss: 1.989895005967882\n",
      "Average test loss: 0.009041373240451019\n",
      "Epoch 25/300\n",
      "Average training loss: 1.9072247492472332\n",
      "Average test loss: 0.5951734644236664\n",
      "Epoch 26/300\n",
      "Average training loss: 1.775656514061822\n",
      "Average test loss: 3.2645755707389776\n",
      "Epoch 27/300\n",
      "Average training loss: 1.7735313562817043\n",
      "Average test loss: 0.010271912971304522\n",
      "Epoch 28/300\n",
      "Average training loss: 1.762873021973504\n",
      "Average test loss: 0.009015537157654763\n",
      "Epoch 29/300\n",
      "Average training loss: 1.6318679503334894\n",
      "Average test loss: 0.0696571449637413\n",
      "Epoch 30/300\n",
      "Average training loss: 1.5164397217432657\n",
      "Average test loss: 0.05340991782810953\n",
      "Epoch 31/300\n",
      "Average training loss: 1.412718540933397\n",
      "Average test loss: 0.012906154738532172\n",
      "Epoch 32/300\n",
      "Average training loss: 1.307791030989753\n",
      "Average test loss: 0.009666986227035523\n",
      "Epoch 33/300\n",
      "Average training loss: 1.210675299750434\n",
      "Average test loss: 0.008135138461987178\n",
      "Epoch 34/300\n",
      "Average training loss: 1.0804122816721597\n",
      "Average test loss: 0.007633779798116949\n",
      "Epoch 35/300\n",
      "Average training loss: 0.9763660346666971\n",
      "Average test loss: 0.009840986106958655\n",
      "Epoch 36/300\n",
      "Average training loss: 0.8871992917060852\n",
      "Average test loss: 0.006876196856300036\n",
      "Epoch 37/300\n",
      "Average training loss: 0.8221932546297709\n",
      "Average test loss: 23.15487419768837\n",
      "Epoch 38/300\n",
      "Average training loss: 0.7347999916076661\n",
      "Average test loss: 0.009054311914576424\n",
      "Epoch 39/300\n",
      "Average training loss: 0.6650448253949484\n",
      "Average test loss: 0.01251693354629808\n",
      "Epoch 40/300\n",
      "Average training loss: 0.6092707452244229\n",
      "Average test loss: 0.008871489374174013\n",
      "Epoch 41/300\n",
      "Average training loss: 0.5583572209676106\n",
      "Average test loss: 0.006951931626432472\n",
      "Epoch 42/300\n",
      "Average training loss: 0.5128425534036425\n",
      "Average test loss: 0.13147675526804395\n",
      "Epoch 43/300\n",
      "Average training loss: 0.42987372030152216\n",
      "Average test loss: 0.006706469205932485\n",
      "Epoch 45/300\n",
      "Average training loss: 0.39512373299068876\n",
      "Average test loss: 0.006617530311147372\n",
      "Epoch 46/300\n",
      "Average training loss: 0.3652863070434994\n",
      "Average test loss: 0.0065628989473399185\n",
      "Epoch 47/300\n",
      "Average training loss: 0.3364819454087151\n",
      "Average test loss: 0.28333746156468986\n",
      "Epoch 48/300\n",
      "Average training loss: 0.31405313854747346\n",
      "Average test loss: 0.006695772189233038\n",
      "Epoch 49/300\n",
      "Average training loss: 0.2923569164276123\n",
      "Average test loss: 0.006775143897367848\n",
      "Epoch 50/300\n",
      "Average training loss: 0.2747392076386346\n",
      "Average test loss: 0.007083728347387579\n",
      "Epoch 51/300\n",
      "Average training loss: 0.2557769274711609\n",
      "Average test loss: 0.006680762113382419\n",
      "Epoch 52/300\n",
      "Average training loss: 0.2399738694164488\n",
      "Average test loss: 0.006335261380506886\n",
      "Epoch 53/300\n",
      "Average training loss: 0.2222850417825911\n",
      "Average test loss: 0.006712266229093075\n",
      "Epoch 54/300\n",
      "Average training loss: 0.20758837042914496\n",
      "Average test loss: 0.006720780510041449\n",
      "Epoch 55/300\n",
      "Average training loss: 0.19517146201928456\n",
      "Average test loss: 0.006372676900691456\n",
      "Epoch 56/300\n",
      "Average training loss: 0.16717371462451086\n",
      "Average test loss: 0.00654855263647106\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1573122895558675\n",
      "Average test loss: 0.007176978445715374\n",
      "Epoch 60/300\n",
      "Average training loss: 0.15303698762257895\n",
      "Average test loss: 0.006555586475878954\n",
      "Epoch 61/300\n",
      "Average training loss: 0.14840085938241745\n",
      "Average test loss: 0.006484537505441242\n",
      "Epoch 62/300\n",
      "Average training loss: 0.1473267773522271\n",
      "Average test loss: 0.006219050860653321\n",
      "Epoch 63/300\n",
      "Average training loss: 0.14092500729031032\n",
      "Average test loss: 0.0064261415878103835\n",
      "Epoch 64/300\n",
      "Average training loss: 0.13678326511383057\n",
      "Average test loss: 0.006630959513700671\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1374625471962823\n",
      "Average test loss: 0.006407948644210895\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1329808164305157\n",
      "Average test loss: 0.006224871451862984\n",
      "Epoch 67/300\n",
      "Average training loss: 0.13111734318070942\n",
      "Average test loss: 0.00629052171525028\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12923866449462043\n",
      "Average test loss: 0.005954177283578449\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12998608983887566\n",
      "Average test loss: 0.007637678388920095\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12653018838829463\n",
      "Average test loss: 0.006071673547642099\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12607344501548343\n",
      "Average test loss: 0.008766837574541568\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1248076199028227\n",
      "Average test loss: 0.00679058120234145\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12283298881848653\n",
      "Average test loss: 0.006408082141644425\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12246920590930514\n",
      "Average test loss: 0.005883350918276443\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12090108385350969\n",
      "Average test loss: 0.006008267128633128\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11940732390350765\n",
      "Average test loss: 0.006434858651624785\n",
      "Epoch 77/300\n",
      "Average training loss: 0.12070105946063996\n",
      "Average test loss: 0.006383458883398109\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11734659554560979\n",
      "Average test loss: 0.0061271875802841455\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11731444652875264\n",
      "Average test loss: 0.006341021877610021\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11560704335901473\n",
      "Average test loss: 0.00574023316303889\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11399350572956933\n",
      "Average test loss: 0.006912349830898974\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11422916850778791\n",
      "Average test loss: 0.006021233012692796\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11251024989287059\n",
      "Average test loss: 0.005909746377418439\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11249433816803826\n",
      "Average test loss: 0.005978191253625685\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11174049801296658\n",
      "Average test loss: 0.005940264061507251\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11112674136956532\n",
      "Average test loss: 0.006077094247771634\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11045721013678444\n",
      "Average test loss: 0.007126771574632989\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10981600297821893\n",
      "Average test loss: 0.005883311226963997\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10953795138332578\n",
      "Average test loss: 0.006033001818590694\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10960691795746486\n",
      "Average test loss: 0.0060507426212231315\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10973499943150414\n",
      "Average test loss: 0.0059546150726576645\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10852551417218315\n",
      "Average test loss: 0.005698738809674978\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10765988742642932\n",
      "Average test loss: 0.0056499791546828215\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10724361793862448\n",
      "Average test loss: 0.005864061952879031\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10810830526219474\n",
      "Average test loss: 0.005783346055282487\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10717937120464113\n",
      "Average test loss: 0.007063448402202792\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10683765225940281\n",
      "Average test loss: 0.0064691473866502446\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10588495123386384\n",
      "Average test loss: 0.006066465265221066\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10806932618882921\n",
      "Average test loss: 0.006129253471063243\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10560575122303432\n",
      "Average training loss: 0.10560773891210556\n",
      "Average test loss: 0.11747137415409088\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10518119683530595\n",
      "Average test loss: 0.005811584282252523\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10510813517702951\n",
      "Average test loss: 0.010570593912568357\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10610876418484581\n",
      "Average test loss: 0.005773036096245051\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10520247575971815\n",
      "Average test loss: 0.005861602541059255\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1043733876215087\n",
      "Average test loss: 0.005932472856508361\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10360734317700068\n",
      "Average test loss: 0.005817737602525287\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10417328491475847\n",
      "Average test loss: 0.0058203363141251935\n",
      "Epoch 113/300\n",
      "Average training loss: 0.1040692330400149\n",
      "Average test loss: 0.006401402817832099\n",
      "Epoch 114/300\n",
      "Average training loss: 0.1034879407286644\n",
      "Average test loss: 0.005669680078824361\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10337903783056471\n",
      "Average test loss: 0.006335504482603735\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10291001531150606\n",
      "Average test loss: 0.005690842723680867\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10266949399312338\n",
      "Average test loss: 0.005898494159181913\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10245793657832676\n",
      "Average test loss: 0.006854821945230166\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10205861613816686\n",
      "Average test loss: 0.006088345361873507\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10309425146712198\n",
      "Average test loss: 0.013918986119329929\n",
      "Epoch 122/300\n",
      "Average training loss: 0.1028817799952295\n",
      "Average test loss: 0.006251384502483739\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10212349452575048\n",
      "Average test loss: 0.005899252106539077\n",
      "Epoch 124/300\n",
      "Average training loss: 0.1019490399758021\n",
      "Average test loss: 0.005852570132248931\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10166987156205708\n",
      "Average test loss: 0.005615272304250135\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10139004360304939\n",
      "Average test loss: 0.006135576362411181\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1013881726331181\n",
      "Average test loss: 0.005625035028076834\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10118081043826209\n",
      "Average test loss: 0.005595270287659433\n",
      "Epoch 130/300\n",
      "Average training loss: 0.1009065958791309\n",
      "Average test loss: 1.189088212384118\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10089657349718942\n",
      "Average test loss: 0.005596889497505294\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10006033751037385\n",
      "Average test loss: 0.005590178099771341\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10112521149714788\n",
      "Average test loss: 0.00553170750869645\n",
      "Epoch 134/300\n",
      "Average training loss: 0.1006262975136439\n",
      "Average test loss: 0.005825389761063788\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10022557349337471\n",
      "Average test loss: 0.0057874777197009985\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09952478540605969\n",
      "Average test loss: 0.005993180035303036\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10000156013170879\n",
      "Average test loss: 0.00567932998885711\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10032615650362439\n",
      "Average test loss: 0.0056755788139998915\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09871431243419647\n",
      "Average test loss: 0.006190735728376442\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09868865872753992\n",
      "Average test loss: 0.006598577312711212\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09982452373703321\n",
      "Average test loss: 0.005741989071170489\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09850822779867384\n",
      "Average test loss: 0.005824128050771024\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09910378224982155\n",
      "Average test loss: 0.005599147695220179\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09871157821019491\n",
      "Average test loss: 0.0072813455416924425\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09893285146024491\n",
      "Average test loss: 0.0055588583971063294\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09907571646902297\n",
      "Average test loss: 0.00709749852369229\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0978446749912368\n",
      "Average test loss: 0.005868725640492307\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09873358470201492\n",
      "Average test loss: 0.00560406843572855\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09811382159921858\n",
      "Average test loss: 0.005851981479260656\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09852835014793608\n",
      "Average test loss: 0.005621557138032383\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09790304420391718\n",
      "Average test loss: 0.00553852981949846\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09749437582492829\n",
      "Average test loss: 0.005542956614659892\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0987658219271236\n",
      "Average test loss: 0.006192118537094858\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09726937739054362\n",
      "Average test loss: 0.005613767427702745\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09721706434090932\n",
      "Average test loss: 0.00564239300125175\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0973910507162412\n",
      "Average test loss: 0.00611644869007998\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09801398244169024\n",
      "Average test loss: 0.005482710238132212\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09719010166327159\n",
      "Average test loss: 0.005702463977245821\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09725681329435773\n",
      "Average test loss: 0.005704569042142895\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09640470325284534\n",
      "Average test loss: 0.005706543891794152\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09706047070026398\n",
      "Average test loss: 0.005737156285179986\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09680562077297104\n",
      "Average test loss: 0.005525472884790765\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09793836871120665\n",
      "Average test loss: 0.0059105840192900765\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09617883099450006\n",
      "Average test loss: 0.00564864052956303\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09616925022337172\n",
      "Average test loss: 0.005978986232644982\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0962698708375295\n",
      "Average test loss: 0.005429767147948345\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09614804055955675\n",
      "Average test loss: 0.005789750982903772\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09580495052205192\n",
      "Average test loss: 0.024422630333238177\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0961157223979632\n",
      "Average test loss: 0.0070503060279621015\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09586239849858814\n",
      "Average test loss: 0.005452580441203382\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09586315975586573\n",
      "Average test loss: 0.005869964888112413\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09579641691181395\n",
      "Average test loss: 0.005461087528202269\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09568524076541265\n",
      "Average test loss: 0.005536581195270022\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09533607939216826\n",
      "Average test loss: 0.005850730040007167\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09514553363455666\n",
      "Average test loss: 0.005486854779637522\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0950233629014757\n",
      "Average test loss: 0.006125522325850195\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09438517286380133\n",
      "Average test loss: 0.0054129798780712815\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09626312528716194\n",
      "Average test loss: 0.006311391981525554\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09523717974954181\n",
      "Average test loss: 0.005526112252225479\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09531734028127459\n",
      "Average test loss: 0.005587707372175323\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09496017174588309\n",
      "Average test loss: 0.007688841248138083\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09523524002896415\n",
      "Average test loss: 0.005615093203054534\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09454466218418545\n",
      "Average test loss: 0.005584349301954111\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09455286698208915\n",
      "Average test loss: 0.0054881092123687265\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09423614137702518\n",
      "Average test loss: 0.006210303814874755\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09453717503613895\n",
      "Average test loss: 0.005740426711738109\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09458767025669416\n",
      "Average test loss: 0.005702751942392853\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09414409373866187\n",
      "Average test loss: 0.0057947752105279105\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09419290738635593\n",
      "Average test loss: 0.005614030778821972\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09404734384351307\n",
      "Average test loss: 0.0056854517484704655\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09383126742972268\n",
      "Average test loss: 0.00555543539300561\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09682175239589479\n",
      "Average test loss: 0.0060837928545143865\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09340361083216138\n",
      "Average test loss: 0.005490196636153592\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09357870774136649\n",
      "Average test loss: 0.0055430760826501585\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09364134667979346\n",
      "Average test loss: 0.005701648559421301\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0929810508357154\n",
      "Average test loss: 0.005569063743783368\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09352903816435072\n",
      "Average test loss: 0.006065467896560828\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0934153569009569\n",
      "Average test loss: 0.005664363807067275\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09414220425817701\n",
      "Average test loss: 0.005591055956979593\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09302289734284083\n",
      "Average test loss: 0.0054323631700956155\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0937105178170734\n",
      "Average test loss: 0.005475611724787288\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09322305034266577\n",
      "Average test loss: 6.9340835664537215\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09318598504198922\n",
      "Average test loss: 0.005881430543959141\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0932465631167094\n",
      "Average test loss: 0.00554303244749705\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09739823167191611\n",
      "Average test loss: 0.005569714596701993\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09239764359262255\n",
      "Average test loss: 0.005539983919097317\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09250411760807037\n",
      "Average test loss: 0.006629615818046861\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09257089229424795\n",
      "Average test loss: 0.005464039665129449\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0925276491244634\n",
      "Average test loss: 0.005518355747891797\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09289888253476884\n",
      "Average test loss: 0.005731300017899937\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09251113757159975\n",
      "Average test loss: 0.0055079322395225365\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09313454110092587\n",
      "Average test loss: 0.005823484247757329\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09281748597158326\n",
      "Average test loss: 0.005453313020782338\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09304466330342823\n",
      "Average test loss: 0.00610505980749925\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09254908510049184\n",
      "Average test loss: 0.006097368687805202\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0925583250257704\n",
      "Average test loss: 0.005531835396670633\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09231010130378935\n",
      "Average test loss: 0.005510181127736965\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09231099425421821\n",
      "Average test loss: 0.0059130832929578095\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09209472658899095\n",
      "Average test loss: 0.005703476903546187\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0919016947084003\n",
      "Average test loss: 0.00572192624790801\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09199578416347504\n",
      "Average test loss: 0.0056812700881726215\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09166124723354975\n",
      "Average test loss: 0.005646917590664493\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09205818362368477\n",
      "Average test loss: 0.005831797945416636\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09236455607414246\n",
      "Average test loss: 0.010054774658961428\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0918682407339414\n",
      "Average test loss: 0.006327777318656445\n",
      "Epoch 235/300\n",
      "Average training loss: 0.091932300110658\n",
      "Average test loss: 0.005864704946262969\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09183193297518624\n",
      "Average test loss: 0.005497801176996694\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09678361803293228\n",
      "Average test loss: 0.005435290111021863\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09254506628380882\n",
      "Average test loss: 0.005470651223013798\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09122092402643628\n",
      "Average test loss: 0.005544299798707167\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09153472917609745\n",
      "Average test loss: 0.005491372245467372\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09150504354635874\n",
      "Average test loss: 0.006104488189849588\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09143424052662319\n",
      "Average test loss: 0.005572663924760289\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09154173819555177\n",
      "Average test loss: 0.006153800857563813\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09238189528385798\n",
      "Average test loss: 0.005529308974328968\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09118218720621533\n",
      "Average test loss: 0.0054025507796969675\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09103895428445605\n",
      "Average test loss: 0.00766440766553084\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09200854545500543\n",
      "Average test loss: 0.005445496362944444\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09131043037441042\n",
      "Average test loss: 0.00600958841211266\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09082512224382824\n",
      "Average test loss: 0.00567997832575606\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09127659955951903\n",
      "Average test loss: 0.0055869146645483045\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09152099615997739\n",
      "Average test loss: 0.005445311147926582\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09097752925422456\n",
      "Average test loss: 0.00548141387436125\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09117776750193701\n",
      "Average test loss: 0.005675695285614994\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09053611928886837\n",
      "Average test loss: 0.005713957210795747\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09078479393985536\n",
      "Average test loss: 0.005466811500075791\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09108445859617657\n",
      "Average test loss: 0.0055158374127414495\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0906733847061793\n",
      "Average test loss: 0.005405058507704072\n",
      "Epoch 259/300\n",
      "Average training loss: 0.090460449927383\n",
      "Average test loss: 0.005619685755421718\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09075743032826318\n",
      "Average test loss: 0.005603471737768915\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09039249305592643\n",
      "Average test loss: 0.005390800837427378\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09043849746386211\n",
      "Average test loss: 0.005539070293307305\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09040815254714754\n",
      "Average test loss: 0.02199735550251272\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09033123509751426\n",
      "Average test loss: 0.005621088904639085\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09021252538760503\n",
      "Average test loss: 0.0054709859842227565\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09026682683494355\n",
      "Average test loss: 0.005544767898403936\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09053167435857985\n",
      "Average test loss: 0.005479739411630565\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09043145025107596\n",
      "Average test loss: 0.005462346677564912\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0901703667971823\n",
      "Average test loss: 0.0056356241893437175\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08998188104232152\n",
      "Average test loss: 0.005498974971473217\n",
      "Epoch 271/300\n",
      "Average training loss: 0.090788400457965\n",
      "Average test loss: 0.005497347628490792\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09042379789882236\n",
      "Average test loss: 0.005454203088457386\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08980680667029486\n",
      "Average test loss: 0.005440854948427942\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09022796843449275\n",
      "Average test loss: 0.005483358350478941\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09004364279905955\n",
      "Average test loss: 0.005602023408023848\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09005958156453239\n",
      "Average test loss: 0.005726718773444494\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08967597618367937\n",
      "Average test loss: 0.005503651740650336\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09009512058231565\n",
      "Average test loss: 0.0056975026147233114\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08950524371862412\n",
      "Average test loss: 0.0057807828026513255\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08992859835094875\n",
      "Average test loss: 0.005509433300544818\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09021977104081048\n",
      "Average test loss: 0.005446552883419726\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0894277553624577\n",
      "Average test loss: 0.005516284579204189\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08966562632057402\n",
      "Average test loss: 0.00562586519949966\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08996153277158737\n",
      "Average test loss: 0.0055110034756362435\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08963897504409155\n",
      "Average test loss: 0.27675531374083623\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08985561168193817\n",
      "Average test loss: 0.19842432250910336\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08980398624142011\n",
      "Average test loss: 0.005695048049920135\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08991581520769332\n",
      "Average test loss: 0.0056047993575533235\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08918485573927562\n",
      "Average test loss: 0.0055781194451782435\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08933786340554556\n",
      "Average test loss: 0.005530627753585577\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08920806144343482\n",
      "Average test loss: 0.0056286648122800724\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0896989810400539\n",
      "Average test loss: 0.005633865213228596\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08910782178905276\n",
      "Average test loss: 0.005540786671555705\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08944326704078251\n",
      "Average test loss: 0.005565523874842458\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08886760933531655\n",
      "Average test loss: 0.005413951296773222\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0891613656812244\n",
      "Average test loss: 0.005452071086400085\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08920453807380464\n",
      "Average test loss: 0.005500617752472559\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0890144029657046\n",
      "Average test loss: 0.005387714079684682\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0890305993490749\n",
      "Average test loss: 0.005698024317622185\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08859884003798167\n",
      "Average test loss: 0.0056596408730579745\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.041619660271539\n",
      "Average test loss: 0.021757450926635\n",
      "Epoch 2/300\n",
      "Average training loss: 3.146976993136936\n",
      "Average test loss: 0.007126064116756121\n",
      "Epoch 3/300\n",
      "Average training loss: 2.778780090543959\n",
      "Average test loss: 0.007035997306721078\n",
      "Epoch 4/300\n",
      "Average training loss: 2.532870783699883\n",
      "Average test loss: 0.00712397849228647\n",
      "Epoch 5/300\n",
      "Average training loss: 2.3302875395880807\n",
      "Average test loss: 0.010205880647732151\n",
      "Epoch 6/300\n",
      "Average training loss: 2.1690502746370104\n",
      "Average test loss: 0.008331208325094647\n",
      "Epoch 7/300\n",
      "Average training loss: 2.0330469862620038\n",
      "Average test loss: 0.00587862800185879\n",
      "Epoch 8/300\n",
      "Average training loss: 1.9074041883680555\n",
      "Average test loss: 0.007779209964805179\n",
      "Epoch 9/300\n",
      "Average training loss: 1.7913091109593708\n",
      "Average test loss: 0.005494049662931098\n",
      "Epoch 10/300\n",
      "Average training loss: 1.6840975195566814\n",
      "Average test loss: 0.0059270310223930416\n",
      "Epoch 11/300\n",
      "Average training loss: 1.5793904866112602\n",
      "Average test loss: 0.005540351744741202\n",
      "Epoch 12/300\n",
      "Average training loss: 1.3685326651467218\n",
      "Average test loss: 0.00920885667287641\n",
      "Epoch 14/300\n",
      "Average training loss: 1.2595889520645143\n",
      "Average test loss: 0.006349205020401213\n",
      "Epoch 15/300\n",
      "Average training loss: 1.1498208554585774\n",
      "Average test loss: 0.004773993802981244\n",
      "Epoch 16/300\n",
      "Average training loss: 1.0396324921713935\n",
      "Average test loss: 0.005171884790476825\n",
      "Epoch 17/300\n",
      "Average training loss: 0.9348084916008843\n",
      "Average test loss: 0.0052431375177370175\n",
      "Epoch 18/300\n",
      "Average training loss: 0.8325893270174662\n",
      "Average test loss: 0.00654559965348906\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7361748417748345\n",
      "Average test loss: 0.00510456583276391\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6491053937806024\n",
      "Average test loss: 0.004881454851064417\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5725630762312147\n",
      "Average test loss: 0.005071177448663446\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5071793648931715\n",
      "Average test loss: 0.004466843094262812\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4538228114181095\n",
      "Average test loss: 0.004788398488528199\n",
      "Epoch 24/300\n",
      "Average training loss: 0.4065955204433865\n",
      "Average test loss: 0.004480895979536904\n",
      "Epoch 25/300\n",
      "Average training loss: 0.36847662160131667\n",
      "Average test loss: 0.005767659494446384\n",
      "Epoch 26/300\n",
      "Average training loss: 0.33191428465313383\n",
      "Average test loss: 0.004357046127112375\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3019909293651581\n",
      "Average test loss: 0.004400075159966946\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2753160526355108\n",
      "Average test loss: 0.00638652638428741\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2504872096379598\n",
      "Average test loss: 0.006183457378298044\n",
      "Epoch 30/300\n",
      "Average training loss: 0.22765301361348894\n",
      "Average test loss: 0.003944205455068085\n",
      "Epoch 31/300\n",
      "Average training loss: 0.20644974222448137\n",
      "Average test loss: 0.004004084645460049\n",
      "Epoch 32/300\n",
      "Average training loss: 0.18818743934896257\n",
      "Average test loss: 0.006367858026590612\n",
      "Epoch 33/300\n",
      "Average training loss: 0.17324252853128644\n",
      "Average test loss: 0.0040078591791292036\n",
      "Epoch 34/300\n",
      "Average training loss: 0.15740000735388862\n",
      "Average test loss: 0.005437236432400015\n",
      "Epoch 35/300\n",
      "Average training loss: 0.14672261521551344\n",
      "Average test loss: 0.004088978295938836\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1225952962372038\n",
      "Average test loss: 0.0041755076092150475\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1149111458990309\n",
      "Average test loss: 0.004200754853586356\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11124664770232306\n",
      "Average test loss: 0.004887462017850743\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10626454275184208\n",
      "Average test loss: 0.0038984061632719304\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10241309654050403\n",
      "Average test loss: 0.003875784840227829\n",
      "Epoch 43/300\n",
      "Average training loss: 0.099584616959095\n",
      "Average test loss: 0.004049746190508206\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09721914154291153\n",
      "Average test loss: 0.003919633787332309\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09515656769275666\n",
      "Average test loss: 0.0038281518125699624\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09031779971387652\n",
      "Average test loss: 0.003725046037385861\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08935222170088027\n",
      "Average test loss: 0.006538852264897691\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0881612766649988\n",
      "Average test loss: 0.0040642636536310115\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0867265640033616\n",
      "Average test loss: 0.00410707118610541\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0860021083454291\n",
      "Average test loss: 0.0036691422719094488\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08544916280110677\n",
      "Average test loss: 0.003949653190043237\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08333192437887192\n",
      "Average test loss: 0.0041769761254804\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08234209695127276\n",
      "Average test loss: 0.0037992218331330354\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08188901867469152\n",
      "Average test loss: 0.003806880372472935\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08136166027519438\n",
      "Average test loss: 0.00429917564884656\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08015177528063457\n",
      "Average test loss: 0.0037818658318784504\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0802642874121666\n",
      "Average test loss: 0.0038204410009913974\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07799399811029434\n",
      "Average test loss: 0.003748362424990369\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07669603869650099\n",
      "Average test loss: 0.0042346484851506026\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07631946476300558\n",
      "Average test loss: 0.003925926272653871\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07688904383447435\n",
      "Average test loss: 0.0036667194832116366\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07485572876532873\n",
      "Average test loss: 0.003603874433371756\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07498593515157699\n",
      "Average test loss: 0.0038662376002305083\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07472326791948743\n",
      "Average test loss: 3.6887504801220365\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07475960856013827\n",
      "Average test loss: 0.003683938751204146\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07294913542932935\n",
      "Average test loss: 0.00478582149454289\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07257493699259228\n",
      "Average test loss: 0.0037476441632542344\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07251392631398307\n",
      "Average test loss: 0.009468839374681313\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07397486639022827\n",
      "Average test loss: 0.00414043585004078\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07185299475987753\n",
      "Average test loss: 0.0035057542340623006\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07155533232953813\n",
      "Average test loss: 0.003643556323937244\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07154986576239268\n",
      "Average test loss: 0.0036816622035370935\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07080447797642814\n",
      "Average test loss: 0.003578314532008436\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07096057617002063\n",
      "Average test loss: 0.0035822884299688868\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07048755392432213\n",
      "Average test loss: 0.0035755418611483443\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07041979061894947\n",
      "Average test loss: 0.003599921592614717\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0703638750049803\n",
      "Average test loss: 0.0036698592528700827\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07021635000573265\n",
      "Average test loss: 0.0036391324870702294\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0697856110797988\n",
      "Average test loss: 0.0040118560747553905\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06944439856211344\n",
      "Average test loss: 0.006617372408509254\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06899715485837725\n",
      "Average test loss: 0.004264592069718573\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06894813436932035\n",
      "Average test loss: 0.0035871808694468606\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06948563183678522\n",
      "Average test loss: 0.003759233685624268\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0697165726919969\n",
      "Average test loss: 0.0035532166390783255\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06875685232215457\n",
      "Average test loss: 0.0035859477118485505\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06849542526404064\n",
      "Average test loss: 0.0037599716153409748\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06846048689550824\n",
      "Average test loss: 0.003802666275865502\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06793817078073819\n",
      "Average test loss: 0.0037806412163707945\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0674872824880812\n",
      "Average test loss: 0.0038499617932571304\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06715663323137495\n",
      "Average test loss: 0.0035792290344834328\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06780989670753479\n",
      "Average test loss: 0.0036715048808190556\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06749069663882255\n",
      "Average test loss: 0.003573798400039474\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06770998306903574\n",
      "Average test loss: 0.0036272218111488555\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06787321810258759\n",
      "Average test loss: 0.0049674431830644605\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06728615213102765\n",
      "Average test loss: 0.005035875908202595\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06666945323348045\n",
      "Average test loss: 0.014169897283737859\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06684419632620282\n",
      "Average test loss: 0.003570600871203674\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0669615445666843\n",
      "Average test loss: 0.0038037173135413065\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06665089753601286\n",
      "Average test loss: 0.0034861855813198618\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0664143761727545\n",
      "Average test loss: 0.004609389294766717\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06713189236985312\n",
      "Average test loss: 0.003611909430887964\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06602530199951595\n",
      "Average test loss: 0.00464128902181983\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06587255402406057\n",
      "Average test loss: 0.0034168310010184843\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06581051855285962\n",
      "Average test loss: 0.0036546400321854485\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06604587880770366\n",
      "Average test loss: 0.003782008464137713\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06594836786720487\n",
      "Average test loss: 0.003405173699061076\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06527749596370591\n",
      "Average test loss: 0.0037107970760100416\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06544593225253953\n",
      "Average test loss: 0.0035057005248963834\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06608622651298841\n",
      "Average test loss: 0.0035426134750660924\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0657489462726646\n",
      "Average test loss: 0.0036180044875169792\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06551454427507189\n",
      "Average test loss: 0.0036806600600894955\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06503370070457458\n",
      "Average test loss: 0.003503111326446136\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06497995418310165\n",
      "Average test loss: 0.0035461343758636053\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06525604771574338\n",
      "Average test loss: 0.003682079604930348\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06453621537155575\n",
      "Average test loss: 0.003967593093713124\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06492704206705094\n",
      "Average test loss: 0.0034477336410846976\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06498325302534633\n",
      "Average test loss: 0.0034358459094332326\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06470927517612775\n",
      "Average test loss: 0.003565845554901494\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06504222610261705\n",
      "Average test loss: 0.003421115634755956\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06472418070501751\n",
      "Average test loss: 0.0034890249628159735\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06534081924623913\n",
      "Average test loss: 0.0038765331177661815\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06431900432043605\n",
      "Average test loss: 0.04293741950392723\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0646232424279054\n",
      "Average test loss: 0.0036298015949626766\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0640136914451917\n",
      "Average test loss: 2.255695203708278\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06509829551312658\n",
      "Average test loss: 0.003688734341205822\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06447790890932083\n",
      "Average test loss: 0.0035732566109961932\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0633697027100457\n",
      "Average test loss: 0.03435671247045199\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0638575727045536\n",
      "Average test loss: 0.12577690486527152\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06383100140094757\n",
      "Average test loss: 0.0037434826068994073\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06416499423980714\n",
      "Average test loss: 0.0034695745663096507\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0633583113418685\n",
      "Average test loss: 0.0037117953329450556\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06348932908640967\n",
      "Average test loss: 0.0034318874639769396\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06375669059488509\n",
      "Average test loss: 0.0033912428046266236\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06398853358957503\n",
      "Average test loss: 0.003364450064798196\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0630630477865537\n",
      "Average test loss: 0.003385612993604607\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0635527274608612\n",
      "Average test loss: 0.003530237196634213\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06344327523642117\n",
      "Average test loss: 0.0038750645750098758\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06286226737830374\n",
      "Average test loss: 0.0033477314898951185\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0635635783639219\n",
      "Average test loss: 0.003504259367783864\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06309599263800515\n",
      "Average test loss: 0.0036035664044320585\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06320334710677465\n",
      "Average test loss: 0.0034696980499558977\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06276033853160011\n",
      "Average test loss: 0.0033718640361395146\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06307179602649476\n",
      "Average test loss: 0.0033908554503901136\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06253927595747842\n",
      "Average test loss: 0.006377551459189918\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06305958751506276\n",
      "Average test loss: 0.003729269398169385\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06292351125677427\n",
      "Average test loss: 0.0037042957227677105\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06279081372419994\n",
      "Average test loss: 0.003810473701606194\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06270958331889577\n",
      "Average test loss: 0.003442495109099481\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06266974385579427\n",
      "Average test loss: 0.004579680085596111\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0628319037258625\n",
      "Average test loss: 0.0036551985525422625\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06244987595081329\n",
      "Average test loss: 0.0035454732895725304\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06291304739647441\n",
      "Average test loss: 0.003452478751954105\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06267878376775318\n",
      "Average test loss: 0.003977388225081894\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06289027434587478\n",
      "Average test loss: 0.0033486318070855407\n",
      "Epoch 156/300\n",
      "Average training loss: 0.061881798555453615\n",
      "Average test loss: 0.0035417737977372276\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06270326458745533\n",
      "Average test loss: 0.0038849658713572557\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06202767127752304\n",
      "Average test loss: 0.003435268084622092\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06212489268183708\n",
      "Average test loss: 0.004377122732914156\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06213955167929332\n",
      "Average test loss: 0.003273416429551111\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06193442196647326\n",
      "Average test loss: 0.0033621553600662285\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06687421772215102\n",
      "Average test loss: 0.003659626381678714\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06195437279012468\n",
      "Average test loss: 0.005113210635466708\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06176369539896647\n",
      "Average test loss: 0.0036596677146024173\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06191351007752948\n",
      "Average test loss: 0.003418608222777645\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06177926469180319\n",
      "Average test loss: 0.0034160309462911553\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06186595110098521\n",
      "Average test loss: 0.003285859749135044\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06196025828520457\n",
      "Average test loss: 0.003296164467309912\n",
      "Epoch 169/300\n",
      "Average training loss: 0.061917847990989684\n",
      "Average test loss: 0.003843029632543524\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06173781910207536\n",
      "Average test loss: 0.003715966796502471\n",
      "Epoch 171/300\n",
      "Average training loss: 0.061592277662621604\n",
      "Average test loss: 0.0035542109650042323\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0615941407084465\n",
      "Average test loss: 0.0033765374591781033\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06168714392185211\n",
      "Average test loss: 0.14148727530861893\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06179965648386213\n",
      "Average test loss: 0.003706138075846765\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06146558381782638\n",
      "Average test loss: 0.003319326606889566\n",
      "Epoch 176/300\n",
      "Average training loss: 0.061404416908820474\n",
      "Average test loss: 0.0036398969599977134\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06101629623439577\n",
      "Average test loss: 0.00330479676504102\n",
      "Epoch 178/300\n",
      "Average training loss: 0.061594819039106366\n",
      "Average test loss: 0.0035351499393582344\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06140495580765936\n",
      "Average test loss: 0.003425887003954914\n",
      "Epoch 180/300\n",
      "Average training loss: 0.060814040349589454\n",
      "Average test loss: 0.003538305497003926\n",
      "Epoch 181/300\n",
      "Average training loss: 0.061676950150065954\n",
      "Average test loss: 0.0039110393474499384\n",
      "Epoch 182/300\n",
      "Average training loss: 0.061035231603516475\n",
      "Average test loss: 29.803117963790893\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06151031032535765\n",
      "Average test loss: 0.003372455725653304\n",
      "Epoch 184/300\n",
      "Average training loss: 0.061032490018341275\n",
      "Average test loss: 0.003428612371906638\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06123332225614124\n",
      "Average test loss: 0.003389143047647344\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06105933248665598\n",
      "Average test loss: 0.0032458356033182806\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06077338742878702\n",
      "Average test loss: 6.531865226076709\n",
      "Epoch 188/300\n",
      "Average training loss: 0.061371574266089336\n",
      "Average test loss: 0.0033530094046145677\n",
      "Epoch 189/300\n",
      "Average training loss: 0.060858989278475446\n",
      "Average test loss: 1.1859100570020575\n",
      "Epoch 190/300\n",
      "Average training loss: 0.060694342348310684\n",
      "Average test loss: 0.004307951064573394\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06097780842251248\n",
      "Average test loss: 0.003261516832643085\n",
      "Epoch 192/300\n",
      "Average training loss: 0.060798407428794436\n",
      "Average test loss: 0.0034406932008763155\n",
      "Epoch 193/300\n",
      "Average training loss: 0.060662822693586346\n",
      "Average test loss: 0.0034952384957836736\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06051582239071528\n",
      "Average test loss: 0.0034090675620569123\n",
      "Epoch 195/300\n",
      "Average training loss: 0.060841359237829844\n",
      "Average test loss: 0.003449298854917288\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06056666098700629\n",
      "Average test loss: 0.0033392174173560406\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06061331242322922\n",
      "Average test loss: 0.004418539308839374\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06087537927097744\n",
      "Average test loss: 0.18753349119755958\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06067197936442163\n",
      "Average test loss: 0.0033541548419743776\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0604607292148802\n",
      "Average test loss: 0.003413101462026437\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06033933624294069\n",
      "Average test loss: 0.003386127575611075\n",
      "Epoch 202/300\n",
      "Average training loss: 0.060806093351708515\n",
      "Average test loss: 0.0033974405154585836\n",
      "Epoch 203/300\n",
      "Average training loss: 0.060087584876351886\n",
      "Average test loss: 0.003453180726617575\n",
      "Epoch 204/300\n",
      "Average training loss: 0.060500408007038965\n",
      "Average test loss: 0.0034365994890944826\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06029893738693661\n",
      "Average test loss: 0.003353061783230967\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06052349194553163\n",
      "Average test loss: 0.0032910133227705956\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06051309032241503\n",
      "Average test loss: 0.09760092638308804\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06000308382180002\n",
      "Average test loss: 0.003242102759165896\n",
      "Epoch 209/300\n",
      "Average training loss: 0.060269924839337664\n",
      "Average test loss: 0.003669688856229186\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06022715476817555\n",
      "Average test loss: 0.008859125565116603\n",
      "Epoch 211/300\n",
      "Average training loss: 0.060025128970543545\n",
      "Average test loss: 0.0034473782185879018\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06006502932641241\n",
      "Average test loss: 0.008043989815852708\n",
      "Epoch 213/300\n",
      "Average training loss: 0.060467277977201674\n",
      "Average test loss: 0.003367102495084206\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05996550768282678\n",
      "Average test loss: 0.0033497981528441113\n",
      "Epoch 215/300\n",
      "Average training loss: 0.060113088313076234\n",
      "Average test loss: 0.0035154105197224353\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05978074716859394\n",
      "Average test loss: 0.003416852960983912\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06011611822578642\n",
      "Average test loss: 0.003392787746671173\n",
      "Epoch 218/300\n",
      "Average training loss: 0.060044078999095495\n",
      "Average test loss: 0.003411296198144555\n",
      "Epoch 219/300\n",
      "Average training loss: 0.059827675021357\n",
      "Average test loss: 0.003272382585538758\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06020320385694504\n",
      "Average test loss: 0.003376638349559572\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05980615135033925\n",
      "Average test loss: 0.05016875705785222\n",
      "Epoch 222/300\n",
      "Average training loss: 0.060044298264715405\n",
      "Average test loss: 0.0033505615908652545\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05953782164388233\n",
      "Average test loss: 0.003321981746910347\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05949242026607195\n",
      "Average test loss: 0.0034998645461681815\n",
      "Epoch 225/300\n",
      "Average training loss: 0.059939007964399126\n",
      "Average test loss: 0.0035265186238620017\n",
      "Epoch 226/300\n",
      "Average training loss: 0.059448484003543856\n",
      "Average test loss: 0.003469003691441483\n",
      "Epoch 227/300\n",
      "Average training loss: 0.059687096738153034\n",
      "Average test loss: 825.4267883750068\n",
      "Epoch 228/300\n",
      "Average training loss: 0.059516779677735436\n",
      "Average test loss: 0.0032886113247109785\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06009462955262926\n",
      "Average test loss: 0.0034746677190479304\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0594772871169779\n",
      "Average test loss: 0.635280797764659\n",
      "Epoch 231/300\n",
      "Average training loss: 0.059384571386708156\n",
      "Average test loss: 0.0035118980440828535\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05930653858515952\n",
      "Average test loss: 0.0036891361104531423\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05965096512768003\n",
      "Average test loss: 0.0034648456244419016\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05940330207016733\n",
      "Average test loss: 0.003330571160962184\n",
      "Epoch 235/300\n",
      "Average training loss: 0.059467989908324345\n",
      "Average test loss: 5.923854497697619\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06201933790577783\n",
      "Average test loss: 0.003317166869425111\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05912045763598548\n",
      "Average test loss: 0.003258515322477453\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05936906309591399\n",
      "Average test loss: 0.003534645894438856\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05942504815260569\n",
      "Average test loss: 0.0032518787203977507\n",
      "Epoch 240/300\n",
      "Average training loss: 0.059091795931259794\n",
      "Average test loss: 0.003294747134877576\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05951488161087036\n",
      "Average test loss: 0.00346478755482369\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05928065303961436\n",
      "Average test loss: 0.0036077845183511573\n",
      "Epoch 243/300\n",
      "Average training loss: 0.059152306805054344\n",
      "Average test loss: 0.0033256126853326955\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05927781231535806\n",
      "Average test loss: 0.03382527326212989\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05923619574970669\n",
      "Average test loss: 0.0035557773193965357\n",
      "Epoch 246/300\n",
      "Average training loss: 0.059100626091162366\n",
      "Average test loss: 0.0033102734349668025\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05901010201043553\n",
      "Average test loss: 0.0034657093551423816\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05893905995620621\n",
      "Average test loss: 0.0032821773986021676\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05957584618528684\n",
      "Average test loss: 0.004201850106732713\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05891947551899486\n",
      "Average test loss: 0.005457649246686035\n",
      "Epoch 251/300\n",
      "Average training loss: 0.059000418580240674\n",
      "Average test loss: 0.003549465563562181\n",
      "Epoch 252/300\n",
      "Average training loss: 0.058742642028464215\n",
      "Average test loss: 0.0033323291360090176\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0589380522635248\n",
      "Average test loss: 0.003219877351489332\n",
      "Epoch 254/300\n",
      "Average training loss: 0.058961170769400065\n",
      "Average test loss: 0.11609240169450641\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05894278319676717\n",
      "Average test loss: 0.003232951189701756\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05879142044319047\n",
      "Average test loss: 0.0032220572086258067\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05885033658809132\n",
      "Average test loss: 0.003421580237026016\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05880786573886871\n",
      "Average test loss: 0.005049579214718607\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05901528837945726\n",
      "Average test loss: 0.0033550024076054496\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05878128564688895\n",
      "Average test loss: 5.69527688685432\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05866493712862333\n",
      "Average test loss: 0.0035771952184538046\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05871812466945913\n",
      "Average test loss: 0.0033613559017992684\n",
      "Epoch 263/300\n",
      "Average training loss: 0.058692036850584876\n",
      "Average test loss: 0.0032674978928019602\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05879016360971663\n",
      "Average test loss: 0.003327177044418123\n",
      "Epoch 265/300\n",
      "Average training loss: 0.058372485240300497\n",
      "Average test loss: 0.0034976761386626295\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05903808767265744\n",
      "Average test loss: 0.0033778615101344054\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05887513926625252\n",
      "Average test loss: 0.0034364214996910757\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05853162751926316\n",
      "Average test loss: 0.003340072291592757\n",
      "Epoch 269/300\n",
      "Average training loss: 0.058534509860806994\n",
      "Average test loss: 0.0032394688895179165\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05835960680908627\n",
      "Average test loss: 0.003487181537474195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0584640944202741\n",
      "Average test loss: 0.003343565320596099\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05846153155962626\n",
      "Average test loss: 0.0032893260582867595\n",
      "Epoch 273/300\n",
      "Average training loss: 0.058374200459983615\n",
      "Average test loss: 0.0034671703887482483\n",
      "Epoch 274/300\n",
      "Average training loss: 0.058413251595364674\n",
      "Average test loss: 0.003405546727693743\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05835661571886804\n",
      "Average test loss: 0.0032131059273249572\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0586435456805759\n",
      "Average test loss: 0.0032366455853399304\n",
      "Epoch 277/300\n",
      "Average training loss: 0.058658974746863046\n",
      "Average test loss: 0.00419932150012917\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0584797041548623\n",
      "Average test loss: 0.0032124979400800333\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0581510055495633\n",
      "Average test loss: 0.0032595966390023628\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05821949344873428\n",
      "Average test loss: 0.0032825942835253146\n",
      "Epoch 281/300\n",
      "Average training loss: 0.058186974078416824\n",
      "Average test loss: 0.0033420829474925997\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05816058003902435\n",
      "Average test loss: 0.003257820395545827\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05807971177167363\n",
      "Average test loss: 0.0032746497732069756\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05835587379998631\n",
      "Average test loss: 0.0033309319184886086\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05837343089779218\n",
      "Average test loss: 0.779477926981118\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0582683069507281\n",
      "Average test loss: 0.003251213750284579\n",
      "Epoch 287/300\n",
      "Average training loss: 0.058061568823125624\n",
      "Average test loss: 0.0032458773545093007\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05797634329398473\n",
      "Average test loss: 0.0032101226867073114\n",
      "Epoch 289/300\n",
      "Average training loss: 0.058138574679692585\n",
      "Average test loss: 0.0033465498925911055\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0594910199145476\n",
      "Average test loss: 0.02710933636791176\n",
      "Epoch 291/300\n",
      "Average training loss: 0.058119892345534434\n",
      "Average test loss: 0.00325699700973928\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05796529440085093\n",
      "Average test loss: 0.11895776430600219\n",
      "Epoch 293/300\n",
      "Average training loss: 0.058040797379281785\n",
      "Average test loss: 0.0033238793222440614\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05798521170020104\n",
      "Average test loss: 0.1681359240822494\n",
      "Epoch 295/300\n",
      "Average training loss: 0.057891783012284176\n",
      "Average test loss: 0.004198060280746884\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05831268505255381\n",
      "Average test loss: 0.003163685880187485\n",
      "Epoch 297/300\n",
      "Average training loss: 0.058015353318717745\n",
      "Average test loss: 0.0032230105484939283\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0577274830573135\n",
      "Average test loss: 0.0033831062016801703\n",
      "Epoch 299/300\n",
      "Average training loss: 0.057820439868503146\n",
      "Average test loss: 0.0032204542466335827\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05778240690628687\n",
      "Average test loss: 0.0037167344681090777\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.5777985286712646\n",
      "Average test loss: 0.009737582385540008\n",
      "Epoch 2/300\n",
      "Average training loss: 2.5762038894229464\n",
      "Average test loss: 0.007513398283471664\n",
      "Epoch 3/300\n",
      "Average training loss: 2.141184854507446\n",
      "Average test loss: 0.0044607367258932855\n",
      "Epoch 4/300\n",
      "Average training loss: 1.822757149166531\n",
      "Average test loss: 0.004858223161763615\n",
      "Epoch 5/300\n",
      "Average training loss: 1.570130839559767\n",
      "Average test loss: 0.005479035145292679\n",
      "Epoch 6/300\n",
      "Average training loss: 1.360851119571262\n",
      "Average test loss: 0.0037282074956844252\n",
      "Epoch 7/300\n",
      "Average training loss: 1.187934468905131\n",
      "Average test loss: 0.004127913425366084\n",
      "Epoch 8/300\n",
      "Average training loss: 1.040481376806895\n",
      "Average test loss: 0.005900946516543627\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9162409204377069\n",
      "Average test loss: 0.008299544468522073\n",
      "Epoch 10/300\n",
      "Average training loss: 0.8088811327616374\n",
      "Average test loss: 0.0035925196044974857\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7133035949071248\n",
      "Average test loss: 0.034562496364116665\n",
      "Epoch 12/300\n",
      "Average training loss: 0.627717392762502\n",
      "Average test loss: 0.003654237504427632\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5538009447256724\n",
      "Average test loss: 0.00368603339460161\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4874894163078732\n",
      "Average test loss: 0.003733834345928497\n",
      "Epoch 15/300\n",
      "Average training loss: 0.4280670760207706\n",
      "Average test loss: 0.0031985921296808456\n",
      "Epoch 16/300\n",
      "Average training loss: 0.38264872153600055\n",
      "Average test loss: 0.0034297454624126356\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3409381364451514\n",
      "Average test loss: 0.0036081289605547984\n",
      "Epoch 18/300\n",
      "Average training loss: 0.30574801097975834\n",
      "Average test loss: 0.006765516185098224\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2751983442968792\n",
      "Average test loss: 0.00343933527254396\n",
      "Epoch 20/300\n",
      "Average training loss: 0.24897907270325556\n",
      "Average test loss: 0.0036253139867136876\n",
      "Epoch 21/300\n",
      "Average training loss: 0.22392438112364874\n",
      "Average test loss: 0.00425679263803694\n",
      "Epoch 22/300\n",
      "Average training loss: 0.20256010643641154\n",
      "Average test loss: 0.0030429213895565932\n",
      "Epoch 23/300\n",
      "Average training loss: 0.18324639643563165\n",
      "Average test loss: 0.003491667654365301\n",
      "Epoch 24/300\n",
      "Average training loss: 0.16450112730926938\n",
      "Average test loss: 0.0033414860965891016\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14878883135318757\n",
      "Average test loss: 0.004207279821650849\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13916425140698752\n",
      "Average test loss: 0.009416173347996342\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12538435037930806\n",
      "Average test loss: 0.0034001054999729\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11634765516387091\n",
      "Average test loss: 0.0029622404405640233\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11040640697876612\n",
      "Average test loss: 0.0030447735941658417\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10218607070710924\n",
      "Average test loss: 0.0026761380231214893\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09613515861829122\n",
      "Average test loss: 0.0028837191990266245\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09203236887852351\n",
      "Average test loss: 0.0057085275368558034\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08766182526614931\n",
      "Average test loss: 0.0028894250829600626\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0836898554166158\n",
      "Average test loss: 0.0033346972490350406\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08213505685991711\n",
      "Average test loss: 0.004127502844358484\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07811027346054714\n",
      "Average test loss: 0.0029882161509659556\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07561786570151648\n",
      "Average test loss: 0.003148773934278223\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0732203551530838\n",
      "Average test loss: 0.005414739211814271\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07212493630912568\n",
      "Average test loss: 0.0026285845179938607\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07093330252832837\n",
      "Average test loss: 0.0027074823509901764\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0686333288749059\n",
      "Average test loss: 0.0027549366183165047\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06836841495831808\n",
      "Average test loss: 0.0025930837097888192\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06620664457148975\n",
      "Average test loss: 0.00292258443745474\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06579027720623547\n",
      "Average test loss: 0.002567298909649253\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06540260197718938\n",
      "Average test loss: 0.0034001803000768024\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06316996468106906\n",
      "Average test loss: 0.0028006957411352133\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06331852428780661\n",
      "Average test loss: 0.002500852779381805\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06141263661119673\n",
      "Average test loss: 0.002508480037252108\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06089185851150089\n",
      "Average test loss: 0.002505225784662697\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06075295342670547\n",
      "Average test loss: 0.003074085755687621\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05891529510418574\n",
      "Average test loss: 0.002490793728373117\n",
      "Epoch 52/300\n",
      "Average training loss: 0.059161008954048154\n",
      "Average test loss: 0.0024881458054814075\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05874690540300475\n",
      "Average test loss: 0.0025974656264815066\n",
      "Epoch 54/300\n",
      "Average training loss: 0.056670952747265496\n",
      "Average test loss: 0.002598736236906714\n",
      "Epoch 55/300\n",
      "Average training loss: 0.056192425661616856\n",
      "Average test loss: 0.002539984989600877\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05592773857381609\n",
      "Average test loss: 0.0027945336836079754\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05541021228167746\n",
      "Average test loss: 0.024082812492218282\n",
      "Epoch 58/300\n",
      "Average training loss: 0.054677093389961455\n",
      "Average test loss: 0.002618610356003046\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05480870529678133\n",
      "Average test loss: 0.0028240542058936423\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05457523801922798\n",
      "Average test loss: 0.002661355147138238\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05360631152325206\n",
      "Average test loss: 0.0025577254437117113\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05384770757291052\n",
      "Average test loss: 0.0026608967559619084\n",
      "Epoch 63/300\n",
      "Average training loss: 0.053308768308824965\n",
      "Average test loss: 0.0026101523701929385\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05340656334492895\n",
      "Average test loss: 0.0031562313019401498\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05610652591453658\n",
      "Average test loss: 0.0023565151149200067\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05256665523515807\n",
      "Average test loss: 0.002472691470136245\n",
      "Epoch 67/300\n",
      "Average training loss: 0.051863356129990684\n",
      "Average test loss: 0.002766172410920262\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0518604362209638\n",
      "Average test loss: 0.002486732279467914\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05157994857099321\n",
      "Average test loss: 0.0024988768769221175\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0517055647207631\n",
      "Average test loss: 0.03212461059623294\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0521808978650305\n",
      "Average test loss: 0.0025515961388332974\n",
      "Epoch 72/300\n",
      "Average training loss: 0.051102350678708816\n",
      "Average test loss: 0.0023694773908290597\n",
      "Epoch 73/300\n",
      "Average training loss: 0.051370482726229565\n",
      "Average test loss: 0.002594066155453523\n",
      "Epoch 74/300\n",
      "Average training loss: 0.051498094810379876\n",
      "Average test loss: 0.0023647610512044694\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05082757134570016\n",
      "Average test loss: 0.0023666071380592053\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05040795667303933\n",
      "Average test loss: 0.0024457570595873726\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05058921051687664\n",
      "Average test loss: 0.0025629824689692923\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05072215270002683\n",
      "Average test loss: 0.0023949178618689376\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05011151619089974\n",
      "Average test loss: 0.002429335988023215\n",
      "Epoch 80/300\n",
      "Average training loss: 0.049818820887141756\n",
      "Average test loss: 0.002491379746132427\n",
      "Epoch 81/300\n",
      "Average training loss: 0.050572804361581805\n",
      "Average test loss: 0.0023931179232895374\n",
      "Epoch 82/300\n",
      "Average training loss: 0.049868279410733116\n",
      "Average test loss: 0.0023901547462575968\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04941592646969689\n",
      "Average test loss: 0.0026063527382082408\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04945615644256274\n",
      "Average test loss: 0.0026279260449939305\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04905960107511944\n",
      "Average test loss: 0.002475312175643113\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04966893493466907\n",
      "Average test loss: 0.0025080757954468328\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04877264420688152\n",
      "Average test loss: 0.002341737201023433\n",
      "Epoch 88/300\n",
      "Average training loss: 0.048844171325365705\n",
      "Average test loss: 0.002679381377581093\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04905589619278908\n",
      "Average test loss: 0.005713947247299883\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05316241042481529\n",
      "Average test loss: 0.00250120795207719\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04842997012204594\n",
      "Average test loss: 0.9529191772821877\n",
      "Epoch 92/300\n",
      "Average training loss: 0.048720000952482224\n",
      "Average test loss: 0.0029382027056482104\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04878337397509151\n",
      "Average test loss: 0.0023179873853094047\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04807540060579777\n",
      "Average test loss: 0.0023105035228654742\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04852615702483389\n",
      "Average test loss: 0.002467394904543956\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04814904175202052\n",
      "Average test loss: 0.0024633227694365713\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04809308699766795\n",
      "Average test loss: 0.0033952067508879634\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04887352180812094\n",
      "Average test loss: 0.002797138831888636\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04801336758666568\n",
      "Average test loss: 0.0024133638251158927\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0476603849000401\n",
      "Average test loss: 0.002390736270075043\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04777869155009588\n",
      "Average test loss: 0.002350939026826786\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04788831537630823\n",
      "Average test loss: 0.0023644827763653464\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04770048505730099\n",
      "Average test loss: 0.02190988912081553\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04752174027429687\n",
      "Average test loss: 0.0023971613879419035\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04825784315003289\n",
      "Average test loss: 0.0024122416058348283\n",
      "Epoch 106/300\n",
      "Average training loss: 0.047343550350930956\n",
      "Average test loss: 0.0023460775343701243\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04754005613923073\n",
      "Average test loss: 0.01430553405587044\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04727180496686035\n",
      "Average test loss: 0.002316508226407071\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04740731196602185\n",
      "Average test loss: 0.0024349798638787536\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04720397521058718\n",
      "Average test loss: 0.0049990522561387885\n",
      "Epoch 111/300\n",
      "Average training loss: 0.047116688433620664\n",
      "Average test loss: 0.0024364144740005333\n",
      "Epoch 112/300\n",
      "Average training loss: 0.047156636092397904\n",
      "Average test loss: 0.0024602145231846307\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04698630558451017\n",
      "Average test loss: 0.0022811260370330677\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04734211560090383\n",
      "Average test loss: 0.002384944493778878\n",
      "Epoch 115/300\n",
      "Average training loss: 0.046733606898122365\n",
      "Average test loss: 0.0022998181098244255\n",
      "Epoch 116/300\n",
      "Average training loss: 0.046712510840760335\n",
      "Average test loss: 0.0022577686792032586\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0465954379538695\n",
      "Average test loss: 0.0023052102538446586\n",
      "Epoch 118/300\n",
      "Average training loss: 0.046527081035905415\n",
      "Average test loss: 0.0024081620569858285\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04680908875664075\n",
      "Average test loss: 0.0024308646904925503\n",
      "Epoch 120/300\n",
      "Average training loss: 0.046610286398066414\n",
      "Average test loss: 0.002343746471322245\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04655024926198854\n",
      "Average test loss: 0.0022638045898121266\n",
      "Epoch 122/300\n",
      "Average training loss: 0.046760089301400715\n",
      "Average test loss: 0.0030183728894011843\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04647447347309854\n",
      "Average test loss: 0.0024320205822587013\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04649236031042205\n",
      "Average test loss: 0.0026638987246486875\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04646053958932559\n",
      "Average test loss: 0.017255796771289574\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04620782142546442\n",
      "Average test loss: 0.002300511913581027\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04597861483030849\n",
      "Average test loss: 0.0022971189233163994\n",
      "Epoch 128/300\n",
      "Average training loss: 0.046422868755128646\n",
      "Average test loss: 0.0027502804485460123\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04608731513553196\n",
      "Average test loss: 0.002283670358773735\n",
      "Epoch 130/300\n",
      "Average training loss: 0.045995289898580975\n",
      "Average test loss: 0.002279382080460588\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0459272278977765\n",
      "Average test loss: 0.006949960080906749\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04606800535321236\n",
      "Average test loss: 0.0022324043052891892\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04653739621904161\n",
      "Average test loss: 0.002627133324328396\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04581799076331986\n",
      "Average test loss: 0.004398010222448243\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04617317569918103\n",
      "Average test loss: 0.002449278383412295\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04557163232896063\n",
      "Average test loss: 0.0023171417106770805\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0457559673719936\n",
      "Average test loss: 0.06110076947924164\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04584144140283267\n",
      "Average test loss: 0.002764186119660735\n",
      "Epoch 139/300\n",
      "Average training loss: 0.045763037969668706\n",
      "Average test loss: 0.0023144072165919675\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04592253495256106\n",
      "Average test loss: 0.00240203925429119\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04567058349317975\n",
      "Average test loss: 0.0023556489280114573\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04542550288306342\n",
      "Average test loss: 0.0022544218096882105\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04527967864274979\n",
      "Average test loss: 0.002512938309046957\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04540843017233743\n",
      "Average test loss: 0.0040387384589347574\n",
      "Epoch 145/300\n",
      "Average training loss: 0.045749759392605886\n",
      "Average test loss: 0.0022859694150586924\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04527774739927716\n",
      "Average test loss: 0.002309074464564522\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0450712783171071\n",
      "Average test loss: 0.002232816387588779\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04549594840904077\n",
      "Average test loss: 0.05526826711971727\n",
      "Epoch 149/300\n",
      "Average training loss: 0.044938055531846155\n",
      "Average test loss: 3124.3184130770846\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0453502089911037\n",
      "Average test loss: 0.0024781872706694736\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04522620224621561\n",
      "Average test loss: 0.027589998753534423\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04510252145595021\n",
      "Average test loss: 0.0026552014156348176\n",
      "Epoch 153/300\n",
      "Average training loss: 0.045273711896604965\n",
      "Average test loss: 0.0021870900496012637\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0451789205107424\n",
      "Average test loss: 0.002302288368344307\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0448044321089983\n",
      "Average test loss: 0.002274032298268543\n",
      "Epoch 156/300\n",
      "Average training loss: 0.044718009839455286\n",
      "Average test loss: 0.002264404276592864\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04467026921775606\n",
      "Average test loss: 0.002331584764437543\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04532136348221037\n",
      "Average test loss: 0.002316493801151713\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04469098280535804\n",
      "Average test loss: 0.0025293979764812523\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04498627162973086\n",
      "Average test loss: 0.0022951178177156384\n",
      "Epoch 161/300\n",
      "Average training loss: 0.044550539741913475\n",
      "Average test loss: 5.437495958287683\n",
      "Epoch 162/300\n",
      "Average training loss: 0.044693797094954385\n",
      "Average test loss: 0.004863832977082995\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04486008791128794\n",
      "Average test loss: 0.0023107676713003053\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0448189371344116\n",
      "Average test loss: 0.00224511897812287\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04467016838656532\n",
      "Average test loss: 0.0022150580804381104\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04441007993949784\n",
      "Average test loss: 0.00220301520689908\n",
      "Epoch 167/300\n",
      "Average training loss: 0.044761253429783716\n",
      "Average test loss: 0.002356113501307037\n",
      "Epoch 168/300\n",
      "Average training loss: 0.044777374575535454\n",
      "Average test loss: 0.0022500085052516725\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0445840348634455\n",
      "Average test loss: 0.0022557733785361052\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04425501530369123\n",
      "Average test loss: 0.0021709406251708666\n",
      "Epoch 171/300\n",
      "Average training loss: 0.044759315497345395\n",
      "Average test loss: 0.0023238160982728003\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04439270625842942\n",
      "Average test loss: 0.002277726287643115\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04464465662340323\n",
      "Average test loss: 0.0022104685581806633\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04414308404922485\n",
      "Average test loss: 0.0022447439256227677\n",
      "Epoch 175/300\n",
      "Average training loss: 0.043955713187654816\n",
      "Average test loss: 0.002558295246420635\n",
      "Epoch 176/300\n",
      "Average training loss: 0.044604579855998354\n",
      "Average test loss: 0.002465637556898097\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04421291235089302\n",
      "Average test loss: 563.658237452873\n",
      "Epoch 178/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DAMP_No_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DAMP_No_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='DAMP', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = DAMP(gaussian_10_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = DAMP(gaussian_20_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = DAMP(gaussian_30_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = DAMP(gaussian_40_normalized, 0.01, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
