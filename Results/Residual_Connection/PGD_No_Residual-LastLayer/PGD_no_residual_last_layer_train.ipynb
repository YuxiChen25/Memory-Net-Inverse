{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.PGD_Network.PGD import PGD\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.024565578940841888\n",
      "Average test loss: 0.01401199309527874\n",
      "Epoch 2/300\n",
      "Average training loss: 0.012755759341021378\n",
      "Average test loss: 0.014111425567004416\n",
      "Epoch 3/300\n",
      "Average training loss: 0.011420922194917997\n",
      "Average test loss: 0.010674019623961714\n",
      "Epoch 4/300\n",
      "Average training loss: 0.010564748879522086\n",
      "Average test loss: 0.009828650866945584\n",
      "Epoch 5/300\n",
      "Average training loss: 0.00971777532580826\n",
      "Average test loss: 0.008998944462173515\n",
      "Epoch 6/300\n",
      "Average training loss: 0.00922019650414586\n",
      "Average test loss: 0.009283309660851955\n",
      "Epoch 7/300\n",
      "Average training loss: 0.008855259122947852\n",
      "Average test loss: 0.009552954388989343\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008525799677603774\n",
      "Average test loss: 0.009228572662091917\n",
      "Epoch 9/300\n",
      "Average training loss: 0.008425515758080615\n",
      "Average test loss: 0.009091655159989992\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008231229959262742\n",
      "Average test loss: 0.018013751099507015\n",
      "Epoch 11/300\n",
      "Average training loss: 0.008046749089327123\n",
      "Average test loss: 0.007949865834580527\n",
      "Epoch 12/300\n",
      "Average training loss: 0.007883721879786915\n",
      "Average test loss: 0.008591551485988828\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007754042926761839\n",
      "Average test loss: 0.007992196303274897\n",
      "Epoch 14/300\n",
      "Average training loss: 0.007581657983362675\n",
      "Average test loss: 0.00771702899535497\n",
      "Epoch 15/300\n",
      "Average training loss: 0.007483032625996405\n",
      "Average test loss: 0.0078983820900321\n",
      "Epoch 16/300\n",
      "Average training loss: 0.007407018014126354\n",
      "Average test loss: 0.007968467384576797\n",
      "Epoch 17/300\n",
      "Average training loss: 0.007378213868786891\n",
      "Average test loss: 0.008299510058429506\n",
      "Epoch 18/300\n",
      "Average training loss: 0.007212888272686137\n",
      "Average test loss: 0.007375995552374257\n",
      "Epoch 19/300\n",
      "Average training loss: 0.00715840069245961\n",
      "Average test loss: 0.007478515304211114\n",
      "Epoch 20/300\n",
      "Average training loss: 0.007028907616105345\n",
      "Average test loss: 0.009374255193604363\n",
      "Epoch 21/300\n",
      "Average training loss: 0.00694245545938611\n",
      "Average test loss: 0.007336578868329525\n",
      "Epoch 22/300\n",
      "Average training loss: 0.006878703635185957\n",
      "Average test loss: 0.007941671900865104\n",
      "Epoch 23/300\n",
      "Average training loss: 0.006804253316587872\n",
      "Average test loss: 0.007423261783189244\n",
      "Epoch 24/300\n",
      "Average training loss: 0.006846523763404952\n",
      "Average test loss: 0.007227212963832749\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00666610995390349\n",
      "Average test loss: 0.007167429189715121\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006605817898280091\n",
      "Average test loss: 0.007179288570251729\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006541696235330568\n",
      "Average test loss: 0.00754632499648465\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006562386352154944\n",
      "Average test loss: 0.007152333868046602\n",
      "Epoch 29/300\n",
      "Average training loss: 0.006493313194562991\n",
      "Average test loss: 0.007324072048068047\n",
      "Epoch 30/300\n",
      "Average training loss: 0.006386637154966593\n",
      "Average test loss: 0.00737821167997188\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006355967592034075\n",
      "Average test loss: 0.0070089341476559635\n",
      "Epoch 32/300\n",
      "Average training loss: 0.006336329068160719\n",
      "Average test loss: 101.85842357042101\n",
      "Epoch 33/300\n",
      "Average training loss: 0.007594496365222666\n",
      "Average test loss: 0.007103712216226591\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006536185564266311\n",
      "Average test loss: 0.0072668675010403\n",
      "Epoch 35/300\n",
      "Average training loss: 0.006381646554917097\n",
      "Average test loss: 0.008287406280222866\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0062141532227396965\n",
      "Average test loss: 0.006843009965701236\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006197837056385146\n",
      "Average test loss: 0.006828952816211515\n",
      "Epoch 38/300\n",
      "Average training loss: 0.006144282897313436\n",
      "Average test loss: 0.007132689736783504\n",
      "Epoch 39/300\n",
      "Average training loss: 0.00608270523034864\n",
      "Average test loss: 0.007579043934328689\n",
      "Epoch 40/300\n",
      "Average training loss: 0.006064290885296133\n",
      "Average test loss: 0.006853957478370931\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006014280556390683\n",
      "Average test loss: 0.00681423782888386\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006006971097240845\n",
      "Average test loss: 0.007241893692976898\n",
      "Epoch 43/300\n",
      "Average training loss: 0.005942135759111908\n",
      "Average test loss: 0.00705243190502127\n",
      "Epoch 44/300\n",
      "Average training loss: 0.005944861432744397\n",
      "Average test loss: 0.00686878448476394\n",
      "Epoch 45/300\n",
      "Average training loss: 0.005894383247941733\n",
      "Average test loss: 0.006886356803692049\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0058885769388741915\n",
      "Average test loss: 0.0069577589035034176\n",
      "Epoch 47/300\n",
      "Average training loss: 0.005850105731023682\n",
      "Average test loss: 0.007470298767917686\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0058296443331572745\n",
      "Average test loss: 0.007748655986040831\n",
      "Epoch 49/300\n",
      "Average training loss: 0.005762854994585117\n",
      "Average test loss: 0.0068617710694670675\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0057509545116788815\n",
      "Average test loss: 0.006966233828001552\n",
      "Epoch 51/300\n",
      "Average training loss: 0.005734274947808849\n",
      "Average test loss: 0.00723538199522429\n",
      "Epoch 52/300\n",
      "Average training loss: 0.005708983816620376\n",
      "Average test loss: 0.0076046872122420206\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005701455277494259\n",
      "Average test loss: 0.0069908930679990185\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0056353111233976155\n",
      "Average test loss: 0.007187576472345325\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005616824127319786\n",
      "Average test loss: 0.007425990583995978\n",
      "Epoch 56/300\n",
      "Average training loss: 0.005651607645882501\n",
      "Average test loss: 0.007177678153746658\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005582960823757781\n",
      "Average test loss: 0.00701774655489458\n",
      "Epoch 58/300\n",
      "Average training loss: 0.005552126026401917\n",
      "Average test loss: 0.006990387952990002\n",
      "Epoch 59/300\n",
      "Average training loss: 0.005544742058134741\n",
      "Average test loss: 0.006906530195226272\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0056257945862081314\n",
      "Average test loss: 0.006883976557602485\n",
      "Epoch 61/300\n",
      "Average training loss: 0.00547470581655701\n",
      "Average test loss: 0.006841972212824557\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005452071300397317\n",
      "Average test loss: 0.006977251096732087\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005478991576366954\n",
      "Average test loss: 0.006898091971874237\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005419835527737935\n",
      "Average test loss: 0.007086073319117228\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005407893328617017\n",
      "Average test loss: 0.006832149496508969\n",
      "Epoch 66/300\n",
      "Average training loss: 0.005400089723368486\n",
      "Average test loss: 0.006848392828471131\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005381017114967108\n",
      "Average test loss: 0.007527067341738277\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005356129695557886\n",
      "Average test loss: 0.007509331107553509\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005342664592795902\n",
      "Average test loss: 0.007021596903188361\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005332119238459401\n",
      "Average test loss: 0.006919685655583938\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005329661645823055\n",
      "Average test loss: 0.006950468610558245\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005282808563361566\n",
      "Average test loss: 0.006848866049614218\n",
      "Epoch 73/300\n",
      "Average training loss: 0.005262578868617614\n",
      "Average test loss: 0.007325390939911206\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00527005208366447\n",
      "Average test loss: 0.006973735784490903\n",
      "Epoch 75/300\n",
      "Average training loss: 0.005260720460779137\n",
      "Average test loss: 0.006902180473837587\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0052332246336672045\n",
      "Average test loss: 0.006841496590938833\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005241607221050395\n",
      "Average test loss: 0.006946749480234252\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005200846202464567\n",
      "Average test loss: 0.0070921492212348515\n",
      "Epoch 79/300\n",
      "Average training loss: 0.005183517645630572\n",
      "Average test loss: 0.006813870364593135\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005192845857184794\n",
      "Average test loss: 0.007202801683710681\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00517425260382394\n",
      "Average test loss: 0.006906624390847153\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005185360334399674\n",
      "Average test loss: 0.006966589883797699\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0052448415601005155\n",
      "Average test loss: 0.007110046782841285\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005117662763843934\n",
      "Average test loss: 0.006799315904163652\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005125906333327293\n",
      "Average test loss: 0.00791902715133296\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005094173424359825\n",
      "Average test loss: 0.006935652969612015\n",
      "Epoch 87/300\n",
      "Average training loss: 0.005076469889117612\n",
      "Average test loss: 0.007384511445131567\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005068653299162785\n",
      "Average test loss: 0.006786254432052374\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0051052404095729195\n",
      "Average test loss: 0.006866341822677188\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005099577907058928\n",
      "Average test loss: 0.007190905747314294\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005030843828701311\n",
      "Average test loss: 0.006792421285476949\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005054026272975736\n",
      "Average test loss: 0.006909367282357481\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005005966526766618\n",
      "Average test loss: 0.0068602069405217965\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005010978440029753\n",
      "Average test loss: 0.007635479932857884\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005026711517737971\n",
      "Average test loss: 0.007284197568893432\n",
      "Epoch 96/300\n",
      "Average training loss: 0.004976814446349938\n",
      "Average test loss: 0.00686677700198359\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00503622287677394\n",
      "Average test loss: 0.006894299250510004\n",
      "Epoch 98/300\n",
      "Average training loss: 0.004972058954752154\n",
      "Average test loss: 0.00696274987856547\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004960323455433051\n",
      "Average test loss: 0.006865574142585198\n",
      "Epoch 100/300\n",
      "Average training loss: 0.004985058248456981\n",
      "Average test loss: 0.006771320146818955\n",
      "Epoch 101/300\n",
      "Average training loss: 0.004937855273071262\n",
      "Average test loss: 0.006937570534646511\n",
      "Epoch 102/300\n",
      "Average training loss: 0.004969566068301598\n",
      "Average test loss: 0.006854748874488804\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0049394308748758504\n",
      "Average test loss: 0.007158074561920431\n",
      "Epoch 104/300\n",
      "Average training loss: 0.004917201272315449\n",
      "Average test loss: 0.006983284407605727\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004898289664338033\n",
      "Average test loss: 0.006788642346858978\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004888029865092702\n",
      "Average test loss: 0.0071658285876943\n",
      "Epoch 107/300\n",
      "Average training loss: 0.004890451783935229\n",
      "Average test loss: 0.007520154616071118\n",
      "Epoch 108/300\n",
      "Average training loss: 0.004897071316838264\n",
      "Average test loss: 0.00699134396099382\n",
      "Epoch 109/300\n",
      "Average training loss: 0.004885774566895432\n",
      "Average test loss: 0.007662445355620649\n",
      "Epoch 110/300\n",
      "Average training loss: 0.004889407269449698\n",
      "Average test loss: 0.0069015163874460595\n",
      "Epoch 111/300\n",
      "Average training loss: 0.004854308710330062\n",
      "Average test loss: 0.007156258584310611\n",
      "Epoch 112/300\n",
      "Average training loss: 0.004845869499776098\n",
      "Average test loss: 0.007415208890206284\n",
      "Epoch 113/300\n",
      "Average training loss: 0.004849219708393017\n",
      "Average test loss: 0.007003628513879246\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00525748411545323\n",
      "Average test loss: 0.006833110970755418\n",
      "Epoch 115/300\n",
      "Average training loss: 0.004841115155981647\n",
      "Average test loss: 0.006836898773494694\n",
      "Epoch 116/300\n",
      "Average training loss: 0.004804018419235945\n",
      "Average test loss: 0.006856376065148248\n",
      "Epoch 117/300\n",
      "Average training loss: 0.004792699115557803\n",
      "Average test loss: 0.007230446314025256\n",
      "Epoch 118/300\n",
      "Average training loss: 0.004795774398992459\n",
      "Average test loss: 0.007280182812155949\n",
      "Epoch 119/300\n",
      "Average training loss: 0.004813298142618603\n",
      "Average test loss: 0.007004372782177395\n",
      "Epoch 120/300\n",
      "Average training loss: 0.004787482616388136\n",
      "Average test loss: 0.006950144790940815\n",
      "Epoch 121/300\n",
      "Average training loss: 0.004766015203462707\n",
      "Average test loss: 0.006895856865992148\n",
      "Epoch 122/300\n",
      "Average training loss: 0.004809442185693317\n",
      "Average test loss: 0.006761375129636791\n",
      "Epoch 123/300\n",
      "Average training loss: 0.004785830122315222\n",
      "Average test loss: 0.006877454568528466\n",
      "Epoch 124/300\n",
      "Average training loss: 0.004779049847275019\n",
      "Average test loss: 0.006942690956095854\n",
      "Epoch 125/300\n",
      "Average training loss: 0.004742153161929713\n",
      "Average test loss: 0.006751633001698388\n",
      "Epoch 126/300\n",
      "Average training loss: 0.004754135436067978\n",
      "Average test loss: 0.0077794299001495044\n",
      "Epoch 127/300\n",
      "Average training loss: 0.004749039702117443\n",
      "Average test loss: 0.006787373690969414\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0047422568967772855\n",
      "Average test loss: 0.006843361064377758\n",
      "Epoch 129/300\n",
      "Average training loss: 0.004718669919917981\n",
      "Average test loss: 0.006894485191752513\n",
      "Epoch 130/300\n",
      "Average training loss: 0.004756388087653452\n",
      "Average test loss: 0.007769317507743835\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0047102187549074494\n",
      "Average test loss: 0.006994698666863971\n",
      "Epoch 132/300\n",
      "Average training loss: 0.004685839573542277\n",
      "Average test loss: 0.006961502730432484\n",
      "Epoch 133/300\n",
      "Average training loss: 0.004700405319531758\n",
      "Average test loss: 0.006784950795272986\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0047199784575237165\n",
      "Average test loss: 0.006780234277248383\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0046871504088242845\n",
      "Average test loss: 0.006812701809323496\n",
      "Epoch 136/300\n",
      "Average training loss: 0.004691868849098682\n",
      "Average test loss: 0.00686921239644289\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0046674491365750635\n",
      "Average test loss: 0.006817256197333336\n",
      "Epoch 138/300\n",
      "Average training loss: 0.004641878397928344\n",
      "Average test loss: 0.007487909441192945\n",
      "Epoch 139/300\n",
      "Average training loss: 0.004670509690004918\n",
      "Average test loss: 0.006865724928677082\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0046889535718493995\n",
      "Average test loss: 0.007296140072660314\n",
      "Epoch 141/300\n",
      "Average training loss: 0.004645729510527518\n",
      "Average test loss: 0.006770816367119551\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00462152438176175\n",
      "Average test loss: 0.0068697801306843755\n",
      "Epoch 143/300\n",
      "Average training loss: 0.004646019640068213\n",
      "Average test loss: 0.006904020898044109\n",
      "Epoch 144/300\n",
      "Average training loss: 0.004997821184496085\n",
      "Average test loss: 0.011119326035181681\n",
      "Epoch 145/300\n",
      "Average training loss: 0.005401270784023735\n",
      "Average test loss: 0.007132971966018279\n",
      "Epoch 146/300\n",
      "Average training loss: 0.004628662014173137\n",
      "Average test loss: 0.006853896345943212\n",
      "Epoch 147/300\n",
      "Average training loss: 0.004599868810839123\n",
      "Average test loss: 0.006892013094077508\n",
      "Epoch 148/300\n",
      "Average training loss: 0.004579240637107028\n",
      "Average test loss: 0.007551108150018586\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0046014897695018184\n",
      "Average test loss: 0.006911812951995267\n",
      "Epoch 150/300\n",
      "Average training loss: 0.004595623039950927\n",
      "Average test loss: 0.0068275081374579005\n",
      "Epoch 151/300\n",
      "Average training loss: 0.004606074806302786\n",
      "Average test loss: 0.007006635748677784\n",
      "Epoch 152/300\n",
      "Average training loss: 0.004596781578742795\n",
      "Average test loss: 0.0068671684385173855\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0045764550459053786\n",
      "Average test loss: 0.0068211006758113705\n",
      "Epoch 154/300\n",
      "Average training loss: 0.004587903554240862\n",
      "Average test loss: 0.006915827488733662\n",
      "Epoch 155/300\n",
      "Average training loss: 0.004587639463858472\n",
      "Average test loss: 0.007083726874656147\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0046240665128247605\n",
      "Average test loss: 0.0069106920717491045\n",
      "Epoch 157/300\n",
      "Average training loss: 0.004559926732132832\n",
      "Average test loss: 0.0068921826630830765\n",
      "Epoch 158/300\n",
      "Average training loss: 0.004570295811527305\n",
      "Average test loss: 0.006787258627514044\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004552367503858275\n",
      "Average test loss: 0.007034360831810369\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004545002142588297\n",
      "Average test loss: 0.006875366255640983\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00456253301517831\n",
      "Average test loss: 0.006844455754591359\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004548123319115903\n",
      "Average test loss: 0.007258656069222423\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004547290764335129\n",
      "Average test loss: 0.00694103187488185\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004540187186871966\n",
      "Average test loss: 0.007197761026935445\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0046709145973953935\n",
      "Average test loss: 0.0069502604823145605\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004511630023519198\n",
      "Average test loss: 0.007765671363721291\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004523748252540827\n",
      "Average test loss: 0.0070534178304175535\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0045746885335279834\n",
      "Average test loss: 0.006833915041966571\n",
      "Epoch 169/300\n",
      "Average training loss: 0.00450670918615328\n",
      "Average test loss: 0.006790298065377606\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0045368138208157485\n",
      "Average test loss: 0.006822192319151428\n",
      "Epoch 171/300\n",
      "Average training loss: 0.004503520094272163\n",
      "Average test loss: 0.007801238203214275\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004517494837443034\n",
      "Average test loss: 0.006872849793483814\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004487019366067317\n",
      "Average test loss: 0.006937189607156648\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004490901908526818\n",
      "Average test loss: 0.006936883231831921\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00448194719851017\n",
      "Average test loss: 0.007826346184644435\n",
      "Epoch 176/300\n",
      "Average training loss: 0.004490836501535442\n",
      "Average test loss: 0.006996430565085676\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004479499050726493\n",
      "Average test loss: 0.00703392834133572\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004511140845509039\n",
      "Average test loss: 0.006864349594546689\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004480466025984949\n",
      "Average test loss: 0.007011946781641907\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004472937800404099\n",
      "Average test loss: 0.006910244910253419\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004450853200422393\n",
      "Average test loss: 0.006874948012746043\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004923489518463612\n",
      "Average test loss: 0.006899035314511921\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004437038669983546\n",
      "Average test loss: 0.007527586390574773\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004440405982236067\n",
      "Average test loss: 0.00684474863898423\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004423460750944084\n",
      "Average test loss: 0.006804405411084493\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00443985234407915\n",
      "Average test loss: 0.006912320943756236\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0044287100724048085\n",
      "Average test loss: 0.007211703362150324\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004440014553566774\n",
      "Average test loss: 0.0074451202443904345\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004451012554268042\n",
      "Average test loss: 0.006910172986073626\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0044372476277252035\n",
      "Average test loss: 0.006983107317652967\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004415396408074432\n",
      "Average test loss: 0.006900688957836893\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004446065369165606\n",
      "Average test loss: 0.006879484308676587\n",
      "Epoch 193/300\n",
      "Average training loss: 0.004405407555608286\n",
      "Average test loss: 0.006874926916427083\n",
      "Epoch 194/300\n",
      "Average training loss: 0.004421891533045305\n",
      "Average test loss: 0.007042224350488849\n",
      "Epoch 195/300\n",
      "Average training loss: 0.004430345250500573\n",
      "Average test loss: 0.006996622123652034\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004415960317270623\n",
      "Average test loss: 0.006967075884756114\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004388538930151197\n",
      "Average test loss: 0.006951251284943687\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00441856796749764\n",
      "Average test loss: 0.006835108228855663\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004404837653454807\n",
      "Average test loss: 0.00694155193037457\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0044494040786392155\n",
      "Average test loss: 0.006875532638281584\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00439195336194502\n",
      "Average test loss: 0.0068993758807579676\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0044058336981882655\n",
      "Average test loss: 0.007105271673450867\n",
      "Epoch 203/300\n",
      "Average training loss: 0.004448095533582899\n",
      "Average test loss: 0.007262112199018399\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004382168059547742\n",
      "Average test loss: 0.006967589231828848\n",
      "Epoch 205/300\n",
      "Average training loss: 0.004352299667480919\n",
      "Average test loss: 0.007439431141648027\n",
      "Epoch 206/300\n",
      "Average training loss: 0.004377148977377348\n",
      "Average test loss: 0.0068512468304899\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00438257568805582\n",
      "Average test loss: 0.0069251447733905585\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004470565992510981\n",
      "Average test loss: 0.007118146702647209\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004483666480208436\n",
      "Average test loss: 0.007346724691904253\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0043490603191571105\n",
      "Average test loss: 0.006843549057013459\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0043432433427208\n",
      "Average test loss: 0.006948974909467829\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004358644942442576\n",
      "Average test loss: 0.007070654338018761\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004339295234531164\n",
      "Average test loss: 0.007065709196031094\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004355905255923669\n",
      "Average test loss: 0.007085945701019632\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00434005180042651\n",
      "Average test loss: 0.006976559854629967\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004380482354511817\n",
      "Average test loss: 0.007064653341141012\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0043398369536217715\n",
      "Average test loss: 0.00691490824893117\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004328567335175143\n",
      "Average test loss: 0.007161125580055846\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0043174676315652\n",
      "Average test loss: 0.0070314987219042245\n",
      "Epoch 220/300\n",
      "Average training loss: 0.004382919396377272\n",
      "Average test loss: 0.006942601864536603\n",
      "Epoch 221/300\n",
      "Average training loss: 0.004319310261143579\n",
      "Average test loss: 0.006946254131694635\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0043078609719458556\n",
      "Average test loss: 0.007043611168033547\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004336127986303634\n",
      "Average test loss: 0.006817732731501261\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0043094506474832695\n",
      "Average test loss: 0.006981797642178006\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00432601366750896\n",
      "Average test loss: 0.007117263037711382\n",
      "Epoch 226/300\n",
      "Average training loss: 0.004309986979183223\n",
      "Average test loss: 0.00727812406917413\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0043386037647724154\n",
      "Average test loss: 0.006843026712950733\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004300980579935842\n",
      "Average test loss: 0.0069955910154514845\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004277887016534805\n",
      "Average test loss: 0.006981239748497804\n",
      "Epoch 230/300\n",
      "Average training loss: 0.004287467385745711\n",
      "Average test loss: 0.006866959993624025\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0043258706604441\n",
      "Average test loss: 0.007283058723227845\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004295888627568881\n",
      "Average test loss: 0.007043071264194118\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004290033084236913\n",
      "Average test loss: 0.006888844145254956\n",
      "Epoch 234/300\n",
      "Average training loss: 0.004289435939656363\n",
      "Average test loss: 0.007010620499650637\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004289111777312226\n",
      "Average test loss: 0.008905988676680458\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0042841715800265474\n",
      "Average test loss: 0.007200685278409057\n",
      "Epoch 237/300\n",
      "Average training loss: 0.004281006003833479\n",
      "Average test loss: 0.006955259789609247\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004282819575733609\n",
      "Average test loss: 0.006950906893445386\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004291397001594305\n",
      "Average test loss: 0.00708314142955674\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004309353888448742\n",
      "Average test loss: 0.006834046655231052\n",
      "Epoch 241/300\n",
      "Average training loss: 0.004242743700535761\n",
      "Average test loss: 0.0069792071816821896\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004250344592250056\n",
      "Average test loss: 0.006946578744798899\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004263128684005804\n",
      "Average test loss: 0.006747004374861717\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0042591225246174464\n",
      "Average test loss: 0.006962450491885344\n",
      "Epoch 245/300\n",
      "Average training loss: 0.004247921439508597\n",
      "Average test loss: 0.010199513095948432\n",
      "Epoch 246/300\n",
      "Average training loss: 0.004247019799219237\n",
      "Average test loss: 0.00692936161864135\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004237506985664368\n",
      "Average test loss: 0.006923667489240567\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0043211222468978825\n",
      "Average test loss: 0.006859942205250263\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004246253124334746\n",
      "Average test loss: 0.007191959658016761\n",
      "Epoch 250/300\n",
      "Average training loss: 0.005251399257116847\n",
      "Average test loss: 0.009463900936974419\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004342310531064868\n",
      "Average test loss: 0.00683939034326209\n",
      "Epoch 252/300\n",
      "Average training loss: 0.004202785473730829\n",
      "Average test loss: 0.006998553106354342\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004189659108304315\n",
      "Average test loss: 0.007069464281201363\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0041929221368498274\n",
      "Average test loss: 0.007059328344547086\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004220199988947975\n",
      "Average test loss: 0.007028973076906469\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004225448279745049\n",
      "Average test loss: 0.007647688066379892\n",
      "Epoch 257/300\n",
      "Average training loss: 0.004205928565313419\n",
      "Average test loss: 0.007009981975787216\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004240984169559346\n",
      "Average test loss: 0.007022028823693593\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0042416900537080235\n",
      "Average test loss: 0.006880460660904646\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004210715809630023\n",
      "Average test loss: 0.007131236303183767\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004222069569759898\n",
      "Average test loss: 0.006894110265705321\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004223512962874439\n",
      "Average test loss: 0.006831990514364507\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004200159060872263\n",
      "Average test loss: 0.0068950059103469056\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004250868058866925\n",
      "Average test loss: 0.006790219445195463\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004203410699135727\n",
      "Average test loss: 0.0070702164529098405\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004220415363709132\n",
      "Average test loss: 0.006879701135473119\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004226397111804949\n",
      "Average test loss: 0.006937016163435247\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004257362407528692\n",
      "Average test loss: 0.006915749767174324\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004171817037380404\n",
      "Average test loss: 0.006814904090431001\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004184453659587436\n",
      "Average test loss: 0.007050103644529978\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004194227771212657\n",
      "Average test loss: 0.006894846936480866\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0041721964648200405\n",
      "Average test loss: 0.006942008234974411\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004213441796600818\n",
      "Average test loss: 0.006895176505876912\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004174686344340444\n",
      "Average test loss: 0.0070364800281822685\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004694368600638377\n",
      "Average test loss: 0.0070913186193340355\n",
      "Epoch 276/300\n",
      "Average training loss: 0.00423291916689939\n",
      "Average test loss: 0.006910008177575138\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0041523840100400975\n",
      "Average test loss: 0.006825714100152254\n",
      "Epoch 278/300\n",
      "Average training loss: 0.004153518877302607\n",
      "Average test loss: 0.007069201567520698\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004164624132836858\n",
      "Average test loss: 0.00689030350993077\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004174869411935409\n",
      "Average test loss: 0.006892920830183559\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0041464710881312684\n",
      "Average test loss: 0.007116144803663095\n",
      "Epoch 282/300\n",
      "Average training loss: 0.004182714911798636\n",
      "Average test loss: 0.006978450154471729\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004169770267067684\n",
      "Average test loss: 0.007211154667867555\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004160692621022463\n",
      "Average test loss: 0.0070894651330179635\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0041840219770868615\n",
      "Average test loss: 0.007196856005324258\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0041613398250192404\n",
      "Average test loss: 0.007026949381248818\n",
      "Epoch 287/300\n",
      "Average training loss: 0.004155370965600013\n",
      "Average test loss: 0.007154497733546628\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004174305552823676\n",
      "Average test loss: 0.007196120348655515\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004165660290254487\n",
      "Average test loss: 0.006838493508183294\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004147865880901615\n",
      "Average test loss: 0.006948654321746694\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004182238474695219\n",
      "Average test loss: 0.006951536796159214\n",
      "Epoch 292/300\n",
      "Average training loss: 0.004140280437138345\n",
      "Average test loss: 0.007023383233282301\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004159411013747255\n",
      "Average test loss: 0.007324222713708878\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004147591729544931\n",
      "Average test loss: 0.006964833374238677\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0041358068036950295\n",
      "Average test loss: 0.0069604499530461095\n",
      "Epoch 296/300\n",
      "Average training loss: 0.004140356313851144\n",
      "Average test loss: 0.006920047484752205\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004143950733666618\n",
      "Average test loss: 0.007147986149622335\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004123257437100013\n",
      "Average test loss: 0.0070278226758042975\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004147995728585455\n",
      "Average test loss: 0.0068997417609724734\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004121541000695692\n",
      "Average test loss: 0.006906349302166038\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.021735805447730752\n",
      "Average test loss: 0.011072021717412605\n",
      "Epoch 2/300\n",
      "Average training loss: 0.00992810814993249\n",
      "Average test loss: 0.012108677349984645\n",
      "Epoch 3/300\n",
      "Average training loss: 0.00859260346616308\n",
      "Average test loss: 0.007851860215266546\n",
      "Epoch 4/300\n",
      "Average training loss: 0.00775820389100247\n",
      "Average test loss: 0.007483749191380209\n",
      "Epoch 5/300\n",
      "Average training loss: 0.007112894353767236\n",
      "Average test loss: 0.0072863044258621\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006814249419089821\n",
      "Average test loss: 0.006468526811649402\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006576410342421796\n",
      "Average test loss: 0.006053128283470869\n",
      "Epoch 8/300\n",
      "Average training loss: 0.006399544753962093\n",
      "Average test loss: 0.009294544874793953\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006076685699737734\n",
      "Average test loss: 0.006081918213930395\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005866981268342998\n",
      "Average test loss: 0.006670435878137747\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005788559518340561\n",
      "Average test loss: 0.005470758512616158\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005534463029768732\n",
      "Average test loss: 0.005596472802675433\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005445577419052521\n",
      "Average test loss: 0.005975061373992099\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005316193834775024\n",
      "Average test loss: 0.011592295452124543\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005199870341767867\n",
      "Average test loss: 0.005557238349897994\n",
      "Epoch 16/300\n",
      "Average training loss: 0.005056000357286798\n",
      "Average test loss: 0.00530004671919677\n",
      "Epoch 17/300\n",
      "Average training loss: 0.004956139870401886\n",
      "Average test loss: 0.005190956670376989\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004942998578564988\n",
      "Average test loss: 0.005105178650882509\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004796733941882849\n",
      "Average test loss: 0.005427770473890835\n",
      "Epoch 20/300\n",
      "Average training loss: 0.004793008477737506\n",
      "Average test loss: 0.005308181545386712\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004672288027902444\n",
      "Average test loss: 0.004944491913335191\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004635625808810194\n",
      "Average test loss: 0.004891394567158487\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004511862234522899\n",
      "Average test loss: 0.004665433685605725\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004594699102971289\n",
      "Average test loss: 0.004786529027339485\n",
      "Epoch 25/300\n",
      "Average training loss: 0.004426824355290996\n",
      "Average test loss: 0.006047994115286403\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004413269814931684\n",
      "Average test loss: 0.00652420688006613\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004409199970049991\n",
      "Average test loss: 0.006798831622219748\n",
      "Epoch 28/300\n",
      "Average training loss: 0.004304058408157693\n",
      "Average test loss: 0.004559903726809555\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004256020283119546\n",
      "Average test loss: 0.004750999264419079\n",
      "Epoch 30/300\n",
      "Average training loss: 0.004246468420657847\n",
      "Average test loss: 0.0047095722361571255\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004194982378640109\n",
      "Average test loss: 0.004579318644478917\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004161897387355566\n",
      "Average test loss: 0.004761862568971184\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0047626940247913205\n",
      "Average test loss: 0.004655509867601925\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004100568835313121\n",
      "Average test loss: 0.004742244021346172\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004085244546333949\n",
      "Average test loss: 0.004599713530805375\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004039133882977896\n",
      "Average test loss: 0.004875114121784766\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004035824655658669\n",
      "Average test loss: 0.004635053846778141\n",
      "Epoch 38/300\n",
      "Average training loss: 0.003993281281656689\n",
      "Average test loss: 0.004697035489396917\n",
      "Epoch 39/300\n",
      "Average training loss: 0.003983438067759077\n",
      "Average test loss: 0.004460227969619963\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0039351928972949585\n",
      "Average test loss: 0.004517711658858591\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00440899577902423\n",
      "Average test loss: 0.005663053449243307\n",
      "Epoch 42/300\n",
      "Average training loss: 0.004049300229176879\n",
      "Average test loss: 0.004496980689879921\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0038873314402169653\n",
      "Average test loss: 0.004562902654210726\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0038444772159887683\n",
      "Average test loss: 0.00445340238718523\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0038276557315968804\n",
      "Average test loss: 0.0043245658977992\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00382916675032013\n",
      "Average test loss: 0.005769384887483385\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0038052280590766007\n",
      "Average test loss: 0.004405984073463413\n",
      "Epoch 48/300\n",
      "Average training loss: 0.003971359187737107\n",
      "Average test loss: 0.005701392236269183\n",
      "Epoch 49/300\n",
      "Average training loss: 0.00377402911024789\n",
      "Average test loss: 0.004472095979998509\n",
      "Epoch 50/300\n",
      "Average training loss: 0.003726546585559845\n",
      "Average test loss: 0.004311487007058329\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003726820617500279\n",
      "Average test loss: 0.005025082960724831\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0036883331226805847\n",
      "Average test loss: 0.004400641331656112\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0036637245522191126\n",
      "Average test loss: 0.006000169470906257\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0036994368938936127\n",
      "Average test loss: 0.004388589532011085\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003718854760958089\n",
      "Average test loss: 0.004786472000388635\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0036221524181051386\n",
      "Average test loss: 0.004596342454560929\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0038726446500255\n",
      "Average test loss: 0.0042749744604031244\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0036222744946264557\n",
      "Average test loss: 0.004308375573406617\n",
      "Epoch 59/300\n",
      "Average training loss: 0.003561821719424592\n",
      "Average test loss: 0.0042263610462347665\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0035689509070167937\n",
      "Average test loss: 0.0045537923189500965\n",
      "Epoch 61/300\n",
      "Average training loss: 0.003598792708168427\n",
      "Average test loss: 0.0043243771559662286\n",
      "Epoch 62/300\n",
      "Average training loss: 0.003536597427808576\n",
      "Average test loss: 0.004349878930383258\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0035379402451217175\n",
      "Average test loss: 0.004403998408052656\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0035273855740411415\n",
      "Average test loss: 0.004400414164695475\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0035680159448335568\n",
      "Average test loss: 0.007709474766006072\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0035539241940197018\n",
      "Average test loss: 0.004209991528756089\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0034673701793783243\n",
      "Average test loss: 0.004499283639921082\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004352990312708749\n",
      "Average test loss: 0.004580877404246065\n",
      "Epoch 69/300\n",
      "Average training loss: 0.003792509724489517\n",
      "Average test loss: 0.004326391327712271\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0034822233888424105\n",
      "Average test loss: 0.004257946181214518\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0034465597820364765\n",
      "Average test loss: 0.004426863402542141\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0034733346870376002\n",
      "Average test loss: 1.3941859805848864\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00412390763954156\n",
      "Average test loss: 0.004377602871921327\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0034714100778930716\n",
      "Average test loss: 0.004243211557467779\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0034196452005869813\n",
      "Average test loss: 0.004134119524103072\n",
      "Epoch 76/300\n",
      "Average training loss: 0.003394450906664133\n",
      "Average test loss: 0.004459925286678804\n",
      "Epoch 77/300\n",
      "Average training loss: 0.003417971175370945\n",
      "Average test loss: 0.0043983372563703195\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0034388130305127965\n",
      "Average test loss: 0.004241664813624488\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0034096485750956668\n",
      "Average test loss: 0.0043028224234779675\n",
      "Epoch 80/300\n",
      "Average training loss: 0.003377887330742346\n",
      "Average test loss: 0.005240014986031586\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0033828280899259777\n",
      "Average test loss: 0.004389647332330545\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0036678805148435962\n",
      "Average test loss: 0.00415367770070831\n",
      "Epoch 83/300\n",
      "Average training loss: 0.003360096212062571\n",
      "Average test loss: 0.0042243916913867\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003354457401360075\n",
      "Average test loss: 0.004304864164234864\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0036202015357298984\n",
      "Average test loss: 0.00423303679873546\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0033222371091445285\n",
      "Average test loss: 0.004172039993935161\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0033065390928337973\n",
      "Average test loss: 0.004286393788953622\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0033362666840354603\n",
      "Average test loss: 0.004289820733790597\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0032838404681533573\n",
      "Average test loss: 0.004166759021580219\n",
      "Epoch 90/300\n",
      "Average training loss: 0.003314916514067186\n",
      "Average test loss: 0.004328733502162827\n",
      "Epoch 91/300\n",
      "Average training loss: 0.003459629884403613\n",
      "Average test loss: 0.004089723289633791\n",
      "Epoch 92/300\n",
      "Average training loss: 0.003262751601222489\n",
      "Average test loss: 0.0045272722397413515\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0032637864841769137\n",
      "Average test loss: 0.004629788773755233\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0032573034523261917\n",
      "Average test loss: 0.004253818818264537\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00431716701740192\n",
      "Average test loss: 0.005522066593998009\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003739248260648714\n",
      "Average test loss: 0.004681304612093502\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0033837303411629466\n",
      "Average test loss: 0.004280564818945196\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0032693487965605324\n",
      "Average test loss: 0.004318739881532059\n",
      "Epoch 99/300\n",
      "Average training loss: 0.003247447734905614\n",
      "Average test loss: 0.004226002392048637\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00324765221381353\n",
      "Average test loss: 0.015321018495493464\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0034257437862041922\n",
      "Average test loss: 0.0042525947044293085\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0032194098693629106\n",
      "Average test loss: 0.004314791485667229\n",
      "Epoch 103/300\n",
      "Average training loss: 0.003209183952667647\n",
      "Average test loss: 0.0041770277470350265\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0032021933450467055\n",
      "Average test loss: 0.004245968204612534\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0032139112196034854\n",
      "Average test loss: 0.004176459657649199\n",
      "Epoch 106/300\n",
      "Average training loss: 0.003203294467387928\n",
      "Average test loss: 0.004196792157573832\n",
      "Epoch 107/300\n",
      "Average training loss: 0.003268648181938463\n",
      "Average test loss: 0.0043562538859744865\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0031970675480034617\n",
      "Average test loss: 0.004306665527116922\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0032011652862032254\n",
      "Average test loss: 0.0042547425656682914\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003166162495397859\n",
      "Average test loss: 0.00410705112417539\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0031660415704051655\n",
      "Average test loss: 0.004181931186674369\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0037861208323803214\n",
      "Average test loss: 0.0044572563966115315\n",
      "Epoch 113/300\n",
      "Average training loss: 0.003192050285637379\n",
      "Average test loss: 0.004145675008495648\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0031431916372643576\n",
      "Average test loss: 0.00418629059733616\n",
      "Epoch 115/300\n",
      "Average training loss: 0.003144952278377281\n",
      "Average test loss: 0.004237019752669665\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0031325630814664895\n",
      "Average test loss: 0.01545936130732298\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0032837502078877554\n",
      "Average test loss: 0.004168037669112285\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0031389967602574163\n",
      "Average test loss: 0.0041369508566955725\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005707220322969887\n",
      "Average test loss: 0.25306424514783754\n",
      "Epoch 120/300\n",
      "Average training loss: 0.00553030275925994\n",
      "Average test loss: 0.07461445176353057\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0043016315441992544\n",
      "Average test loss: 0.21589595117482047\n",
      "Epoch 122/300\n",
      "Average training loss: 0.003977378158933587\n",
      "Average test loss: 0.0060653745209177335\n",
      "Epoch 123/300\n",
      "Average training loss: 0.003797575209082829\n",
      "Average test loss: 0.029240863295065034\n",
      "Epoch 124/300\n",
      "Average training loss: 0.003558750448127588\n",
      "Average test loss: 1.1672590052402683\n",
      "Epoch 125/300\n",
      "Average training loss: 0.003435388205572963\n",
      "Average test loss: 0.004803087895322177\n",
      "Epoch 126/300\n",
      "Average training loss: 0.003349098946692215\n",
      "Average test loss: 1.564003226934208\n",
      "Epoch 127/300\n",
      "Average training loss: 0.003248570342858632\n",
      "Average test loss: 0.0057682906687259675\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0033046674384839005\n",
      "Average test loss: 0.2950487298601204\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0032554270778265264\n",
      "Average test loss: 0.004170335302439829\n",
      "Epoch 130/300\n",
      "Average training loss: 0.003205784570632709\n",
      "Average test loss: 0.004638374492939976\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0031886241148329445\n",
      "Average test loss: 0.004406528381009896\n",
      "Epoch 132/300\n",
      "Average training loss: 0.003187148784804675\n",
      "Average test loss: 0.0041667473307914205\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0031583042548348505\n",
      "Average test loss: 0.011049148079835706\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0031908720735874443\n",
      "Average test loss: 0.00417406298344334\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0031572869720144403\n",
      "Average test loss: 0.004177305840576688\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0031441591715233195\n",
      "Average test loss: 0.0046947135236114265\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0031363462859557733\n",
      "Average test loss: 0.004125975295901299\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0031094390095935927\n",
      "Average test loss: 0.004322154914753304\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0031125360110567674\n",
      "Average test loss: 0.004261527216475871\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0033860503886308933\n",
      "Average test loss: 0.004526701884137259\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0031860914888481298\n",
      "Average test loss: 0.004476548235449526\n",
      "Epoch 142/300\n",
      "Average training loss: 0.003069129370359911\n",
      "Average test loss: 0.004934743754151795\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0030946387940396867\n",
      "Average test loss: 0.004158112481650379\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0030529588347093925\n",
      "Average test loss: 0.007060844679673513\n",
      "Epoch 145/300\n",
      "Average training loss: 0.003092874405077762\n",
      "Average test loss: 0.00619345893462499\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0031091258445133767\n",
      "Average test loss: 0.0041559457542995615\n",
      "Epoch 147/300\n",
      "Average training loss: 0.003046018133146895\n",
      "Average test loss: 0.10303024005558756\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0031107355465905535\n",
      "Average test loss: 0.004090567112382915\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0030298720602360035\n",
      "Average test loss: 0.004690952184713549\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0030258588674995633\n",
      "Average test loss: 0.004118884457482232\n",
      "Epoch 151/300\n",
      "Average training loss: 0.003206783093098137\n",
      "Average test loss: 0.0042634150286515555\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0030777391464345983\n",
      "Average test loss: 0.00509397355094552\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0030220789849344227\n",
      "Average test loss: 0.004118039458783137\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002998229841184285\n",
      "Average test loss: 0.004201353552440802\n",
      "Epoch 155/300\n",
      "Average training loss: 0.003001573104618324\n",
      "Average test loss: 0.004362375852237973\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00299830971347789\n",
      "Average test loss: 0.00421431901699139\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00332403512009316\n",
      "Average test loss: 0.004210709780868556\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0030419816710054873\n",
      "Average test loss: 0.00424255131640368\n",
      "Epoch 159/300\n",
      "Average training loss: 0.003007122617214918\n",
      "Average test loss: 0.00420580204617646\n",
      "Epoch 160/300\n",
      "Average training loss: 0.003000221131576432\n",
      "Average test loss: 0.004233000396440426\n",
      "Epoch 161/300\n",
      "Average training loss: 0.00298458661366668\n",
      "Average test loss: 0.004243806071165535\n",
      "Epoch 162/300\n",
      "Average training loss: 0.003501226644963026\n",
      "Average test loss: 0.004112144115484423\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0031254563776569235\n",
      "Average test loss: 0.0041124722082167865\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0029861044625027313\n",
      "Average test loss: 0.00411041707617955\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0029573627480616173\n",
      "Average test loss: 0.0041384158478014995\n",
      "Epoch 166/300\n",
      "Average training loss: 0.002955861199233267\n",
      "Average test loss: 0.0042932780006279545\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0030605381046318347\n",
      "Average test loss: 0.004171749462890956\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0029461146528936095\n",
      "Average test loss: 0.0040943137763275045\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0029573797227607835\n",
      "Average test loss: 0.0043850777484476565\n",
      "Epoch 170/300\n",
      "Average training loss: 0.002964700747384793\n",
      "Average test loss: 0.004463479198101494\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0030016293041408063\n",
      "Average test loss: 0.005333923071208928\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0029531761116037766\n",
      "Average test loss: 0.005028865014927255\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0029897476256721548\n",
      "Average test loss: 0.00440428192458219\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002949182315108677\n",
      "Average test loss: 0.004198508965679341\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0029471544393648704\n",
      "Average test loss: 0.004253031169167823\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0029534807049979767\n",
      "Average test loss: 0.004207415991359287\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002924237274254362\n",
      "Average test loss: 0.004223080881353882\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0030462640778472026\n",
      "Average test loss: 0.004199452975143989\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0029115627221763135\n",
      "Average test loss: 0.004269242935296562\n",
      "Epoch 180/300\n",
      "Average training loss: 0.002968304242110915\n",
      "Average test loss: 0.0042888108218709625\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0029411571338358853\n",
      "Average test loss: 0.004309926435765293\n",
      "Epoch 182/300\n",
      "Average training loss: 0.002940141578722331\n",
      "Average test loss: 0.004125909006430043\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0029213997942085066\n",
      "Average test loss: 0.00439210676194893\n",
      "Epoch 184/300\n",
      "Average training loss: 0.002917249181204372\n",
      "Average test loss: 0.004152043076439036\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0029262196094625528\n",
      "Average test loss: 0.004324692002601094\n",
      "Epoch 186/300\n",
      "Average training loss: 0.002923603826512893\n",
      "Average test loss: 0.004583317382054196\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0029230718279464377\n",
      "Average test loss: 0.004169245306195484\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0029139492344111206\n",
      "Average test loss: 0.004172638338887029\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0029033729140129356\n",
      "Average test loss: 0.004491733340546489\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0029057448332508403\n",
      "Average test loss: 0.0041587875166700945\n",
      "Epoch 191/300\n",
      "Average training loss: 0.002892033193053471\n",
      "Average test loss: 0.004239805416928397\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0029026585929095745\n",
      "Average test loss: 0.00429703278798196\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0029147477087875205\n",
      "Average test loss: 0.004298595155692763\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0029181402909259\n",
      "Average test loss: 0.004315547679861386\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002895000832155347\n",
      "Average test loss: 0.004541428776043984\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0031570909041911364\n",
      "Average test loss: 0.004299398929294613\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0028540096153608628\n",
      "Average test loss: 0.004260883199671904\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0029020904283970595\n",
      "Average test loss: 0.014621206808421347\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0037998591758724716\n",
      "Average test loss: 0.004174958584623204\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0030196698976473675\n",
      "Average test loss: 0.00423837330730425\n",
      "Epoch 201/300\n",
      "Average training loss: 0.002894145819875929\n",
      "Average test loss: 0.004188547912985087\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0028944172888166375\n",
      "Average test loss: 0.004482558229317268\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0028583176792081858\n",
      "Average test loss: 0.0043303271180225745\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0028516536456429297\n",
      "Average test loss: 0.004218335601604647\n",
      "Epoch 205/300\n",
      "Average training loss: 0.002862686138600111\n",
      "Average test loss: 0.0042154816829909885\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0028828324381675987\n",
      "Average test loss: 0.004179169757291675\n",
      "Epoch 207/300\n",
      "Average training loss: 0.003151387465910779\n",
      "Average test loss: 0.004396328618956937\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0028411082681268456\n",
      "Average test loss: 0.004195826376477877\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002827226273094614\n",
      "Average test loss: 21.977865398830836\n",
      "Epoch 210/300\n",
      "Average training loss: 0.003501620713621378\n",
      "Average test loss: 0.004289007039533721\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0029259896677815253\n",
      "Average test loss: 0.004641998741982712\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0028420136202540664\n",
      "Average test loss: 0.004136978545536597\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0028350295428600574\n",
      "Average test loss: 0.004552096240843336\n",
      "Epoch 214/300\n",
      "Average training loss: 0.002890489016763038\n",
      "Average test loss: 0.004903117166625129\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00283987017100056\n",
      "Average test loss: 0.0043073648831082716\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0028277107187443308\n",
      "Average test loss: 0.004178446140140295\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0028590022828429936\n",
      "Average test loss: 0.004187181462430292\n",
      "Epoch 218/300\n",
      "Average training loss: 0.003011182631055514\n",
      "Average test loss: 0.02993461244304975\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0028609383093814054\n",
      "Average test loss: 0.004492456825657023\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002815165091512932\n",
      "Average test loss: 0.004187023492323028\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0028098310712311005\n",
      "Average test loss: 0.004169085862321986\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0028529585947593052\n",
      "Average test loss: 0.004254968950317965\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0028409039092560607\n",
      "Average test loss: 0.004284423257120781\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0028298749008940326\n",
      "Average test loss: 0.004193819986242387\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0028206460206872887\n",
      "Average test loss: 0.004296061568790012\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0028336894835035006\n",
      "Average test loss: 0.004569043358580934\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002836808721845349\n",
      "Average test loss: 0.004220289817286863\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0028893738550444443\n",
      "Average test loss: 0.004169306626750364\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002825383755688866\n",
      "Average test loss: 0.004337902487359113\n",
      "Epoch 230/300\n",
      "Average training loss: 0.002850581913979517\n",
      "Average test loss: 0.005645302930225929\n",
      "Epoch 231/300\n",
      "Average training loss: 0.002908211009576917\n",
      "Average test loss: 0.004231450570126374\n",
      "Epoch 232/300\n",
      "Average training loss: 0.002797581809469395\n",
      "Average test loss: 0.0042204740612457195\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0027998817374722826\n",
      "Average test loss: 0.005618588540703059\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0028020413648337126\n",
      "Average test loss: 0.004168286652200752\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0028058822059796916\n",
      "Average test loss: 0.004373478887809647\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0028010433216889698\n",
      "Average test loss: 0.004224103098321292\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0028848293961750136\n",
      "Average test loss: 0.004208909305433432\n",
      "Epoch 238/300\n",
      "Average training loss: 0.002843447357002232\n",
      "Average test loss: 0.004273538556363848\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0027865650438600115\n",
      "Average test loss: 0.004194610724432601\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0028053583649711475\n",
      "Average test loss: 0.004205278753820393\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0028187378730831873\n",
      "Average test loss: 0.004241032746516996\n",
      "Epoch 242/300\n",
      "Average training loss: 0.002789983648392889\n",
      "Average test loss: 0.004232913545022408\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002802439704951313\n",
      "Average test loss: 0.0042525889465792315\n",
      "Epoch 244/300\n",
      "Average training loss: 0.002817345761176613\n",
      "Average test loss: 0.004242694734285275\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0027764679222471184\n",
      "Average test loss: 0.004289307609614399\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0027776373071182105\n",
      "Average test loss: 0.004214572690013382\n",
      "Epoch 247/300\n",
      "Average training loss: 0.002781490679209431\n",
      "Average test loss: 0.004532442253289951\n",
      "Epoch 248/300\n",
      "Average training loss: 0.002783222279407912\n",
      "Average test loss: 0.004116849193556441\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0027945471445305478\n",
      "Average test loss: 0.005997120877934827\n",
      "Epoch 250/300\n",
      "Average training loss: 0.002803617544886139\n",
      "Average test loss: 0.004264507295356857\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0027765479023671813\n",
      "Average test loss: 0.004777251290364398\n",
      "Epoch 252/300\n",
      "Average training loss: 0.002763432552003198\n",
      "Average test loss: 0.004236609052452776\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0027692983568542535\n",
      "Average test loss: 0.004309880017406411\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0027551028095185756\n",
      "Average test loss: 0.004419367202454143\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0028379620920038887\n",
      "Average test loss: 0.004399893917971187\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0027582057317097983\n",
      "Average test loss: 0.004401135648083356\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002766826732498076\n",
      "Average test loss: 0.00427922281747063\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0027501358451942604\n",
      "Average test loss: 0.006182841987245613\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0027677406167818442\n",
      "Average test loss: 0.004544062733650207\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002746076782544454\n",
      "Average test loss: 0.004270999866848191\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0028016099733197027\n",
      "Average test loss: 0.004370177827982439\n",
      "Epoch 262/300\n",
      "Average training loss: 0.002757755490640799\n",
      "Average test loss: 0.004170621626906925\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002756091277425488\n",
      "Average test loss: 0.004455096753314137\n",
      "Epoch 264/300\n",
      "Average training loss: 0.002745950148958299\n",
      "Average test loss: 0.004477543945527739\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0027632764799313415\n",
      "Average test loss: 0.005877835777484708\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002771576436236501\n",
      "Average test loss: 0.004663448224878974\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0027455325989673533\n",
      "Average test loss: 0.004319031671103504\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0028201486892584296\n",
      "Average test loss: 0.004287482757742206\n",
      "Epoch 269/300\n",
      "Average training loss: 0.002728281517409616\n",
      "Average test loss: 0.004232599554376469\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0027282518175327113\n",
      "Average test loss: 0.005769054309775432\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002741103901631302\n",
      "Average test loss: 0.0044374295766982765\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0027429902839163937\n",
      "Average test loss: 0.00425580506813195\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0027227869783010747\n",
      "Average test loss: 0.004262743334596356\n",
      "Epoch 274/300\n",
      "Average training loss: 0.002773422584455046\n",
      "Average test loss: 0.004342007437513934\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0027298699801580773\n",
      "Average test loss: 0.006738497327185339\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0027424762073076435\n",
      "Average test loss: 0.004253555040392611\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0027371011693030594\n",
      "Average test loss: 0.004270003969884581\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0027564114317711855\n",
      "Average test loss: 0.00420676513409449\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0027363924247523147\n",
      "Average test loss: 0.004304859823650784\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0027366941165592935\n",
      "Average test loss: 0.004351202466007736\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0027260182110799684\n",
      "Average test loss: 0.004237498910062843\n",
      "Epoch 282/300\n",
      "Average training loss: 0.002729067431348893\n",
      "Average test loss: 0.004213042895827028\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0027177316730635035\n",
      "Average test loss: 0.005836307895266348\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0027221111212339666\n",
      "Average test loss: 0.7789282730023066\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0027407378324617944\n",
      "Average test loss: 0.008441559428142177\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002737877454401718\n",
      "Average test loss: 0.004214131012558937\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0028783509638160465\n",
      "Average test loss: 0.005358156170282099\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002828899783185787\n",
      "Average test loss: 0.004333027287075917\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0026825840019931396\n",
      "Average test loss: 0.0043354651772727565\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00268402040273779\n",
      "Average test loss: 0.007166006545225779\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002713660043560796\n",
      "Average test loss: 0.005181709214631054\n",
      "Epoch 292/300\n",
      "Average training loss: 0.002730881936641203\n",
      "Average test loss: 0.0041608838169938985\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0026868934749315183\n",
      "Average test loss: 0.004412082891083426\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0028445638525817127\n",
      "Average test loss: 0.00428835175683101\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0026998421421481505\n",
      "Average test loss: 0.004159519420729743\n",
      "Epoch 296/300\n",
      "Average training loss: 0.002694755307502217\n",
      "Average test loss: 0.004736012654172049\n",
      "Epoch 297/300\n",
      "Average training loss: 0.002707522332875265\n",
      "Average test loss: 0.00430011960822675\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0027026466199507318\n",
      "Average test loss: 0.0042564490878333645\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0027005093855162463\n",
      "Average test loss: 0.004287086200176014\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0026873561617814833\n",
      "Average test loss: 0.004289100872973601\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.020233989492886597\n",
      "Average test loss: 0.00923043676548534\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008522463416473733\n",
      "Average test loss: 0.006806005568554004\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007167024122758044\n",
      "Average test loss: 0.00623947826317615\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0063880611235896745\n",
      "Average test loss: 0.007807961202330059\n",
      "Epoch 5/300\n",
      "Average training loss: 0.005873600448792179\n",
      "Average test loss: 0.005403172283122937\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005342009009172519\n",
      "Average test loss: 0.0050667440322124295\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005164877350131671\n",
      "Average test loss: 0.005055800413505899\n",
      "Epoch 8/300\n",
      "Average training loss: 0.005024452366762691\n",
      "Average test loss: 0.004912451257929206\n",
      "Epoch 9/300\n",
      "Average training loss: 0.004729592892858717\n",
      "Average test loss: 0.0060114806012974845\n",
      "Epoch 10/300\n",
      "Average training loss: 0.004538408086531692\n",
      "Average test loss: 0.007475093613482184\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004533586201568444\n",
      "Average test loss: 0.005848130414883296\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004325789531071981\n",
      "Average test loss: 0.00545610321395927\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004151452409724394\n",
      "Average test loss: 0.004568251373867194\n",
      "Epoch 14/300\n",
      "Average training loss: 0.004378748968864481\n",
      "Average test loss: 0.005108714318523804\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004045878475945857\n",
      "Average test loss: 0.004621717715428935\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0040015883532663185\n",
      "Average test loss: 0.005379761330369446\n",
      "Epoch 17/300\n",
      "Average training loss: 0.003785584285441372\n",
      "Average test loss: 0.003970754920608468\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0038272395928700765\n",
      "Average test loss: 0.0037319601888044012\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0037102749227649638\n",
      "Average test loss: 0.028941592822472256\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0037157182273351483\n",
      "Average test loss: 0.004931059380372365\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0037764995692090857\n",
      "Average test loss: 0.03764501706427998\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0035749043789174823\n",
      "Average test loss: 0.005065338218791618\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003596712865970201\n",
      "Average test loss: 0.003719938972344001\n",
      "Epoch 24/300\n",
      "Average training loss: 0.003563222299226456\n",
      "Average test loss: 0.003886546900909808\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0034725686291025744\n",
      "Average test loss: 0.004642663210630417\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0034099761539449296\n",
      "Average test loss: 0.003526197979433669\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0033302447702735663\n",
      "Average test loss: 0.007489524018433359\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0032894914504140614\n",
      "Average test loss: 0.00653669825134178\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0033117631665534443\n",
      "Average test loss: 0.0034840581653018793\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0032632776000019576\n",
      "Average test loss: 0.0035455942414700987\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0031701776441186665\n",
      "Average test loss: 0.003422154928247134\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0031810729913413524\n",
      "Average test loss: 0.003455065380781889\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0032699916099922523\n",
      "Average test loss: 0.00457996734438671\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0031624925546348097\n",
      "Average test loss: 0.0035470958337601687\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0031104927954988346\n",
      "Average test loss: 0.0035040769196218913\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003071466650813818\n",
      "Average test loss: 0.0033488450654678877\n",
      "Epoch 37/300\n",
      "Average training loss: 0.003040467747590608\n",
      "Average test loss: 0.004583181069956885\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0030419912779082853\n",
      "Average test loss: 0.003459358306808604\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0030098629786322515\n",
      "Average test loss: 0.0030964708322038254\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0032411266141053703\n",
      "Average test loss: 0.0036453642890685134\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00300006375172072\n",
      "Average test loss: 0.003888088126356403\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002931536311490668\n",
      "Average test loss: 0.006846054178145196\n",
      "Epoch 43/300\n",
      "Average training loss: 0.003017064374768072\n",
      "Average test loss: 0.003387868234059877\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0030022484324872495\n",
      "Average test loss: 0.0036082459021773605\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0028922966896659796\n",
      "Average test loss: 0.005965564279920525\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0029848476621425813\n",
      "Average test loss: 0.004079772301432159\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0029836517069488765\n",
      "Average test loss: 0.0033783766103701457\n",
      "Epoch 48/300\n",
      "Average training loss: 0.002818112168668045\n",
      "Average test loss: 0.003384451178006\n",
      "Epoch 49/300\n",
      "Average training loss: 0.002827968718070123\n",
      "Average test loss: 0.0030222006694724164\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0028176922980282042\n",
      "Average test loss: 0.0030180513742897245\n",
      "Epoch 51/300\n",
      "Average training loss: 0.002896548064942989\n",
      "Average test loss: 0.004294774956173367\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002851686821008722\n",
      "Average test loss: 0.003388069019135502\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0027666060380223726\n",
      "Average test loss: 0.006026652926165197\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0027970741778198215\n",
      "Average test loss: 0.005584269377299481\n",
      "Epoch 55/300\n",
      "Average training loss: 0.002790992310891549\n",
      "Average test loss: 0.003197094563808706\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0027127921515040926\n",
      "Average test loss: 0.0030119702311025725\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0028809842434194353\n",
      "Average test loss: 0.003057415550160739\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0027001627707439993\n",
      "Average test loss: 0.0029561494733724328\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0027108832268665233\n",
      "Average test loss: 0.003041133255180385\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0027193293776363136\n",
      "Average test loss: 0.005118098826871978\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002872483251409398\n",
      "Average test loss: 0.003200419896799657\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0026766636261923446\n",
      "Average test loss: 0.0031227428391575815\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0026586472688035832\n",
      "Average test loss: 0.003018249216179053\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002684253486287263\n",
      "Average test loss: 0.0035467213454345864\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0026560243024594254\n",
      "Average test loss: 0.003146882884825269\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0026302040635297697\n",
      "Average test loss: 0.0043103920974665215\n",
      "Epoch 67/300\n",
      "Average training loss: 0.002719400063984924\n",
      "Average test loss: 0.003399041201091475\n",
      "Epoch 68/300\n",
      "Average training loss: 0.002644850332289934\n",
      "Average test loss: 0.003099176256193055\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002642688675266173\n",
      "Average test loss: 0.0030603554350220495\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0026118118533243737\n",
      "Average test loss: 0.07009238569604026\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0026580077034110824\n",
      "Average test loss: 0.003020333742515908\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0036587284019009934\n",
      "Average test loss: 0.033237176751097046\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0038869201857596636\n",
      "Average test loss: 5.205211267963052\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0031669935126685434\n",
      "Average test loss: 0.003037223605852988\n",
      "Epoch 75/300\n",
      "Average training loss: 0.002978413028228614\n",
      "Average test loss: 0.003298329282965925\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002866296599101689\n",
      "Average test loss: 0.12248570058784551\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002938684453566869\n",
      "Average test loss: 0.017309569641533826\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0028505886755883694\n",
      "Average test loss: 2.4064111807859607\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0028180092515216935\n",
      "Average test loss: 0.08692747585806582\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0026961209380792247\n",
      "Average test loss: 0.0029211941642893684\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0026254517044872047\n",
      "Average test loss: 0.0030348674870199627\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002654910082825356\n",
      "Average test loss: 0.003008177012619045\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0026006565636230838\n",
      "Average test loss: 0.0029978373356991344\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0026725931014451717\n",
      "Average test loss: 0.003331655285631617\n",
      "Epoch 85/300\n",
      "Average training loss: 0.002689828819905718\n",
      "Average test loss: 168.73032809787327\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0025806785279677974\n",
      "Average test loss: 0.613082247255577\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0025172544593612353\n",
      "Average test loss: 97.12868203057184\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002586992060351703\n",
      "Average test loss: 0.0036271513259659213\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0025160990154577626\n",
      "Average test loss: 0.005213576689155565\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0025217637146512668\n",
      "Average test loss: 0.002953813428680102\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0025182937189108796\n",
      "Average test loss: 0.010515069619648987\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002493001960010992\n",
      "Average test loss: 0.0028606552181558477\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0025410536639392375\n",
      "Average test loss: 0.12299725669622422\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0027112739549742804\n",
      "Average test loss: 0.007871458667847846\n",
      "Epoch 95/300\n",
      "Average training loss: 0.002480547067191866\n",
      "Average test loss: 0.004149550208614932\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0024929474155522056\n",
      "Average test loss: 0.0028405436525742213\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0024541309943629636\n",
      "Average test loss: 0.002954796684698926\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0024669423593829077\n",
      "Average test loss: 0.003119770162221458\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0024973349343571398\n",
      "Average test loss: 27.46734353742997\n",
      "Epoch 100/300\n",
      "Average training loss: 0.002542343003468381\n",
      "Average test loss: 0.0034904901993771395\n",
      "Epoch 101/300\n",
      "Average training loss: 0.002426879909924335\n",
      "Average test loss: 0.016682282230920262\n",
      "Epoch 102/300\n",
      "Average training loss: 0.00274150659599238\n",
      "Average test loss: 0.008622576981368993\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0024096577351705897\n",
      "Average test loss: 0.003419664873017205\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002404816560861137\n",
      "Average test loss: 0.0029718652829113933\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0024253459771474205\n",
      "Average test loss: 0.07117978558606572\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0024289039763518505\n",
      "Average test loss: 0.00421133597753942\n",
      "Epoch 107/300\n",
      "Average training loss: 0.002404232436998023\n",
      "Average test loss: 0.03622384891493453\n",
      "Epoch 108/300\n",
      "Average training loss: 0.002387887815013528\n",
      "Average test loss: 0.002988374313339591\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0023965105060487985\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'PGD_No_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'PGD_No_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.023119256890482373\n",
      "Average test loss: 1.4966868931253752\n",
      "Epoch 2/300\n",
      "Average training loss: 0.013103751473956638\n",
      "Average test loss: 4.596778882483641\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01239166689829694\n",
      "Average test loss: 13.663041976710161\n",
      "Epoch 4/300\n",
      "Average training loss: 0.011993690114882258\n",
      "Average test loss: 0.500378106618921\n",
      "Epoch 5/300\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'PGD_No_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
