{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import LastLayerLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Nesterov_Network.Nesterov import Nesterov\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Last Layer Loss\n",
    "loss_function = LastLayerLoss()\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.026142521444294187\n",
      "Average test loss: 0.016222729234231844\n",
      "Epoch 2/300\n",
      "Average training loss: 0.013197788668175538\n",
      "Average test loss: 0.0172041621092293\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01182920008401076\n",
      "Average test loss: 0.010165562195910348\n",
      "Epoch 4/300\n",
      "Average training loss: 0.010960471067163679\n",
      "Average test loss: 0.009417658103836907\n",
      "Epoch 5/300\n",
      "Average training loss: 0.010192210784388913\n",
      "Average test loss: 0.009647640440613031\n",
      "Epoch 6/300\n",
      "Average training loss: 0.009446591855751144\n",
      "Average test loss: 0.012707167245447636\n",
      "Epoch 7/300\n",
      "Average training loss: 0.00906806975768672\n",
      "Average test loss: 0.011678324224220382\n",
      "Epoch 8/300\n",
      "Average training loss: 0.008732845351099968\n",
      "Average test loss: 0.008854575311972035\n",
      "Epoch 9/300\n",
      "Average training loss: 0.00859469635412097\n",
      "Average test loss: 0.01096303126629856\n",
      "Epoch 10/300\n",
      "Average training loss: 0.008507199810610877\n",
      "Average test loss: 0.017917518756455846\n",
      "Epoch 11/300\n",
      "Average training loss: 0.008164184297538466\n",
      "Average test loss: 0.009686855170461866\n",
      "Epoch 12/300\n",
      "Average training loss: 0.008171591532313161\n",
      "Average test loss: 0.009008547360698381\n",
      "Epoch 13/300\n",
      "Average training loss: 0.007991204019221994\n",
      "Average test loss: 0.00822909138061934\n",
      "Epoch 14/300\n",
      "Average training loss: 0.007814049721592003\n",
      "Average test loss: 0.007715674795210362\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0077481929419769184\n",
      "Average test loss: 0.00891545707732439\n",
      "Epoch 16/300\n",
      "Average training loss: 0.007691339465479056\n",
      "Average test loss: 0.008226764117678007\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0075475844219326975\n",
      "Average test loss: 0.007773014912174808\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0074874333863457045\n",
      "Average test loss: 0.007863120637834072\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0074662218992080955\n",
      "Average test loss: 0.007908841577668985\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0073362712259921765\n",
      "Average test loss: 0.007840573875440491\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0073138556778430935\n",
      "Average test loss: 0.008460594756735695\n",
      "Epoch 22/300\n",
      "Average training loss: 0.007201066351599164\n",
      "Average test loss: 0.007484963997370667\n",
      "Epoch 23/300\n",
      "Average training loss: 0.007148395486589935\n",
      "Average test loss: 0.008091592295302284\n",
      "Epoch 24/300\n",
      "Average training loss: 0.007097074368347724\n",
      "Average test loss: 0.0075339527738591035\n",
      "Epoch 25/300\n",
      "Average training loss: 0.007029839748723639\n",
      "Average test loss: 0.007426560008691417\n",
      "Epoch 26/300\n",
      "Average training loss: 0.006963288815485106\n",
      "Average test loss: 0.0076132444110181595\n",
      "Epoch 27/300\n",
      "Average training loss: 0.006938638048039542\n",
      "Average test loss: 0.007721983628554477\n",
      "Epoch 28/300\n",
      "Average training loss: 0.006866193125231399\n",
      "Average test loss: 0.007347139284428623\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0068801172462602455\n",
      "Average test loss: 0.0075283373904724915\n",
      "Epoch 30/300\n",
      "Average training loss: 0.00674857959151268\n",
      "Average test loss: 0.007251333161360687\n",
      "Epoch 31/300\n",
      "Average training loss: 0.006750463690194819\n",
      "Average test loss: 0.007569779134872887\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0067212987744973765\n",
      "Average test loss: 0.05624234247207641\n",
      "Epoch 33/300\n",
      "Average training loss: 0.006759122483846214\n",
      "Average test loss: 0.007721750122598476\n",
      "Epoch 34/300\n",
      "Average training loss: 0.006639214381161663\n",
      "Average test loss: 0.007548695039418009\n",
      "Epoch 35/300\n",
      "Average training loss: 0.006654135754538907\n",
      "Average test loss: 0.007458662780622642\n",
      "Epoch 36/300\n",
      "Average training loss: 0.006557137016620901\n",
      "Average test loss: 0.0071517599183652135\n",
      "Epoch 37/300\n",
      "Average training loss: 0.006531000696950489\n",
      "Average test loss: 0.007203069081323015\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0064891412407159805\n",
      "Average test loss: 0.007423460249271658\n",
      "Epoch 39/300\n",
      "Average training loss: 0.006482115219450659\n",
      "Average test loss: 0.007365853026923206\n",
      "Epoch 40/300\n",
      "Average training loss: 0.006455540511343214\n",
      "Average test loss: 0.007569867595202393\n",
      "Epoch 41/300\n",
      "Average training loss: 0.006390566567165984\n",
      "Average test loss: 0.00729373980479108\n",
      "Epoch 42/300\n",
      "Average training loss: 0.006397541157073445\n",
      "Average test loss: 0.007253416051881181\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0063414683995975385\n",
      "Average test loss: 0.007212897831367122\n",
      "Epoch 44/300\n",
      "Average training loss: 0.006336367393533389\n",
      "Average test loss: 0.007276333513359229\n",
      "Epoch 45/300\n",
      "Average training loss: 0.006302202857202953\n",
      "Average test loss: 0.007542944598529074\n",
      "Epoch 46/300\n",
      "Average training loss: 0.006287019382334418\n",
      "Average test loss: 0.0075748820400072464\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0062575708611143956\n",
      "Average test loss: 0.007726378161874083\n",
      "Epoch 48/300\n",
      "Average training loss: 0.006238812888041139\n",
      "Average test loss: 0.00709234590207537\n",
      "Epoch 49/300\n",
      "Average training loss: 0.006186758500420385\n",
      "Average test loss: 0.007307550199329853\n",
      "Epoch 50/300\n",
      "Average training loss: 0.006176307951824532\n",
      "Average test loss: 0.007436013223810329\n",
      "Epoch 51/300\n",
      "Average training loss: 0.006169025755176942\n",
      "Average test loss: 0.007487781263887882\n",
      "Epoch 52/300\n",
      "Average training loss: 0.006121208387116591\n",
      "Average test loss: 0.007412921234965324\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00614971709334188\n",
      "Average test loss: 0.0072392521194285814\n",
      "Epoch 54/300\n",
      "Average training loss: 0.006091230774505271\n",
      "Average test loss: 0.007933737995309963\n",
      "Epoch 55/300\n",
      "Average training loss: 0.006060897944288122\n",
      "Average test loss: 0.00852353768547376\n",
      "Epoch 56/300\n",
      "Average training loss: 0.006116380293336179\n",
      "Average test loss: 0.007513716456376844\n",
      "Epoch 57/300\n",
      "Average training loss: 0.006002433837287956\n",
      "Average test loss: 0.0073248900137841706\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00599794209914075\n",
      "Average test loss: 0.007188940372731951\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0059843521850804484\n",
      "Average test loss: 0.0070978975610600575\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0059680796075198385\n",
      "Average test loss: 0.007376500439726644\n",
      "Epoch 61/300\n",
      "Average training loss: 0.005967387059496509\n",
      "Average test loss: 0.007139205678883526\n",
      "Epoch 62/300\n",
      "Average training loss: 0.005917332789550225\n",
      "Average test loss: 0.007452661149203777\n",
      "Epoch 63/300\n",
      "Average training loss: 0.005925624306003252\n",
      "Average test loss: 0.007261668591035737\n",
      "Epoch 64/300\n",
      "Average training loss: 0.005897011629202299\n",
      "Average test loss: 0.007525538294679589\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005875366618235906\n",
      "Average test loss: 0.007296493920187155\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0058941682755119275\n",
      "Average test loss: 0.007130831255680985\n",
      "Epoch 67/300\n",
      "Average training loss: 0.005834578246706062\n",
      "Average test loss: 0.0073134315858284634\n",
      "Epoch 68/300\n",
      "Average training loss: 0.005835251369824012\n",
      "Average test loss: 0.0072440007862945395\n",
      "Epoch 69/300\n",
      "Average training loss: 0.005846346315410402\n",
      "Average test loss: 0.0072683127290672726\n",
      "Epoch 70/300\n",
      "Average training loss: 0.005818843295176824\n",
      "Average test loss: 0.007682370407299863\n",
      "Epoch 71/300\n",
      "Average training loss: 0.005776309549394581\n",
      "Average test loss: 0.007252410349746545\n",
      "Epoch 72/300\n",
      "Average training loss: 0.005770793488870065\n",
      "Average test loss: 0.007324302764402496\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00575090603530407\n",
      "Average test loss: 0.007393352595468362\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0057394924176235994\n",
      "Average test loss: 0.007338220918344127\n",
      "Epoch 75/300\n",
      "Average training loss: 0.00574118500492639\n",
      "Average test loss: 0.007178810553418265\n",
      "Epoch 76/300\n",
      "Average training loss: 0.005699170134961605\n",
      "Average test loss: 0.009469518973595567\n",
      "Epoch 77/300\n",
      "Average training loss: 0.005694691343026029\n",
      "Average test loss: 0.007420122391647762\n",
      "Epoch 78/300\n",
      "Average training loss: 0.005685007151216268\n",
      "Average test loss: 0.007453552959693802\n",
      "Epoch 79/300\n",
      "Average training loss: 0.005660953319734997\n",
      "Average test loss: 0.007104906683166822\n",
      "Epoch 80/300\n",
      "Average training loss: 0.005663761069998145\n",
      "Average test loss: 0.008584701708621448\n",
      "Epoch 81/300\n",
      "Average training loss: 0.005657881504545609\n",
      "Average test loss: 0.008660132761630747\n",
      "Epoch 82/300\n",
      "Average training loss: 0.005672641548017661\n",
      "Average test loss: 0.007079093660331434\n",
      "Epoch 83/300\n",
      "Average training loss: 0.005610807648963398\n",
      "Average test loss: 0.0076122950828737684\n",
      "Epoch 84/300\n",
      "Average training loss: 0.005636287322474851\n",
      "Average test loss: 0.00741416087332699\n",
      "Epoch 85/300\n",
      "Average training loss: 0.005602046655284034\n",
      "Average test loss: 0.008005903604957792\n",
      "Epoch 86/300\n",
      "Average training loss: 0.005594156383640236\n",
      "Average test loss: 0.008522982570032278\n",
      "Epoch 87/300\n",
      "Average training loss: 0.005574164727081855\n",
      "Average test loss: 0.007519891312552823\n",
      "Epoch 88/300\n",
      "Average training loss: 0.005571442360679309\n",
      "Average test loss: 0.007211907993174262\n",
      "Epoch 89/300\n",
      "Average training loss: 0.005558459885418415\n",
      "Average test loss: 0.008516487181186676\n",
      "Epoch 90/300\n",
      "Average training loss: 0.005534098301496771\n",
      "Average test loss: 0.0073374650474223825\n",
      "Epoch 91/300\n",
      "Average training loss: 0.005518203299078676\n",
      "Average test loss: 0.007230783755580584\n",
      "Epoch 92/300\n",
      "Average training loss: 0.005520996918694841\n",
      "Average test loss: 0.00732305099732346\n",
      "Epoch 93/300\n",
      "Average training loss: 0.005492360427561733\n",
      "Average test loss: 0.007927567366924551\n",
      "Epoch 94/300\n",
      "Average training loss: 0.005496615042289098\n",
      "Average test loss: 0.010556264664563868\n",
      "Epoch 95/300\n",
      "Average training loss: 0.005483383252802823\n",
      "Average test loss: 0.007892950834499465\n",
      "Epoch 96/300\n",
      "Average training loss: 0.005469422386338314\n",
      "Average test loss: 0.00728906938019726\n",
      "Epoch 97/300\n",
      "Average training loss: 0.005464192900392744\n",
      "Average test loss: 0.007461610175669193\n",
      "Epoch 98/300\n",
      "Average training loss: 0.005467711228049464\n",
      "Average test loss: 0.0075769369221395915\n",
      "Epoch 99/300\n",
      "Average training loss: 0.00544048963735501\n",
      "Average test loss: 0.00756615050137043\n",
      "Epoch 100/300\n",
      "Average training loss: 0.005441897762732373\n",
      "Average test loss: 0.007138202657302221\n",
      "Epoch 101/300\n",
      "Average training loss: 0.005424826795442237\n",
      "Average test loss: 0.007252207743624846\n",
      "Epoch 102/300\n",
      "Average training loss: 0.005424289920677741\n",
      "Average test loss: 0.007257062569260597\n",
      "Epoch 103/300\n",
      "Average training loss: 0.005405333396047354\n",
      "Average test loss: 0.008952537874380747\n",
      "Epoch 104/300\n",
      "Average training loss: 0.005400893631080786\n",
      "Average test loss: 0.007200288181503614\n",
      "Epoch 105/300\n",
      "Average training loss: 0.005404280477927791\n",
      "Average test loss: 0.007227713400704993\n",
      "Epoch 106/300\n",
      "Average training loss: 0.005380095324375563\n",
      "Average test loss: 0.007737781388892068\n",
      "Epoch 107/300\n",
      "Average training loss: 0.00536191323854857\n",
      "Average test loss: 0.007111546088423994\n",
      "Epoch 108/300\n",
      "Average training loss: 0.005349839500048094\n",
      "Average test loss: 0.00728427897600664\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00535348394802875\n",
      "Average test loss: 0.008949888655708896\n",
      "Epoch 110/300\n",
      "Average training loss: 0.00535706898301012\n",
      "Average test loss: 0.0073781826396783195\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00532749513309035\n",
      "Average test loss: 0.009497977569699287\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0053298543571598\n",
      "Average test loss: 0.007376963392727905\n",
      "Epoch 113/300\n",
      "Average training loss: 0.005314381771203545\n",
      "Average test loss: 0.0072805192247033116\n",
      "Epoch 114/300\n",
      "Average training loss: 0.005318328433152702\n",
      "Average test loss: 0.0071547548042403325\n",
      "Epoch 115/300\n",
      "Average training loss: 0.005296157121244404\n",
      "Average test loss: 0.007556880131363869\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0053021859663228196\n",
      "Average test loss: 0.007300054446690613\n",
      "Epoch 117/300\n",
      "Average training loss: 0.005267176801959673\n",
      "Average test loss: 0.00771125637822681\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0052618306440611684\n",
      "Average test loss: 0.009378501218640142\n",
      "Epoch 119/300\n",
      "Average training loss: 0.005282712557249599\n",
      "Average test loss: 0.00852041377996405\n",
      "Epoch 120/300\n",
      "Average training loss: 0.005275732614927821\n",
      "Average test loss: 0.007362820626960861\n",
      "Epoch 121/300\n",
      "Average training loss: 0.005226069157736169\n",
      "Average test loss: 0.007296508266694016\n",
      "Epoch 122/300\n",
      "Average training loss: 0.005237753846993049\n",
      "Average test loss: 0.0072773790525065525\n",
      "Epoch 123/300\n",
      "Average training loss: 0.005252169489032692\n",
      "Average test loss: 0.007323944622857703\n",
      "Epoch 124/300\n",
      "Average training loss: 0.005207520854141977\n",
      "Average test loss: 0.0074136124062869286\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0052319044462508625\n",
      "Average test loss: 0.0071241533284385995\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00522372373772992\n",
      "Average test loss: 0.007444541867408488\n",
      "Epoch 127/300\n",
      "Average training loss: 0.005210030197683308\n",
      "Average test loss: 0.007686294119391176\n",
      "Epoch 128/300\n",
      "Average training loss: 0.005201928736021121\n",
      "Average test loss: 0.007544974601517121\n",
      "Epoch 129/300\n",
      "Average training loss: 0.005183195968882905\n",
      "Average test loss: 0.007253012266837888\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0052001658876736955\n",
      "Average test loss: 0.007363731687267621\n",
      "Epoch 131/300\n",
      "Average training loss: 0.005173998680793577\n",
      "Average test loss: 0.007298312275773949\n",
      "Epoch 132/300\n",
      "Average training loss: 0.005148097797814342\n",
      "Average test loss: 0.0073433960556156105\n",
      "Epoch 133/300\n",
      "Average training loss: 0.005161607273750835\n",
      "Average test loss: 0.007137691687378618\n",
      "Epoch 134/300\n",
      "Average training loss: 0.005160047005448076\n",
      "Average test loss: 0.007310455628567272\n",
      "Epoch 135/300\n",
      "Average training loss: 0.005135763745341036\n",
      "Average test loss: 0.007325486791630586\n",
      "Epoch 136/300\n",
      "Average training loss: 0.005153446508364545\n",
      "Average test loss: 0.007192298260827859\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0051153888180851936\n",
      "Average test loss: 0.007242233213451173\n",
      "Epoch 138/300\n",
      "Average training loss: 0.005136822570529249\n",
      "Average test loss: 0.007808281702299913\n",
      "Epoch 139/300\n",
      "Average training loss: 0.005106854559232791\n",
      "Average test loss: 0.007447008070018557\n",
      "Epoch 140/300\n",
      "Average training loss: 0.005112457084572978\n",
      "Average test loss: 0.00787523535018166\n",
      "Epoch 141/300\n",
      "Average training loss: 0.005099842361691925\n",
      "Average test loss: 0.007126706399437454\n",
      "Epoch 142/300\n",
      "Average training loss: 0.005092635717449917\n",
      "Average test loss: 0.007368146864076455\n",
      "Epoch 143/300\n",
      "Average training loss: 0.005086552335570256\n",
      "Average test loss: 0.007981451569332016\n",
      "Epoch 144/300\n",
      "Average training loss: 0.005100543027950657\n",
      "Average test loss: 0.007230610557728344\n",
      "Epoch 145/300\n",
      "Average training loss: 0.005099403539465534\n",
      "Average test loss: 0.007241574284103182\n",
      "Epoch 146/300\n",
      "Average training loss: 0.005053081730587615\n",
      "Average test loss: 0.0072864808895521695\n",
      "Epoch 147/300\n",
      "Average training loss: 0.005089877958099048\n",
      "Average test loss: 0.007573904462572601\n",
      "Epoch 148/300\n",
      "Average training loss: 0.005049215050621165\n",
      "Average test loss: 0.007306123525732094\n",
      "Epoch 149/300\n",
      "Average training loss: 0.005042846739706066\n",
      "Average test loss: 0.007202941084487571\n",
      "Epoch 150/300\n",
      "Average training loss: 0.005033631360365285\n",
      "Average test loss: 0.007169043167183796\n",
      "Epoch 151/300\n",
      "Average training loss: 0.005041520011921724\n",
      "Average test loss: 0.008632108339418968\n",
      "Epoch 152/300\n",
      "Average training loss: 0.005047546061376731\n",
      "Average test loss: 0.007129652187642124\n",
      "Epoch 153/300\n",
      "Average training loss: 0.004997004922893312\n",
      "Average test loss: 0.007696835872613722\n",
      "Epoch 154/300\n",
      "Average training loss: 0.005002321055365934\n",
      "Average test loss: 0.007589496490028169\n",
      "Epoch 155/300\n",
      "Average training loss: 0.005015271901670429\n",
      "Average test loss: 0.008077457286831406\n",
      "Epoch 156/300\n",
      "Average training loss: 0.005017811053329044\n",
      "Average test loss: 0.007526822744144334\n",
      "Epoch 157/300\n",
      "Average training loss: 0.005006493115590678\n",
      "Average test loss: 0.007252755487544669\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00499436963804894\n",
      "Average test loss: 0.007131423291646772\n",
      "Epoch 159/300\n",
      "Average training loss: 0.004983722062160571\n",
      "Average test loss: 0.007509277652535173\n",
      "Epoch 160/300\n",
      "Average training loss: 0.004983738962974813\n",
      "Average test loss: 0.007323052343808942\n",
      "Epoch 161/300\n",
      "Average training loss: 0.004982096840524012\n",
      "Average test loss: 0.007575299291974968\n",
      "Epoch 162/300\n",
      "Average training loss: 0.004970337790747484\n",
      "Average test loss: 0.007648355348656575\n",
      "Epoch 163/300\n",
      "Average training loss: 0.004964434418827295\n",
      "Average test loss: 0.0076502416725787855\n",
      "Epoch 164/300\n",
      "Average training loss: 0.004957795946134462\n",
      "Average test loss: 0.009806359540257189\n",
      "Epoch 165/300\n",
      "Average training loss: 0.004976607913979226\n",
      "Average test loss: 0.007306742102735572\n",
      "Epoch 166/300\n",
      "Average training loss: 0.004948671653866768\n",
      "Average test loss: 0.008414702577309476\n",
      "Epoch 167/300\n",
      "Average training loss: 0.004947608947133025\n",
      "Average test loss: 0.007586841382914119\n",
      "Epoch 168/300\n",
      "Average training loss: 0.004947776342017783\n",
      "Average test loss: 0.007203724040339391\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0049490383908980425\n",
      "Average test loss: 0.007210328748656644\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0049203876927495\n",
      "Average test loss: 0.007383722345034281\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0049342251399325\n",
      "Average test loss: 0.010230753679242399\n",
      "Epoch 172/300\n",
      "Average training loss: 0.004941012198312415\n",
      "Average test loss: 0.007397522939576043\n",
      "Epoch 173/300\n",
      "Average training loss: 0.004891609279438853\n",
      "Average test loss: 0.007865389246907498\n",
      "Epoch 174/300\n",
      "Average training loss: 0.004918059747252199\n",
      "Average test loss: 0.007410490949948629\n",
      "Epoch 175/300\n",
      "Average training loss: 0.004907882982658015\n",
      "Average test loss: 0.007360142832828893\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00489378972310159\n",
      "Average test loss: 0.007596561562269926\n",
      "Epoch 177/300\n",
      "Average training loss: 0.004899866569373343\n",
      "Average test loss: 0.007339623954147101\n",
      "Epoch 178/300\n",
      "Average training loss: 0.004891677942954832\n",
      "Average test loss: 0.007451353559891383\n",
      "Epoch 179/300\n",
      "Average training loss: 0.004888572671347194\n",
      "Average test loss: 0.007816687335570654\n",
      "Epoch 180/300\n",
      "Average training loss: 0.004869372214708063\n",
      "Average test loss: 0.007273870181706217\n",
      "Epoch 181/300\n",
      "Average training loss: 0.004878560974780056\n",
      "Average test loss: 0.007243247291694085\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004872144900262356\n",
      "Average test loss: 0.007885535051011376\n",
      "Epoch 183/300\n",
      "Average training loss: 0.004873627996693055\n",
      "Average test loss: 0.0079745107959542\n",
      "Epoch 184/300\n",
      "Average training loss: 0.004869029032687346\n",
      "Average test loss: 0.007309121106647783\n",
      "Epoch 185/300\n",
      "Average training loss: 0.004856068807343642\n",
      "Average test loss: 0.007446722978519069\n",
      "Epoch 186/300\n",
      "Average training loss: 0.004850147136383587\n",
      "Average test loss: 0.007275704311413897\n",
      "Epoch 187/300\n",
      "Average training loss: 0.004833599326097303\n",
      "Average test loss: 0.007368618787990676\n",
      "Epoch 188/300\n",
      "Average training loss: 0.004834316367697385\n",
      "Average test loss: 0.007791220753557152\n",
      "Epoch 189/300\n",
      "Average training loss: 0.004836620402004984\n",
      "Average test loss: 0.007474246801187595\n",
      "Epoch 190/300\n",
      "Average training loss: 0.004839965752015511\n",
      "Average test loss: 0.007639281858172682\n",
      "Epoch 191/300\n",
      "Average training loss: 0.004822681625684102\n",
      "Average test loss: 0.007349225689553552\n",
      "Epoch 192/300\n",
      "Average training loss: 0.004841891021778186\n",
      "Average test loss: 0.00764751420252853\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0048177804156310025\n",
      "Average test loss: 0.0074699110512932146\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0047991093606170685\n",
      "Average test loss: 0.007425218961305088\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0048295762236747475\n",
      "Average test loss: 0.0076425535099373925\n",
      "Epoch 196/300\n",
      "Average training loss: 0.004812099104540215\n",
      "Average test loss: 0.007424801231672366\n",
      "Epoch 197/300\n",
      "Average training loss: 0.004800431824806664\n",
      "Average test loss: 0.007519980954213275\n",
      "Epoch 198/300\n",
      "Average training loss: 0.004785176317724917\n",
      "Average test loss: 0.007164597451686859\n",
      "Epoch 199/300\n",
      "Average training loss: 0.004805938721530967\n",
      "Average test loss: 0.007249776957763566\n",
      "Epoch 200/300\n",
      "Average training loss: 0.004837514937751823\n",
      "Average test loss: 0.007596304655075073\n",
      "Epoch 201/300\n",
      "Average training loss: 0.004786963874267207\n",
      "Average test loss: 0.007259514897647831\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0047737885725994906\n",
      "Average test loss: 0.0074493543778856594\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00477982333716419\n",
      "Average test loss: 0.007484592316879166\n",
      "Epoch 204/300\n",
      "Average training loss: 0.004821943415535821\n",
      "Average test loss: 0.007343270802663432\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0047639925289485185\n",
      "Average test loss: 0.007109761481483777\n",
      "Epoch 206/300\n",
      "Average training loss: 0.00476717063681119\n",
      "Average test loss: 0.007230608234388961\n",
      "Epoch 207/300\n",
      "Average training loss: 0.004745618717124065\n",
      "Average test loss: 0.007526277735001511\n",
      "Epoch 208/300\n",
      "Average training loss: 0.004757384999344747\n",
      "Average test loss: 0.008223604260633389\n",
      "Epoch 209/300\n",
      "Average training loss: 0.004752454388472769\n",
      "Average test loss: 0.007430886625415749\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00476343916149603\n",
      "Average test loss: 0.007445685642047061\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0047568546537723804\n",
      "Average test loss: 0.007677020959970024\n",
      "Epoch 212/300\n",
      "Average training loss: 0.004736907513605224\n",
      "Average test loss: 0.007545260802739196\n",
      "Epoch 213/300\n",
      "Average training loss: 0.004750664771637983\n",
      "Average test loss: 0.007322595535053147\n",
      "Epoch 214/300\n",
      "Average training loss: 0.004749256523533\n",
      "Average test loss: 0.0072069226089451046\n",
      "Epoch 215/300\n",
      "Average training loss: 0.00473516489027275\n",
      "Average test loss: 0.007374069557421737\n",
      "Epoch 216/300\n",
      "Average training loss: 0.004742594437052806\n",
      "Average test loss: 0.007366150379180908\n",
      "Epoch 217/300\n",
      "Average training loss: 0.004728651237984498\n",
      "Average test loss: 0.007671896710991859\n",
      "Epoch 218/300\n",
      "Average training loss: 0.004715502094063494\n",
      "Average test loss: 0.0073835139299432435\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0047269908007648255\n",
      "Average test loss: 0.00842537465153469\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0047278127277063\n",
      "Average test loss: 0.007454318237801393\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0047114203510185085\n",
      "Average test loss: 0.007412382971909311\n",
      "Epoch 222/300\n",
      "Average training loss: 0.004712926955272754\n",
      "Average test loss: 0.007480319131579664\n",
      "Epoch 223/300\n",
      "Average training loss: 0.004711900127016836\n",
      "Average test loss: 0.007278450914555126\n",
      "Epoch 224/300\n",
      "Average training loss: 0.004719322770420048\n",
      "Average test loss: 0.007609243730703989\n",
      "Epoch 225/300\n",
      "Average training loss: 0.004703575175255537\n",
      "Average test loss: 0.0074043498349686465\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0046969062621808715\n",
      "Average test loss: 0.007400696188211441\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00468895857863956\n",
      "Average test loss: 0.007432604365050792\n",
      "Epoch 228/300\n",
      "Average training loss: 0.004677701227366924\n",
      "Average test loss: 0.00745311598562532\n",
      "Epoch 229/300\n",
      "Average training loss: 0.004688200887292624\n",
      "Average test loss: 0.0074560252444611655\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0046864411826762885\n",
      "Average test loss: 0.007357260937078131\n",
      "Epoch 231/300\n",
      "Average training loss: 0.004693390953458018\n",
      "Average test loss: 0.007303762472752068\n",
      "Epoch 232/300\n",
      "Average training loss: 0.004676984784296817\n",
      "Average test loss: 0.007446358054876328\n",
      "Epoch 233/300\n",
      "Average training loss: 0.004679611486693223\n",
      "Average test loss: 0.007261054933071136\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0046637119497689936\n",
      "Average test loss: 0.007289403629799684\n",
      "Epoch 235/300\n",
      "Average training loss: 0.004663005214184523\n",
      "Average test loss: 0.00783741378866964\n",
      "Epoch 236/300\n",
      "Average training loss: 0.004674088838613696\n",
      "Average test loss: 0.007534772808353106\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0046464336551725865\n",
      "Average test loss: 0.007254060502681467\n",
      "Epoch 238/300\n",
      "Average training loss: 0.004661817164056831\n",
      "Average test loss: 0.00741353614628315\n",
      "Epoch 239/300\n",
      "Average training loss: 0.004644024286005232\n",
      "Average test loss: 0.007373296049733957\n",
      "Epoch 240/300\n",
      "Average training loss: 0.004646320927888155\n",
      "Average test loss: 0.007270014322880242\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00463456349944075\n",
      "Average test loss: 0.007366521759165658\n",
      "Epoch 242/300\n",
      "Average training loss: 0.004642688626216518\n",
      "Average test loss: 0.007189313151356247\n",
      "Epoch 243/300\n",
      "Average training loss: 0.004636913153860304\n",
      "Average test loss: 0.007949186977826886\n",
      "Epoch 244/300\n",
      "Average training loss: 0.004653710996939077\n",
      "Average test loss: 0.007405017079578506\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0046331222876906395\n",
      "Average test loss: 0.008589534097247654\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00462573863276177\n",
      "Average test loss: 0.007562521365367704\n",
      "Epoch 247/300\n",
      "Average training loss: 0.004612780476609866\n",
      "Average test loss: 0.007438803114410903\n",
      "Epoch 248/300\n",
      "Average training loss: 0.004647835370153189\n",
      "Average test loss: 0.00735504126879904\n",
      "Epoch 249/300\n",
      "Average training loss: 0.004616998132525219\n",
      "Average test loss: 0.007892950241350465\n",
      "Epoch 250/300\n",
      "Average training loss: 0.004615137212392356\n",
      "Average test loss: 0.008142939587848053\n",
      "Epoch 251/300\n",
      "Average training loss: 0.004599428739605679\n",
      "Average test loss: 0.0073134223388301\n",
      "Epoch 252/300\n",
      "Average training loss: 0.00460238341987133\n",
      "Average test loss: 0.007399379166464011\n",
      "Epoch 253/300\n",
      "Average training loss: 0.004609639945750435\n",
      "Average test loss: 0.0074506234861910344\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00459325882875257\n",
      "Average test loss: 0.007478483753071891\n",
      "Epoch 255/300\n",
      "Average training loss: 0.004612797893790735\n",
      "Average test loss: 0.007687978756096628\n",
      "Epoch 256/300\n",
      "Average training loss: 0.004604805222402016\n",
      "Average test loss: 0.007468545369389984\n",
      "Epoch 257/300\n",
      "Average training loss: 0.00459165630944901\n",
      "Average test loss: 0.007324698777662383\n",
      "Epoch 258/300\n",
      "Average training loss: 0.004596511274990108\n",
      "Average test loss: 0.0076298687019281915\n",
      "Epoch 259/300\n",
      "Average training loss: 0.004607006759693225\n",
      "Average test loss: 0.007519536065972513\n",
      "Epoch 260/300\n",
      "Average training loss: 0.004590896551807722\n",
      "Average test loss: 0.008577966976496909\n",
      "Epoch 261/300\n",
      "Average training loss: 0.004595757749345567\n",
      "Average test loss: 0.008138572953641414\n",
      "Epoch 262/300\n",
      "Average training loss: 0.004580913251886765\n",
      "Average test loss: 0.0072305112365219325\n",
      "Epoch 263/300\n",
      "Average training loss: 0.004585334153225025\n",
      "Average test loss: 0.007112062082108524\n",
      "Epoch 264/300\n",
      "Average training loss: 0.004574683687753147\n",
      "Average test loss: 0.007353832234938939\n",
      "Epoch 265/300\n",
      "Average training loss: 0.004555422806491454\n",
      "Average test loss: 0.0074446359351277355\n",
      "Epoch 266/300\n",
      "Average training loss: 0.004584979063106908\n",
      "Average test loss: 0.007417186073131032\n",
      "Epoch 267/300\n",
      "Average training loss: 0.004597477204683754\n",
      "Average test loss: 0.007525444461239709\n",
      "Epoch 268/300\n",
      "Average training loss: 0.004551069423556328\n",
      "Average test loss: 0.007384155587603649\n",
      "Epoch 269/300\n",
      "Average training loss: 0.004561205686380466\n",
      "Average test loss: 0.007261480902632077\n",
      "Epoch 270/300\n",
      "Average training loss: 0.004561280456889007\n",
      "Average test loss: 0.007476829926586813\n",
      "Epoch 271/300\n",
      "Average training loss: 0.004569957716597451\n",
      "Average test loss: 0.007341753403345743\n",
      "Epoch 272/300\n",
      "Average training loss: 0.004548744086590078\n",
      "Average test loss: 0.007635741023139821\n",
      "Epoch 273/300\n",
      "Average training loss: 0.004546752252512508\n",
      "Average test loss: 0.007231212043513855\n",
      "Epoch 274/300\n",
      "Average training loss: 0.004533486882017718\n",
      "Average test loss: 0.007350843013988601\n",
      "Epoch 275/300\n",
      "Average training loss: 0.004555461582417289\n",
      "Average test loss: 0.0077964940385686025\n",
      "Epoch 276/300\n",
      "Average training loss: 0.004552356503903866\n",
      "Average test loss: 0.007451816039780776\n",
      "Epoch 277/300\n",
      "Average training loss: 0.00455410969712668\n",
      "Average test loss: 0.007651041415830453\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0045473266128036715\n",
      "Average test loss: 0.007592097971174452\n",
      "Epoch 279/300\n",
      "Average training loss: 0.004551787029951811\n",
      "Average test loss: 0.007605017013847828\n",
      "Epoch 280/300\n",
      "Average training loss: 0.004535415727231238\n",
      "Average test loss: 0.00722202284137408\n",
      "Epoch 281/300\n",
      "Average training loss: 0.004521251203285323\n",
      "Average test loss: 0.007407061327662733\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0045371235662864315\n",
      "Average test loss: 0.0077016368762900436\n",
      "Epoch 283/300\n",
      "Average training loss: 0.004539379393474923\n",
      "Average test loss: 0.0073805701625016\n",
      "Epoch 284/300\n",
      "Average training loss: 0.004530013173404667\n",
      "Average test loss: 0.007291982166055176\n",
      "Epoch 285/300\n",
      "Average training loss: 0.004520999236032367\n",
      "Average test loss: 0.007547782924026251\n",
      "Epoch 286/300\n",
      "Average training loss: 0.004526814308431414\n",
      "Average test loss: 0.007507449296613534\n",
      "Epoch 287/300\n",
      "Average training loss: 0.004521491588403781\n",
      "Average test loss: 0.0074975263592269685\n",
      "Epoch 288/300\n",
      "Average training loss: 0.004529229329278072\n",
      "Average test loss: 0.007844238503939576\n",
      "Epoch 289/300\n",
      "Average training loss: 0.004518539843459924\n",
      "Average test loss: 0.007911079614112774\n",
      "Epoch 290/300\n",
      "Average training loss: 0.004513279158208106\n",
      "Average test loss: 0.007338892489257786\n",
      "Epoch 291/300\n",
      "Average training loss: 0.004529167563964923\n",
      "Average test loss: 0.007261737137205071\n",
      "Epoch 292/300\n",
      "Average training loss: 0.00450042998418212\n",
      "Average test loss: 0.007341656558215618\n",
      "Epoch 293/300\n",
      "Average training loss: 0.004484847311137451\n",
      "Average test loss: 0.007925455139742956\n",
      "Epoch 294/300\n",
      "Average training loss: 0.004493262865063217\n",
      "Average test loss: 0.00759564014027516\n",
      "Epoch 295/300\n",
      "Average training loss: 0.004506789348398646\n",
      "Average test loss: 0.007345869621882836\n",
      "Epoch 296/300\n",
      "Average training loss: 0.00450687688175175\n",
      "Average test loss: 0.00741825985825724\n",
      "Epoch 297/300\n",
      "Average training loss: 0.004485516562230057\n",
      "Average test loss: 0.007723345593445831\n",
      "Epoch 298/300\n",
      "Average training loss: 0.004499446851925717\n",
      "Average test loss: 0.007453749172389507\n",
      "Epoch 299/300\n",
      "Average training loss: 0.004486441992223263\n",
      "Average test loss: 0.007477856205983294\n",
      "Epoch 300/300\n",
      "Average training loss: 0.004487310397956106\n",
      "Average test loss: 0.007483885802328586\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.022154530233807035\n",
      "Average test loss: 0.011427337196138171\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009907000541687012\n",
      "Average test loss: 0.012082488792637984\n",
      "Epoch 3/300\n",
      "Average training loss: 0.008463697006305058\n",
      "Average test loss: 0.008005096833325094\n",
      "Epoch 4/300\n",
      "Average training loss: 0.007728622530897458\n",
      "Average test loss: 0.0071332946796384125\n",
      "Epoch 5/300\n",
      "Average training loss: 0.007120352914763822\n",
      "Average test loss: 0.0070859573640757135\n",
      "Epoch 6/300\n",
      "Average training loss: 0.006710760100434224\n",
      "Average test loss: 0.006126869336184528\n",
      "Epoch 7/300\n",
      "Average training loss: 0.006537286424802409\n",
      "Average test loss: 0.006061218901226918\n",
      "Epoch 8/300\n",
      "Average training loss: 0.006538203858253029\n",
      "Average test loss: 0.006951075169775221\n",
      "Epoch 9/300\n",
      "Average training loss: 0.006102655265066359\n",
      "Average test loss: 0.005896556577748723\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005953646043936412\n",
      "Average test loss: 0.011437616578406758\n",
      "Epoch 11/300\n",
      "Average training loss: 0.005896112966454692\n",
      "Average test loss: 0.005449474583897326\n",
      "Epoch 12/300\n",
      "Average training loss: 0.005607948589242167\n",
      "Average test loss: 0.00592262158749832\n",
      "Epoch 13/300\n",
      "Average training loss: 0.005490051290227308\n",
      "Average test loss: 0.00679503171145916\n",
      "Epoch 14/300\n",
      "Average training loss: 0.005420800423870484\n",
      "Average test loss: 0.010314522945218616\n",
      "Epoch 15/300\n",
      "Average training loss: 0.005303176459752851\n",
      "Average test loss: 0.005199345206220945\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0052161473331766\n",
      "Average test loss: 0.005004955557899342\n",
      "Epoch 17/300\n",
      "Average training loss: 0.005061725607348813\n",
      "Average test loss: 0.00548448154495822\n",
      "Epoch 18/300\n",
      "Average training loss: 0.005003685942747527\n",
      "Average test loss: 0.0050653924263185926\n",
      "Epoch 19/300\n",
      "Average training loss: 0.005050011275543107\n",
      "Average test loss: 0.004985519363234441\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0048411532019575435\n",
      "Average test loss: 0.005235311327709092\n",
      "Epoch 21/300\n",
      "Average training loss: 0.004806230189071762\n",
      "Average test loss: 0.005426220659580496\n",
      "Epoch 22/300\n",
      "Average training loss: 0.004731765468915304\n",
      "Average test loss: 0.005107662175264624\n",
      "Epoch 23/300\n",
      "Average training loss: 0.004808417851312293\n",
      "Average test loss: 0.004659551541631421\n",
      "Epoch 24/300\n",
      "Average training loss: 0.004626486902849542\n",
      "Average test loss: 0.004703108956002526\n",
      "Epoch 25/300\n",
      "Average training loss: 0.005090283786671029\n",
      "Average test loss: 0.008398557184471024\n",
      "Epoch 26/300\n",
      "Average training loss: 0.004580206397506926\n",
      "Average test loss: 0.006181268301688962\n",
      "Epoch 27/300\n",
      "Average training loss: 0.004536470003426075\n",
      "Average test loss: 0.005812782333128982\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0044419553135004305\n",
      "Average test loss: 0.004572410439865456\n",
      "Epoch 29/300\n",
      "Average training loss: 0.004449372920932042\n",
      "Average test loss: 0.004996308204200533\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0043571822912328775\n",
      "Average test loss: 0.006668951999810007\n",
      "Epoch 31/300\n",
      "Average training loss: 0.004419257147030698\n",
      "Average test loss: 0.004706413583416078\n",
      "Epoch 32/300\n",
      "Average training loss: 0.004284504385044178\n",
      "Average test loss: 0.004605448982574874\n",
      "Epoch 33/300\n",
      "Average training loss: 0.004307537379364172\n",
      "Average test loss: 0.004481681504597266\n",
      "Epoch 34/300\n",
      "Average training loss: 0.004256699712119168\n",
      "Average test loss: 0.005119345811920034\n",
      "Epoch 35/300\n",
      "Average training loss: 0.004223545558336708\n",
      "Average test loss: 0.004503153661265969\n",
      "Epoch 36/300\n",
      "Average training loss: 0.004245227642771271\n",
      "Average test loss: 0.004833483704676231\n",
      "Epoch 37/300\n",
      "Average training loss: 0.004117261113805904\n",
      "Average test loss: 0.004994535961705777\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00415574754236473\n",
      "Average test loss: 0.0047196720784737\n",
      "Epoch 39/300\n",
      "Average training loss: 0.004075573888917764\n",
      "Average test loss: 0.004427313109652864\n",
      "Epoch 40/300\n",
      "Average training loss: 0.00420552163467639\n",
      "Average test loss: 0.006109064124938514\n",
      "Epoch 41/300\n",
      "Average training loss: 0.004219012443804079\n",
      "Average test loss: 0.004365621860656473\n",
      "Epoch 42/300\n",
      "Average training loss: 0.003997736017737123\n",
      "Average test loss: 0.004290960277120272\n",
      "Epoch 43/300\n",
      "Average training loss: 0.004281765749884976\n",
      "Average test loss: 0.0047872553086943095\n",
      "Epoch 44/300\n",
      "Average training loss: 0.003949990339991119\n",
      "Average test loss: 0.004381655871040291\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003919909330291881\n",
      "Average test loss: 0.004455606833100319\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0039493557396862245\n",
      "Average test loss: 0.00579685700395041\n",
      "Epoch 47/300\n",
      "Average training loss: 0.004011549514821834\n",
      "Average test loss: 0.004438390462348858\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0038455182001408605\n",
      "Average test loss: 0.004697790450106065\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0048885471386214095\n",
      "Average test loss: 0.004565314530498452\n",
      "Epoch 50/300\n",
      "Average training loss: 0.004114974741513531\n",
      "Average test loss: 0.004310725844568677\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0063367401431832044\n",
      "Average test loss: 2.9203350349830255\n",
      "Epoch 52/300\n",
      "Average training loss: 0.006789594931734933\n",
      "Average test loss: 8.83665033085106\n",
      "Epoch 53/300\n",
      "Average training loss: 0.005571314635376135\n",
      "Average test loss: 23.77041568307413\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00513504940064417\n",
      "Average test loss: 47.365608609391586\n",
      "Epoch 55/300\n",
      "Average training loss: 0.005148427516428961\n",
      "Average test loss: 1.1760509397406131\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0049888663718269935\n",
      "Average test loss: 2.828162762604033\n",
      "Epoch 57/300\n",
      "Average training loss: 0.005031188322438134\n",
      "Average test loss: 9.723983496901061\n",
      "Epoch 58/300\n",
      "Average training loss: 0.004890323433404168\n",
      "Average test loss: 0.008049825488693185\n",
      "Epoch 59/300\n",
      "Average training loss: 0.004558210754974021\n",
      "Average test loss: 0.6463017119322386\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0045692627947363585\n",
      "Average test loss: 0.01395851514240106\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0044772358275949956\n",
      "Average test loss: 0.5579241438607375\n",
      "Epoch 62/300\n",
      "Average training loss: 0.004482369331436024\n",
      "Average test loss: 92.51804254838162\n",
      "Epoch 63/300\n",
      "Average training loss: 0.004520875180140138\n",
      "Average test loss: 0.048201609501822125\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0048349150361286265\n",
      "Average test loss: 0.005879867381519742\n",
      "Epoch 65/300\n",
      "Average training loss: 0.005069766932477553\n",
      "Average test loss: 34.20821564330657\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0057609920973579086\n",
      "Average test loss: 0.005135459264119466\n",
      "Epoch 67/300\n",
      "Average training loss: 0.004879568618204859\n",
      "Average test loss: 0.0044650221611890525\n",
      "Epoch 68/300\n",
      "Average training loss: 0.004483100318246417\n",
      "Average test loss: 0.006707498512334294\n",
      "Epoch 69/300\n",
      "Average training loss: 0.004295719805484017\n",
      "Average test loss: 0.005697496247788271\n",
      "Epoch 70/300\n",
      "Average training loss: 0.004254132104416689\n",
      "Average test loss: 0.00895640512307485\n",
      "Epoch 71/300\n",
      "Average training loss: 0.004431381699939569\n",
      "Average test loss: 30.31584459588925\n",
      "Epoch 72/300\n",
      "Average training loss: 0.004259957584655947\n",
      "Average test loss: 58.92316497779513\n",
      "Epoch 73/300\n",
      "Average training loss: 0.004203876682453685\n",
      "Average test loss: 0.31586599771844015\n",
      "Epoch 74/300\n",
      "Average training loss: 0.004502159076846308\n",
      "Average test loss: 0.00627172083242072\n",
      "Epoch 75/300\n",
      "Average training loss: 0.004333533843358358\n",
      "Average test loss: 0.7040953120411271\n",
      "Epoch 76/300\n",
      "Average training loss: 0.004080417477008369\n",
      "Average test loss: 233.27799362184274\n",
      "Epoch 77/300\n",
      "Average training loss: 0.004294191035959456\n",
      "Average test loss: 0.005362481495572461\n",
      "Epoch 78/300\n",
      "Average training loss: 0.004474384472394983\n",
      "Average test loss: 0.6692607737415367\n",
      "Epoch 79/300\n",
      "Average training loss: 0.004311498686671257\n",
      "Average test loss: 379.63649782562254\n",
      "Epoch 80/300\n",
      "Average training loss: 0.004036224757217699\n",
      "Average test loss: 1246.7819247470431\n",
      "Epoch 81/300\n",
      "Average training loss: 0.004011336673878961\n",
      "Average test loss: 0.20358909307089118\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0040306618147426185\n",
      "Average test loss: 0.4050939846651422\n",
      "Epoch 83/300\n",
      "Average training loss: 0.004045346883436044\n",
      "Average test loss: 0.004488442102861073\n",
      "Epoch 84/300\n",
      "Average training loss: 0.003934832462627027\n",
      "Average test loss: 183.2215465101103\n",
      "Epoch 85/300\n",
      "Average training loss: 0.003946334446883864\n",
      "Average test loss: 89713.05130208333\n",
      "Epoch 86/300\n",
      "Average training loss: 0.004140709825274017\n",
      "Average test loss: 0.004005125194787979\n",
      "Epoch 87/300\n",
      "Average training loss: 0.004015004287784298\n",
      "Average test loss: 21.64582588946488\n",
      "Epoch 88/300\n",
      "Average training loss: 0.003918451437519657\n",
      "Average test loss: 0.11179021922643814\n",
      "Epoch 89/300\n",
      "Average training loss: 0.003864868636553486\n",
      "Average test loss: 0.19217545980670386\n",
      "Epoch 90/300\n",
      "Average training loss: 0.004946894174855616\n",
      "Average test loss: 10.007006701588631\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00415470703670548\n",
      "Average test loss: 165.94420761445\n",
      "Epoch 92/300\n",
      "Average training loss: 0.004034815392560429\n",
      "Average test loss: 0.03052663200183047\n",
      "Epoch 93/300\n",
      "Average training loss: 0.003990480291967591\n",
      "Average test loss: 0.33981188867820633\n",
      "Epoch 94/300\n",
      "Average training loss: 0.004390555163017578\n",
      "Average test loss: 0.07045709718929397\n",
      "Epoch 95/300\n",
      "Average training loss: 0.003972137376045187\n",
      "Average test loss: 0.754331464327044\n",
      "Epoch 96/300\n",
      "Average training loss: 0.003924543072986934\n",
      "Average test loss: 0.5124451314231587\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0039535305570397115\n",
      "Average test loss: 57.99700335030423\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0038591692029602\n",
      "Average test loss: 444.77350109185113\n",
      "Epoch 99/300\n",
      "Average training loss: 0.004216564402812057\n",
      "Average test loss: 0.010720556244874994\n",
      "Epoch 100/300\n",
      "Average training loss: 0.003930912618835767\n",
      "Average test loss: 0.008168093281901545\n",
      "Epoch 101/300\n",
      "Average training loss: 0.003862920183274481\n",
      "Average test loss: 208.9254014963044\n",
      "Epoch 102/300\n",
      "Average training loss: 0.003813186821424299\n",
      "Average test loss: 0.03830985825426049\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0038414087914344338\n",
      "Average test loss: 129.50535360216765\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0038171821182800664\n",
      "Average test loss: 0.15955290030067165\n",
      "Epoch 105/300\n",
      "Average training loss: 0.004631307100877165\n",
      "Average test loss: 0.006410895047916306\n",
      "Epoch 106/300\n",
      "Average training loss: 0.004150828739007314\n",
      "Average test loss: 3.6766506320503023\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0039014949515048\n",
      "Average test loss: 0.021106163067950143\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0038413468870437807\n",
      "Average test loss: 4.163512660942144\n",
      "Epoch 109/300\n",
      "Average training loss: 0.003906609401520755\n",
      "Average test loss: 10.691689727038145\n",
      "Epoch 110/300\n",
      "Average training loss: 0.003800026211266716\n",
      "Average test loss: 0.1356223910500606\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0038021463935987818\n",
      "Average test loss: 0.004199580886090795\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0037775306548509333\n",
      "Average test loss: 0.01313736226202713\n",
      "Epoch 113/300\n",
      "Average training loss: 0.00399354354167978\n",
      "Average test loss: 0.24192631029089293\n",
      "Epoch 114/300\n",
      "Average training loss: 0.003819195636237661\n",
      "Average test loss: 0.11774787233769894\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0037513661454949116\n",
      "Average test loss: 20.906846852408513\n",
      "Epoch 116/300\n",
      "Average training loss: 0.003682013930131992\n",
      "Average test loss: 0.0064801838625636364\n",
      "Epoch 117/300\n",
      "Average training loss: 0.003701797435267104\n",
      "Average test loss: 0.026636523057189252\n",
      "Epoch 118/300\n",
      "Average training loss: 0.003741169614717364\n",
      "Average test loss: 0.00518473712561859\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0036985101165870823\n",
      "Average test loss: 0.006146860609865851\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0036540059138917263\n",
      "Average test loss: 7.991158786929316\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0037079042765415376\n",
      "Average test loss: 0.009735922619700433\n",
      "Epoch 122/300\n",
      "Average training loss: 0.003637518822525938\n",
      "Average test loss: 1.9517812529934777\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00363241575161616\n",
      "Average test loss: 0.012221364199287361\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0036646551204224426\n",
      "Average test loss: 0.029184290340791147\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0036367197881142298\n",
      "Average test loss: 46.389156322903105\n",
      "Epoch 126/300\n",
      "Average training loss: 0.003580247867645489\n",
      "Average test loss: 11.22058851608634\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0036381662715640334\n",
      "Average test loss: 0.5101322031997972\n",
      "Epoch 128/300\n",
      "Average training loss: 0.003684063107603126\n",
      "Average test loss: 0.030886247427720163\n",
      "Epoch 129/300\n",
      "Average training loss: 0.003566945505224996\n",
      "Average test loss: 6.579270836533771\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0035867527580509583\n",
      "Average test loss: 0.16671973712411192\n",
      "Epoch 131/300\n",
      "Average training loss: 0.003666810472599334\n",
      "Average test loss: 0.0692367817527718\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0035953863221738075\n",
      "Average test loss: 0.0043378267046064135\n",
      "Epoch 133/300\n",
      "Average training loss: 0.003559940382838249\n",
      "Average test loss: 0.007551613973246681\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0035276379322426187\n",
      "Average test loss: 0.2350953671435515\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0035733925242804818\n",
      "Average test loss: 0.00416442396864295\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0035221421654439635\n",
      "Average test loss: 0.08035978322124315\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0035907513356457153\n",
      "Average test loss: 0.003928665211631192\n",
      "Epoch 138/300\n",
      "Average training loss: 0.003460986153740022\n",
      "Average test loss: 0.0041084618618090945\n",
      "Epoch 139/300\n",
      "Average training loss: 0.003523999513644311\n",
      "Average test loss: 0.008541069558511178\n",
      "Epoch 140/300\n",
      "Average training loss: 0.003524364298209548\n",
      "Average test loss: 6.393459124811821\n",
      "Epoch 141/300\n",
      "Average training loss: 0.003474373149375121\n",
      "Average test loss: 4.450707250778046\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0035130999833345414\n",
      "Average test loss: 0.00615706868055794\n",
      "Epoch 143/300\n",
      "Average training loss: 0.003467908449057076\n",
      "Average test loss: 0.054961838654759856\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0034892966356128456\n",
      "Average test loss: 1.341221257209778\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0034748449039955936\n",
      "Average test loss: 2.0406069628571473\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0034659431051048965\n",
      "Average test loss: 0.11526767621686061\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0035203422562529642\n",
      "Average test loss: 0.06105198902429806\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0034656009276707966\n",
      "Average test loss: 10.96805735223492\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0034290412242213887\n",
      "Average test loss: 0.0051046664654794665\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0034306254546261497\n",
      "Average test loss: 0.3037398499002059\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0034788701234178413\n",
      "Average test loss: 1.4521103666416473\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0034386487232728135\n",
      "Average test loss: 0.008472006238996983\n",
      "Epoch 153/300\n",
      "Average training loss: 0.003396627434425884\n",
      "Average test loss: 74.67811834011806\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0034559594471421508\n",
      "Average test loss: 0.5718600500929687\n",
      "Epoch 155/300\n",
      "Average training loss: 0.003376209197979834\n",
      "Average test loss: 0.4964581859658162\n",
      "Epoch 156/300\n",
      "Average training loss: 0.003410607221846779\n",
      "Average test loss: 0.05784239221281476\n",
      "Epoch 157/300\n",
      "Average training loss: 0.003475151856740316\n",
      "Average test loss: 0.005682515582069754\n",
      "Epoch 158/300\n",
      "Average training loss: 0.003766306650928325\n",
      "Average test loss: 0.4671035479373402\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0034668065895222954\n",
      "Average test loss: 0.147906554043293\n",
      "Epoch 160/300\n",
      "Average training loss: 0.003448216504934761\n",
      "Average test loss: 2.288757870475451\n",
      "Epoch 161/300\n",
      "Average training loss: 0.003495157141652372\n",
      "Average test loss: 1.3800821261786753\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0034185962590078515\n",
      "Average test loss: 0.588811259213421\n",
      "Epoch 163/300\n",
      "Average training loss: 0.003427950178790424\n",
      "Average test loss: 264.06768600461385\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0035112429993848005\n",
      "Average test loss: 0.006339362627102269\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0033763003738390075\n",
      "Average test loss: 0.005964453217883905\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0033722196229630046\n",
      "Average test loss: 0.07563805126316017\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0033386095787088076\n",
      "Average test loss: 0.038224986994018155\n",
      "Epoch 168/300\n",
      "Average training loss: 0.003404686443507671\n",
      "Average test loss: 0.003932714756578207\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0033364666200553375\n",
      "Average test loss: 0.004382386423854364\n",
      "Epoch 170/300\n",
      "Average training loss: 0.003384987228239576\n",
      "Average test loss: 1.2176535406443807\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0033305040817293856\n",
      "Average test loss: 8.058127237205705\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0033193053338262768\n",
      "Average test loss: 0.42053832020693355\n",
      "Epoch 173/300\n",
      "Average training loss: 0.003453832374471757\n",
      "Average test loss: 0.40811973424586984\n",
      "Epoch 174/300\n",
      "Average training loss: 0.003308453969657421\n",
      "Average test loss: 98.95697381890979\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0033217884856793617\n",
      "Average test loss: 0.026264320978480908\n",
      "Epoch 176/300\n",
      "Average training loss: 0.003302802124992013\n",
      "Average test loss: 0.16049601058827506\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0032788881959600583\n",
      "Average test loss: 0.03539366101121737\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0033761595096439122\n",
      "Average test loss: 0.09576671694053544\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0032868478747291696\n",
      "Average test loss: 0.0053560419490353926\n",
      "Epoch 180/300\n",
      "Average training loss: 0.003299238199161159\n",
      "Average test loss: 0.011426286273946365\n",
      "Epoch 181/300\n",
      "Average training loss: 0.003305345650451879\n",
      "Average test loss: 0.005397130915688144\n",
      "Epoch 182/300\n",
      "Average training loss: 0.003268226444307301\n",
      "Average test loss: 0.012422393318679597\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0032540173356731734\n",
      "Average test loss: 2509.5055932939317\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0036858979401489097\n",
      "Average test loss: 0.01021190466814571\n",
      "Epoch 185/300\n",
      "Average training loss: 0.003274622667166922\n",
      "Average test loss: 0.06673933267013894\n",
      "Epoch 186/300\n",
      "Average training loss: 0.003242554455374678\n",
      "Average test loss: 0.0956396325416863\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0032502721088627975\n",
      "Average test loss: 0.026640108365772498\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003224697984755039\n",
      "Average test loss: 0.36495625533494686\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0032382076633059318\n",
      "Average test loss: 2.5523899042438716\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0032449818067252635\n",
      "Average test loss: 0.009884792709516154\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0033729847628209326\n",
      "Average test loss: 2.016205403543181\n",
      "Epoch 192/300\n",
      "Average training loss: 0.003251177112261454\n",
      "Average test loss: 8.899181731906202\n",
      "Epoch 193/300\n",
      "Average training loss: 0.003234956636610958\n",
      "Average test loss: 0.01884008354693651\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0032687577866017817\n",
      "Average test loss: 0.050357124196779394\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0032144088461581203\n",
      "Average test loss: 315.0607108106348\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0032496587466448543\n",
      "Average test loss: 0.32360335994594625\n",
      "Epoch 197/300\n",
      "Average training loss: 0.003199933843480216\n",
      "Average test loss: 0.5762863109819591\n",
      "Epoch 198/300\n",
      "Average training loss: 0.003205300574087434\n",
      "Average test loss: 0.5240966378566292\n",
      "Epoch 199/300\n",
      "Average training loss: 0.003230517376214266\n",
      "Average test loss: 0.021731727141266068\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0032523372667945093\n",
      "Average test loss: 0.11447815013014608\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0031992191334979402\n",
      "Average test loss: 0.00636433291186889\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0032017277249445516\n",
      "Average test loss: 0.06853193698492314\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0031933607982678547\n",
      "Average test loss: 21.53658200426731\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0032032030502127276\n",
      "Average test loss: 2.022744936598672\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0031804852589137024\n",
      "Average test loss: 0.7124452525327603\n",
      "Epoch 206/300\n",
      "Average training loss: 0.003161564343712396\n",
      "Average test loss: 198.4975815268788\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0032147478306045136\n",
      "Average test loss: 0.10187231003906992\n",
      "Epoch 208/300\n",
      "Average training loss: 0.003159843851915664\n",
      "Average test loss: 0.004909123433132967\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0031944488558090394\n",
      "Average test loss: 0.6455348557891945\n",
      "Epoch 210/300\n",
      "Average training loss: 0.003164189226304491\n",
      "Average test loss: 7.525432316654259\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0031627977466624642\n",
      "Average test loss: 0.011318666325675117\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0031775249544945027\n",
      "Average test loss: 0.08363196502170629\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0031544739914437137\n",
      "Average test loss: 0.6697071132643355\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0031516115164591206\n",
      "Average test loss: 4.409381331165632\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0031412462583846517\n",
      "Average test loss: 4.696015223397149\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0031223335918039085\n",
      "Average test loss: 0.031422564137727024\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0031163674061083133\n",
      "Average test loss: 5.015312697581119\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0031912013430976205\n",
      "Average test loss: 602.6469301607973\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0031780619954483377\n",
      "Average test loss: 8457.946642390765\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0030991990940852296\n",
      "Average test loss: 0.06122467558830977\n",
      "Epoch 221/300\n",
      "Average training loss: 0.003133438421206342\n",
      "Average test loss: 0.004237877718690369\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0031499437851210434\n",
      "Average test loss: 0.08060437282919884\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0031239113623483315\n",
      "Average test loss: 0.02089488899314569\n",
      "Epoch 224/300\n",
      "Average training loss: 0.003091659160744813\n",
      "Average test loss: 0.06187863059259123\n",
      "Epoch 225/300\n",
      "Average training loss: 0.003124461323643724\n",
      "Average test loss: 1.5128566355721818\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0030779418976356588\n",
      "Average test loss: 6.520016553382079\n",
      "Epoch 227/300\n",
      "Average training loss: 0.00309626914622883\n",
      "Average test loss: 0.020329247042122814\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0031783004152692025\n",
      "Average test loss: 25.967522595971822\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0031699692741450334\n",
      "Average test loss: 0.009252667971162332\n",
      "Epoch 230/300\n",
      "Average training loss: 0.003180786771906747\n",
      "Average test loss: 4.899189608934439\n",
      "Epoch 231/300\n",
      "Average training loss: 0.003095454967684216\n",
      "Average test loss: 0.024124021775192683\n",
      "Epoch 232/300\n",
      "Average training loss: 0.003086254526550571\n",
      "Average test loss: 141.04784235331084\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0030724631138145924\n",
      "Average test loss: 0.06043543557781312\n",
      "Epoch 234/300\n",
      "Average training loss: 0.003075360918003652\n",
      "Average test loss: 241.57530991505584\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0031066413656291036\n",
      "Average test loss: 0.030056346474215387\n",
      "Epoch 236/300\n",
      "Average training loss: 0.003145860854536295\n",
      "Average test loss: 0.020758525993674993\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0031203367457621626\n",
      "Average test loss: 211.31298743693034\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0030850005785210263\n",
      "Average test loss: 47.307226615650784\n",
      "Epoch 239/300\n",
      "Average training loss: 0.003190962997989522\n",
      "Average test loss: 0.04296844054096275\n",
      "Epoch 240/300\n",
      "Average training loss: 0.003100618919564618\n",
      "Average test loss: 0.23023321111748615\n",
      "Epoch 241/300\n",
      "Average training loss: 0.003054959154998263\n",
      "Average test loss: 0.02609906239952478\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0030437255828744837\n",
      "Average test loss: 7.323091312115391\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0031958750306318204\n",
      "Average test loss: 0.005902885403277145\n",
      "Epoch 244/300\n",
      "Average training loss: 0.003148365763326486\n",
      "Average test loss: 0.03578967564097709\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0030273296340472167\n",
      "Average test loss: 0.015327780358493328\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0030136254945149024\n",
      "Average test loss: 0.004680782036234935\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0030801031034853724\n",
      "Average test loss: 13.799421414454779\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0030359527023716107\n",
      "Average test loss: 62.77280081862708\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0030748222081197632\n",
      "Average test loss: 0.14138955390618907\n",
      "Epoch 250/300\n",
      "Average training loss: 0.003041942906876405\n",
      "Average test loss: 1.8228146739494469\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0030504945007463298\n",
      "Average test loss: 0.00503554703709152\n",
      "Epoch 252/300\n",
      "Average training loss: 0.003080064959410164\n",
      "Average test loss: 0.004132109479357799\n",
      "Epoch 253/300\n",
      "Average training loss: 0.003059632690106001\n",
      "Average test loss: 0.0040756533837152855\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0030079862183580797\n",
      "Average test loss: 0.1268206954714325\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0030519998512334293\n",
      "Average test loss: 4042.0723534603253\n",
      "Epoch 256/300\n",
      "Average training loss: 0.003048383430267374\n",
      "Average test loss: 0.06183963060379028\n",
      "Epoch 257/300\n",
      "Average training loss: 0.003006375434497992\n",
      "Average test loss: 14.242497446065148\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0030678318049758672\n",
      "Average test loss: 0.004860768914636639\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0030257776127093367\n",
      "Average test loss: 0.5639588610993491\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0030439579128805133\n",
      "Average test loss: 11.289489050125082\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003022313855174515\n",
      "Average test loss: 0.013333509842140807\n",
      "Epoch 262/300\n",
      "Average training loss: 0.003081072297775083\n",
      "Average test loss: 1.4569138578511776\n",
      "Epoch 263/300\n",
      "Average training loss: 0.003004547093477514\n",
      "Average test loss: 1232.428544669893\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0029923850492470793\n",
      "Average test loss: 7.683418597128656\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0030116636864840982\n",
      "Average test loss: 0.012793453459524446\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0029886263372997444\n",
      "Average test loss: 0.43306869128180875\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0031373254431204664\n",
      "Average test loss: 0.03922423797763056\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0029749515230456988\n",
      "Average test loss: 0.005176556534858214\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0030384757435984082\n",
      "Average test loss: 2.5284556403358778\n",
      "Epoch 270/300\n",
      "Average training loss: 0.003036287619007958\n",
      "Average test loss: 0.012369107351534896\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002978084000241425\n",
      "Average test loss: 0.00417436492898398\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0029652428339338966\n",
      "Average test loss: 0.22242765680576365\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0030122030838910076\n",
      "Average test loss: 11.88952190958129\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0029827044037067226\n",
      "Average test loss: 2.5654362318366766\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0029642721807791126\n",
      "Average test loss: 1.1942636257525947\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0029820975930326516\n",
      "Average test loss: 0.2915936109887229\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0030037430007424618\n",
      "Average test loss: 0.012672008078959253\n",
      "Epoch 278/300\n",
      "Average training loss: 0.003055942358656062\n",
      "Average test loss: 0.15486649345006379\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0029718528669327496\n",
      "Average test loss: 2.457384196912249\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002978491007246905\n",
      "Average test loss: 1099.2576244899515\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0029632661355038486\n",
      "Average test loss: 0.1459175626159542\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0029430461728738415\n",
      "Average test loss: 0.06765501140389178\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002958667438270317\n",
      "Average test loss: 0.01236406724775831\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002927851023359431\n",
      "Average test loss: 0.02577794797718525\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0030026150734888185\n",
      "Average test loss: 8.39356704323656\n",
      "Epoch 286/300\n",
      "Average training loss: 0.002986401825522383\n",
      "Average test loss: 18.08308729167448\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0029589529676983754\n",
      "Average test loss: 16.580331241584485\n",
      "Epoch 288/300\n",
      "Average training loss: 0.002971975812481509\n",
      "Average test loss: 0.16091579354968336\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002946701163839963\n",
      "Average test loss: 0.004054566202271316\n",
      "Epoch 290/300\n",
      "Average training loss: 0.002948833425425821\n",
      "Average test loss: 16215213.550986111\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0030791998385555214\n",
      "Average test loss: 0.19021215026246177\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0029347915531446537\n",
      "Average test loss: 5.309520039808419\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002950287014659908\n",
      "Average test loss: 0.006291072474585639\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0029458595437722073\n",
      "Average test loss: 1.849103459301922\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0029341122839185926\n",
      "Average test loss: 2.1661979971826075\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0029762804514418045\n",
      "Average test loss: 1403.9315827060275\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0029554386685291924\n",
      "Average test loss: 1.0687479215578901\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00293875861643917\n",
      "Average test loss: 0.4702841663422684\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0029151528307961093\n",
      "Average test loss: 0.1385990329819421\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002926398866913385\n",
      "Average test loss: 0.003994278181758192\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.02144568058186107\n",
      "Average test loss: 0.010323142276042037\n",
      "Epoch 2/300\n",
      "Average training loss: 0.009148203293482463\n",
      "Average test loss: 0.007143194912622372\n",
      "Epoch 3/300\n",
      "Average training loss: 0.007690374110721879\n",
      "Average test loss: 0.006646200341069036\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0069943793362213505\n",
      "Average test loss: 0.008366647282408343\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006232749722070164\n",
      "Average test loss: 0.0062270027928882175\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005844652393211921\n",
      "Average test loss: 0.00553800774531232\n",
      "Epoch 7/300\n",
      "Average training loss: 0.00565848081269198\n",
      "Average test loss: 0.005330679778009653\n",
      "Epoch 8/300\n",
      "Average training loss: 0.00543021383550432\n",
      "Average test loss: 0.005103965699672699\n",
      "Epoch 9/300\n",
      "Average training loss: 0.005235809866752889\n",
      "Average test loss: 0.0064394366890192034\n",
      "Epoch 10/300\n",
      "Average training loss: 0.005037029671172301\n",
      "Average test loss: 0.005194536752998829\n",
      "Epoch 11/300\n",
      "Average training loss: 0.004870809300492207\n",
      "Average test loss: 0.006045329846441746\n",
      "Epoch 12/300\n",
      "Average training loss: 0.004781883883807394\n",
      "Average test loss: 0.004754382274630997\n",
      "Epoch 13/300\n",
      "Average training loss: 0.004474560370461809\n",
      "Average test loss: 0.004371124907500214\n",
      "Epoch 14/300\n",
      "Average training loss: 0.00437461597368949\n",
      "Average test loss: 0.005483873835040463\n",
      "Epoch 15/300\n",
      "Average training loss: 0.004387053603513373\n",
      "Average test loss: 0.0045806997658477885\n",
      "Epoch 16/300\n",
      "Average training loss: 0.00422326411658691\n",
      "Average test loss: 0.005981406028072039\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0041457005888223646\n",
      "Average test loss: 0.004394231186144882\n",
      "Epoch 18/300\n",
      "Average training loss: 0.004144844027443064\n",
      "Average test loss: 0.004081903603341844\n",
      "Epoch 19/300\n",
      "Average training loss: 0.004029852831322286\n",
      "Average test loss: 0.029715820756223466\n",
      "Epoch 20/300\n",
      "Average training loss: 0.003879415107373562\n",
      "Average test loss: 0.003944765793780486\n",
      "Epoch 21/300\n",
      "Average training loss: 0.003938653570082453\n",
      "Average test loss: 0.004111697632405493\n",
      "Epoch 22/300\n",
      "Average training loss: 0.003949762360089355\n",
      "Average test loss: 0.005229800741291708\n",
      "Epoch 23/300\n",
      "Average training loss: 0.003691593893079294\n",
      "Average test loss: 0.0038339928980502818\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0036787262374742163\n",
      "Average test loss: 0.0038524247225787903\n",
      "Epoch 25/300\n",
      "Average training loss: 0.003853205996048119\n",
      "Average test loss: 0.004151836440174116\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0036275377919276555\n",
      "Average test loss: 0.003748977860642804\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0035525369515849482\n",
      "Average test loss: 0.007373091061909994\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0035224317881382174\n",
      "Average test loss: 0.00403730860994094\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0035393477270586624\n",
      "Average test loss: 0.003586609492699305\n",
      "Epoch 30/300\n",
      "Average training loss: 0.003555503113816182\n",
      "Average test loss: 0.003706629747731818\n",
      "Epoch 31/300\n",
      "Average training loss: 0.003409528593429261\n",
      "Average test loss: 0.003406998069336017\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0033561167942567005\n",
      "Average test loss: 0.0038952191221631235\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0036327927251242928\n",
      "Average test loss: 0.005274330281135109\n",
      "Epoch 34/300\n",
      "Average training loss: 0.003389014409027166\n",
      "Average test loss: 0.003735785886645317\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0033240129937314325\n",
      "Average test loss: 0.003458055214335521\n",
      "Epoch 36/300\n",
      "Average training loss: 0.003386328490037057\n",
      "Average test loss: 0.0034872232265770434\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0033451890986826685\n",
      "Average test loss: 0.0036260606745878855\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0034533425490889283\n",
      "Average test loss: 0.004537887705696954\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0032582035097810957\n",
      "Average test loss: 0.003363973691024714\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0032485004586891996\n",
      "Average test loss: 0.0034317706848184266\n",
      "Epoch 41/300\n",
      "Average training loss: 0.003314443715951509\n",
      "Average test loss: 0.003839846239114801\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0031733555077678626\n",
      "Average test loss: 0.004266851998658644\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0032622715052631167\n",
      "Average test loss: 600.0107540298038\n",
      "Epoch 44/300\n",
      "Average training loss: 0.004233101602850689\n",
      "Average test loss: 0.0034123280164268283\n",
      "Epoch 45/300\n",
      "Average training loss: 0.003205805217847228\n",
      "Average test loss: 0.003606041999740733\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0031251302547752858\n",
      "Average test loss: 0.003560169098070926\n",
      "Epoch 47/300\n",
      "Average training loss: 0.003176494386047125\n",
      "Average test loss: 0.0034410065739519068\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0036357675718350544\n",
      "Average test loss: 0.0036152486523820296\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0032713026872111693\n",
      "Average test loss: 0.0036115216302374997\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0032252797161539397\n",
      "Average test loss: 0.0036960866018715833\n",
      "Epoch 51/300\n",
      "Average training loss: 0.003170674847231971\n",
      "Average test loss: 0.0036892320464054743\n",
      "Epoch 52/300\n",
      "Average training loss: 0.003192962479467193\n",
      "Average test loss: 0.0036336777984268135\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0030732092538641557\n",
      "Average test loss: 0.003910269613895151\n",
      "Epoch 54/300\n",
      "Average training loss: 0.003122822472618686\n",
      "Average test loss: 0.003449996238988307\n",
      "Epoch 55/300\n",
      "Average training loss: 0.003060951360190908\n",
      "Average test loss: 0.00327518678497937\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0030245165375785694\n",
      "Average test loss: 0.003188605621043179\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0030515156884988147\n",
      "Average test loss: 0.003933152828158604\n",
      "Epoch 58/300\n",
      "Average training loss: 0.003161565917957988\n",
      "Average test loss: 0.0031968779497676424\n",
      "Epoch 59/300\n",
      "Average training loss: 0.002987771833315492\n",
      "Average test loss: 0.0033285548090934754\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0030178246214571927\n",
      "Average test loss: 0.003737843520525429\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0032212030763427418\n",
      "Average test loss: 0.0033577856820904547\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0029508155114534827\n",
      "Average test loss: 0.0033135640687412685\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0029441509414464234\n",
      "Average test loss: 0.0031893277642213638\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0030187664236873387\n",
      "Average test loss: 0.0032574569477389257\n",
      "Epoch 65/300\n",
      "Average training loss: 0.002953401716839936\n",
      "Average test loss: 0.005209563326711456\n",
      "Epoch 66/300\n",
      "Average training loss: 0.002926877152795593\n",
      "Average test loss: 0.01362551680372821\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0030532126321146887\n",
      "Average test loss: 0.003231437486285965\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0029088443863309094\n",
      "Average test loss: 0.003916878900180261\n",
      "Epoch 69/300\n",
      "Average training loss: 0.002943962453554074\n",
      "Average test loss: 0.0036820715131858986\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0029084112397912477\n",
      "Average test loss: 0.3321553906003634\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0029072697361310325\n",
      "Average test loss: 0.0031640692992756765\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0028479074883378214\n",
      "Average test loss: 0.0030738171521160337\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0036867067265427774\n",
      "Average test loss: 0.003316638814492358\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0029812027427057425\n",
      "Average test loss: 0.0034406530598385466\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0028658781138559183\n",
      "Average test loss: 0.0031044277805421087\n",
      "Epoch 76/300\n",
      "Average training loss: 0.002861196862740649\n",
      "Average test loss: 0.0031164125653190744\n",
      "Epoch 77/300\n",
      "Average training loss: 0.002843052733482586\n",
      "Average test loss: 0.0034977251378198464\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0028212648261752393\n",
      "Average test loss: 0.003107420758654674\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002887510097482138\n",
      "Average test loss: 0.003474414364538259\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0027819780649410352\n",
      "Average test loss: 0.0033270522952079775\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0028543213518957295\n",
      "Average test loss: 0.00330364559052719\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0029113454148173333\n",
      "Average test loss: 0.0032551048929906554\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0027947258547776277\n",
      "Average test loss: 0.0032443676164580714\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002774258789088991\n",
      "Average test loss: 0.005218857028418117\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0028398449474738704\n",
      "Average test loss: 0.7597453021473355\n",
      "Epoch 86/300\n",
      "Average training loss: 0.003000725713248054\n",
      "Average test loss: 0.12346789558542272\n",
      "Epoch 87/300\n",
      "Average training loss: 0.003922876021307376\n",
      "Average test loss: 0.007710164138012462\n",
      "Epoch 88/300\n",
      "Average training loss: 0.002943249773234129\n",
      "Average test loss: 0.0032548561071356137\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0027922678767806955\n",
      "Average test loss: 0.0032809367450988957\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0028249171283096074\n",
      "Average test loss: 0.003057802989044123\n",
      "Epoch 91/300\n",
      "Average training loss: 0.00275128912118574\n",
      "Average test loss: 0.003995547812845971\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0027247158183405797\n",
      "Average test loss: 0.0030257454564174015\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002763720371863908\n",
      "Average test loss: 0.08422123363448514\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0028494655162923867\n",
      "Average test loss: 0.005199835125356912\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00275054506253865\n",
      "Average test loss: 0.004151279708577527\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0027390301804989577\n",
      "Average test loss: 0.0030255187665008836\n",
      "Epoch 97/300\n",
      "Average training loss: 0.002714360094215307\n",
      "Average test loss: 0.003288241946034961\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0027218394958310656\n",
      "Average test loss: 0.003290899196225736\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0026879191193729638\n",
      "Average test loss: 0.12578979566031032\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0028489600595914657\n",
      "Average test loss: 0.0030669668194734387\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0026608116258349684\n",
      "Average test loss: 0.0048666970547702575\n",
      "Epoch 102/300\n",
      "Average training loss: 0.003043740630667243\n",
      "Average test loss: 0.009019613271786107\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0027028009494145713\n",
      "Average test loss: 0.0038470095342232123\n",
      "Epoch 104/300\n",
      "Average training loss: 0.002698745713672704\n",
      "Average test loss: 0.0030782119186802044\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002697013875676526\n",
      "Average test loss: 0.004975059045271741\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002647521027777758\n",
      "Average test loss: 0.009680038400822216\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0026574644479486677\n",
      "Average test loss: 0.007236656739893887\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0027002075490438274\n",
      "Average test loss: 0.0030391999023656052\n",
      "Epoch 109/300\n",
      "Average training loss: 0.002614631573152211\n",
      "Average test loss: 0.004319513309126099\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0026677105168087614\n",
      "Average test loss: 0.0031320034905026354\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0028700977661129503\n",
      "Average test loss: 0.10673359290427632\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002612461553265651\n",
      "Average test loss: 0.3657427231470744\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0027515114367836053\n",
      "Average test loss: 0.0035851886827084753\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0026089985720399353\n",
      "Average test loss: 0.0030991820871002144\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0026074048419379527\n",
      "Average test loss: 0.05969139630223314\n",
      "Epoch 116/300\n",
      "Average training loss: 0.002672145197995835\n",
      "Average test loss: 0.0033571579112774796\n",
      "Epoch 117/300\n",
      "Average training loss: 0.00261207669828501\n",
      "Average test loss: 0.004478232386418515\n",
      "Epoch 118/300\n",
      "Average training loss: 0.002615680604138308\n",
      "Average test loss: 0.018606972268886038\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002610744398708145\n",
      "Average test loss: 0.003478619328389565\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0025969739113416938\n",
      "Average test loss: 0.01359755023361908\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0025815108423638674\n",
      "Average test loss: 0.003391608496507009\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0028665953050884935\n",
      "Average test loss: 0.005099481016397477\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0025913614531358083\n",
      "Average test loss: 0.006750688717597061\n",
      "Epoch 124/300\n",
      "Average training loss: 0.002568886456700663\n",
      "Average test loss: 0.02007617984132634\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0026075490907662446\n",
      "Average test loss: 1.0275966163708103\n",
      "Epoch 126/300\n",
      "Average training loss: 0.002565418009335796\n",
      "Average test loss: 0.0033167166894094813\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002619572477415204\n",
      "Average test loss: 0.012440911127461327\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0025607878551301027\n",
      "Average test loss: 0.051692070433042116\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00257274896863641\n",
      "Average test loss: 0.003250788300505115\n",
      "Epoch 130/300\n",
      "Average training loss: 0.002554811177568303\n",
      "Average test loss: 0.003723946956710683\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0025562151757379374\n",
      "Average test loss: 2.277487430009577\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0027025376523120534\n",
      "Average test loss: 0.014327632720271747\n",
      "Epoch 133/300\n",
      "Average training loss: 0.002546238042000267\n",
      "Average test loss: 0.0032130402276913327\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0031033988433579606\n",
      "Average test loss: 0.003077685135623647\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0025787665140297677\n",
      "Average test loss: 0.003026726209662027\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0025318197154750428\n",
      "Average test loss: 0.0042521408970157305\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0025212922325978677\n",
      "Average test loss: 0.004293158886333306\n",
      "Epoch 138/300\n",
      "Average training loss: 0.002525405888963077\n",
      "Average test loss: 0.0031081766583439378\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0027456767244471445\n",
      "Average test loss: 382.86204286024304\n",
      "Epoch 140/300\n",
      "Average training loss: 0.003469310638184349\n",
      "Average test loss: 0.0030907411947846413\n",
      "Epoch 141/300\n",
      "Average training loss: 0.002702628499517838\n",
      "Average test loss: 0.0032240587030020027\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0025995548226767115\n",
      "Average test loss: 0.0030793436136510636\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0025526144365883535\n",
      "Average test loss: 0.0034591351749582422\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0025362133531727725\n",
      "Average test loss: 0.0030215608581072753\n",
      "Epoch 145/300\n",
      "Average training loss: 0.002539809949799544\n",
      "Average test loss: 0.0032824946116242143\n",
      "Epoch 146/300\n",
      "Average training loss: 0.002534668171778321\n",
      "Average test loss: 0.03682099629441897\n",
      "Epoch 147/300\n",
      "Average training loss: 0.002500305194614662\n",
      "Average test loss: 0.0030759553048345777\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0025032672415383986\n",
      "Average test loss: 0.0032395243216305973\n",
      "Epoch 149/300\n",
      "Average training loss: 0.002838858877826068\n",
      "Average test loss: 0.003245122305634949\n",
      "Epoch 150/300\n",
      "Average training loss: 0.002567344068860014\n",
      "Average test loss: 0.22606129115157658\n",
      "Epoch 151/300\n",
      "Average training loss: 0.002504462589406305\n",
      "Average test loss: 0.03183124096029335\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0024922139092038074\n",
      "Average test loss: 0.003023705309463872\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0024913270223057933\n",
      "Average test loss: 0.9080720113598638\n",
      "Epoch 154/300\n",
      "Average training loss: 0.002520447542890906\n",
      "Average test loss: 0.0033243678104546335\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0024783937550253337\n",
      "Average test loss: 0.004552562950385941\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0025033884100202058\n",
      "Average test loss: 0.03306013598044713\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0025724532670444913\n",
      "Average test loss: 0.0105614090508057\n",
      "Epoch 158/300\n",
      "Average training loss: 0.002438889931887388\n",
      "Average test loss: 0.010626926458544201\n",
      "Epoch 159/300\n",
      "Average training loss: 0.002491002699153291\n",
      "Average test loss: 0.003078702283402284\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0025035924752139383\n",
      "Average test loss: 0.008151740816318327\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0024650819945252604\n",
      "Average test loss: 0.0030838637637595335\n",
      "Epoch 162/300\n",
      "Average training loss: 0.002497281981839074\n",
      "Average test loss: 0.0872850119912376\n",
      "Epoch 163/300\n",
      "Average training loss: 0.002471928398642275\n",
      "Average test loss: 0.003125438836713632\n",
      "Epoch 164/300\n",
      "Average training loss: 0.002459222552056114\n",
      "Average test loss: 0.0035034865198863875\n",
      "Epoch 165/300\n",
      "Average training loss: 0.002545572110555238\n",
      "Average test loss: 0.0030938276532623504\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0024479602821585204\n",
      "Average test loss: 0.0031275452381620806\n",
      "Epoch 167/300\n",
      "Average training loss: 0.002435486914796962\n",
      "Average test loss: 0.09399615873230828\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0024490509322947925\n",
      "Average test loss: 0.0032328018510921134\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0024474273479233185\n",
      "Average test loss: 0.008636501625180244\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0024553558878186678\n",
      "Average test loss: 0.6213011274867588\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0024734276976022453\n",
      "Average test loss: 0.005511116188847356\n",
      "Epoch 172/300\n",
      "Average training loss: 0.002458532159941064\n",
      "Average test loss: 0.0037059737383905384\n",
      "Epoch 173/300\n",
      "Average training loss: 0.002468057440800799\n",
      "Average test loss: 0.005756674309571584\n",
      "Epoch 174/300\n",
      "Average training loss: 0.002527705827520953\n",
      "Average test loss: 0.006686567143019703\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0024148343669043648\n",
      "Average test loss: 0.025499930429375834\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0023984213579032157\n",
      "Average test loss: 0.0030111624859273435\n",
      "Epoch 177/300\n",
      "Average training loss: 0.002445578754362133\n",
      "Average test loss: 0.003069778089101116\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0023858795497152543\n",
      "Average test loss: 0.0034272380591266684\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0024524242064605157\n",
      "Average test loss: 0.0031296525171233547\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0024171329074435762\n",
      "Average test loss: 0.003502492884794871\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0023870371433181896\n",
      "Average test loss: 0.01090160937452068\n",
      "Epoch 182/300\n",
      "Average training loss: 0.004628898224482933\n",
      "Average test loss: 8.021304114990764\n",
      "Epoch 183/300\n",
      "Average training loss: 0.005479030069998569\n",
      "Average test loss: 0.907382903933525\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0038144080947256753\n",
      "Average test loss: 8.733043304598993\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0036044114354170033\n",
      "Average test loss: 18.960054262396362\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0034621946008669005\n",
      "Average test loss: 19.710628512495095\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0032890963852405548\n",
      "Average test loss: 0.003466149744888147\n",
      "Epoch 188/300\n",
      "Average training loss: 0.003378778516418404\n",
      "Average test loss: 38.00960267205371\n",
      "Epoch 189/300\n",
      "Average training loss: 0.003198019596437613\n",
      "Average test loss: 17.837310433619553\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00309023258731597\n",
      "Average test loss: 5.693679736763239\n",
      "Epoch 191/300\n",
      "Average training loss: 0.003002382558046116\n",
      "Average test loss: 0.7389790859698422\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0029467261359095573\n",
      "Average test loss: 0.01452386961898042\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0029703411770363648\n",
      "Average test loss: 0.003771796081629064\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0029251656304630966\n",
      "Average test loss: 0.02554275797204011\n",
      "Epoch 195/300\n",
      "Average training loss: 0.002891090757523974\n",
      "Average test loss: 0.016585799297317863\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00281192387930221\n",
      "Average test loss: 1984.401861307356\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0033478751629590986\n",
      "Average test loss: 0.010501954658991761\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0028374490506119197\n",
      "Average test loss: 0.005172601361241606\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0027704204983181424\n",
      "Average test loss: 1.1424116809488172\n",
      "Epoch 200/300\n",
      "Average training loss: 0.002763375502700607\n",
      "Average test loss: 6186.98974286645\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0030635439082980155\n",
      "Average test loss: 0.011950266174972057\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00276570173787574\n",
      "Average test loss: 1.389791982870549\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0027440887555066082\n",
      "Average test loss: 0.011771232645958663\n",
      "Epoch 204/300\n",
      "Average training loss: 0.002725879833723108\n",
      "Average test loss: 5.634711181276375\n",
      "Epoch 205/300\n",
      "Average training loss: 0.002686940021192034\n",
      "Average test loss: 0.00304219163229896\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0026666627168241475\n",
      "Average test loss: 0.8576722098572387\n",
      "Epoch 207/300\n",
      "Average training loss: 0.002695610930936204\n",
      "Average test loss: 228.96191869242654\n",
      "Epoch 208/300\n",
      "Average training loss: 0.002682009413010544\n",
      "Average test loss: 15.994961472673548\n",
      "Epoch 209/300\n",
      "Average training loss: 0.002685567255442341\n",
      "Average test loss: 0.0030393292704183196\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0026932441269358\n",
      "Average test loss: 0.010052619771411022\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0025821739617321224\n",
      "Average test loss: 78.7483338172887\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0028560142810973854\n",
      "Average test loss: 0.0032805366791370844\n",
      "Epoch 213/300\n",
      "Average training loss: 0.002792308298043079\n",
      "Average test loss: 0.0030603679911130003\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0026431209136628443\n",
      "Average test loss: 0.03475603186152875\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0026788425226178434\n",
      "Average test loss: 1.265827719858951\n",
      "Epoch 216/300\n",
      "Average training loss: 0.002605375803800093\n",
      "Average test loss: 0.3625916398531861\n",
      "Epoch 217/300\n",
      "Average training loss: 0.002547468171765407\n",
      "Average test loss: 0.004355696791576015\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0025619763146258064\n",
      "Average test loss: 6.777777940127585\n",
      "Epoch 219/300\n",
      "Average training loss: 0.002587123829043574\n",
      "Average test loss: 0.43730755439069535\n",
      "Epoch 220/300\n",
      "Average training loss: 0.002547630097096165\n",
      "Average test loss: 4.908671470988128\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0025926628212134045\n",
      "Average test loss: 2.4874021570947433\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0025149252009060647\n",
      "Average test loss: 0.003872551234232055\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0025896305756436455\n",
      "Average test loss: 0.8048826791565451\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0025442932560626004\n",
      "Average test loss: 0.0031896155927744175\n",
      "Epoch 225/300\n",
      "Average training loss: 0.002952949217003253\n",
      "Average test loss: 0.08911202669805951\n",
      "Epoch 226/300\n",
      "Average training loss: 0.002618502813701828\n",
      "Average test loss: 10.676956978230013\n",
      "Epoch 227/300\n",
      "Average training loss: 0.002525085512134764\n",
      "Average test loss: 2.2388305597040388\n",
      "Epoch 228/300\n",
      "Average training loss: 0.002584115657541487\n",
      "Average test loss: 0.0036224508384863534\n",
      "Epoch 229/300\n",
      "Average training loss: 0.002504642102660404\n",
      "Average test loss: 0.00330088709294796\n",
      "Epoch 230/300\n",
      "Average training loss: 0.002476784161809418\n",
      "Average test loss: 0.010758528427945243\n",
      "Epoch 231/300\n",
      "Average training loss: 0.002506866156951421\n",
      "Average test loss: 0.006614611816902955\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0025121203400194645\n",
      "Average test loss: 50.87558819384045\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0025452627514799438\n",
      "Average test loss: 13.443758274826738\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0025216928651142453\n",
      "Average test loss: 0.005323935564814342\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0024560149237513543\n",
      "Average test loss: 0.5691506163697276\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0024636837626910873\n",
      "Average test loss: 0.10628766576759517\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0024918143293923803\n",
      "Average test loss: 0.003147505648434162\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0024440923848499854\n",
      "Average test loss: 0.33542077508403195\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0025826078500184747\n",
      "Average test loss: 0.004747757008506192\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0024288290701806547\n",
      "Average test loss: 0.0036532404981553554\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0024475349336862563\n",
      "Average test loss: 0.05000348187486331\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0027056076632191737\n",
      "Average test loss: 17.125804707578485\n",
      "Epoch 243/300\n",
      "Average training loss: 0.002490306811614169\n",
      "Average test loss: 0.022905275865975353\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0023985702912840577\n",
      "Average test loss: 0.017399619774685966\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0024126232088439994\n",
      "Average test loss: 0.003064762745673458\n",
      "Epoch 246/300\n",
      "Average training loss: 0.002409133028652933\n",
      "Average test loss: 58.09862747685114\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0024404527061722345\n",
      "Average test loss: 0.30252159617510105\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0024277535013647543\n",
      "Average test loss: 0.003137639889286624\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0024128350340243843\n",
      "Average test loss: 0.025132530828317005\n",
      "Epoch 250/300\n",
      "Average training loss: 0.002423044853947229\n",
      "Average test loss: 0.662208391663101\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0023736894455634885\n",
      "Average test loss: 0.03435269151462449\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0024189890327139035\n",
      "Average test loss: 0.05200801005048884\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0028526094986332786\n",
      "Average test loss: 0.0029883330485059156\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0025167815289977523\n",
      "Average test loss: 505.11463201226127\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0024101728678991396\n",
      "Average test loss: 0.004570514473650191\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00241133774485853\n",
      "Average test loss: 0.005158194998900096\n",
      "Epoch 257/300\n",
      "Average training loss: 0.002390187029933764\n",
      "Average test loss: 0.003376579987299111\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0023752309537182254\n",
      "Average test loss: 0.4820230528546704\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0023595491972648436\n",
      "Average test loss: 14.027697175461386\n",
      "Epoch 260/300\n",
      "Average training loss: 0.002424549019998974\n",
      "Average test loss: 7.064704789115323\n",
      "Epoch 261/300\n",
      "Average training loss: 0.003593279716869195\n",
      "Average test loss: 3.075508927019106\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0031652463802860843\n",
      "Average test loss: 4.265287324845791\n",
      "Epoch 263/300\n",
      "Average training loss: 0.002914591486048367\n",
      "Average test loss: 30.365497734602126\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0031255403169327314\n",
      "Average test loss: 2.347586999922991\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0031393966793807015\n",
      "Average test loss: 1.1201329574102743\n",
      "Epoch 266/300\n",
      "Average training loss: 0.002802835625389384\n",
      "Average test loss: 1354.420148262408\n",
      "Epoch 267/300\n",
      "Average training loss: 0.002849571612974008\n",
      "Average test loss: 0.003068881069827411\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0027032421663817433\n",
      "Average test loss: 511.3805436952379\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0026945458688876697\n",
      "Average test loss: 16312.21735410452\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0026286014724108907\n",
      "Average test loss: 0.5045341488238838\n",
      "Epoch 271/300\n",
      "Average training loss: 0.002604548290785816\n",
      "Average test loss: 0.20795481632649898\n",
      "Epoch 272/300\n",
      "Average training loss: 0.002560325263482001\n",
      "Average test loss: 0.14315837626490327\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0032919806184040176\n",
      "Average test loss: 0.0043943984450565444\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0028240662970476682\n",
      "Average test loss: 0.0033358085201018388\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0026616850847171413\n",
      "Average test loss: 0.10203680838644505\n",
      "Epoch 276/300\n",
      "Average training loss: 0.002590650422912505\n",
      "Average test loss: 0.002995101469879349\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0025377724325905244\n",
      "Average test loss: 0.2045802712680565\n",
      "Epoch 278/300\n",
      "Average training loss: 0.002511096447085341\n",
      "Average test loss: 0.010314178239968087\n",
      "Epoch 279/300\n",
      "Average training loss: 0.002532235952715079\n",
      "Average test loss: 0.003477506142937475\n",
      "Epoch 280/300\n",
      "Average training loss: 0.002511834014000164\n",
      "Average test loss: 2.769510009325213\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0024840036169108417\n",
      "Average test loss: 1636745.7948609444\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0032684369263135723\n",
      "Average test loss: 0.11753271453496483\n",
      "Epoch 283/300\n",
      "Average training loss: 0.002585764958212773\n",
      "Average test loss: 0.006153223184661733\n",
      "Epoch 284/300\n",
      "Average training loss: 0.002512929944321513\n",
      "Average test loss: 0.39441383088587056\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0024989410473240745\n",
      "Average test loss: 6.597859553492731\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0024738359256751006\n",
      "Average test loss: 0.2153674015502135\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0024466290711942645\n",
      "Average test loss: 0.03997526931017637\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0024125936103777754\n",
      "Average test loss: 0.003127105584161149\n",
      "Epoch 289/300\n",
      "Average training loss: 0.002443170424550772\n",
      "Average test loss: 0.45445798214773336\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0033099368303600284\n",
      "Average test loss: 1.479495960412754\n",
      "Epoch 291/300\n",
      "Average training loss: 0.002630016334147917\n",
      "Average test loss: 3.558586419336498\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0026094036110573347\n",
      "Average test loss: 1.2657909059578345\n",
      "Epoch 293/300\n",
      "Average training loss: 0.002521348575750987\n",
      "Average test loss: 0.3437395313721564\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0024718467634585167\n",
      "Average test loss: 0.2728631881672061\n",
      "Epoch 295/300\n",
      "Average training loss: 0.002446154970365266\n",
      "Average test loss: 0.02412451956203828\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0024367591552436354\n",
      "Average test loss: 0.008641010670198335\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0024422808090845743\n",
      "Average test loss: 0.03421962709310982\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0024181985751622254\n",
      "Average test loss: 0.0036315541034564374\n",
      "Epoch 299/300\n",
      "Average training loss: 0.002396838547765381\n",
      "Average test loss: 15.282951772212568\n",
      "Epoch 300/300\n",
      "Average training loss: 0.002513709168053336\n",
      "Average test loss: 0.01551412401182784\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.021861893875731363\n",
      "Average test loss: 0.008692355559104019\n",
      "Epoch 2/300\n",
      "Average training loss: 0.008845353294577864\n",
      "Average test loss: 0.01652802549633715\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0075342183754675916\n",
      "Average test loss: 0.010477453240917789\n",
      "Epoch 4/300\n",
      "Average training loss: 0.006738806698471308\n",
      "Average test loss: 0.009728095319535997\n",
      "Epoch 5/300\n",
      "Average training loss: 0.006203074595580498\n",
      "Average test loss: 0.004654574572212166\n",
      "Epoch 6/300\n",
      "Average training loss: 0.005712550989869568\n",
      "Average test loss: 0.0063816164831320445\n",
      "Epoch 7/300\n",
      "Average training loss: 0.005127244177791808\n",
      "Average test loss: 0.017925602269669372\n",
      "Epoch 8/300\n",
      "Average training loss: 0.004377170433186823\n",
      "Average test loss: 0.007870383843779563\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0041940353268550504\n",
      "Average test loss: 0.02369135750995742\n",
      "Epoch 10/300\n",
      "Average training loss: 0.003944932917339934\n",
      "Average test loss: 0.016466578902883663\n",
      "Epoch 11/300\n",
      "Average training loss: 0.003961780081192652\n",
      "Average test loss: 42.7690818813112\n",
      "Epoch 12/300\n",
      "Average training loss: 0.003727862218808797\n",
      "Average test loss: 0.038343794226646424\n",
      "Epoch 13/300\n",
      "Average training loss: 0.003480074073498448\n",
      "Average test loss: 0.38111400832277204\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0034387626263002553\n",
      "Average test loss: 0.10594380966780914\n",
      "Epoch 15/300\n",
      "Average training loss: 0.003373579569367899\n",
      "Average test loss: 14.590075141815676\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0032645482290536167\n",
      "Average test loss: 0.012300368145522144\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0031587546540217267\n",
      "Average test loss: 0.003932667875041564\n",
      "Epoch 18/300\n",
      "Average training loss: 0.003097198807944854\n",
      "Average test loss: 0.029098139156483942\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0030426649790671135\n",
      "Average test loss: 0.017454734260009395\n",
      "Epoch 20/300\n",
      "Average training loss: 0.002978112161780397\n",
      "Average test loss: 0.005448016984181272\n",
      "Epoch 21/300\n",
      "Average training loss: 0.002904432151466608\n",
      "Average test loss: 0.003204803687416845\n",
      "Epoch 22/300\n",
      "Average training loss: 0.002819347674234046\n",
      "Average test loss: 0.02680622045447429\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0028229278994517195\n",
      "Average test loss: 11.001539855697503\n",
      "Epoch 24/300\n",
      "Average training loss: 0.003100911760702729\n",
      "Average test loss: 0.4218466072270854\n",
      "Epoch 25/300\n",
      "Average training loss: 0.002733423687931564\n",
      "Average test loss: 1.8952426052364624\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0027213974164591896\n",
      "Average test loss: 1.3753158673213588\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0026592565996365414\n",
      "Average test loss: 0.6755957449119951\n",
      "Epoch 28/300\n",
      "Average training loss: 0.002698746424375309\n",
      "Average test loss: 0.003733142952641679\n",
      "Epoch 29/300\n",
      "Average training loss: 0.002619433963050445\n",
      "Average test loss: 1.6467833044172988\n",
      "Epoch 30/300\n",
      "Average training loss: 0.002676323110651639\n",
      "Average test loss: 0.004499469944172435\n",
      "Epoch 31/300\n",
      "Average training loss: 0.002571051005067097\n",
      "Average test loss: 0.302016243217306\n",
      "Epoch 32/300\n",
      "Average training loss: 0.002610990603350931\n",
      "Average test loss: 158.25173491488562\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0025233210809528827\n",
      "Average test loss: 3.4514535061272067\n",
      "Epoch 34/300\n",
      "Average training loss: 0.002821518700983789\n",
      "Average test loss: 752.0979459396468\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0038062686485548815\n",
      "Average test loss: 0.02956503058390485\n",
      "Epoch 36/300\n",
      "Average training loss: 0.002911904383864668\n",
      "Average test loss: 1.0005200765000448\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0027826942505521906\n",
      "Average test loss: 0.8307670268929667\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0033649963753090966\n",
      "Average test loss: 0.0067505016103386875\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0030195635804492566\n",
      "Average test loss: 0.0048057780681798855\n",
      "Epoch 40/300\n",
      "Average training loss: 0.002777435172763136\n",
      "Average test loss: 9.204724409454398\n",
      "Epoch 41/300\n",
      "Average training loss: 0.002743617703931199\n",
      "Average test loss: 2.6359935889840127\n",
      "Epoch 42/300\n",
      "Average training loss: 0.002652729746989078\n",
      "Average test loss: 0.003051776463786761\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0026542970453285507\n",
      "Average test loss: 12.000297750737932\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0025319669897564583\n",
      "Average test loss: 0.00265318397556742\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0025695811152044268\n",
      "Average test loss: 79921.01294097223\n",
      "Epoch 46/300\n",
      "Average training loss: 0.002511557423406177\n",
      "Average test loss: 11.803289569674266\n",
      "Epoch 47/300\n",
      "Average training loss: 0.002420570140911473\n",
      "Average test loss: 28.227437846314576\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0024401042130258346\n",
      "Average test loss: 5240.241507175022\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0023887258865353134\n",
      "Average test loss: 16.819147999086315\n",
      "Epoch 50/300\n",
      "Average training loss: 0.002332020346075296\n",
      "Average test loss: 3.7258095740278563\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0024652972399360606\n",
      "Average test loss: 0.08280114673015972\n",
      "Epoch 52/300\n",
      "Average training loss: 0.002327014892672499\n",
      "Average test loss: 0.04697408632602956\n",
      "Epoch 53/300\n",
      "Average training loss: 0.002601132742025786\n",
      "Average test loss: 0.8421016442163123\n",
      "Epoch 54/300\n",
      "Average training loss: 0.002504240140939752\n",
      "Average test loss: 0.26012530589186483\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0023059634562167856\n",
      "Average test loss: 0.03696910283507572\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0025121627470685377\n",
      "Average test loss: 0.06807413591796325\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00236471971983297\n",
      "Average test loss: 2.3954482234177283\n",
      "Epoch 58/300\n",
      "Average training loss: 0.002289567377935681\n",
      "Average test loss: 0.0023251231388292377\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0022699001975771452\n",
      "Average test loss: 1.2616822706386448\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0022606164227343267\n",
      "Average test loss: 0.007242685906589031\n",
      "Epoch 61/300\n",
      "Average training loss: 0.002261776793127259\n",
      "Average test loss: 0.11073731713824803\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0022910288280496996\n",
      "Average test loss: 272.2317385930568\n",
      "Epoch 63/300\n",
      "Average training loss: 0.002217748652315802\n",
      "Average test loss: 2.0119767329825295\n",
      "Epoch 64/300\n",
      "Average training loss: 0.002201619410266479\n",
      "Average test loss: 0.012786117153242231\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0021857976341206164\n",
      "Average test loss: 489.83346593568393\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0022269553909492162\n",
      "Average test loss: 4.46212329193453\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0026578447833243346\n",
      "Average test loss: 0.8794934701952669\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0024436350516561004\n",
      "Average test loss: 2.3607622719870673\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0022467531216227347\n",
      "Average test loss: 0.14658907800416152\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0022129571714128056\n",
      "Average test loss: 6.815255259959234\n",
      "Epoch 71/300\n",
      "Average training loss: 0.002192050544389834\n",
      "Average test loss: 0.03221914716478851\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0021807708215589323\n",
      "Average test loss: 0.23531489529212316\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0021563360018448698\n",
      "Average test loss: 0.003286595166557365\n",
      "Epoch 74/300\n",
      "Average training loss: 0.002171008502961033\n",
      "Average test loss: 0.5980529853171772\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0021395628191530703\n",
      "Average test loss: 1.3823482383092245\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0023774259229087167\n",
      "Average test loss: 0.0022271432901422184\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0021956429433905417\n",
      "Average test loss: 0.16152203476346202\n",
      "Epoch 78/300\n",
      "Average training loss: 0.002172348440003892\n",
      "Average test loss: 0.281243920504219\n",
      "Epoch 79/300\n",
      "Average training loss: 0.002149858336895704\n",
      "Average test loss: 0.4472628222538365\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0021633666772395374\n",
      "Average test loss: 10.551528438482848\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0021173483574142057\n",
      "Average test loss: 1.0108181364900537\n",
      "Epoch 82/300\n",
      "Average training loss: 0.002095775501285162\n",
      "Average test loss: 8.446078584003056\n",
      "Epoch 83/300\n",
      "Average training loss: 0.002130661609686083\n",
      "Average test loss: 147.7518785445773\n",
      "Epoch 84/300\n",
      "Average training loss: 0.002198503503162\n",
      "Average test loss: 10.977475056992636\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0020976560262756217\n",
      "Average test loss: 5.812945906649033\n",
      "Epoch 86/300\n",
      "Average training loss: 0.002255901474298702\n",
      "Average test loss: 0.015434165924994482\n",
      "Epoch 87/300\n",
      "Average training loss: 0.002209949126570589\n",
      "Average test loss: 27.508004446082644\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0021284434991992182\n",
      "Average test loss: 0.24620988446225722\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0020876384768635033\n",
      "Average test loss: 2.7229401236727004\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0020780519720994765\n",
      "Average test loss: 1.4835537394012015\n",
      "Epoch 91/300\n",
      "Average training loss: 0.002075772245000634\n",
      "Average test loss: 69.89086319583323\n",
      "Epoch 92/300\n",
      "Average training loss: 0.002081421426911321\n",
      "Average test loss: 4.4926978219168054\n",
      "Epoch 93/300\n",
      "Average training loss: 0.002455982088525262\n",
      "Average test loss: 0.031156018507149483\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0022125196516927744\n",
      "Average test loss: 0.2144190223819266\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0020866617826330995\n",
      "Average test loss: 16.48231833474835\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00204866187998818\n",
      "Average test loss: 0.012733717480467425\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0020600031965101757\n",
      "Average test loss: 3.7016357046961783\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0020618162255527247\n",
      "Average test loss: 0.3123731756189631\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0020487519440551597\n",
      "Average test loss: 4.712332013448907\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0022992458403524427\n",
      "Average test loss: 0.005486099481582641\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0022240337814307875\n",
      "Average test loss: 0.04125825607776642\n",
      "Epoch 102/300\n",
      "Average training loss: 0.002092171940538618\n",
      "Average test loss: 0.25423922692404854\n",
      "Epoch 103/300\n",
      "Average training loss: 0.002047225589450035\n",
      "Average test loss: 7.573575276401308\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0020630760269446506\n",
      "Average test loss: 63.283142666914394\n",
      "Epoch 105/300\n",
      "Average training loss: 0.002148076775173346\n",
      "Average test loss: 0.006931597811480363\n",
      "Epoch 106/300\n",
      "Average training loss: 0.002058173729417225\n",
      "Average test loss: 5.798738999127514\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0020296476415048044\n",
      "Average test loss: 6.328617869668537\n",
      "Epoch 108/300\n",
      "Average training loss: 0.002053356526626481\n",
      "Average test loss: 0.19350350718386472\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0021275874382505815\n",
      "Average test loss: 0.1958909070558018\n",
      "Epoch 110/300\n",
      "Average training loss: 0.002089040170527167\n",
      "Average test loss: 0.7740771164700596\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0020240016383015445\n",
      "Average test loss: 0.0022500157132744787\n",
      "Epoch 112/300\n",
      "Average training loss: 0.002014989337676929\n",
      "Average test loss: 2.3099980876015294\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0020251174286628764\n",
      "Average test loss: 61.57039154653168\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0020166101501219802\n",
      "Average test loss: 0.027557630988665753\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0020094160431375105\n",
      "Average test loss: 0.19937265147092856\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0020633556974223918\n",
      "Average test loss: 0.7899553536122872\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0019964571617957618\n",
      "Average test loss: 78.37259870511707\n",
      "Epoch 118/300\n",
      "Average training loss: 0.001993593987594876\n",
      "Average test loss: 0.008954542949795722\n",
      "Epoch 119/300\n",
      "Average training loss: 0.002149652594390015\n",
      "Average test loss: 6.8679172994999425\n",
      "Epoch 120/300\n",
      "Average training loss: 0.001998714250719382\n",
      "Average test loss: 0.0528252499178052\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0019937500100996757\n",
      "Average test loss: 0.2551166298781625\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0020227466361183257\n",
      "Average test loss: 37.85665004823109\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00197661766782403\n",
      "Average test loss: 0.8229092782346739\n",
      "Epoch 124/300\n",
      "Average training loss: 0.001995938032865524\n",
      "Average test loss: 0.0026218067846364443\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0019792268175838723\n",
      "Average test loss: 0.026755488496273756\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0019722430024089084\n",
      "Average test loss: 18.508813353411853\n",
      "Epoch 127/300\n",
      "Average training loss: 0.002070830887907909\n",
      "Average test loss: 0.07059083484237393\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0019615445183590054\n",
      "Average test loss: 460.9655975290537\n",
      "Epoch 129/300\n",
      "Average training loss: 0.001961770331280099\n",
      "Average test loss: 10.948536127984937\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0019430516589846877\n",
      "Average test loss: 256.62341600227853\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0019413694278854463\n",
      "Average test loss: 0.04981679769191477\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0020044283227374157\n",
      "Average test loss: 49.25274340483298\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0020178965833038093\n",
      "Average test loss: 2.4627856401186436\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0019434907053493791\n",
      "Average test loss: 0.7229604069156986\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0019472983684390784\n",
      "Average test loss: 0.04169058077616824\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0019219597144466308\n",
      "Average test loss: 11551.802186482748\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0019495664909482003\n",
      "Average test loss: 7.3625549011528495\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0019235619593204723\n",
      "Average test loss: 125.99602771068447\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0019314196261887749\n",
      "Average test loss: 6.952233369973385\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0019426464427055583\n",
      "Average test loss: 109.77037030956895\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0019499560231342912\n",
      "Average test loss: 138.99600152578452\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0018991731330752373\n",
      "Average test loss: 0.1344167169150379\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0019222327288654114\n",
      "Average test loss: 73.02916549698512\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0019240762696911891\n",
      "Average test loss: 0.20804307907488612\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0019045137914104593\n",
      "Average test loss: 0.023168834721255634\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0019055354813010328\n",
      "Average test loss: 1.2218821479531212\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0019230455042173464\n",
      "Average test loss: 0.06854048042671962\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0018968290648319656\n",
      "Average test loss: 0.015438626978546382\n",
      "Epoch 149/300\n",
      "Average training loss: 0.001911679168852667\n",
      "Average test loss: 412.08044312201605\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0018734324213324322\n",
      "Average test loss: 0.3186255889513219\n",
      "Epoch 151/300\n",
      "Average training loss: 0.00187846659289466\n",
      "Average test loss: 0.2673031085714077\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0018855638330181439\n",
      "Average test loss: 0.08356691302152143\n",
      "Epoch 153/300\n",
      "Average training loss: 0.001951014635256595\n",
      "Average test loss: 0.9434757352694869\n",
      "Epoch 154/300\n",
      "Average training loss: 0.001863481308747497\n",
      "Average test loss: 18.977731831356056\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0019114540784309307\n",
      "Average test loss: 10.787665622744296\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0018748227340272732\n",
      "Average test loss: 20.82357906192541\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0018630570653412077\n",
      "Average test loss: 3.2842087727089724\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0018635772785378828\n",
      "Average test loss: 0.13222283638517063\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0018838967198712957\n",
      "Average test loss: 0.810923185404804\n",
      "Epoch 160/300\n",
      "Average training loss: 0.001858309022978776\n",
      "Average test loss: 47.99914523403036\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0019109213867535193\n",
      "Average test loss: 0.14875340738768378\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0018793646084765594\n",
      "Average test loss: 0.8874028418320749\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0018567001004185942\n",
      "Average test loss: 0.011137419920414686\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0018568636809165279\n",
      "Average test loss: 1.5655550822019577\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0018748823892739085\n",
      "Average test loss: 1.0917161839900331\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0018909266515531474\n",
      "Average test loss: 483.81559067902134\n",
      "Epoch 167/300\n",
      "Average training loss: 0.001867641024912397\n",
      "Average test loss: 5.408479964419372\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0018397307795368963\n",
      "Average test loss: 3.048178066350106\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0018400394417759445\n",
      "Average test loss: 2.8405967609071068\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0018373697590496805\n",
      "Average test loss: 1.8217059728287988\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0018406041499434247\n",
      "Average test loss: 3.879169452921177\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0018433443538637625\n",
      "Average test loss: 0.2840875981003046\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0020691615933966304\n",
      "Average test loss: 0.009582353839443789\n",
      "Epoch 174/300\n",
      "Average training loss: 0.001959067026877569\n",
      "Average test loss: 0.5043366581598918\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0018839799397521548\n",
      "Average test loss: 0.9530402166578504\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0018525309566822317\n",
      "Average test loss: 0.019847793394285773\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0018574489862140681\n",
      "Average test loss: 0.03979590503209167\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0018314976018543045\n",
      "Average test loss: 217.27595664670733\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0018330706573194927\n",
      "Average test loss: 0.02092353035012881\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0018192930441970627\n",
      "Average test loss: 0.03542091332727836\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0018155481942618886\n",
      "Average test loss: 0.32319202077233544\n",
      "Epoch 183/300\n",
      "Average training loss: 0.001822824922700723\n",
      "Average test loss: 6.6534829735259216\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0018076093608720436\n",
      "Average test loss: 5.062170674448212\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0018228850044930975\n",
      "Average test loss: 43.85481105047961\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0018084420038180218\n",
      "Average test loss: 7.247043778692062\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0019711829516001872\n",
      "Average test loss: 0.073983058264686\n",
      "Epoch 188/300\n",
      "Average training loss: 0.002012237999588251\n",
      "Average test loss: 0.31061022437198293\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0018478465960878464\n",
      "Average test loss: 29.866541038636534\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00182621164392266\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Nesterov_No_Residual-LastLayer/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Nesterov_No_Residual-LastLayer/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Nesterov_No_Residual-LastLayer/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
