{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee6282c-4511-472e-98d8-f8d738471540",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14201101458734935\n",
      "Average test loss: 0.016370616761346657\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06550575502382384\n",
      "Average test loss: 0.011507978910373317\n",
      "Epoch 3/300\n",
      "Average training loss: 0.060538575467136174\n",
      "Average test loss: 0.010974263471033838\n",
      "Epoch 4/300\n",
      "Average training loss: 0.057604623357454934\n",
      "Average test loss: 0.010092835817072127\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05649955741564433\n",
      "Average test loss: 0.016451093285448023\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05372118057807287\n",
      "Average test loss: 0.010368676736123033\n",
      "Epoch 7/300\n",
      "Average training loss: 0.051806418269872666\n",
      "Average test loss: 0.009090521318217118\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04960369743572341\n",
      "Average test loss: 0.00864109536425935\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04836365667978922\n",
      "Average test loss: 0.009697176029284795\n",
      "Epoch 10/300\n",
      "Average training loss: 0.047526915007167395\n",
      "Average test loss: 0.008498518246743414\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04671963034073512\n",
      "Average test loss: 0.008799641403059165\n",
      "Epoch 12/300\n",
      "Average training loss: 0.045353261080053116\n",
      "Average test loss: 0.007899750020768908\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04423729835947355\n",
      "Average test loss: 0.007637398397756947\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04364454181989034\n",
      "Average test loss: 0.011819718697004848\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04293288287851545\n",
      "Average test loss: 0.007525813961194621\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04281701615452766\n",
      "Average test loss: 0.007738375369873312\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0425077517595556\n",
      "Average test loss: 0.007644187418123087\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04166966949237717\n",
      "Average test loss: 0.007742687134279145\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04083791305621465\n",
      "Average test loss: 0.00790969627060824\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04143340813451343\n",
      "Average test loss: 0.007300161478420099\n",
      "Epoch 21/300\n",
      "Average training loss: 0.040459150569306476\n",
      "Average test loss: 0.007125612460904651\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0396408094200823\n",
      "Average test loss: 0.0069568957972029844\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03944050235218472\n",
      "Average test loss: 0.007756811087330183\n",
      "Epoch 24/300\n",
      "Average training loss: 0.039453118354082106\n",
      "Average test loss: 0.007151879624360137\n",
      "Epoch 25/300\n",
      "Average training loss: 0.038994595213068854\n",
      "Average test loss: 0.007512330224944485\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03864126278956731\n",
      "Average test loss: 0.006999762066950401\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0384179216755761\n",
      "Average test loss: 0.006658446651779943\n",
      "Epoch 28/300\n",
      "Average training loss: 0.037894682857725355\n",
      "Average test loss: 0.006710728398627705\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03779284700420168\n",
      "Average test loss: 0.006585079320602947\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03778740806380908\n",
      "Average test loss: 0.007738111870984237\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03733359804087215\n",
      "Average test loss: 0.006787681936182909\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03712170548902618\n",
      "Average test loss: 0.006757013434337245\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03707014551758766\n",
      "Average test loss: 0.0070016185690959295\n",
      "Epoch 34/300\n",
      "Average training loss: 0.036821766753991444\n",
      "Average test loss: 0.007248084928426477\n",
      "Epoch 35/300\n",
      "Average training loss: 0.036651085330380334\n",
      "Average test loss: 0.006747900115119086\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03634076871474584\n",
      "Average test loss: 0.006813964921981096\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03619680698050393\n",
      "Average test loss: 0.006538215475777785\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03616449913713667\n",
      "Average test loss: 0.006530012231734064\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03582817110419274\n",
      "Average test loss: 0.006407948796947797\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03582721549272537\n",
      "Average test loss: 0.007017224089139038\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03560277892318037\n",
      "Average test loss: 0.006342385800348388\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03576810943418079\n",
      "Average test loss: 0.006393880989816454\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03557207040985425\n",
      "Average test loss: 0.006394822356187635\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03523215663101938\n",
      "Average test loss: 0.006320922660330931\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03501068728831079\n",
      "Average test loss: 0.006204343102044529\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0351846423347791\n",
      "Average test loss: 0.006313447625272804\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0351808210975594\n",
      "Average test loss: 0.006423999078571796\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03501471199260817\n",
      "Average test loss: 0.008566601226727168\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03467603658967548\n",
      "Average test loss: 0.006689279050462776\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03463355139229033\n",
      "Average test loss: 0.006686929496212138\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03447671702173021\n",
      "Average test loss: 0.006251868706610468\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0345462859471639\n",
      "Average test loss: 0.0066937810277773275\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03437491930193371\n",
      "Average test loss: 0.0062743281784156954\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03443443527817726\n",
      "Average test loss: 0.006470294118341473\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03422284060385492\n",
      "Average test loss: 0.006084282297227118\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03419409482346641\n",
      "Average test loss: 0.006195306363619036\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03393922217686971\n",
      "Average test loss: 0.006417157536579503\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03404849827786287\n",
      "Average test loss: 0.006294602152581016\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03390271176728937\n",
      "Average test loss: 0.00646721828273601\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03383337865107589\n",
      "Average test loss: 0.006082269332475132\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0339320821331607\n",
      "Average test loss: 0.006144141961303022\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03370596854223146\n",
      "Average test loss: 0.0064328103599449\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03377532641092936\n",
      "Average test loss: 0.006317216478702095\n",
      "Epoch 64/300\n",
      "Average training loss: 0.033570979207754134\n",
      "Average test loss: 0.0060759496270782415\n",
      "Epoch 65/300\n",
      "Average training loss: 0.033601682278845045\n",
      "Average test loss: 0.02046410926183065\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03355403596493933\n",
      "Average test loss: 0.006068411394539806\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03337372311618593\n",
      "Average test loss: 0.006269124026099841\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03337110212776396\n",
      "Average test loss: 0.006289396409359244\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0334642392595609\n",
      "Average test loss: 0.006032697460717625\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0333027425126897\n",
      "Average test loss: 0.006226243113891946\n",
      "Epoch 71/300\n",
      "Average training loss: 0.033269491924179925\n",
      "Average test loss: 0.006052061828060283\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03308359011179871\n",
      "Average test loss: 0.006088797417365842\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03301199285354879\n",
      "Average test loss: 0.007500737789604399\n",
      "Epoch 74/300\n",
      "Average training loss: 0.033071270359887016\n",
      "Average test loss: 0.006368924348304669\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03301006062163247\n",
      "Average test loss: 0.006132697934905688\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03302389221058952\n",
      "Average test loss: 0.005959029079725345\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03289438483119011\n",
      "Average test loss: 0.005962741302119361\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03301167060931524\n",
      "Average test loss: 0.0060908415901164215\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03284247840444247\n",
      "Average test loss: 0.006327397959927718\n",
      "Epoch 80/300\n",
      "Average training loss: 0.032754048950142334\n",
      "Average test loss: 0.006187575609733661\n",
      "Epoch 81/300\n",
      "Average training loss: 0.032715996808475915\n",
      "Average test loss: 0.0062598264374666744\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03273481475313505\n",
      "Average test loss: 0.09057109388377932\n",
      "Epoch 83/300\n",
      "Average training loss: 0.032940009358856416\n",
      "Average test loss: 0.006038204182767206\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03274344122078684\n",
      "Average test loss: 0.006213467710961898\n",
      "Epoch 85/300\n",
      "Average training loss: 0.032560917561252914\n",
      "Average test loss: 0.005933780292669932\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03258041484488381\n",
      "Average test loss: 0.005998708467930556\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03249398992127842\n",
      "Average test loss: 0.006152111318376329\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03239032492703862\n",
      "Average test loss: 0.005943944995601972\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03249749566780196\n",
      "Average test loss: 0.006063148196372721\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03237583441866769\n",
      "Average test loss: 0.005970592375430796\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03246667797366778\n",
      "Average test loss: 0.006001941596054368\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03224817696875996\n",
      "Average test loss: 0.005994840229964918\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03236266728904512\n",
      "Average test loss: 0.006072268697122732\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03228251984715462\n",
      "Average test loss: 0.0060382785867485736\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03224182286030716\n",
      "Average test loss: 0.006105674739927054\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03222420100039906\n",
      "Average test loss: 0.005993208238648044\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03215599283244875\n",
      "Average test loss: 0.005905024577346113\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03207211819291115\n",
      "Average test loss: 0.006308885151313411\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03210871171785726\n",
      "Average test loss: 0.006044774962796105\n",
      "Epoch 100/300\n",
      "Average training loss: 0.032179529615574416\n",
      "Average test loss: 0.005997938373850452\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03204958722823196\n",
      "Average test loss: 0.005892361700534821\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03198752074937026\n",
      "Average test loss: 0.005935317868573798\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03197163454360432\n",
      "Average test loss: 0.006046874443689982\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0319069379137622\n",
      "Average test loss: 0.005892361801531579\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03186734562449985\n",
      "Average test loss: 0.005983034656693538\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03180731034444438\n",
      "Average test loss: 0.006038210838619206\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03185664310720232\n",
      "Average test loss: 0.00586505435158809\n",
      "Epoch 108/300\n",
      "Average training loss: 0.031767393993006814\n",
      "Average test loss: 0.006656694506605466\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03186965164542198\n",
      "Average test loss: 0.0059708791586260005\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03180907939871152\n",
      "Average test loss: 0.0062263852746950255\n",
      "Epoch 111/300\n",
      "Average training loss: 0.031747172077496846\n",
      "Average test loss: 0.005865466750330395\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03166739721099535\n",
      "Average test loss: 0.006364444649881787\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03166818210482597\n",
      "Average test loss: 0.006135962271855937\n",
      "Epoch 114/300\n",
      "Average training loss: 0.031683589923712945\n",
      "Average test loss: 0.006521471897761027\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03160126253300243\n",
      "Average test loss: 0.006202010371204879\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031660650928815204\n",
      "Average test loss: 0.005973467495706346\n",
      "Epoch 117/300\n",
      "Average training loss: 0.031510700191060705\n",
      "Average test loss: 0.0061099052867955635\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03158067818482717\n",
      "Average test loss: 0.006088108899278773\n",
      "Epoch 119/300\n",
      "Average training loss: 0.031519306250744394\n",
      "Average test loss: 0.005964322809543875\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03153380266494221\n",
      "Average test loss: 0.005953198794275522\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03141598513060146\n",
      "Average test loss: 0.005866019582582845\n",
      "Epoch 122/300\n",
      "Average training loss: 0.031468748380740486\n",
      "Average test loss: 0.006289331603381369\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03143570606244935\n",
      "Average test loss: 0.0059797403216362\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03146843668487337\n",
      "Average test loss: 0.005937111655871074\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03141688678165277\n",
      "Average test loss: 0.005955275562902292\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03128812125325203\n",
      "Average test loss: 0.005989919433163272\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03143480266133944\n",
      "Average test loss: 0.006029095844262176\n",
      "Epoch 128/300\n",
      "Average training loss: 0.031316376686096194\n",
      "Average test loss: 0.005913138430979517\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0312846077332894\n",
      "Average test loss: 0.005898587670591143\n",
      "Epoch 130/300\n",
      "Average training loss: 0.031370879275931254\n",
      "Average test loss: 0.005940004842976729\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03128491690920459\n",
      "Average test loss: 0.005935931386633052\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03123020033041636\n",
      "Average test loss: 0.006201202047781812\n",
      "Epoch 133/300\n",
      "Average training loss: 0.031189222494761148\n",
      "Average test loss: 0.005909552021159066\n",
      "Epoch 134/300\n",
      "Average training loss: 0.031173775864972008\n",
      "Average test loss: 0.005910542968660593\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03126295498013496\n",
      "Average test loss: 0.006167622927576303\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031203384863005744\n",
      "Average test loss: 0.006221907601588302\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031109605959720083\n",
      "Average test loss: 0.006099616443945302\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031189642869763903\n",
      "Average test loss: 0.005815009466889832\n",
      "Epoch 139/300\n",
      "Average training loss: 0.031052929735845988\n",
      "Average test loss: 0.005957216332770056\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0310884352558189\n",
      "Average test loss: 0.005979836537813147\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03110489065448443\n",
      "Average test loss: 0.006308578745772442\n",
      "Epoch 142/300\n",
      "Average training loss: 0.031069679526819122\n",
      "Average test loss: 0.0059023847012884085\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03107505143351025\n",
      "Average test loss: 0.006016826648679045\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03100712541076872\n",
      "Average test loss: 0.0059303600593573515\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030979601052072314\n",
      "Average test loss: 0.006180016212579277\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030987793581353295\n",
      "Average test loss: 0.005957254189790951\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030908768196900686\n",
      "Average test loss: 0.005885206101669206\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03096318206853337\n",
      "Average test loss: 0.005858768792615997\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0309026917003923\n",
      "Average test loss: 0.005831021428315176\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030868576203783353\n",
      "Average test loss: 0.005988215864946445\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03090047419567903\n",
      "Average test loss: 0.006438788282788462\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030873926603131825\n",
      "Average test loss: 0.005987540259957314\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030851000972919995\n",
      "Average test loss: 0.006496071097337537\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030836760280860794\n",
      "Average test loss: 0.0059684089256657495\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03082195481657982\n",
      "Average test loss: 0.006100121270865202\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030745081951220832\n",
      "Average test loss: 0.00602903080234925\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030764946970674726\n",
      "Average test loss: 0.005919113974811302\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030711104119817416\n",
      "Average test loss: 0.005895052745938301\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030750639052854644\n",
      "Average test loss: 0.006048227145026127\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03069369240767426\n",
      "Average test loss: 0.006222863731698857\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03078729164103667\n",
      "Average test loss: 0.005895446653167407\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030586389881041315\n",
      "Average test loss: 0.006018644425604078\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030698246729042793\n",
      "Average test loss: 0.006462634994337956\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030644263986084195\n",
      "Average test loss: 0.005978108134948545\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030632526343067488\n",
      "Average test loss: 0.005952076569613483\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030583992348776925\n",
      "Average test loss: 0.0059132465211053685\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030539473800195588\n",
      "Average test loss: 0.0061294510629442\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030573038200537365\n",
      "Average test loss: 0.006132267149372234\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030532599300146103\n",
      "Average test loss: 0.006365826221803824\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03059139424893591\n",
      "Average test loss: 0.0063580671834448975\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03054753988981247\n",
      "Average test loss: 0.005943857489774625\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030622445588310558\n",
      "Average test loss: 0.00582729403840171\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030504630625247955\n",
      "Average test loss: 0.005957095981885989\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030428185598717794\n",
      "Average test loss: 0.00604728431523674\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03054465361105071\n",
      "Average test loss: 0.00609327767458227\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03043082242541843\n",
      "Average test loss: 0.008489160135388375\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030393601949016254\n",
      "Average test loss: 0.006212941025694211\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030410473439428543\n",
      "Average test loss: 0.006643851182113091\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030482010496987236\n",
      "Average test loss: 0.006013178730176555\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03043619373606311\n",
      "Average test loss: 0.005987879916611645\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030410326460997263\n",
      "Average test loss: 0.0059899446529646716\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03034706473019388\n",
      "Average test loss: 0.00614127714621524\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0303683423962858\n",
      "Average test loss: 0.00591165748052299\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03029847624235683\n",
      "Average test loss: 0.0063834315521849525\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03035721066594124\n",
      "Average test loss: 0.0061380521994498045\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030310892735918363\n",
      "Average test loss: 0.005911604861832328\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03023181481162707\n",
      "Average test loss: 0.006283116816646523\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030330291996399562\n",
      "Average test loss: 0.005924408591455883\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030275183454155922\n",
      "Average test loss: 0.00616655976490842\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03033465410106712\n",
      "Average test loss: 0.005868959483587079\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030185736586650212\n",
      "Average test loss: 0.005958819778429138\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030264794137742784\n",
      "Average test loss: 0.005929640121757984\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030234624692135385\n",
      "Average test loss: 0.0060114629401101\n",
      "Epoch 194/300\n",
      "Average training loss: 0.030256132965286574\n",
      "Average test loss: 0.006057635846237342\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030187391151984534\n",
      "Average test loss: 0.006045558189766274\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030157005161046982\n",
      "Average test loss: 0.0062645594875017805\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03014826766649882\n",
      "Average test loss: 0.006005892659640974\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03016123738553789\n",
      "Average test loss: 0.0059916627489858204\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030165787551138135\n",
      "Average test loss: 0.006345190828045209\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030178885772824286\n",
      "Average test loss: 0.00618547572940588\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030177955634064144\n",
      "Average test loss: 0.00597768410667777\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030064094863004154\n",
      "Average test loss: 0.006167815017617411\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030146755114197732\n",
      "Average test loss: 0.005852774677177271\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03002875349422296\n",
      "Average test loss: 0.006015488100962506\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03010212748911646\n",
      "Average test loss: 0.006323569304413266\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030048780351877212\n",
      "Average test loss: 0.005994523311654727\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030054076886839337\n",
      "Average test loss: 0.006193495745874114\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030083062438501253\n",
      "Average test loss: 0.0059750379576451245\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030028653376632266\n",
      "Average test loss: 0.0059052893283466495\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03000496356520388\n",
      "Average test loss: 0.006252547681331634\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02995863991975784\n",
      "Average test loss: 0.007365632231864664\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029977206425534354\n",
      "Average test loss: 0.006061740067270067\n",
      "Epoch 213/300\n",
      "Average training loss: 0.029987749435835414\n",
      "Average test loss: 0.006095579809198777\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02995923858880997\n",
      "Average test loss: 0.0060180740824176205\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029909851296080484\n",
      "Average test loss: 0.006140777281175057\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030016230036815007\n",
      "Average test loss: 0.0062011649977001876\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02990773386425442\n",
      "Average test loss: 0.006054566154877344\n",
      "Epoch 218/300\n",
      "Average training loss: 0.029954671972327762\n",
      "Average test loss: 0.005973373341891501\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02994389545586374\n",
      "Average test loss: 0.005897671355555455\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02992717311448521\n",
      "Average test loss: 0.006051521405163738\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029895546913146973\n",
      "Average test loss: 0.006553543681899706\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02984686751001411\n",
      "Average test loss: 0.007152530881265799\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02987350401116742\n",
      "Average test loss: 0.006094184586571323\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029960195822848215\n",
      "Average test loss: 0.006196467770056592\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029889178678393365\n",
      "Average test loss: 0.006069842344770829\n",
      "Epoch 226/300\n",
      "Average training loss: 0.029793454746405285\n",
      "Average test loss: 0.006375114964942138\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02982123166322708\n",
      "Average test loss: 0.006152714644455247\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02982917653189765\n",
      "Average test loss: 0.006488782716708051\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029785840844114623\n",
      "Average test loss: 0.005961271138654815\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029774719410472446\n",
      "Average test loss: 0.006002227682827248\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029818672224879265\n",
      "Average test loss: 0.005971663527604606\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029863802363475165\n",
      "Average test loss: 0.005973688082148631\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029776555801431338\n",
      "Average test loss: 0.00620308071664638\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029820413364304438\n",
      "Average test loss: 0.006079099496205648\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029808794054720138\n",
      "Average test loss: 0.005864493476019965\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02972613607843717\n",
      "Average test loss: 0.005919476806703541\n",
      "Epoch 237/300\n",
      "Average training loss: 0.029727599806255764\n",
      "Average test loss: 0.006081623936692874\n",
      "Epoch 238/300\n",
      "Average training loss: 0.029775602438383633\n",
      "Average test loss: 0.0060337577553259\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029771475540267097\n",
      "Average test loss: 0.006091464231411616\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029655353320969476\n",
      "Average test loss: 0.00607972536691361\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029695356301135487\n",
      "Average test loss: 0.0060683862769769295\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029629719706045256\n",
      "Average test loss: 0.006023590732779768\n",
      "Epoch 243/300\n",
      "Average training loss: 0.029695472805036437\n",
      "Average test loss: 0.006183748485313521\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02970077712337176\n",
      "Average test loss: 0.005910617055992285\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029616781584090656\n",
      "Average test loss: 0.006048467050823901\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02969723886417018\n",
      "Average test loss: 0.0059774158555600376\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029600517567661074\n",
      "Average test loss: 0.005960510468317403\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02958519657784038\n",
      "Average test loss: 0.006067078681455718\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029627128947112294\n",
      "Average test loss: 0.0060098378910786576\n",
      "Epoch 250/300\n",
      "Average training loss: 0.029669370936022864\n",
      "Average test loss: 0.0065040148575272825\n",
      "Epoch 251/300\n",
      "Average training loss: 0.029592439002460905\n",
      "Average test loss: 0.006069322454018726\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029633284323745304\n",
      "Average test loss: 0.006346710109876262\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029606558329529232\n",
      "Average test loss: 0.005976843371987343\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029602088544103834\n",
      "Average test loss: 0.005943602223363187\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029540218599968485\n",
      "Average test loss: 0.006029636141326692\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02949286021788915\n",
      "Average test loss: 0.005921704597357247\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029550344241989983\n",
      "Average test loss: 0.006627868000004027\n",
      "Epoch 258/300\n",
      "Average training loss: 0.029601728863186308\n",
      "Average test loss: 0.006339560827033387\n",
      "Epoch 259/300\n",
      "Average training loss: 0.029479777046375805\n",
      "Average test loss: 0.006575516660594278\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02958430723183685\n",
      "Average test loss: 0.006028088752180338\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02955132073495123\n",
      "Average test loss: 0.005986139980041319\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02951875139441755\n",
      "Average test loss: 0.006056352239516046\n",
      "Epoch 263/300\n",
      "Average training loss: 0.029509727974732718\n",
      "Average test loss: 0.006079721202452978\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02947894380821122\n",
      "Average test loss: 0.006361958517796464\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029482935421996648\n",
      "Average test loss: 0.006373049910697672\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02944791678753164\n",
      "Average test loss: 0.0060738411235312624\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02947082587083181\n",
      "Average test loss: 0.005958058450784948\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029425733748409483\n",
      "Average test loss: 0.0060448530610236854\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029452968070904414\n",
      "Average test loss: 0.005948179812894927\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029413425544897714\n",
      "Average test loss: 0.006351667365680138\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02941093965702587\n",
      "Average test loss: 0.005980423398729827\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029487241312861442\n",
      "Average test loss: 0.00615936747358905\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029429173689749507\n",
      "Average test loss: 0.006290330990321107\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029423985885249245\n",
      "Average test loss: 0.006093963505079349\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02937357924381892\n",
      "Average test loss: 0.02279700654082828\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02939088632000817\n",
      "Average test loss: 0.006366864591009087\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029359952443175845\n",
      "Average test loss: 0.006242948522584306\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029365271053380437\n",
      "Average test loss: 0.00599389286339283\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029398741049898994\n",
      "Average test loss: 0.006600447064472569\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029301237765285704\n",
      "Average test loss: 0.006188526699112521\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02931345969438553\n",
      "Average test loss: 0.005972232725057337\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02928295770287514\n",
      "Average test loss: 0.0060111949228578146\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029353058725595474\n",
      "Average test loss: 0.00615198073329197\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029302957107623418\n",
      "Average test loss: 0.00613085580451621\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02925318753057056\n",
      "Average test loss: 0.0060706192892458705\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029280537045664256\n",
      "Average test loss: 0.006112751415206326\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029368736690945096\n",
      "Average test loss: 0.006445454734067122\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029314442755447492\n",
      "Average test loss: 0.006198278225544426\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029254341170191763\n",
      "Average test loss: 0.0062259816423886356\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029297952198319963\n",
      "Average test loss: 0.006000510973648892\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029223225245873133\n",
      "Average test loss: 0.006030333705246449\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02926953670051363\n",
      "Average test loss: 0.006108991758070058\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029253748594058885\n",
      "Average test loss: 0.006557806964135832\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029205122083425523\n",
      "Average test loss: 0.006097789178291957\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029315134902795154\n",
      "Average test loss: 0.006243559480955204\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029177069139149452\n",
      "Average test loss: 0.006255251500341627\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029186728374825582\n",
      "Average test loss: 0.006178862490173844\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029236227308710415\n",
      "Average test loss: 0.006047861621197727\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029173906611071693\n",
      "Average test loss: 0.006106750105404192\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029233449753787784\n",
      "Average test loss: 0.006203737164537112\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12267458488543828\n",
      "Average test loss: 0.0109779915685455\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05282461584938897\n",
      "Average test loss: 0.012665659018688732\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04788912143309911\n",
      "Average test loss: 0.008507500410493878\n",
      "Epoch 4/300\n",
      "Average training loss: 0.044845106369919244\n",
      "Average test loss: 0.006696016168428792\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04214426311850548\n",
      "Average test loss: 0.007711038496345282\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04083220303720898\n",
      "Average test loss: 0.00798575676812066\n",
      "Epoch 7/300\n",
      "Average training loss: 0.038505558335118824\n",
      "Average test loss: 0.006538842039803664\n",
      "Epoch 8/300\n",
      "Average training loss: 0.036351239528920914\n",
      "Average test loss: 0.007233656040910218\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03652264075477918\n",
      "Average test loss: 0.006264366731461551\n",
      "Epoch 10/300\n",
      "Average training loss: 0.034881169077422884\n",
      "Average test loss: 0.005005611148352424\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03325820034742356\n",
      "Average test loss: 0.004883536488852567\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03269147367941009\n",
      "Average test loss: 0.006856449062625567\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03159906911849976\n",
      "Average test loss: 0.005481469713565376\n",
      "Epoch 14/300\n",
      "Average training loss: 0.031101985464493432\n",
      "Average test loss: 0.005298141839603583\n",
      "Epoch 15/300\n",
      "Average training loss: 0.030333401176664564\n",
      "Average test loss: 0.006543936077091429\n",
      "Epoch 16/300\n",
      "Average training loss: 0.029600652140047815\n",
      "Average test loss: 0.0047900019257018965\n",
      "Epoch 17/300\n",
      "Average training loss: 0.029278941108120814\n",
      "Average test loss: 0.004801005682390597\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02906407077776061\n",
      "Average test loss: 0.004515394445094798\n",
      "Epoch 19/300\n",
      "Average training loss: 0.028690270204510954\n",
      "Average test loss: 0.005103226790411605\n",
      "Epoch 20/300\n",
      "Average training loss: 0.028139525406890447\n",
      "Average test loss: 0.004359853320030703\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027633422495590317\n",
      "Average test loss: 0.004112203818228509\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027541159234113162\n",
      "Average test loss: 0.004490643861393134\n",
      "Epoch 23/300\n",
      "Average training loss: 0.027435314698351754\n",
      "Average test loss: 0.004438767707389262\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02705498556792736\n",
      "Average test loss: 0.004357444839965966\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02669606742097272\n",
      "Average test loss: 0.00499405949562788\n",
      "Epoch 26/300\n",
      "Average training loss: 0.026735520258545874\n",
      "Average test loss: 0.004115188888584574\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02630536849465635\n",
      "Average test loss: 0.004313597891479731\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026085714250802992\n",
      "Average test loss: 0.004197895902312464\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02581665562424395\n",
      "Average test loss: 0.004088980736417903\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02612822102341387\n",
      "Average test loss: 0.003999574628348152\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02571858284705215\n",
      "Average test loss: 0.0040933779494629965\n",
      "Epoch 32/300\n",
      "Average training loss: 0.025493247184488508\n",
      "Average test loss: 0.003995391913172271\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02538952981101142\n",
      "Average test loss: 0.003869840666444765\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02516449630094899\n",
      "Average test loss: 0.004176142087413205\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025185911907090082\n",
      "Average test loss: 0.004548461724900537\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025037790189186733\n",
      "Average test loss: 0.00635676352514161\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024830010135968528\n",
      "Average test loss: 0.0038106739268534712\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024713731706142425\n",
      "Average test loss: 0.0039857092690136695\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02463648618095451\n",
      "Average test loss: 0.0037147999054027926\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024707619986600347\n",
      "Average test loss: 0.004347448988921113\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02443604333864318\n",
      "Average test loss: 0.00371317989544736\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02453383374048604\n",
      "Average test loss: 0.003670957191122903\n",
      "Epoch 43/300\n",
      "Average training loss: 0.024207486949033206\n",
      "Average test loss: 0.0038531281803217197\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02414274970855978\n",
      "Average test loss: 0.0036795802981489235\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024238194139467345\n",
      "Average test loss: 0.0036899983887043264\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023975603479478094\n",
      "Average test loss: 0.003596355906377236\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02400905184282197\n",
      "Average test loss: 0.0036698937339501247\n",
      "Epoch 48/300\n",
      "Average training loss: 0.024074104997846815\n",
      "Average test loss: 0.004661472127669387\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02384539580676291\n",
      "Average test loss: 0.003648247065229548\n",
      "Epoch 50/300\n",
      "Average training loss: 0.023762476114763155\n",
      "Average test loss: 0.003850666523186697\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023827774056129986\n",
      "Average test loss: 0.003801859537760417\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023815263715055254\n",
      "Average test loss: 0.003667040331496133\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023587672117683624\n",
      "Average test loss: 0.003767637305789524\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02359146142668194\n",
      "Average test loss: 0.0036987382086614766\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023562104725175434\n",
      "Average test loss: 0.003969694689330128\n",
      "Epoch 56/300\n",
      "Average training loss: 0.023584921954406632\n",
      "Average test loss: 0.0038913505880369082\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023560934911171595\n",
      "Average test loss: 0.0038924158720506563\n",
      "Epoch 58/300\n",
      "Average training loss: 0.023325446183482805\n",
      "Average test loss: 0.003548417287775212\n",
      "Epoch 59/300\n",
      "Average training loss: 0.023353365815348096\n",
      "Average test loss: 0.003711975729920798\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02330241195526388\n",
      "Average test loss: 0.0037714299567871623\n",
      "Epoch 61/300\n",
      "Average training loss: 0.023343940986527336\n",
      "Average test loss: 0.003539202048132817\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02331199451122019\n",
      "Average test loss: 0.003599493886447615\n",
      "Epoch 63/300\n",
      "Average training loss: 0.023190183957417804\n",
      "Average test loss: 0.003742948432556457\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02328703109257751\n",
      "Average test loss: 0.0037278086936308277\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02309207671549585\n",
      "Average test loss: 0.003549180054416259\n",
      "Epoch 66/300\n",
      "Average training loss: 0.023288969279991256\n",
      "Average test loss: 0.004009976206140385\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02311738725999991\n",
      "Average test loss: 0.0038930312821434604\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02304572443995211\n",
      "Average test loss: 0.003570000217606624\n",
      "Epoch 69/300\n",
      "Average training loss: 0.023014342044790585\n",
      "Average test loss: 0.0038313071680151755\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023064321908685896\n",
      "Average test loss: 0.004651985632876555\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02290329784154892\n",
      "Average test loss: 0.0036877348890735043\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022900431541932952\n",
      "Average test loss: 0.0035518620217012036\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022843674295478397\n",
      "Average test loss: 0.003578964840620756\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0228649935838249\n",
      "Average test loss: 0.0036884788912203577\n",
      "Epoch 75/300\n",
      "Average training loss: 0.022815906117359796\n",
      "Average test loss: 0.003574261903762817\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02295272087223\n",
      "Average test loss: 0.0035696962314347427\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02278777852157752\n",
      "Average test loss: 0.003582015332662397\n",
      "Epoch 78/300\n",
      "Average training loss: 0.022807734494407973\n",
      "Average test loss: 0.0034992571322040424\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022757006612088945\n",
      "Average test loss: 0.0035579836894240645\n",
      "Epoch 80/300\n",
      "Average training loss: 0.022654530121220484\n",
      "Average test loss: 0.0036577563828064335\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022630662437942293\n",
      "Average test loss: 0.003541629614515437\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022643077005942664\n",
      "Average test loss: 0.003568853098899126\n",
      "Epoch 83/300\n",
      "Average training loss: 0.022748420991831355\n",
      "Average test loss: 0.0034932900127023457\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02252647356854545\n",
      "Average test loss: 0.0035673561903337638\n",
      "Epoch 85/300\n",
      "Average training loss: 0.022556643802258705\n",
      "Average test loss: 0.0036062054460247357\n",
      "Epoch 86/300\n",
      "Average training loss: 0.022543001502752305\n",
      "Average test loss: 0.0037560600936412812\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02251388534075684\n",
      "Average test loss: 0.003594574749055836\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022504131972789766\n",
      "Average test loss: 0.003887211184948683\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02246665835214986\n",
      "Average test loss: 0.0036696769297122956\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0224484716852506\n",
      "Average test loss: 0.003742390424013138\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0224262583239211\n",
      "Average test loss: 0.003566104449538721\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022393195047974587\n",
      "Average test loss: 0.00363167941570282\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022512602239847183\n",
      "Average test loss: 0.003643331218510866\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022363840570052466\n",
      "Average test loss: 0.0035659765675663947\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02234509036441644\n",
      "Average test loss: 0.003481171566165156\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02230791886978679\n",
      "Average test loss: 0.003695989060215652\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02232543551756276\n",
      "Average test loss: 0.0036348608765337203\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02225866638786263\n",
      "Average test loss: 0.003448859898787406\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022264926973316404\n",
      "Average test loss: 0.003549663748592138\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022258489287561842\n",
      "Average test loss: 0.0035979733876883983\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022382627879579862\n",
      "Average test loss: 0.003789863870789607\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022239178326394823\n",
      "Average test loss: 0.003525886341308554\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02220550449523661\n",
      "Average test loss: 0.003490068707201216\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022228972415129344\n",
      "Average test loss: 0.0034769671586238676\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02211211412317223\n",
      "Average test loss: 0.0034753228877153664\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02217547121975157\n",
      "Average test loss: 0.003704760833332936\n",
      "Epoch 107/300\n",
      "Average training loss: 0.022152661752369667\n",
      "Average test loss: 0.003634462690601746\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022136229837934177\n",
      "Average test loss: 0.0034732227697968484\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02207755905555354\n",
      "Average test loss: 0.0034791885773754784\n",
      "Epoch 110/300\n",
      "Average training loss: 0.022113277971744536\n",
      "Average test loss: 0.003539117386771573\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022065331695808304\n",
      "Average test loss: 0.003530998250986967\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02200976910524898\n",
      "Average test loss: 0.0035845524755616985\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02202166618572341\n",
      "Average test loss: 0.0036428009739352596\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02208860047823853\n",
      "Average test loss: 0.0035293011930253772\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022016102019283508\n",
      "Average test loss: 0.0035237165238294335\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021966314180029763\n",
      "Average test loss: 0.0035067772002269825\n",
      "Epoch 117/300\n",
      "Average training loss: 0.021974382533795304\n",
      "Average test loss: 0.0036225273371156717\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02197509123881658\n",
      "Average test loss: 0.0035407033171504735\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02192111204399003\n",
      "Average test loss: 0.0036232197179148595\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021947749212384224\n",
      "Average test loss: 0.00352200007190307\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021864599727922016\n",
      "Average test loss: 0.0036927244882616734\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021889923105637234\n",
      "Average test loss: 0.0038500404076443778\n",
      "Epoch 123/300\n",
      "Average training loss: 0.021891210855709182\n",
      "Average test loss: 0.003805125427742799\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021823088576396307\n",
      "Average test loss: 0.0035352067825280956\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02188383782406648\n",
      "Average test loss: 0.003466081517024173\n",
      "Epoch 126/300\n",
      "Average training loss: 0.021820032368103662\n",
      "Average test loss: 0.0034686656105849477\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02184209841489792\n",
      "Average test loss: 0.003608699562235011\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021756255560451083\n",
      "Average test loss: 0.003576770623731944\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021769855769144163\n",
      "Average test loss: 0.0034627006202936172\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02181167718105846\n",
      "Average test loss: 0.003500109093470706\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021834117157591714\n",
      "Average test loss: 0.0037838765825662347\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02175272528992759\n",
      "Average test loss: 0.0036740500072224274\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02178118698630068\n",
      "Average test loss: 0.0037916271413366\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02170684652030468\n",
      "Average test loss: 0.0035341973944256703\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02172694319486618\n",
      "Average test loss: 0.0036661923664311567\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02168517140381866\n",
      "Average test loss: 0.003931938482655419\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021685789881481066\n",
      "Average test loss: 0.0034827190780391294\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021699128364523253\n",
      "Average test loss: 0.0035329088249968156\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021661491821209588\n",
      "Average test loss: 0.003677159112567703\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021661515225966772\n",
      "Average test loss: 0.004266942081765996\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021778771630591817\n",
      "Average test loss: 0.0035914570949971674\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021611203629109593\n",
      "Average test loss: 0.0038927867424984773\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02160807018313143\n",
      "Average test loss: 0.0035283905151817533\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02158284596933259\n",
      "Average test loss: 0.003415742280168666\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021578804067439502\n",
      "Average test loss: 0.0035780025236308573\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02167951433526145\n",
      "Average test loss: 0.003489141792472866\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02152322358555264\n",
      "Average test loss: 0.0035221956159091658\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021569956524504556\n",
      "Average test loss: 0.004292789654599296\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02157968310183949\n",
      "Average test loss: 0.0036742461609343686\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0215453638980786\n",
      "Average test loss: 0.003561700049580799\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02157753335767322\n",
      "Average test loss: 0.0036151128843840627\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021480243803726304\n",
      "Average test loss: 0.003494776506287356\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021523245924048955\n",
      "Average test loss: 0.0035226694035033384\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02152653180890613\n",
      "Average test loss: 0.003772683207359579\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02148098737001419\n",
      "Average test loss: 0.003543409726478987\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021417587917712\n",
      "Average test loss: 0.003849975897827082\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02149458838502566\n",
      "Average test loss: 0.0035922808121475907\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0213979463312361\n",
      "Average test loss: 0.0037619777367346816\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021568799283769396\n",
      "Average test loss: 0.0034673870446988277\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021475818956891695\n",
      "Average test loss: 0.0036158214627454678\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02146952121787601\n",
      "Average test loss: 0.0036125594601035117\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021328152183029386\n",
      "Average test loss: 0.003417362538683746\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021398242619302538\n",
      "Average test loss: 0.003499503296903438\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02142394065691365\n",
      "Average test loss: 0.0035047558955848216\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02138594798081451\n",
      "Average test loss: 0.0036284372133927214\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02135460427072313\n",
      "Average test loss: 0.0036327775832679537\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021374065559771324\n",
      "Average test loss: 0.003535955074139767\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02136071680651771\n",
      "Average test loss: 0.0036087096730868023\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021347034634815323\n",
      "Average test loss: 0.0035250190974523625\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02137205779386891\n",
      "Average test loss: 0.0035481043346226216\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02134591780271795\n",
      "Average test loss: 0.0034527404541149736\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02129207348326842\n",
      "Average test loss: 0.0036880220940543546\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021328676701419882\n",
      "Average test loss: 0.0036984179371760953\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021283146179384656\n",
      "Average test loss: 0.004041298064920637\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021376057884759374\n",
      "Average test loss: 0.003796862739345266\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021214472552140554\n",
      "Average test loss: 0.0035677578774177365\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021243685742219288\n",
      "Average test loss: 0.003681457080360916\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02125136992832025\n",
      "Average test loss: 0.0035640739316327706\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021269257941179804\n",
      "Average test loss: 0.0035682735016776455\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02126096857753065\n",
      "Average test loss: 0.0035167932152334186\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02125877547926373\n",
      "Average test loss: 0.003611608773883846\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02124889424774382\n",
      "Average test loss: 0.003533733376198345\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021242990157670446\n",
      "Average test loss: 0.0035494379463295142\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021195284043749173\n",
      "Average test loss: 0.003551672312327557\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021207530033257274\n",
      "Average test loss: 0.003601391626521945\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02127711733844545\n",
      "Average test loss: 0.004035047365559472\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02120379491812653\n",
      "Average test loss: 0.003501847242522571\n",
      "Epoch 188/300\n",
      "Average training loss: 0.021152474944790205\n",
      "Average test loss: 0.003933179644660817\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021179009502132733\n",
      "Average test loss: 0.0035483451067573495\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02119672011335691\n",
      "Average test loss: 0.0034858886243568526\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021102304187085894\n",
      "Average test loss: 0.003745518348697159\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021162268140249782\n",
      "Average test loss: 0.0034452447642882664\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021130736213591364\n",
      "Average test loss: 0.003631778493523598\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021160804750190842\n",
      "Average test loss: 0.0035231759515073563\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021126454123192362\n",
      "Average test loss: 0.0037630772503713765\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02112570962972111\n",
      "Average test loss: 0.0036938586632410684\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021142829173141055\n",
      "Average test loss: 0.003671065533326732\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021143181673354574\n",
      "Average test loss: 0.003941194139007065\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021167991482549244\n",
      "Average test loss: 0.0036838924205965465\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02104404445985953\n",
      "Average test loss: 0.0034461909383535386\n",
      "Epoch 201/300\n",
      "Average training loss: 0.021047779753804206\n",
      "Average test loss: 0.003509816461139255\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02111328161921766\n",
      "Average test loss: 0.0034520503452254666\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021074942863649793\n",
      "Average test loss: 0.003533514052008589\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02103421324160364\n",
      "Average test loss: 0.003512213484280639\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021032723933458328\n",
      "Average test loss: 0.0035323577146563266\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021234299225939644\n",
      "Average test loss: 0.0036420779166122276\n",
      "Epoch 207/300\n",
      "Average training loss: 0.021042518268028894\n",
      "Average test loss: 0.003460691260587838\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020969930110706223\n",
      "Average test loss: 0.0039445770960301165\n",
      "Epoch 209/300\n",
      "Average training loss: 0.021002966124150486\n",
      "Average test loss: 0.003539988160133362\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021011607491307788\n",
      "Average test loss: 0.0035242680909319058\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021118328683906132\n",
      "Average test loss: 0.0036473182156268095\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02098961286743482\n",
      "Average test loss: 0.003741766805657082\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02101447773973147\n",
      "Average test loss: 0.0035523024114469687\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02096040006975333\n",
      "Average test loss: 0.003565156607164277\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020955027621653344\n",
      "Average test loss: 0.003913747268211511\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02109524277349313\n",
      "Average test loss: 0.003907151877052254\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020978981122374535\n",
      "Average test loss: 0.003792684763049086\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0209486456927326\n",
      "Average test loss: 0.003550946187848846\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02096906609336535\n",
      "Average test loss: 0.0036328424772040712\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020913846049043866\n",
      "Average test loss: 0.003465644262317154\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02092778654727671\n",
      "Average test loss: 0.0034265101262264783\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020920072267452877\n",
      "Average test loss: 0.004277814494652881\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02088883854283227\n",
      "Average test loss: 0.00360267786660956\n",
      "Epoch 224/300\n",
      "Average training loss: 0.020935007689727676\n",
      "Average test loss: 0.0035904275240997473\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02089324763417244\n",
      "Average test loss: 0.0034181397499309647\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020898530671993893\n",
      "Average test loss: 0.00353329206423627\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02088505858514044\n",
      "Average test loss: 0.003785262929689553\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0209608753754033\n",
      "Average test loss: 0.00356614300629331\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020926184342967138\n",
      "Average test loss: 0.003648772093777855\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020886385260356796\n",
      "Average test loss: 0.003528683158879479\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020909923603137336\n",
      "Average test loss: 0.0035386532627873953\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02087212567196952\n",
      "Average test loss: 0.003462988505967789\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020837675946454207\n",
      "Average test loss: 0.003673581212759018\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020913743616806136\n",
      "Average test loss: 0.0035153550232450165\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020840849783685472\n",
      "Average test loss: 0.003506727853996886\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020864939080344307\n",
      "Average test loss: 0.0036273999541170067\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020833430611424977\n",
      "Average test loss: 0.0035775954518467187\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02077525986234347\n",
      "Average test loss: 0.0038257307455771498\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020829967768655883\n",
      "Average test loss: 0.0037330293241474364\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020857739897237885\n",
      "Average test loss: 0.0034912033588108088\n",
      "Epoch 241/300\n",
      "Average training loss: 0.020805512636899948\n",
      "Average test loss: 0.0035486763819224305\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020847373970680767\n",
      "Average test loss: 0.003600444580945704\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020768502778477138\n",
      "Average test loss: 0.003443546195824941\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020860390133327907\n",
      "Average test loss: 0.0037185043814695544\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020791786349482005\n",
      "Average test loss: 0.00399948930947317\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020810959041118623\n",
      "Average test loss: 0.0036018375154170724\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020764369868569905\n",
      "Average test loss: 0.003729278156740798\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020795815468662316\n",
      "Average test loss: 0.0035270526121473976\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02079939089053207\n",
      "Average test loss: 0.003930381380849415\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020786823050843346\n",
      "Average test loss: 0.0034880059535304705\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020746529027819635\n",
      "Average test loss: 0.0035916367709222766\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020735281576712925\n",
      "Average test loss: 0.003502669132418103\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020668705973360273\n",
      "Average test loss: 0.0037700237673189904\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020762358001536793\n",
      "Average test loss: 0.003568862025108602\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020747252442770533\n",
      "Average test loss: 0.003666795396970378\n",
      "Epoch 256/300\n",
      "Average training loss: 0.020737529471516608\n",
      "Average test loss: 0.0036856501845435965\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020764660772350098\n",
      "Average test loss: 0.003492238376289606\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020729365410076247\n",
      "Average test loss: 0.0036866485768308243\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020764747651914755\n",
      "Average test loss: 0.0037942725328935518\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02065225429667367\n",
      "Average test loss: 0.0034733450673520563\n",
      "Epoch 261/300\n",
      "Average training loss: 0.020730327528383996\n",
      "Average test loss: 0.0037688749647802776\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020659193178017933\n",
      "Average test loss: 0.0038036137256357407\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02062739280694061\n",
      "Average test loss: 0.0037685559002889526\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02066549602482054\n",
      "Average test loss: 0.00362210743708743\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020671930061446298\n",
      "Average test loss: 0.0035595101012537876\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020702411807245677\n",
      "Average test loss: 0.0035852308641705246\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020630947013696034\n",
      "Average test loss: 0.003580797548716267\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020705647961960897\n",
      "Average test loss: 0.0035757019985467194\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020670218156443702\n",
      "Average test loss: 0.0036762042937593326\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020619780821932686\n",
      "Average test loss: 0.0037300468809488745\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020651273247268464\n",
      "Average test loss: 0.0036069827839318247\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020609726303153567\n",
      "Average test loss: 0.003680372835447391\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020655103908644782\n",
      "Average test loss: 0.0038834104281332757\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020630953169531292\n",
      "Average test loss: 0.0034444328072584336\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02064231238265832\n",
      "Average test loss: 0.0035173636228880947\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020692245880762735\n",
      "Average test loss: 0.0037963896606945328\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020627973177366785\n",
      "Average test loss: 0.0036191094695693917\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020589296834336388\n",
      "Average test loss: 0.00372681740567916\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020590535995033053\n",
      "Average test loss: 0.0035497941936676702\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020609731425013808\n",
      "Average test loss: 0.00402101385841767\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020588647339079114\n",
      "Average test loss: 0.0035294224808199537\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02060050742659304\n",
      "Average test loss: 0.003581061826397975\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02056369623541832\n",
      "Average test loss: 0.0039158741006006794\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020549041522873773\n",
      "Average test loss: 0.0037603447985731894\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020561508726742533\n",
      "Average test loss: 0.003822054092047943\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020597412687208917\n",
      "Average test loss: 0.0035260938447382716\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020552710513273877\n",
      "Average test loss: 0.00430688340548012\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02057302944527732\n",
      "Average test loss: 0.0035043685854309137\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02052416566179858\n",
      "Average test loss: 0.003621402426726288\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02053304647571511\n",
      "Average test loss: 0.0034956620554957127\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020578018292784692\n",
      "Average test loss: 0.0035221098822851974\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020527831474939983\n",
      "Average test loss: 0.004127197448578146\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020564791902899743\n",
      "Average test loss: 0.00359553644226657\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02055486022763782\n",
      "Average test loss: 0.0036047734338790177\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0204958060781161\n",
      "Average test loss: 0.003857371765913235\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020532213201953305\n",
      "Average test loss: 0.0037913125908623137\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02056263664199246\n",
      "Average test loss: 0.0036441018467562067\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020506647010644277\n",
      "Average test loss: 0.0036830480390538773\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020535290183292493\n",
      "Average test loss: 0.0036288147976415025\n",
      "Epoch 300/300\n",
      "Average training loss: 0.020560394868254662\n",
      "Average test loss: 0.003496439991518855\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11470877823895878\n",
      "Average test loss: 0.007504781663417816\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04448096913099289\n",
      "Average test loss: 0.007810990280989144\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0412246749997139\n",
      "Average test loss: 0.005271103767471181\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03796713860664103\n",
      "Average test loss: 0.005129515435960558\n",
      "Epoch 5/300\n",
      "Average training loss: 0.035478922343916364\n",
      "Average test loss: 0.0052155485182172725\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03340678684910139\n",
      "Average test loss: 0.004399964051942031\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03168302913175689\n",
      "Average test loss: 0.00464893444813788\n",
      "Epoch 8/300\n",
      "Average training loss: 0.029553853200541602\n",
      "Average test loss: 0.005573410905483696\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02839360496401787\n",
      "Average test loss: 0.0041285482729888625\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02696997720334265\n",
      "Average test loss: 0.004544187831381957\n",
      "Epoch 11/300\n",
      "Average training loss: 0.026229980402522616\n",
      "Average test loss: 0.004580607380304072\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0251132464143965\n",
      "Average test loss: 0.005183926230503453\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024901760977175502\n",
      "Average test loss: 0.0032610567543241713\n",
      "Epoch 14/300\n",
      "Average training loss: 0.024321162379450267\n",
      "Average test loss: 0.0033107753451913596\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023931301194760536\n",
      "Average test loss: 0.010170552966495355\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023261454948120645\n",
      "Average test loss: 0.004430832317305936\n",
      "Epoch 17/300\n",
      "Average training loss: 0.022844213003913563\n",
      "Average test loss: 0.004626138456579712\n",
      "Epoch 18/300\n",
      "Average training loss: 0.022534942910075188\n",
      "Average test loss: 0.0032596569003330335\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0222357832259602\n",
      "Average test loss: 0.0030107652822302447\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021786354740460714\n",
      "Average test loss: 0.0029000240692661867\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021676422014832497\n",
      "Average test loss: 0.004183693546387884\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021706330299377442\n",
      "Average test loss: 0.0029979863179226715\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021246935991777315\n",
      "Average test loss: 0.002958533564582467\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020950353802906142\n",
      "Average test loss: 0.00296253312813739\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02087759798599614\n",
      "Average test loss: 0.002838698206883338\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020784496848781902\n",
      "Average test loss: 0.0035528080090880395\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020548407793045044\n",
      "Average test loss: 0.00283970214260949\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02027334426012304\n",
      "Average test loss: 0.0028422674499452116\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020562138511074912\n",
      "Average test loss: 0.003406084701832798\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02019086105293698\n",
      "Average test loss: 0.0035793216596874925\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01995520007610321\n",
      "Average test loss: 0.0026100968826148247\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02001159699923462\n",
      "Average test loss: 0.002726496642662419\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019739352992839285\n",
      "Average test loss: 0.002581671082207726\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019684987652632924\n",
      "Average test loss: 0.003176048196024365\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0195968655033244\n",
      "Average test loss: 0.0027206895243790416\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019542429579628838\n",
      "Average test loss: 0.002791027785589298\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019309551974137622\n",
      "Average test loss: 0.0028654107944005064\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01928222605586052\n",
      "Average test loss: 0.00263559566044973\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01924484943598509\n",
      "Average test loss: 0.002480044338852167\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01915556191653013\n",
      "Average test loss: 0.002475710564189487\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019188529595732688\n",
      "Average test loss: 0.0025814757512675392\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019037387556499905\n",
      "Average test loss: 0.0035247890609833928\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018956136468384\n",
      "Average test loss: 0.002464637360846003\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0189781360005339\n",
      "Average test loss: 0.0025360013391408655\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018869568910863666\n",
      "Average test loss: 0.002693492042935557\n",
      "Epoch 46/300\n",
      "Average training loss: 0.018847215129269495\n",
      "Average test loss: 0.0025174240447166894\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01882756638030211\n",
      "Average test loss: 0.0025588480482498805\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01871003480255604\n",
      "Average test loss: 0.002460755216371682\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0187265138907565\n",
      "Average test loss: 0.002493431002108587\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01864777226249377\n",
      "Average test loss: 0.0027137107338963285\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01852782972653707\n",
      "Average test loss: 0.002485675685521629\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018595095058282215\n",
      "Average test loss: 0.00248878430802789\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018443663282526866\n",
      "Average test loss: 0.002453215118911531\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018382094994187356\n",
      "Average test loss: 0.0024854430541810064\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018419648581080965\n",
      "Average test loss: 0.0025891665698339543\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018451855512956777\n",
      "Average test loss: 0.002569964428121845\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01846487485534615\n",
      "Average test loss: 0.0029739796119845575\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01831797074112627\n",
      "Average test loss: 0.002461369312368333\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018232401908271842\n",
      "Average test loss: 0.0026874417472216817\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01822517790397008\n",
      "Average test loss: 0.0024820413569816287\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018229259591963556\n",
      "Average test loss: 0.002564160783878631\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018168461007376513\n",
      "Average test loss: 0.0024970464925799106\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01817034765250153\n",
      "Average test loss: 0.0023807147002468504\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018141476369566388\n",
      "Average test loss: 0.0025975831488354337\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018107293711768255\n",
      "Average test loss: 0.0025002592626131243\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01805217445972893\n",
      "Average test loss: 0.0024126100511186652\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018067450303170415\n",
      "Average test loss: 0.0024819324471884304\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017985025980406338\n",
      "Average test loss: 0.0024466738649126557\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017961084014839597\n",
      "Average test loss: 0.0024200795342524846\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017979501876566144\n",
      "Average test loss: 0.0024264737990581328\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01791431962947051\n",
      "Average test loss: 0.002478792280993528\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017906339998046555\n",
      "Average test loss: 0.0025176141344838672\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017835992431475056\n",
      "Average test loss: 0.002636801829975512\n",
      "Epoch 74/300\n",
      "Average training loss: 0.017844045302934118\n",
      "Average test loss: 0.002462469760949413\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01791149632467164\n",
      "Average test loss: 0.002476816498984893\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017752340414457852\n",
      "Average test loss: 0.002426787723890609\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017757088020443915\n",
      "Average test loss: 0.0025714027689148983\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017754959278636507\n",
      "Average test loss: 0.002539074852027827\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017715826782915327\n",
      "Average test loss: 0.0024893576941556403\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01782122562163406\n",
      "Average test loss: 0.002462655848823488\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017665995656616158\n",
      "Average test loss: 0.002937007081798381\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017647271752357484\n",
      "Average test loss: 0.0023743673589908415\n",
      "Epoch 83/300\n",
      "Average training loss: 0.017602853317227628\n",
      "Average test loss: 0.0026067785501687063\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01766064769360754\n",
      "Average test loss: 0.002400975687222348\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017573460548288292\n",
      "Average test loss: 0.0025545315332710745\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017583754362331495\n",
      "Average test loss: 0.00452183397155669\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017533946081168122\n",
      "Average test loss: 0.002528361493928565\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017613493311736318\n",
      "Average test loss: 0.0023426576792779895\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01757039584716161\n",
      "Average test loss: 0.0026682173181325195\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01753124714559979\n",
      "Average test loss: 0.0035705997722430363\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017462622263365323\n",
      "Average test loss: 0.0024763575345277786\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01749757595608632\n",
      "Average test loss: 0.0023449066351685257\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017420971454017692\n",
      "Average test loss: 0.002484706100490358\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017417627667387327\n",
      "Average test loss: 0.0024956521273901064\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017430512147645155\n",
      "Average test loss: 0.0027433205646359257\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017379867784678937\n",
      "Average test loss: 0.0025167215228494673\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017322494217091138\n",
      "Average test loss: 0.0024324415381997825\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01740668642024199\n",
      "Average test loss: 0.0023518483168962928\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017334611834751237\n",
      "Average test loss: 0.002434895031878518\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017362193814582295\n",
      "Average test loss: 0.002310780459601018\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017294724992579884\n",
      "Average test loss: 0.0025076151993125677\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017283548249138726\n",
      "Average test loss: 0.002514932182410525\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01731115436222818\n",
      "Average test loss: 0.002467482047672901\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017260796647932796\n",
      "Average test loss: 0.0023438135577986637\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017284615529908075\n",
      "Average test loss: 0.0028454530348794326\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017263232284122042\n",
      "Average test loss: 0.002482450500751535\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01723728249967098\n",
      "Average test loss: 0.0024359463498824173\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01721355938580301\n",
      "Average test loss: 0.0023859083619382645\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017258963247140248\n",
      "Average test loss: 0.002405549329602056\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01720557468632857\n",
      "Average test loss: 0.002431937959873014\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017147100993328623\n",
      "Average test loss: 0.002315225251019001\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017120748216907185\n",
      "Average test loss: 0.002493518355819914\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01715674556295077\n",
      "Average test loss: 0.002464095972271429\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017142235724462402\n",
      "Average test loss: 0.009408174698965417\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017097830346888967\n",
      "Average test loss: 0.0025025136871263385\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017129957920975155\n",
      "Average test loss: 0.002374666885369354\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017096044234103627\n",
      "Average test loss: 0.0025007695389083692\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017050871385468378\n",
      "Average test loss: 0.002522458174472882\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017152496172322167\n",
      "Average test loss: 0.0025392791014164687\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017071724911530813\n",
      "Average test loss: 0.0023844581140826148\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017231866100596058\n",
      "Average test loss: 0.002513682674513095\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016954196617007256\n",
      "Average test loss: 0.002740809460170567\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016988473730782667\n",
      "Average test loss: 0.002354037336849918\n",
      "Epoch 124/300\n",
      "Average training loss: 0.016997848565379777\n",
      "Average test loss: 0.002296948440165983\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01696756209101942\n",
      "Average test loss: 0.0023390525336273844\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01698351989686489\n",
      "Average test loss: 0.0023672345308586957\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016947265598509047\n",
      "Average test loss: 0.0023434742571165164\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017000972809890907\n",
      "Average test loss: 0.0025099021838977933\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017044334467914368\n",
      "Average test loss: 0.00250293649526106\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016915655605494975\n",
      "Average test loss: 0.003191001910716295\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01691857159965568\n",
      "Average test loss: 0.002457304340476791\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016967466132508383\n",
      "Average test loss: 0.0024280082966304487\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016884974160128168\n",
      "Average test loss: 0.0024934304015090066\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016928443282842636\n",
      "Average test loss: 0.0023580496669229536\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016947155267000198\n",
      "Average test loss: 0.0025645385295566584\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016841333561473423\n",
      "Average test loss: 0.0027216573477619223\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01695214908487267\n",
      "Average test loss: 0.0023788147670113376\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01684552158166965\n",
      "Average test loss: 0.0023714046182317866\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016873153386016686\n",
      "Average test loss: 0.0024932451111574967\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016893742683033147\n",
      "Average test loss: 0.002379768182006147\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01682724702523814\n",
      "Average test loss: 0.0023312054133663575\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016790033237801656\n",
      "Average test loss: 0.002367161569082075\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016806672246919736\n",
      "Average test loss: 0.002490404339300262\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016791759201221997\n",
      "Average test loss: 0.0023495994872517055\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016810829559961955\n",
      "Average test loss: 0.0023676792970961993\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01678757214960125\n",
      "Average test loss: 0.00261402181080646\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016764514384998215\n",
      "Average test loss: 0.0023938176795426343\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01675980816202031\n",
      "Average test loss: 0.002489604866753022\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016811405388017495\n",
      "Average test loss: 0.0026162178330123424\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016708150640130044\n",
      "Average test loss: 0.002390467247201337\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016757783341738913\n",
      "Average test loss: 0.002403739757525424\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01668377769490083\n",
      "Average test loss: 0.00239321365321262\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016708177585568692\n",
      "Average test loss: 0.0026623563460177846\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01673037384947141\n",
      "Average test loss: 0.003403047768192159\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016714308412538636\n",
      "Average test loss: 0.0023021668665524986\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016703245215117932\n",
      "Average test loss: 0.0023066278968213332\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016677144828769894\n",
      "Average test loss: 0.002322966947323746\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01665826069894764\n",
      "Average test loss: 0.0024105542846437956\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016668518601192367\n",
      "Average test loss: 0.0024135129942248266\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01671954073260228\n",
      "Average test loss: 0.002448327489197254\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01666140725215276\n",
      "Average test loss: 0.0023476762829555404\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01663785063723723\n",
      "Average test loss: 0.0023324905128942596\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016589895773265098\n",
      "Average test loss: 0.002521568589740329\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016623630419373512\n",
      "Average test loss: 0.002386921343497104\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016664167042407724\n",
      "Average test loss: 0.0024046486927610305\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016574609560271104\n",
      "Average test loss: 0.002343268400679032\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01658476072963741\n",
      "Average test loss: 0.002432053481849531\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016598368086748652\n",
      "Average test loss: 0.0023613411854538652\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016589987206790183\n",
      "Average test loss: 0.002608124162380894\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016558635168605382\n",
      "Average test loss: 0.002457240333987607\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016559387192130088\n",
      "Average test loss: 0.002357032468956378\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016547859226663908\n",
      "Average test loss: 0.0023684439005123245\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016554518663220934\n",
      "Average test loss: 0.002324742671309246\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016562318426039484\n",
      "Average test loss: 0.002368126482392351\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016518698091308275\n",
      "Average test loss: 0.0024400375675823955\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016528542444109916\n",
      "Average test loss: 0.00238363211746845\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016498074636691146\n",
      "Average test loss: 0.00234244371453921\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01650304166889853\n",
      "Average test loss: 0.002484516597456402\n",
      "Epoch 179/300\n",
      "Average training loss: 0.016522153907352025\n",
      "Average test loss: 0.002536370082447926\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016530858024954794\n",
      "Average test loss: 0.00259512130336629\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01648690890769164\n",
      "Average test loss: 0.0024619837997274267\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016520153207911385\n",
      "Average test loss: 0.002480332024809387\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016468179224265946\n",
      "Average test loss: 0.0023292961033682027\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01648832710004515\n",
      "Average test loss: 0.0023428657806168\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016469634705119664\n",
      "Average test loss: 0.009554915506806639\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016428406902485423\n",
      "Average test loss: 0.0023686418177353012\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01640746941914161\n",
      "Average test loss: 0.0023043093955558208\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01648187250395616\n",
      "Average test loss: 0.0027324030221336417\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016443557896547847\n",
      "Average test loss: 0.0023644581921398638\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016418453954988057\n",
      "Average test loss: 0.002363692245963547\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016414912126130526\n",
      "Average test loss: 0.0024582563071615165\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016469359755516052\n",
      "Average test loss: 0.0025819905618619586\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01642096678084797\n",
      "Average test loss: 0.0023614684628943602\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01641394782397482\n",
      "Average test loss: 0.0023759520161483025\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01640639577474859\n",
      "Average test loss: 0.002452430307451222\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016421318107181126\n",
      "Average test loss: 0.0025320416409522293\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016388863157067033\n",
      "Average test loss: 0.002377818283935388\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016348766844305727\n",
      "Average test loss: 0.002374352040183213\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016352048136293887\n",
      "Average test loss: 0.007734087607926793\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016346510107318562\n",
      "Average test loss: 0.0024955975882088146\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01643176662011279\n",
      "Average test loss: 0.002503854589433306\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01638583487437831\n",
      "Average test loss: 0.0023517526135676437\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016315495141678387\n",
      "Average test loss: 0.0022786234468221665\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01635992329319318\n",
      "Average test loss: 0.0025221061930060388\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01641663688669602\n",
      "Average test loss: 0.0024094357983105714\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016338832926419047\n",
      "Average test loss: 0.0023334872418393693\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01642657801343335\n",
      "Average test loss: 0.003487954256228275\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01628404540154669\n",
      "Average test loss: 0.0024244698439207344\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016285929236147138\n",
      "Average test loss: 0.0023768631058434645\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016359051927924157\n",
      "Average test loss: 0.0023932981722884707\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016251143525044122\n",
      "Average test loss: 0.002473240725696087\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016269108706878293\n",
      "Average test loss: 0.0023401260303540362\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01630126553442743\n",
      "Average test loss: 0.002372189754206273\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01631097208129035\n",
      "Average test loss: 0.003430706234027942\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01626741397049692\n",
      "Average test loss: 0.0023299443622430164\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016277760578526392\n",
      "Average test loss: 0.002412130285675327\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016271094800697432\n",
      "Average test loss: 0.0023926880507626467\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01625555139117771\n",
      "Average test loss: 0.002335282590550681\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01627392247815927\n",
      "Average test loss: 0.0025009361563457385\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016244490722815197\n",
      "Average test loss: 0.0023611991194387276\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016261958270437187\n",
      "Average test loss: 0.002577272616326809\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016274868497418032\n",
      "Average test loss: 0.002555939928938945\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01619556506474813\n",
      "Average test loss: 0.002410506813062562\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016229801696207788\n",
      "Average test loss: 0.0025278127243121467\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016202286013298563\n",
      "Average test loss: 0.0024136791597637866\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01622147408127785\n",
      "Average test loss: 0.002391803457091252\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016264395164118872\n",
      "Average test loss: 0.0022882322842876115\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01625361984057559\n",
      "Average test loss: 0.002408174715315302\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016201018527150154\n",
      "Average test loss: 0.0024229547538691097\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016194478667444653\n",
      "Average test loss: 0.00254957506329649\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01619669811262025\n",
      "Average test loss: 0.0023234345481420557\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01621087998151779\n",
      "Average test loss: 0.002314883674805363\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016196396704349254\n",
      "Average test loss: 0.0023814362584509783\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016196984872221945\n",
      "Average test loss: 0.0023814890030771493\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01614562943412198\n",
      "Average test loss: 0.002354407423693273\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016140371557739046\n",
      "Average test loss: 0.0024232947731183635\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01618462275548114\n",
      "Average test loss: 0.002409219260430998\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016180431498421563\n",
      "Average test loss: 0.0038223759225673145\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016143568608495924\n",
      "Average test loss: 0.0025552118012888564\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016135746119750872\n",
      "Average test loss: 0.0025146499742857284\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01612585444582833\n",
      "Average test loss: 0.0023056782502681016\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016156214389536117\n",
      "Average test loss: 0.002586116741514868\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016152658321791226\n",
      "Average test loss: 0.0023469247294382917\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01615140873193741\n",
      "Average test loss: 0.0024372400167501636\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016151162261764208\n",
      "Average test loss: 0.0023613161754070058\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016132927690943082\n",
      "Average test loss: 0.0024559844254205625\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016160522228313816\n",
      "Average test loss: 0.006296575876987643\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01610195457107491\n",
      "Average test loss: 0.0027482412567155228\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016090595283442075\n",
      "Average test loss: 0.002426111277089351\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016148009546928934\n",
      "Average test loss: 0.002760792535315785\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0160602769644724\n",
      "Average test loss: 0.002353192848670814\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016099025406771238\n",
      "Average test loss: 0.002406369738901655\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016093800850212572\n",
      "Average test loss: 0.0025534365740087295\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016111817563573518\n",
      "Average test loss: 0.002325487846715583\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016098760273721483\n",
      "Average test loss: 0.002386688381743928\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016048003044393327\n",
      "Average test loss: 0.002358408020084931\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016080407257709237\n",
      "Average test loss: 0.0023242018630521164\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016135076955788665\n",
      "Average test loss: 0.0024207674603288374\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01603041965597206\n",
      "Average test loss: 0.0023710578641750746\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01606158760521147\n",
      "Average test loss: 0.0024177862708974216\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016051429336269695\n",
      "Average test loss: 0.0024277198678917355\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016053723555472164\n",
      "Average test loss: 0.002411393186284436\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016054658924539885\n",
      "Average test loss: 0.0023125530800057782\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0160258892443445\n",
      "Average test loss: 0.002379172102962103\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016052667594618267\n",
      "Average test loss: 0.002470083586561183\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016027701713972622\n",
      "Average test loss: 0.0023886017062597804\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016026223906212382\n",
      "Average test loss: 0.0024445092853986553\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01605997679548131\n",
      "Average test loss: 0.0024586484875116082\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016025248262617323\n",
      "Average test loss: 0.0024434619401064186\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016023844334814283\n",
      "Average test loss: 0.002381933790217671\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0160534905112452\n",
      "Average test loss: 0.002409320927328534\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015976276527676317\n",
      "Average test loss: 0.002416214993740949\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016010636079642506\n",
      "Average test loss: 0.0026937596241219178\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016036044450269803\n",
      "Average test loss: 0.0024081002357933257\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016017387193110254\n",
      "Average test loss: 0.0023484089016500447\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015983721911079354\n",
      "Average test loss: 0.002624803776024944\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015992662071353858\n",
      "Average test loss: 0.002450352080787222\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015978289230002298\n",
      "Average test loss: 0.002517012200007836\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015989089597430495\n",
      "Average test loss: 0.0025807206610010728\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015944909329215686\n",
      "Average test loss: 0.0023812493874380987\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016016147813035383\n",
      "Average test loss: 0.0023552394724554488\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015958205882045957\n",
      "Average test loss: 0.00245321017358866\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01598862962093618\n",
      "Average test loss: 0.002627268655018674\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015963727846741677\n",
      "Average test loss: 0.002409926414696707\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015980163573390906\n",
      "Average test loss: 0.0023479475072688528\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015972695776157907\n",
      "Average test loss: 0.0023716965357048645\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015949539182914627\n",
      "Average test loss: 0.0023254241083438196\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015984582454793984\n",
      "Average test loss: 0.002447706740970413\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015942705001268122\n",
      "Average test loss: 0.002365234037033386\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015971678947408995\n",
      "Average test loss: 0.0023826711513102055\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01595783746904797\n",
      "Average test loss: 0.00233524132375088\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01590014154381222\n",
      "Average test loss: 0.002336027384011282\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015898848777843845\n",
      "Average test loss: 0.0024276625216007233\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01593927933110131\n",
      "Average test loss: 0.0025606142530838647\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01592725253933006\n",
      "Average test loss: 0.0025945319541626505\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015936955789724986\n",
      "Average test loss: 0.002484308932183517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015905954700377253\n",
      "Average test loss: 0.0023378491202990215\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015930522602465417\n",
      "Average test loss: 0.0023778168873654472\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015931086268689897\n",
      "Average test loss: 0.002613862517186337\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015893922236230637\n",
      "Average test loss: 0.002320048244256112\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1071302157441775\n",
      "Average test loss: 0.009745732439888848\n",
      "Epoch 2/300\n",
      "Average training loss: 0.039308691359228556\n",
      "Average test loss: 0.006704947706725862\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03566585351692306\n",
      "Average test loss: 0.010748844934006532\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03233618626660771\n",
      "Average test loss: 0.0042077083492444625\n",
      "Epoch 5/300\n",
      "Average training loss: 0.030374147140317494\n",
      "Average test loss: 0.004145114944626887\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02792346021698581\n",
      "Average test loss: 0.004121411948982212\n",
      "Epoch 7/300\n",
      "Average training loss: 0.027094276931550768\n",
      "Average test loss: 0.003987558545130822\n",
      "Epoch 8/300\n",
      "Average training loss: 0.024849570219715435\n",
      "Average test loss: 0.003742043491452932\n",
      "Epoch 9/300\n",
      "Average training loss: 0.024087393255697358\n",
      "Average test loss: 0.00317244855976767\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022906532179978158\n",
      "Average test loss: 0.0030010260610530775\n",
      "Epoch 11/300\n",
      "Average training loss: 0.022237923517823218\n",
      "Average test loss: 0.002744268143342601\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02182437282303969\n",
      "Average test loss: 0.006697837532808383\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021116981201701693\n",
      "Average test loss: 0.002711924463717474\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0208308422267437\n",
      "Average test loss: 0.003490674198915561\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02007306609219975\n",
      "Average test loss: 0.0030129312763197556\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01950834279590183\n",
      "Average test loss: 0.0023265829759960372\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019254513131247625\n",
      "Average test loss: 0.0024955289975429574\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01908980642259121\n",
      "Average test loss: 0.002197200153643886\n",
      "Epoch 19/300\n",
      "Average training loss: 0.019144921359088685\n",
      "Average test loss: 0.0022720033780982097\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01844901427957747\n",
      "Average test loss: 0.002343654663612445\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018233519890242154\n",
      "Average test loss: 0.0023317602372003926\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01807851351963149\n",
      "Average test loss: 0.0025815728362649677\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017873042534622883\n",
      "Average test loss: 0.002216962123910586\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017589426947964563\n",
      "Average test loss: 0.005936451882537868\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017549516025516722\n",
      "Average test loss: 0.0021408717250451445\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01761168730755647\n",
      "Average test loss: 0.0022100425414327118\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017194330715470843\n",
      "Average test loss: 0.0021512229964137076\n",
      "Epoch 28/300\n",
      "Average training loss: 0.017169239746199712\n",
      "Average test loss: 0.002492684490357836\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016896283876564767\n",
      "Average test loss: 0.002221686717329754\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016961578140656155\n",
      "Average test loss: 0.002474267824863394\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016860591500997543\n",
      "Average test loss: 0.0020034529245975944\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016478316956096226\n",
      "Average test loss: 0.002117200738957359\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016519858697222337\n",
      "Average test loss: 0.00203697667322639\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016694882969061534\n",
      "Average test loss: 0.001982343930337164\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01644071928328938\n",
      "Average test loss: 0.0018998682600342566\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01623100507921643\n",
      "Average test loss: 0.0022007398044483527\n",
      "Epoch 37/300\n",
      "Average training loss: 0.016254420386420356\n",
      "Average test loss: 0.0019113054272408287\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01604767567747169\n",
      "Average test loss: 0.001965829946204192\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01610250077976121\n",
      "Average test loss: 0.0022491259846008486\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01610596813261509\n",
      "Average test loss: 0.0018761356864124537\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015927976955142285\n",
      "Average test loss: 0.0019966597011726764\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015915120247337555\n",
      "Average test loss: 0.0019403159009913603\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01576105895638466\n",
      "Average test loss: 0.002000886491810282\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015671896842618785\n",
      "Average test loss: 0.0019447628623909421\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01578020162714852\n",
      "Average test loss: 0.002081836446912752\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01587176249341832\n",
      "Average test loss: 0.0019897511938793792\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015502970870170328\n",
      "Average test loss: 0.0018402940168355901\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015661491044693523\n",
      "Average test loss: 0.001837461069226265\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015410484737820095\n",
      "Average test loss: 0.001884344830074244\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015561252923475372\n",
      "Average test loss: 0.00188943601720449\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015490915925966369\n",
      "Average test loss: 0.0018087052654268013\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01533954387737645\n",
      "Average test loss: 0.0017643371865981156\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015388850107789039\n",
      "Average test loss: 0.0017859622202813625\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015283293597400188\n",
      "Average test loss: 0.0018714563231915235\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015324463970131345\n",
      "Average test loss: 0.0020195034437088502\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01526981148786015\n",
      "Average test loss: 0.0018269850986285343\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015207341167661878\n",
      "Average test loss: 0.0017901054119898213\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015146412315467993\n",
      "Average test loss: 0.0019214255052308242\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015183508015341228\n",
      "Average test loss: 0.0017853439244338207\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015185834454579486\n",
      "Average test loss: 0.0019381595539549987\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01510327354984151\n",
      "Average test loss: 0.0018970974350555076\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015061616445581119\n",
      "Average test loss: 0.0023088114805933503\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015119184893038538\n",
      "Average test loss: 0.0018117465401689212\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015013586569991376\n",
      "Average test loss: 0.0018338625666995843\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014995311338868406\n",
      "Average test loss: 0.001743449825172623\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01496392825908131\n",
      "Average test loss: 0.0018126903962757852\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014879018371303876\n",
      "Average test loss: 0.0019483721003764206\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01496959090895123\n",
      "Average test loss: 0.001809606238371796\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015069102578692966\n",
      "Average test loss: 0.001805290057634314\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014818365575538742\n",
      "Average test loss: 0.001766588173289266\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014874806061387063\n",
      "Average test loss: 0.0017331943991076616\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014758043389353487\n",
      "Average test loss: 0.0017650238290015195\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014831652679377132\n",
      "Average test loss: 0.0018195699760690332\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014799499125944244\n",
      "Average test loss: 0.002296902509509689\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014830707559982935\n",
      "Average test loss: 0.0019720860018084445\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01470590302017\n",
      "Average test loss: 0.0017821740604316194\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014733024299144745\n",
      "Average test loss: 0.0019227652659432755\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014692830631302462\n",
      "Average test loss: 0.0019456828503559033\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01468169623033868\n",
      "Average test loss: 0.0018271076209429237\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014668749867214097\n",
      "Average test loss: 0.0020139269400388004\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014587050524850686\n",
      "Average test loss: 0.001840088721882138\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014627918787300586\n",
      "Average test loss: 0.0017173180013067193\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014554983529779646\n",
      "Average test loss: 0.0017806062545213434\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014581698841518826\n",
      "Average test loss: 0.0018511720562560691\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014544756918317742\n",
      "Average test loss: 0.0017304252203967836\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014559875098367533\n",
      "Average test loss: 0.001731766904393832\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014485474252452453\n",
      "Average test loss: 0.0018747747359383437\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014505611312058237\n",
      "Average test loss: 0.0017783940320627556\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014488119559983413\n",
      "Average test loss: 0.0018677064612921742\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014476670602129565\n",
      "Average test loss: 0.0020303961928519938\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014550723385479716\n",
      "Average test loss: 0.001774404198759132\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014396048746175236\n",
      "Average test loss: 0.0017828487233362265\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014387472319934104\n",
      "Average test loss: 0.0018331328100628323\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014407428067591455\n",
      "Average test loss: 0.0017380838699431883\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014451209232211113\n",
      "Average test loss: 0.002076680321660307\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014369608078565863\n",
      "Average test loss: 0.0020753798205405474\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014376390390925937\n",
      "Average test loss: 0.0019049923825595113\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014372174168212546\n",
      "Average test loss: 0.0017370849248642723\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014322480234007041\n",
      "Average test loss: 0.0018410077214034068\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014265970332754983\n",
      "Average test loss: 0.0017185964909278684\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014297082607116963\n",
      "Average test loss: 0.0017882123010026084\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014327286395761701\n",
      "Average test loss: 0.0018071050342793266\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014281844959490829\n",
      "Average test loss: 0.001769039203826752\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014234185943173038\n",
      "Average test loss: 0.0017373060606833961\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014258545501364602\n",
      "Average test loss: 0.001726974062414633\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014243490323424339\n",
      "Average test loss: 0.0018349077099313338\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014236860444976223\n",
      "Average test loss: 0.0016925771939050821\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014270482860505581\n",
      "Average test loss: 0.001739653419703245\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014208371796541743\n",
      "Average test loss: 0.0017658222276303503\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014141299323075347\n",
      "Average test loss: 0.0023791203391220835\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014157654833462503\n",
      "Average test loss: 0.001982461081403825\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014173462980323367\n",
      "Average test loss: 0.0019157697619456384\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014176653650071886\n",
      "Average test loss: 0.0017547213242699704\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0142038503314058\n",
      "Average test loss: 0.0017442959402170447\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01409659341060453\n",
      "Average test loss: 0.0017143649318152004\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014148196707169216\n",
      "Average test loss: 0.0018799231505642335\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014103943783375952\n",
      "Average test loss: 0.001841451961443656\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0140949129851328\n",
      "Average test loss: 0.0017605189596199328\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014061015017330647\n",
      "Average test loss: 0.0017319374043080543\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014086427420377731\n",
      "Average test loss: 0.0016714508135078682\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01405935138463974\n",
      "Average test loss: 0.001749002852373653\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01408908831493722\n",
      "Average test loss: 0.025614893534945116\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014076798118650913\n",
      "Average test loss: 0.0017985633362291589\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013988839991390705\n",
      "Average test loss: 0.0016876251264992688\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014006901052263048\n",
      "Average test loss: 0.0017069375299745137\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014023034963342878\n",
      "Average test loss: 0.0018237025803989834\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014029624638458093\n",
      "Average test loss: 0.0017469437221686046\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013958800572488043\n",
      "Average test loss: 0.001805946153950774\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01398849491112762\n",
      "Average test loss: 0.0018703271322366264\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014014541923171944\n",
      "Average test loss: 0.001748161696518461\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013963967229757044\n",
      "Average test loss: 0.0017702585390458505\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013969853047695425\n",
      "Average test loss: 0.0017400919545648827\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013912739470601081\n",
      "Average test loss: 0.001679488968414565\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013938743480377727\n",
      "Average test loss: 0.0017877959843931926\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013928677217413981\n",
      "Average test loss: 0.0018166813012212514\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01387810866203573\n",
      "Average test loss: 0.001826657442582978\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013961817281941572\n",
      "Average test loss: 0.001774478942155838\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013858884750141038\n",
      "Average test loss: 0.001791228169782294\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013872130152251985\n",
      "Average test loss: 0.0017850985612927212\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013879533953136867\n",
      "Average test loss: 0.0018060928177502421\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013893668038149674\n",
      "Average test loss: 0.001738180615628759\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013905718456539843\n",
      "Average test loss: 0.0017269775403870476\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01383890217045943\n",
      "Average test loss: 0.001689592325852977\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01389646791997883\n",
      "Average test loss: 0.0017920370176434516\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013861936960783269\n",
      "Average test loss: 0.0017382533801719546\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013818323975635899\n",
      "Average test loss: 0.0017748388743235005\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013815877300997575\n",
      "Average test loss: 0.0017273705454750193\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013795124457942115\n",
      "Average test loss: 0.0016666872697985835\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013797324837081962\n",
      "Average test loss: 0.0017293623338143032\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013807792767882347\n",
      "Average test loss: 0.0017532120140062437\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013785076767206191\n",
      "Average test loss: 0.0018409562766965892\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013783236490355597\n",
      "Average test loss: 0.0016822618587563436\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01380039227174388\n",
      "Average test loss: 0.0019768720131574406\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01378904676106241\n",
      "Average test loss: 0.0016921311797988084\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013754031656516922\n",
      "Average test loss: 0.0018262461228296162\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013727025883893173\n",
      "Average test loss: 0.0017384349971802699\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013746383795307742\n",
      "Average test loss: 0.001771726667881012\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013734224480887253\n",
      "Average test loss: 0.0018277549172441164\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013745381537410948\n",
      "Average test loss: 0.0018590112852139606\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01370479709158341\n",
      "Average test loss: 0.001755876778314511\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013719942142566045\n",
      "Average test loss: 0.001777882118191984\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013689419469071758\n",
      "Average test loss: 0.001704551751518415\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013712118681934145\n",
      "Average test loss: 0.001726571846753359\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013709776679674785\n",
      "Average test loss: 0.0017658105437747306\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013653623642192947\n",
      "Average test loss: 0.0018838427564543154\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01371206388870875\n",
      "Average test loss: 0.0017458451270229286\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01367314916352431\n",
      "Average test loss: 0.0017683169293320842\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013708983074459764\n",
      "Average test loss: 0.0017997657232400443\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013734543775518734\n",
      "Average test loss: 0.0017767822532397178\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013648038614955213\n",
      "Average test loss: 0.0018419380519125196\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013644107831848992\n",
      "Average test loss: 0.0017753196112397643\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013626413183907667\n",
      "Average test loss: 0.001901119133664502\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013626346212294367\n",
      "Average test loss: 0.0018665959859887758\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013602902878489758\n",
      "Average test loss: 0.0018694649702972836\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013625031837158732\n",
      "Average test loss: 0.001749692950087289\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013646244099570645\n",
      "Average test loss: 0.0016659083044570353\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013603641974429289\n",
      "Average test loss: 0.00178752386280232\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013577007302807437\n",
      "Average test loss: 0.001828784004267719\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013629158896704515\n",
      "Average test loss: 0.0017898876224127082\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013651155029733976\n",
      "Average test loss: 0.0017718572587085268\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013597830147378975\n",
      "Average test loss: 0.0016840581933243407\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013544092127846347\n",
      "Average test loss: 0.0016677802745252847\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01356437195920282\n",
      "Average test loss: 0.0016942751705646515\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013570734346078502\n",
      "Average test loss: 0.001735555891878903\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013546622675326135\n",
      "Average test loss: 0.002346119371139341\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013596279963850976\n",
      "Average test loss: 0.001762097521788544\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013561763093703322\n",
      "Average test loss: 0.0021097162533551457\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013555866118106576\n",
      "Average test loss: 0.0018415016369480225\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01356144825120767\n",
      "Average test loss: 0.0017267582195086611\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013532008468276924\n",
      "Average test loss: 0.001785007539515694\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013538441217607921\n",
      "Average test loss: 0.001829379314970639\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013486316656900777\n",
      "Average test loss: 0.0022931359958731465\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013540208876960808\n",
      "Average test loss: 0.0016985944590220848\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013514010763002767\n",
      "Average test loss: 0.0017778570827924542\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01348475026504861\n",
      "Average test loss: 0.001938029766186244\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01350146709713671\n",
      "Average test loss: 0.0017377421737441586\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01353048888179991\n",
      "Average test loss: 0.0017293746068866717\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01355678903311491\n",
      "Average test loss: 0.0016789268438393871\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01345414817912711\n",
      "Average test loss: 0.009093382677270306\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013579232642220126\n",
      "Average test loss: 0.001953938025256826\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01346793785939614\n",
      "Average test loss: 0.0017173103409198424\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013488750707772043\n",
      "Average test loss: 0.0016884264723501271\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013479467084010443\n",
      "Average test loss: 0.0017699835270436274\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013463347419268554\n",
      "Average test loss: 0.00181679343059659\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013452704609268241\n",
      "Average test loss: 0.0017421706953189438\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013469600006110138\n",
      "Average test loss: 0.001727952141314745\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01348305178185304\n",
      "Average test loss: 0.0017955561776955922\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01342209448499812\n",
      "Average test loss: 0.0017397247415242923\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013492279105716282\n",
      "Average test loss: 0.0016716769660512607\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013415977737969823\n",
      "Average test loss: 0.001758802830448581\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013414590858750873\n",
      "Average test loss: 0.0018703512493520975\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013463787015941408\n",
      "Average test loss: 0.0017465407515151632\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01348606730831994\n",
      "Average test loss: 0.0017226370078408056\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01342477903680669\n",
      "Average test loss: 0.001673698342198299\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01344930025935173\n",
      "Average test loss: 0.001784079448837373\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01341618877152602\n",
      "Average test loss: 0.0018050516883118285\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013430856468776863\n",
      "Average test loss: 0.0017683252051679625\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013392240702278084\n",
      "Average test loss: 0.0017149533415213227\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013414002502130137\n",
      "Average test loss: 0.0018055324130174185\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013413660971654785\n",
      "Average test loss: 0.0017484899151863323\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013420028590493733\n",
      "Average test loss: 0.001771422692678041\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013387456988294919\n",
      "Average test loss: 0.0017918538578475516\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013389381130536396\n",
      "Average test loss: 0.0017180616952892807\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013399431871043312\n",
      "Average test loss: 0.001725035714606444\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01336853023370107\n",
      "Average test loss: 0.0017868490148749616\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01340977998615967\n",
      "Average test loss: 0.0018793357098475098\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01337167440354824\n",
      "Average test loss: 0.001825875852463974\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013376590692334704\n",
      "Average test loss: 0.0017526807782964574\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013337276914053493\n",
      "Average test loss: 0.001663912516604695\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013352675926354196\n",
      "Average test loss: 0.0018020185659536057\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013339722529881531\n",
      "Average test loss: 0.0017273430196154448\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013326070275571611\n",
      "Average test loss: 0.0017155989862771498\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013345152772963047\n",
      "Average test loss: 0.001818477893455161\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013334891904559401\n",
      "Average test loss: 0.0016806161523693138\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013381794426176282\n",
      "Average test loss: 0.0018396308972603746\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013306651465594769\n",
      "Average test loss: 0.0017844187193032767\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013303208703796069\n",
      "Average test loss: 0.0016808052884621752\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013307642609708839\n",
      "Average test loss: 0.0017205394361582067\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013340861491031118\n",
      "Average test loss: 0.001694682422094047\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013308453453083832\n",
      "Average test loss: 0.0016917178829511006\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013316854153242377\n",
      "Average test loss: 0.0018325319662172762\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013279192852477233\n",
      "Average test loss: 0.002129273352834086\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013278811074793338\n",
      "Average test loss: 0.0016969924919928113\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01327943742606375\n",
      "Average test loss: 0.0017203964578608672\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013326808458401097\n",
      "Average test loss: 0.0017694121855828498\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013283508270978928\n",
      "Average test loss: 0.0017768764664005073\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01326483635438813\n",
      "Average test loss: 0.0016900003624873029\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013246827323403623\n",
      "Average test loss: 0.0017315832667259706\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013352409337129858\n",
      "Average test loss: 0.0016858676735104787\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013332613196637895\n",
      "Average test loss: 0.001804517177450988\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013254044652813012\n",
      "Average test loss: 0.0017788316566083166\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013268605204919974\n",
      "Average test loss: 0.0017562591406620211\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01324669077164597\n",
      "Average test loss: 0.0018350043721083137\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013288398488528199\n",
      "Average test loss: 0.0017058365297400288\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013235845385326279\n",
      "Average test loss: 0.0016818231682603558\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013225410193204879\n",
      "Average test loss: 0.0017233236149056918\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013259348452091218\n",
      "Average test loss: 0.0018134681683861548\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013255736180477672\n",
      "Average test loss: 0.0017475425648606486\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013245324384835031\n",
      "Average test loss: 0.0017054477972495887\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013222141976157824\n",
      "Average test loss: 0.0017754106488492754\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013200775656435225\n",
      "Average test loss: 0.00182126923236582\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01323465390668975\n",
      "Average test loss: 0.0017038718917303615\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013249252344999048\n",
      "Average test loss: 0.0016846500373341971\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01319424239711629\n",
      "Average test loss: 0.0017214308734983205\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01320625913474295\n",
      "Average test loss: 0.0018404267342347238\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013246759398116006\n",
      "Average test loss: 0.001675187568904625\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013230109526051415\n",
      "Average test loss: 0.0019496648712083698\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013185409265259901\n",
      "Average test loss: 0.001729136481674181\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013193835409979026\n",
      "Average test loss: 0.0016920779362424381\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013183656413522031\n",
      "Average test loss: 0.001691933736205101\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013196897987690236\n",
      "Average test loss: 0.001701943537323839\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013197750124666426\n",
      "Average test loss: 0.0016722200770551959\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013203441467550066\n",
      "Average test loss: 0.0016620424169426164\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013157036546203826\n",
      "Average test loss: 0.0017336291786697177\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01318223468048705\n",
      "Average test loss: 0.0018202124807155794\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013216107801430755\n",
      "Average test loss: 0.011564594608214166\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013881700899865892\n",
      "Average test loss: 0.0017121718165775141\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013142532645000352\n",
      "Average test loss: 0.0017133149908752077\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01315380822122097\n",
      "Average test loss: 0.0017887337967339488\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013140612412658003\n",
      "Average test loss: 0.0017968180813930101\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013149643059406016\n",
      "Average test loss: 0.002157743251779013\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013160803544024626\n",
      "Average test loss: 0.0018179849576618937\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013175236821174621\n",
      "Average test loss: 0.0017019660232795609\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013164533524877495\n",
      "Average test loss: 0.0017168847443018523\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013130902710060278\n",
      "Average test loss: 0.0016848201668924756\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01323718762397766\n",
      "Average test loss: 0.001731199223237733\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013142444888750712\n",
      "Average test loss: 0.0017586255819640225\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013129407545758618\n",
      "Average test loss: 0.0018229193778501616\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013119809152351485\n",
      "Average test loss: 0.0016882512465947203\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01312812993509902\n",
      "Average test loss: 0.0017663622301899724\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013142415814929538\n",
      "Average test loss: 0.001660867863955597\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013138077813718054\n",
      "Average test loss: 0.0017280155468939078\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013137299478881889\n",
      "Average test loss: 0.0017403854236213696\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013084153956837125\n",
      "Average test loss: 0.001711170533258054\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013148556996550825\n",
      "Average test loss: 0.0016666585715073678\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013134907764693102\n",
      "Average test loss: 0.0017609138803349601\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013113394280274708\n",
      "Average test loss: 0.001989959585583872\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013171567424303955\n",
      "Average test loss: 0.001815351023649176\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013127568322751257\n",
      "Average test loss: 0.0018813223614253932\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01311682675530513\n",
      "Average test loss: 0.00175743024982512\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_No_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.42\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.32\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.59\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.09\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.60\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.87\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.457170759518941\n",
      "Average test loss: 0.07650970202022128\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2975448967615764\n",
      "Average test loss: 0.022411427047517564\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9177110050519307\n",
      "Average test loss: 0.009764006386200588\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7130711278915405\n",
      "Average test loss: 0.009758155439462927\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5865824982855055\n",
      "Average test loss: 0.40483477034005855\n",
      "Epoch 6/300\n",
      "Average training loss: 0.5024199463526408\n",
      "Average test loss: 0.009001222601367367\n",
      "Epoch 7/300\n",
      "Average training loss: 0.43228174130121866\n",
      "Average test loss: 0.008590020127594472\n",
      "Epoch 8/300\n",
      "Average training loss: 0.3803470731311374\n",
      "Average test loss: 0.007509587290386359\n",
      "Epoch 9/300\n",
      "Average training loss: 0.3407760899066925\n",
      "Average test loss: 0.010471469581127166\n",
      "Epoch 10/300\n",
      "Average training loss: 0.3067407729095883\n",
      "Average test loss: 0.010846761105789079\n",
      "Epoch 11/300\n",
      "Average training loss: 0.27621448273128935\n",
      "Average test loss: 0.012560705691576003\n",
      "Epoch 12/300\n",
      "Average training loss: 0.25346434020996095\n",
      "Average test loss: 0.007997329201963213\n",
      "Epoch 13/300\n",
      "Average training loss: 0.23441612341668872\n",
      "Average test loss: 0.01135016811473502\n",
      "Epoch 14/300\n",
      "Average training loss: 0.22147239277097913\n",
      "Average test loss: 0.009557923427058591\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20642823448446063\n",
      "Average test loss: 0.0071235626050167615\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1929253436062071\n",
      "Average test loss: 0.008325345553457738\n",
      "Epoch 17/300\n",
      "Average training loss: 0.18431477320194245\n",
      "Average test loss: 0.008503805852598615\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1776560656229655\n",
      "Average test loss: 0.012534396655029721\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1700977132452859\n",
      "Average test loss: 0.008384099077019426\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16259843685891892\n",
      "Average test loss: 0.007544582669933637\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15740187144941753\n",
      "Average test loss: 0.010119231674406263\n",
      "Epoch 22/300\n",
      "Average training loss: 0.15231800140274895\n",
      "Average test loss: 0.006788642488833931\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1469634569618437\n",
      "Average test loss: 0.007834937782751189\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1438548178540336\n",
      "Average test loss: 0.0069397644690341416\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14086114609241485\n",
      "Average test loss: 0.006651325001484818\n",
      "Epoch 26/300\n",
      "Average training loss: 0.135981374224027\n",
      "Average test loss: 0.006653849052058326\n",
      "Epoch 27/300\n",
      "Average training loss: 0.13548546722862456\n",
      "Average test loss: 0.006395848254362742\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13011899773279825\n",
      "Average test loss: 0.006547830008798175\n",
      "Epoch 29/300\n",
      "Average training loss: 0.127450723376539\n",
      "Average test loss: 0.0066183837958508065\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1261564315425025\n",
      "Average test loss: 0.007057684376421902\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12319864332675934\n",
      "Average test loss: 0.006046132644017537\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1229063483874003\n",
      "Average test loss: 0.006134650293323729\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11843657547897762\n",
      "Average test loss: 0.006141798034724262\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1176970470017857\n",
      "Average test loss: 0.005924232328103648\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11606203218301138\n",
      "Average test loss: 0.006536925508744187\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11369951256778506\n",
      "Average test loss: 0.006519005232387119\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11209260403447681\n",
      "Average test loss: 0.0062290515841709245\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1095324051645067\n",
      "Average test loss: 0.006181556342790524\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10934002343151304\n",
      "Average test loss: 0.006106179195145766\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1071728906499015\n",
      "Average test loss: 0.006038445600618919\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10684828369485007\n",
      "Average test loss: 0.005867551373110877\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10600671165850427\n",
      "Average test loss: 0.005754292818821139\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10462812792592578\n",
      "Average test loss: 0.0058009296109279\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10336238029930327\n",
      "Average test loss: 0.006153374108589358\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10183863098753823\n",
      "Average test loss: 0.005995128357575999\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10164236285951403\n",
      "Average test loss: 0.0060761492438614366\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10049125068055259\n",
      "Average test loss: 0.006094920938213667\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09956684987412559\n",
      "Average test loss: 0.005922931658724944\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09836042323377397\n",
      "Average test loss: 0.005668467756774691\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09933995085292392\n",
      "Average test loss: 0.005974923030369812\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09838181770510143\n",
      "Average test loss: 0.005834726895309157\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09757939052581788\n",
      "Average test loss: 0.005642724594929152\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09661746584044563\n",
      "Average test loss: 0.006220449225356181\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09672620831595527\n",
      "Average test loss: 0.00599057190782494\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09557987155516942\n",
      "Average test loss: 0.0056424104145003685\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09550422325399187\n",
      "Average test loss: 0.0061174298682146605\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09501591778463787\n",
      "Average test loss: 0.00574237417926391\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09409341078334385\n",
      "Average test loss: 0.005895551907105578\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09399476795395216\n",
      "Average test loss: 0.005678913401646747\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0938400268289778\n",
      "Average test loss: 0.00580568014147381\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09303962560494741\n",
      "Average test loss: 0.005702710237767961\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09258005548848046\n",
      "Average test loss: 0.005837305364509424\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09269025356239742\n",
      "Average test loss: 0.006176583570324712\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09212920671039157\n",
      "Average test loss: 0.0055823784462279745\n",
      "Epoch 65/300\n",
      "Average training loss: 0.092065741803911\n",
      "Average test loss: 0.005841015658444829\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09141554068856769\n",
      "Average test loss: 0.0061577279600832195\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09143449802531137\n",
      "Average test loss: 0.005787700376162926\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09092561036348343\n",
      "Average test loss: 0.005614070448610517\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0900629715985722\n",
      "Average test loss: 0.006285700203229984\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09027339615424475\n",
      "Average test loss: 0.005667377492619885\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09010777252250247\n",
      "Average test loss: 0.005532600322531329\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08976583672894371\n",
      "Average test loss: 0.005721723291195102\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08891381698846818\n",
      "Average test loss: 0.0055946803978747794\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08902367123630311\n",
      "Average test loss: 0.005498788197421365\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08914761994282405\n",
      "Average test loss: 0.005488789120482074\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08893584543466568\n",
      "Average test loss: 0.005512812176512347\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0885723739001486\n",
      "Average test loss: 0.005778154608276155\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08920038622617722\n",
      "Average test loss: 0.005530311612619294\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08803236524264017\n",
      "Average test loss: 0.006045651126652956\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08810899664296044\n",
      "Average test loss: 0.006152433887124061\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08697748239835103\n",
      "Average test loss: 0.005535448564423455\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08751025834348466\n",
      "Average test loss: 0.006184471437707543\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08715268584092459\n",
      "Average test loss: 0.005772465046495199\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0866917701628473\n",
      "Average test loss: 0.0060425210495789845\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08725975508822335\n",
      "Average test loss: 0.005428783068640365\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08668284946680069\n",
      "Average test loss: 0.005712662174055974\n",
      "Epoch 87/300\n",
      "Average training loss: 0.086452255639765\n",
      "Average test loss: 0.005542040008223719\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08588865098026063\n",
      "Average test loss: 0.005683073434564802\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08574199187755585\n",
      "Average test loss: 0.005636560702489482\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0861175934208764\n",
      "Average test loss: 0.005544333447598749\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08621768854061762\n",
      "Average test loss: 0.005645888021836678\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08575325551960203\n",
      "Average test loss: 0.005773298960593012\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08472844029466312\n",
      "Average test loss: 0.0059045603209071686\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08561198380258349\n",
      "Average test loss: 0.005559674445953634\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08517719507879681\n",
      "Average test loss: 0.005560906514111492\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08498022245036231\n",
      "Average test loss: 0.005879983241773314\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08465538940164778\n",
      "Average test loss: 0.0054635849537120924\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08457203054096964\n",
      "Average test loss: 0.005563808485451672\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08451689164506064\n",
      "Average test loss: 0.005484674634825852\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08396746352314949\n",
      "Average test loss: 0.005977217317869266\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08423671526379055\n",
      "Average test loss: 0.005698892792893781\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08465185281303193\n",
      "Average test loss: 0.005706466291927629\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08371557407246695\n",
      "Average test loss: 0.005874653663486242\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08370198415385352\n",
      "Average test loss: 0.005538560015873777\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08401973664098315\n",
      "Average test loss: 0.005622592451671759\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08374028351571824\n",
      "Average test loss: 0.005718674850960573\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0831912750005722\n",
      "Average test loss: 0.005723833793567287\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08321265878611141\n",
      "Average test loss: 0.00554779462930229\n",
      "Epoch 109/300\n",
      "Average training loss: 0.08301301626364391\n",
      "Average test loss: 0.005814624544233084\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08262374283207788\n",
      "Average test loss: 0.0057077801794641544\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08272445570760303\n",
      "Average test loss: 0.00580153685890966\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08371597635746002\n",
      "Average test loss: 0.0055723949227896\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08229783424403932\n",
      "Average test loss: 0.005920345204985804\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08233452982372708\n",
      "Average test loss: 0.005689323408322202\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0825253585378329\n",
      "Average test loss: 0.00557004839885566\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08382533838351568\n",
      "Average test loss: 0.00613304496390952\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0822861856619517\n",
      "Average test loss: 0.005503710500068135\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08192125260167651\n",
      "Average test loss: 0.0055600107825464675\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08207846559418572\n",
      "Average test loss: 0.005561006834937467\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08179387802547879\n",
      "Average test loss: 0.005670014333600799\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08200076619121763\n",
      "Average test loss: 0.005806127501444684\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0816828149954478\n",
      "Average test loss: 0.005348438843670819\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08167842649751239\n",
      "Average test loss: 0.005889570843014452\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08143350431323051\n",
      "Average test loss: 0.005668639898714092\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08118899008962843\n",
      "Average test loss: 0.00586712520983484\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08233296245336533\n",
      "Average test loss: 0.005497501056227419\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08079385217693116\n",
      "Average test loss: 0.0054336366065674355\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08085639411873288\n",
      "Average test loss: 0.005555008676110042\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0820519265068902\n",
      "Average test loss: 0.0054527557194232945\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08084724371963077\n",
      "Average test loss: 0.0072152999639511104\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0806281509730551\n",
      "Average test loss: 0.00545085644763377\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08154610750410292\n",
      "Average test loss: 0.0056883277309437595\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08044642694791158\n",
      "Average test loss: 0.005501366074714396\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08037868848111894\n",
      "Average test loss: 0.0053993027003275025\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0803555510573917\n",
      "Average test loss: 0.00563616042625573\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08055212889446152\n",
      "Average test loss: 0.005669538053373496\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08030691020356284\n",
      "Average test loss: 0.005680137479884757\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08012438611189525\n",
      "Average test loss: 0.005765704861531655\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08000250398450427\n",
      "Average test loss: 0.005472385362618499\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0802311937212944\n",
      "Average test loss: 0.0055920582359863654\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08033185474740134\n",
      "Average test loss: 0.005431097605576118\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0795937074787087\n",
      "Average test loss: 0.005524921141978767\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07983716728289922\n",
      "Average test loss: 0.005560581643962198\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08012292092376286\n",
      "Average test loss: 0.0057782680119077365\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07972615735398399\n",
      "Average test loss: 0.006118362864686383\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07935164162847731\n",
      "Average test loss: 0.0054402945981257495\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07955408409568998\n",
      "Average test loss: 0.005625473517096705\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07939053081803851\n",
      "Average test loss: 0.005476294059720305\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07930706167883343\n",
      "Average test loss: 0.005721368501583735\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07912723345226712\n",
      "Average test loss: 0.005540929665168126\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0791497417887052\n",
      "Average test loss: 0.005655717614624235\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07896492682562933\n",
      "Average test loss: 0.005734036182363828\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07888030863470501\n",
      "Average test loss: 0.008140108779072761\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0789128345714675\n",
      "Average test loss: 0.005662718844496542\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07861437606811524\n",
      "Average test loss: 0.00543461953351895\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07920650027195612\n",
      "Average test loss: 0.005449826555740502\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07855612752834956\n",
      "Average test loss: 0.005473240069217152\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07839459878868527\n",
      "Average test loss: 0.005639065309531159\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07909134288628895\n",
      "Average test loss: 0.005669176778445641\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08037238462103738\n",
      "Average test loss: 0.005647986646741629\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07784665826956431\n",
      "Average test loss: 0.005528923130697674\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07882303881645203\n",
      "Average test loss: 0.005556817116422786\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07789856190151638\n",
      "Average test loss: 0.005656834691349003\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0781120890378952\n",
      "Average test loss: 0.005773727933979697\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07814958315425449\n",
      "Average test loss: 0.005660514275232951\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07827513589461645\n",
      "Average test loss: 0.0057555047182573215\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0776490600042873\n",
      "Average test loss: 0.005660843503557974\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07815505522489548\n",
      "Average test loss: 0.005588608865108755\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07802760173214807\n",
      "Average test loss: 0.005545438396020068\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07748338535759185\n",
      "Average test loss: 0.005804718880603711\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07785663325256771\n",
      "Average test loss: 0.005880419565157758\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07804413576258554\n",
      "Average test loss: 0.005511478457185957\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0773818099366294\n",
      "Average test loss: 0.005612998760615786\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07769824919435712\n",
      "Average test loss: 0.005704104310522476\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0772307652036349\n",
      "Average test loss: 0.005558580699480242\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07729228968090482\n",
      "Average test loss: 0.005587908489836587\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07715882261594137\n",
      "Average test loss: 0.0055780891585681175\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0776037452618281\n",
      "Average test loss: 0.005923623868160778\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07716078939702775\n",
      "Average test loss: 0.005539766144421365\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0773048915333218\n",
      "Average test loss: 0.0059380863747662965\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07719410794973373\n",
      "Average test loss: 0.005454772784064214\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07715024740828408\n",
      "Average test loss: 0.005613675926294592\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07659752132495244\n",
      "Average test loss: 0.005678706314000819\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0768881932662593\n",
      "Average test loss: 0.0061894228594998515\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07691264231999716\n",
      "Average test loss: 0.005540401445908679\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07635014921426773\n",
      "Average test loss: 0.005626445951561133\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07690779272715251\n",
      "Average test loss: 0.00548988135655721\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07687891819079717\n",
      "Average test loss: 0.005630316203667058\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07653695808516608\n",
      "Average test loss: 0.005687358580943611\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07636489157213106\n",
      "Average test loss: 0.005575382917291588\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0763110236691104\n",
      "Average test loss: 0.005758268910356694\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07708291002776888\n",
      "Average test loss: 0.005626077192525069\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07671786722209718\n",
      "Average test loss: 0.005510139494306511\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07585689525471793\n",
      "Average test loss: 0.005482462643542223\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07662155871258841\n",
      "Average test loss: 0.005455770449505912\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07694063505199221\n",
      "Average test loss: 0.005617977622482511\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07634610638684697\n",
      "Average test loss: 0.0061649089708096455\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07629155589474572\n",
      "Average test loss: 0.005929323899663157\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0759426506360372\n",
      "Average test loss: 0.005579077042225334\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07585625535911984\n",
      "Average test loss: 0.00554516523083051\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07570426399509111\n",
      "Average test loss: 0.005604933440271351\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07574356376462513\n",
      "Average test loss: 0.005547967602395349\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07595550405979157\n",
      "Average test loss: 0.005517691473166148\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0753225049773852\n",
      "Average test loss: 0.005566093148870601\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07562288455168406\n",
      "Average test loss: 0.005559951557881302\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07568306492434608\n",
      "Average test loss: 0.005572347377323442\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07583079420195686\n",
      "Average test loss: 0.005625771293209659\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07538697847392824\n",
      "Average test loss: 0.005620151564064953\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07542060551378461\n",
      "Average test loss: 0.005643336753050486\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07564981585741043\n",
      "Average test loss: 0.005629442794041501\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07532363667090734\n",
      "Average test loss: 0.005623531069606543\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0754659830364916\n",
      "Average test loss: 0.0057240814032653966\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07498906795183817\n",
      "Average test loss: 0.005861040043334167\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07531519091129303\n",
      "Average test loss: 0.005431003250595596\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07482799341943529\n",
      "Average test loss: 0.005587699137628079\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07530449412928687\n",
      "Average test loss: 0.00549265099151267\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0753635725511445\n",
      "Average test loss: 0.005565284762531519\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07538159867127736\n",
      "Average test loss: 0.005593139129380385\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07497872451941172\n",
      "Average test loss: 0.00572873685343398\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07475291758775711\n",
      "Average test loss: 0.005562247801986006\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07474539628293779\n",
      "Average test loss: 0.0055685090016987585\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07467121477921804\n",
      "Average test loss: 0.00549894465216332\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07473115153445138\n",
      "Average test loss: 0.005526465185814434\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07451513717571895\n",
      "Average test loss: 0.005561295851237244\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07478812815083398\n",
      "Average test loss: 0.005822156907783614\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07493866490655475\n",
      "Average test loss: 0.005682573290748729\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0743886206679874\n",
      "Average test loss: 0.0061405117110245756\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0744873284233941\n",
      "Average test loss: 0.00591206174550785\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07468641390403112\n",
      "Average test loss: 0.005834251262661483\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07504657878478369\n",
      "Average test loss: 0.005694831572473049\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0740833691060543\n",
      "Average test loss: 0.006291053074929449\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07443522632784313\n",
      "Average test loss: 0.005626376087259915\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0746352363758617\n",
      "Average test loss: 0.005623745430674818\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07537883482376734\n",
      "Average test loss: 0.005683398554722468\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07401341231664021\n",
      "Average test loss: 0.005595199257549312\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0738633263806502\n",
      "Average test loss: 0.005566795255574915\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0741498192747434\n",
      "Average test loss: 0.0056817867817978065\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0740131410294109\n",
      "Average test loss: 0.005888157253877984\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07431821886698405\n",
      "Average test loss: 0.005842308476567268\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0734657977686988\n",
      "Average test loss: 0.005719886581930849\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07396410620212555\n",
      "Average test loss: 0.005568817404409249\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07396730748812358\n",
      "Average test loss: 0.00597409735702806\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07418544923596912\n",
      "Average test loss: 0.00565713430609968\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07378712509738074\n",
      "Average test loss: 0.0056644954633795555\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07394940260383819\n",
      "Average test loss: 0.005543582395133045\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07344443248377906\n",
      "Average test loss: 0.005545667053924667\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07333928651279874\n",
      "Average test loss: 0.006056931181086435\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07384212171369128\n",
      "Average test loss: 0.0054631990107397235\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07402272752920787\n",
      "Average test loss: 0.005563668076776796\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07371835770209631\n",
      "Average test loss: 0.005428100560067428\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07389719936582777\n",
      "Average test loss: 0.005522251194963853\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07363708027203877\n",
      "Average test loss: 0.005691101803133885\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07304269921779633\n",
      "Average test loss: 0.005572674112187492\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07369171704848608\n",
      "Average test loss: 0.0055146851227101355\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0735937667687734\n",
      "Average test loss: 0.0055443661196364295\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07341741988393996\n",
      "Average test loss: 0.006000667937099934\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07293928862280316\n",
      "Average test loss: 0.00588754781252808\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07312119965420828\n",
      "Average test loss: 0.005845155044148366\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07307662747303645\n",
      "Average test loss: 0.0056794607270922925\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07301146629783842\n",
      "Average test loss: 0.0055618248068624076\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07299546587467194\n",
      "Average test loss: 0.005559209381540617\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07381049703889422\n",
      "Average test loss: 0.005716403146584829\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07353496525684992\n",
      "Average test loss: 0.005907418585071961\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07285712238152822\n",
      "Average test loss: 0.005752195763091246\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07322417872813013\n",
      "Average test loss: 0.00578873014367289\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07267370789580875\n",
      "Average test loss: 0.005553549009271794\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07286264949705866\n",
      "Average test loss: 0.005703254388645291\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0729075349966685\n",
      "Average test loss: 0.00822093572964271\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07297215292188856\n",
      "Average test loss: 0.005613218128681183\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07256340659326978\n",
      "Average test loss: 0.00562150266352627\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07254827290773391\n",
      "Average test loss: 0.005581071286979649\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07328066938453251\n",
      "Average test loss: 0.0056074396988583936\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07241728574699825\n",
      "Average test loss: 0.0057508044503629206\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07327636552188131\n",
      "Average test loss: 0.005541631885286834\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07288818803098467\n",
      "Average test loss: 0.005555597664995326\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0722053235967954\n",
      "Average test loss: 0.005764309991978937\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07255505129694939\n",
      "Average test loss: 0.005565893555266989\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07244882647196452\n",
      "Average test loss: 0.005670329550074207\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07262428157197104\n",
      "Average test loss: 0.007312219548970461\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0724272034102016\n",
      "Average test loss: 0.0058665173004070915\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07221111758550008\n",
      "Average test loss: 0.005573956581453482\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0727091232670678\n",
      "Average test loss: 0.0057687551677227025\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07257623942693074\n",
      "Average test loss: 0.005558514369444715\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07266026783651776\n",
      "Average test loss: 0.006192719253400962\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07314333895511098\n",
      "Average test loss: 0.005672575518488884\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0724462958905432\n",
      "Average test loss: 0.005567186104754607\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07194312345981597\n",
      "Average test loss: 0.0056016962776581445\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07206830061475436\n",
      "Average test loss: 0.005641596315635575\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07209632833136452\n",
      "Average test loss: 0.005680133634143406\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07214520525270038\n",
      "Average test loss: 0.005672673143032525\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07493488855494393\n",
      "Average test loss: 0.005667617560674747\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07214548585812251\n",
      "Average test loss: 0.006245657220482826\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07141859722799725\n",
      "Average test loss: 0.005658368971612719\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07163327857520846\n",
      "Average test loss: 0.005610072785781489\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07182514556248983\n",
      "Average test loss: 0.005746864628460672\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07174322082599004\n",
      "Average test loss: 0.005634901015294923\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07211922325690587\n",
      "Average test loss: 0.0064029677071505125\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07160287376244863\n",
      "Average test loss: 0.005517451246579488\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07199622533718746\n",
      "Average test loss: 0.005750560599482721\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0722721851600541\n",
      "Average test loss: 0.005760767281469371\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.0263576455646093\n",
      "Average test loss: 0.06311199553310871\n",
      "Epoch 2/300\n",
      "Average training loss: 0.8232062833044264\n",
      "Average test loss: 0.009967837469445335\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5456529518233405\n",
      "Average test loss: 0.013989434078335762\n",
      "Epoch 4/300\n",
      "Average training loss: 0.41955892859564886\n",
      "Average test loss: 0.01111256795956029\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3466844804816776\n",
      "Average test loss: 0.005692150300989548\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2973184554974238\n",
      "Average test loss: 0.006204737666994333\n",
      "Epoch 7/300\n",
      "Average training loss: 0.25729343265957305\n",
      "Average test loss: 0.007250711942298545\n",
      "Epoch 8/300\n",
      "Average training loss: 0.22828274737464058\n",
      "Average test loss: 0.008948157732685407\n",
      "Epoch 9/300\n",
      "Average training loss: 0.20488588914606307\n",
      "Average test loss: 0.006794498314460119\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18486729835139382\n",
      "Average test loss: 0.005301381991555294\n",
      "Epoch 11/300\n",
      "Average training loss: 0.16940529176923963\n",
      "Average test loss: 0.004674621951662832\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15702378074328105\n",
      "Average test loss: 0.01356517687605487\n",
      "Epoch 13/300\n",
      "Average training loss: 0.14565423464775085\n",
      "Average test loss: 0.0046608055465751225\n",
      "Epoch 14/300\n",
      "Average training loss: 0.13736885452270509\n",
      "Average test loss: 0.004922302204287714\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12840943045748604\n",
      "Average test loss: 0.007490374291936557\n",
      "Epoch 16/300\n",
      "Average training loss: 0.12551022803783415\n",
      "Average test loss: 0.010054353246258365\n",
      "Epoch 17/300\n",
      "Average training loss: 0.11779030605157216\n",
      "Average test loss: 0.00402215015101764\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11324591465791066\n",
      "Average test loss: 0.003946469984948635\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11157785777250925\n",
      "Average test loss: 0.004186016014052762\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10616144463751051\n",
      "Average test loss: 0.004367717855506473\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10311431400643455\n",
      "Average test loss: 0.004001293119457033\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09867225727770064\n",
      "Average test loss: 0.0046656294138067295\n",
      "Epoch 23/300\n",
      "Average training loss: 0.09618644310368432\n",
      "Average test loss: 0.004135510271001193\n",
      "Epoch 24/300\n",
      "Average training loss: 0.09479696914222506\n",
      "Average test loss: 0.00447394745838311\n",
      "Epoch 25/300\n",
      "Average training loss: 0.09181087587277094\n",
      "Average test loss: 0.004049183915472693\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08713626370827357\n",
      "Average test loss: 0.004364262785141667\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08703373150693046\n",
      "Average test loss: 0.0036876115805159013\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08388152419858509\n",
      "Average test loss: 0.0038271474084920353\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08277924547592799\n",
      "Average test loss: 0.003831040672543976\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07983561573425929\n",
      "Average test loss: 0.0037923945846656958\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0779354520506329\n",
      "Average test loss: 0.0035018738706906636\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07842461240291596\n",
      "Average test loss: 0.003510709642122189\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07637653283940421\n",
      "Average test loss: 0.0039519945387211115\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07327747666835785\n",
      "Average test loss: 0.0038571566293636957\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07287508089674843\n",
      "Average test loss: 0.003588816452357504\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07041797661119037\n",
      "Average test loss: 0.0037324069175455306\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06950738172398672\n",
      "Average test loss: 0.003800399273633957\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06925045173698001\n",
      "Average test loss: 0.00345803632421626\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07047898463408152\n",
      "Average test loss: 0.004784477667262157\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06705823164847162\n",
      "Average test loss: 0.0033619302738871837\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06651224348942439\n",
      "Average test loss: 0.0034417559570736355\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06525091718965106\n",
      "Average test loss: 0.0035582018738819494\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06470882838964462\n",
      "Average test loss: 0.0035716784108016228\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06481467965576383\n",
      "Average test loss: 0.003523190754569239\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06414356425073411\n",
      "Average test loss: 0.0036109614496429763\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06315007829003864\n",
      "Average test loss: 0.0033411465846002104\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06253749993774627\n",
      "Average test loss: 0.0036214693658467795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06274704066912333\n",
      "Average test loss: 0.003578419739380479\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06266615872912937\n",
      "Average test loss: 0.003470858811918232\n",
      "Epoch 50/300\n",
      "Average training loss: 0.061887468874454496\n",
      "Average test loss: 0.0035784034662776523\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06211954954266548\n",
      "Average test loss: 0.003337847278970811\n",
      "Epoch 52/300\n",
      "Average training loss: 0.061382179700666006\n",
      "Average test loss: 0.0033493708175503546\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06129702937934134\n",
      "Average test loss: 0.0034732459322032003\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06061513172255622\n",
      "Average test loss: 0.0032784560517304475\n",
      "Epoch 55/300\n",
      "Average training loss: 0.060933570663134255\n",
      "Average test loss: 0.0032945108477854066\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05981390611992942\n",
      "Average test loss: 0.003203056205271019\n",
      "Epoch 57/300\n",
      "Average training loss: 0.060199788255823984\n",
      "Average test loss: 0.0032946036915398307\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05937939980294969\n",
      "Average test loss: 0.004568445341454612\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0588164829744233\n",
      "Average test loss: 0.0036442627157602047\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05989646915594737\n",
      "Average test loss: 0.0034781692926254535\n",
      "Epoch 61/300\n",
      "Average training loss: 0.059004042506217956\n",
      "Average test loss: 0.0032245602160692216\n",
      "Epoch 62/300\n",
      "Average training loss: 0.059472713142633436\n",
      "Average test loss: 0.0033123611025512216\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05879762224356334\n",
      "Average test loss: 0.003244076449217068\n",
      "Epoch 64/300\n",
      "Average training loss: 0.058494842214716805\n",
      "Average test loss: 0.0032155957385483717\n",
      "Epoch 65/300\n",
      "Average training loss: 0.058184109783834884\n",
      "Average test loss: 0.003165653909039166\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05823336232039664\n",
      "Average test loss: 0.003218253444466326\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05785631130801307\n",
      "Average test loss: 0.00318792724609375\n",
      "Epoch 68/300\n",
      "Average training loss: 0.057324903812673356\n",
      "Average test loss: 0.003225264001223776\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05711586406164699\n",
      "Average test loss: 0.004109868266516261\n",
      "Epoch 70/300\n",
      "Average training loss: 0.057979113287395904\n",
      "Average test loss: 0.0032104567272795573\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05872832334041595\n",
      "Average test loss: 0.003313311626513799\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05768174624774191\n",
      "Average test loss: 0.003151331422229608\n",
      "Epoch 73/300\n",
      "Average training loss: 0.056598163629571596\n",
      "Average test loss: 0.0035141799222264025\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05661887848377228\n",
      "Average test loss: 0.0036621727753016683\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05627068826887343\n",
      "Average test loss: 0.0031885121603392893\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05656241764293777\n",
      "Average test loss: 0.003307392749107546\n",
      "Epoch 77/300\n",
      "Average training loss: 0.056447839717070264\n",
      "Average test loss: 0.0037403480847262673\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05605929858485858\n",
      "Average test loss: 0.0032468321348230046\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05824587702088886\n",
      "Average test loss: 0.003230232224282291\n",
      "Epoch 80/300\n",
      "Average training loss: 0.056404027763340206\n",
      "Average test loss: 0.0034744800043602785\n",
      "Epoch 81/300\n",
      "Average training loss: 0.055589209877782396\n",
      "Average test loss: 0.003557290615513921\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05593958963950475\n",
      "Average test loss: 0.0031740521233942776\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05546679742799865\n",
      "Average test loss: 0.010464548089438015\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05535527641243405\n",
      "Average test loss: 0.004044681577011943\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0553138328856892\n",
      "Average test loss: 0.0031466185663723283\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05565116728345553\n",
      "Average test loss: 0.0032444227577911483\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05535530916684204\n",
      "Average test loss: 0.0031284370281630092\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05524675263961156\n",
      "Average test loss: 0.0031189872592480646\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05509572582112418\n",
      "Average test loss: 0.007408450115058157\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05485331402884589\n",
      "Average test loss: 0.0031039985619071456\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05490507567591137\n",
      "Average test loss: 0.0032970666988856264\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05447462384237183\n",
      "Average test loss: 0.0032371455002576112\n",
      "Epoch 93/300\n",
      "Average training loss: 0.054729025264581045\n",
      "Average test loss: 0.0030820249455670514\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05462545629342397\n",
      "Average test loss: 0.0030898612812161447\n",
      "Epoch 95/300\n",
      "Average training loss: 0.054429926504691445\n",
      "Average test loss: 0.003169232187171777\n",
      "Epoch 96/300\n",
      "Average training loss: 0.053956364045540495\n",
      "Average test loss: 0.0031512793155594005\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05451875505182478\n",
      "Average test loss: 0.003225549590256479\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05404911549223794\n",
      "Average test loss: 0.0034382474821888736\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05386620194713275\n",
      "Average test loss: 0.003236537655505041\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05502913563781314\n",
      "Average test loss: 0.003986430318405231\n",
      "Epoch 101/300\n",
      "Average training loss: 0.053524221640494134\n",
      "Average test loss: 0.0032781672924757006\n",
      "Epoch 102/300\n",
      "Average training loss: 0.053840525699986354\n",
      "Average test loss: 0.003199142560155855\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05480219063493941\n",
      "Average test loss: 0.0031618804625338976\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05325171973970201\n",
      "Average test loss: 0.0031415014101399317\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05336739922894372\n",
      "Average test loss: 0.003317730411473248\n",
      "Epoch 106/300\n",
      "Average training loss: 0.053976298719644544\n",
      "Average test loss: 0.003133478490014871\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05384197453657786\n",
      "Average test loss: 0.0030488005694415832\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05286727146307627\n",
      "Average test loss: 0.003197425987157557\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05350917856229676\n",
      "Average test loss: 0.0031001892168488767\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05274352492888768\n",
      "Average test loss: 0.0031020437915705973\n",
      "Epoch 111/300\n",
      "Average training loss: 0.052943274941709306\n",
      "Average test loss: 0.003104330394210087\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05237031196554502\n",
      "Average test loss: 0.0030504472638583848\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05292986850937208\n",
      "Average test loss: 0.003089003685447905\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05383949426147673\n",
      "Average test loss: 0.003126024308717913\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05303533383872774\n",
      "Average test loss: 0.003095231073598067\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0523275647825665\n",
      "Average test loss: 0.0031577670712851814\n",
      "Epoch 117/300\n",
      "Average training loss: 0.052420787271526124\n",
      "Average test loss: 0.00310858296085563\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05389189232720269\n",
      "Average test loss: 0.0034066208367132477\n",
      "Epoch 119/300\n",
      "Average training loss: 0.052362608644697405\n",
      "Average test loss: 0.003045660146822532\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05222789845863978\n",
      "Average test loss: 0.003339904613999857\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0521606415145927\n",
      "Average test loss: 0.003154371636816197\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05275079290072123\n",
      "Average test loss: 0.0032047067435665264\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0514958522717158\n",
      "Average test loss: 0.0031704246561146446\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05222147019041909\n",
      "Average test loss: 0.003147818291766776\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05217761794394917\n",
      "Average test loss: 0.0031901143352604574\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05203558113508754\n",
      "Average test loss: 0.0031523718252364132\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05160437266031901\n",
      "Average test loss: 0.0031302262528075113\n",
      "Epoch 128/300\n",
      "Average training loss: 0.051846632851494684\n",
      "Average test loss: 0.0031386690735816956\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05192114914457003\n",
      "Average test loss: 0.003204890512757831\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05225159219900767\n",
      "Average test loss: 0.003318717743166619\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05147974830534723\n",
      "Average test loss: 0.0030339682278119856\n",
      "Epoch 132/300\n",
      "Average training loss: 0.051261005544000204\n",
      "Average test loss: 0.003004473923188117\n",
      "Epoch 133/300\n",
      "Average training loss: 0.051835397286547555\n",
      "Average test loss: 0.0030498179340114197\n",
      "Epoch 134/300\n",
      "Average training loss: 0.051208202173312505\n",
      "Average test loss: 0.02644300694266955\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05140869652562671\n",
      "Average test loss: 0.0031602773333175315\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0514282759461138\n",
      "Average test loss: 0.003296939683664176\n",
      "Epoch 137/300\n",
      "Average training loss: 0.052981738229592644\n",
      "Average test loss: 0.0030443691226343314\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05110163974099689\n",
      "Average test loss: 0.003078265292156074\n",
      "Epoch 139/300\n",
      "Average training loss: 0.050757975505457985\n",
      "Average test loss: 0.0031754426943759123\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05115862681468328\n",
      "Average test loss: 0.0034403283670544622\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05137145435810089\n",
      "Average test loss: 0.0033074128691934876\n",
      "Epoch 142/300\n",
      "Average training loss: 0.051780519783496855\n",
      "Average test loss: 0.00303561236605876\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05085027694371012\n",
      "Average test loss: 0.0031321155027382902\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05088737518588702\n",
      "Average test loss: 0.0032480530860937305\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0510861508945624\n",
      "Average test loss: 0.003198913663211796\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05107379902733697\n",
      "Average test loss: 0.003125944875180721\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05081218349436919\n",
      "Average test loss: 0.0031397131228198608\n",
      "Epoch 148/300\n",
      "Average training loss: 0.050940496274166636\n",
      "Average test loss: 0.0037604770279592937\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05026084197560946\n",
      "Average test loss: 0.0031235447594275077\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05110209988223182\n",
      "Average test loss: 0.0032726250340541206\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05074574264552858\n",
      "Average test loss: 0.0030746389490862685\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05087912443611357\n",
      "Average test loss: 0.0030792999673220846\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05131465397940742\n",
      "Average test loss: 0.0031556515564314194\n",
      "Epoch 154/300\n",
      "Average training loss: 0.049827944424417285\n",
      "Average test loss: 0.003219917322612471\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05071413044134776\n",
      "Average test loss: 0.008989575118654305\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05030591452452871\n",
      "Average test loss: 0.0032760943037768203\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05071959860788451\n",
      "Average test loss: 0.0031383197286890614\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05011300332016415\n",
      "Average test loss: 0.0034210808635171917\n",
      "Epoch 159/300\n",
      "Average training loss: 0.050308260245455635\n",
      "Average test loss: 0.0031000967116819487\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05084019464585516\n",
      "Average test loss: 0.003028519475625621\n",
      "Epoch 161/300\n",
      "Average training loss: 0.050166480902168485\n",
      "Average test loss: 0.003346681009978056\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04971777705020375\n",
      "Average test loss: 0.0030609786373873554\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04977292860878838\n",
      "Average test loss: 0.003240624993004733\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05055520151721107\n",
      "Average test loss: 0.003066548189976149\n",
      "Epoch 165/300\n",
      "Average training loss: 0.049802690896723005\n",
      "Average test loss: 0.003182290135572354\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05066976304186715\n",
      "Average test loss: 0.0030803830484445725\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04975459377302064\n",
      "Average test loss: 0.004658382289111614\n",
      "Epoch 168/300\n",
      "Average training loss: 0.049937406241893766\n",
      "Average test loss: 0.0031517192671696344\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04935552060272959\n",
      "Average test loss: 0.0031980219698614543\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04971475879020161\n",
      "Average test loss: 0.0030696558881964948\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04939821277062098\n",
      "Average test loss: 0.003163440065872338\n",
      "Epoch 172/300\n",
      "Average training loss: 0.051064181708627275\n",
      "Average test loss: 0.0034175894235571225\n",
      "Epoch 173/300\n",
      "Average training loss: 0.049903532303041884\n",
      "Average test loss: 0.0031483471761975025\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05013784470823076\n",
      "Average test loss: 0.0032735632320659027\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05012240129709244\n",
      "Average test loss: 0.0031470158795515696\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04933826440572739\n",
      "Average test loss: 0.0029817335144099264\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04901129231850306\n",
      "Average test loss: 0.003115769373252988\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04940650249520938\n",
      "Average test loss: 0.003107165362685919\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04936145686440998\n",
      "Average test loss: 0.0030732254118969043\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04915737622645166\n",
      "Average test loss: 0.0032469290300375887\n",
      "Epoch 181/300\n",
      "Average training loss: 0.049385827326112325\n",
      "Average test loss: 0.0031311003011133937\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05106248147951232\n",
      "Average test loss: 0.00321218865737319\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04930286418894927\n",
      "Average test loss: 0.003064056502551668\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04866595461302333\n",
      "Average test loss: 0.003464482105233603\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0489953871137566\n",
      "Average test loss: 0.003156537509212891\n",
      "Epoch 186/300\n",
      "Average training loss: 0.048951857146289614\n",
      "Average test loss: 0.0031708321933531097\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04971183167894681\n",
      "Average test loss: 0.0031480837856522866\n",
      "Epoch 188/300\n",
      "Average training loss: 0.049105951868825486\n",
      "Average test loss: 0.0030401981468829845\n",
      "Epoch 189/300\n",
      "Average training loss: 0.049254245479901634\n",
      "Average test loss: 0.0033080667480826376\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04887956988480356\n",
      "Average test loss: 0.0030271841556661658\n",
      "Epoch 191/300\n",
      "Average training loss: 0.048820173260238436\n",
      "Average test loss: 0.003179511112160981\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04898524278402328\n",
      "Average test loss: 0.0030660854313108656\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04876539266109466\n",
      "Average test loss: 0.0032180354456520744\n",
      "Epoch 194/300\n",
      "Average training loss: 0.048867662373516293\n",
      "Average test loss: 0.0033195122045775256\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04862327100170983\n",
      "Average test loss: 0.0030521338497185046\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04878611085481114\n",
      "Average test loss: 0.003151879161596298\n",
      "Epoch 197/300\n",
      "Average training loss: 0.048667775375975504\n",
      "Average test loss: 0.0030625156319389743\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04929456369082133\n",
      "Average test loss: 0.0030821045140425364\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04870839173263974\n",
      "Average test loss: 0.003184251368873649\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04864668195777469\n",
      "Average test loss: 0.0033531759002556403\n",
      "Epoch 201/300\n",
      "Average training loss: 0.048653756726119254\n",
      "Average test loss: 0.004668982094360723\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04829944586422708\n",
      "Average test loss: 0.0030853975241382915\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04856574140323533\n",
      "Average test loss: 0.003195522020881375\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04884051936202579\n",
      "Average test loss: 0.0030168258351170353\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04833233217729462\n",
      "Average test loss: 0.003067913115852409\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04822244008051024\n",
      "Average test loss: 0.003025703661971622\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04868190495504273\n",
      "Average test loss: 0.003071208042610023\n",
      "Epoch 208/300\n",
      "Average training loss: 0.049188103417555494\n",
      "Average test loss: 0.0029899242965297566\n",
      "Epoch 209/300\n",
      "Average training loss: 0.048973637799421946\n",
      "Average test loss: 0.0031285319042702515\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047935852971341876\n",
      "Average test loss: 0.0033166684744258723\n",
      "Epoch 211/300\n",
      "Average training loss: 0.048498397943046355\n",
      "Average test loss: 0.00309121836307976\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0479531328479449\n",
      "Average test loss: 0.003165488059529\n",
      "Epoch 213/300\n",
      "Average training loss: 0.048101383199294405\n",
      "Average test loss: 0.0031516773394412462\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04795359209842152\n",
      "Average test loss: 0.003049282709757487\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04846243927545018\n",
      "Average test loss: 0.0030046888258722093\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04854650575584835\n",
      "Average test loss: 0.0030822270348047215\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04831916875971688\n",
      "Average test loss: 0.003062095161837836\n",
      "Epoch 218/300\n",
      "Average training loss: 0.048024615400367315\n",
      "Average test loss: 0.00315327913645241\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0484909034702513\n",
      "Average test loss: 0.0032376814023074175\n",
      "Epoch 220/300\n",
      "Average training loss: 0.047493287675910524\n",
      "Average test loss: 0.003330895363870594\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04802158544792069\n",
      "Average test loss: 0.0031021315238128106\n",
      "Epoch 222/300\n",
      "Average training loss: 0.047796253499057556\n",
      "Average test loss: 0.0030220544737660222\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04787876951032215\n",
      "Average test loss: 0.0031458723040090667\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04825977728101942\n",
      "Average test loss: 0.0030511946133855317\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0478158572614193\n",
      "Average test loss: 0.0030983949038717483\n",
      "Epoch 226/300\n",
      "Average training loss: 0.047796434928973515\n",
      "Average test loss: 0.003412588060316112\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0477106792959902\n",
      "Average test loss: 0.006921034156448311\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0480804985165596\n",
      "Average test loss: 0.0035855424162000416\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04770897225538889\n",
      "Average test loss: 0.003305040989898973\n",
      "Epoch 230/300\n",
      "Average training loss: 0.048193797343307074\n",
      "Average test loss: 0.0030159091494149633\n",
      "Epoch 231/300\n",
      "Average training loss: 0.047452869060966706\n",
      "Average test loss: 0.0033215603691836198\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04774049348963631\n",
      "Average test loss: 0.003451059883667363\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04778624488247765\n",
      "Average test loss: 0.0029895069292849965\n",
      "Epoch 234/300\n",
      "Average training loss: 0.047887464261717264\n",
      "Average test loss: 0.0032058424519168004\n",
      "Epoch 235/300\n",
      "Average training loss: 0.047701784465048046\n",
      "Average test loss: 0.003239600283404191\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04753254899713728\n",
      "Average test loss: 0.003188206120911572\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04747483518719673\n",
      "Average test loss: 0.00331161116850045\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04764295744564798\n",
      "Average test loss: 0.0030709250124378337\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04749885417686568\n",
      "Average test loss: 0.003014944422990084\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04751448513401879\n",
      "Average test loss: 0.00311880133052667\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04724075862434175\n",
      "Average test loss: 0.003016224129953318\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04746890499856737\n",
      "Average test loss: 0.003262399377094375\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04746712056795756\n",
      "Average test loss: 0.00309647002360887\n",
      "Epoch 244/300\n",
      "Average training loss: 0.047197228597270116\n",
      "Average test loss: 0.013498367314537366\n",
      "Epoch 245/300\n",
      "Average training loss: 0.047383871797058316\n",
      "Average test loss: 0.0030867217992328936\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04729034454954995\n",
      "Average test loss: 0.0030028888079234295\n",
      "Epoch 247/300\n",
      "Average training loss: 0.047294236537483006\n",
      "Average test loss: 0.0030673845012982688\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04700398264328639\n",
      "Average test loss: 0.0033324609249830244\n",
      "Epoch 249/300\n",
      "Average training loss: 0.047158771637413235\n",
      "Average test loss: 0.0030594430067059065\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04699987821777662\n",
      "Average test loss: 0.0033104469053861166\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04697989928391245\n",
      "Average test loss: 0.0033256036962072055\n",
      "Epoch 252/300\n",
      "Average training loss: 0.047262402365605034\n",
      "Average test loss: 0.003121094288511409\n",
      "Epoch 253/300\n",
      "Average training loss: 0.047466297924518584\n",
      "Average test loss: 0.0030569599831683767\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04674694704347187\n",
      "Average test loss: 0.0032246040410051744\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04741803194416894\n",
      "Average test loss: 0.0030395762026309966\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04702567335300975\n",
      "Average test loss: 0.0034776860672152705\n",
      "Epoch 257/300\n",
      "Average training loss: 0.046638087428278394\n",
      "Average test loss: 0.0030745893377396795\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04701647823055585\n",
      "Average test loss: 0.03112659734653102\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04679002719786432\n",
      "Average test loss: 0.0030104942319707736\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04692547629608048\n",
      "Average test loss: 0.003076795074260897\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0472334203587638\n",
      "Average test loss: 0.0030191734180682237\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0471836724281311\n",
      "Average test loss: 0.0030941846892237664\n",
      "Epoch 263/300\n",
      "Average training loss: 0.046864482627974616\n",
      "Average test loss: 0.0030014586864660184\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04665154375467036\n",
      "Average test loss: 0.0031014397968020706\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04663883164525032\n",
      "Average test loss: 0.0030817033898913196\n",
      "Epoch 266/300\n",
      "Average training loss: 0.046848556611273025\n",
      "Average test loss: 0.004772461894899607\n",
      "Epoch 267/300\n",
      "Average training loss: 0.047279562152094314\n",
      "Average test loss: 0.003134748471279939\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04651583538783921\n",
      "Average test loss: 0.003076924931257963\n",
      "Epoch 269/300\n",
      "Average training loss: 0.047096381651030646\n",
      "Average test loss: 0.0032279127589944336\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04654376003146172\n",
      "Average test loss: 0.0031835135567105477\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04649179212252299\n",
      "Average test loss: 0.003155355761655503\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04677476978633139\n",
      "Average test loss: 0.003154929716553953\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04650338694784376\n",
      "Average test loss: 0.00311118248042961\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04678168928954336\n",
      "Average test loss: 0.003246303650447064\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04652183561192619\n",
      "Average test loss: 0.0033973842962748474\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04657610648870468\n",
      "Average test loss: 0.0030855652288430267\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04668612251182397\n",
      "Average test loss: 0.003101260928023193\n",
      "Epoch 278/300\n",
      "Average training loss: 0.046227859844764074\n",
      "Average test loss: 0.003083057741117146\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04645530559619268\n",
      "Average test loss: 0.0030334026935613817\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04651622720559438\n",
      "Average test loss: 0.0032360464576631784\n",
      "Epoch 281/300\n",
      "Average training loss: 0.046299963778919644\n",
      "Average test loss: 0.00309452509548929\n",
      "Epoch 282/300\n",
      "Average training loss: 0.046546014222833845\n",
      "Average test loss: 0.00313952375555204\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04656371123757627\n",
      "Average test loss: 0.0034042030622561774\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04625204156504737\n",
      "Average test loss: 0.0031321446069826685\n",
      "Epoch 285/300\n",
      "Average training loss: 0.046387577652931215\n",
      "Average test loss: 0.0030772037861040895\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04604353961017397\n",
      "Average test loss: 0.0029566838685423135\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04629625937011507\n",
      "Average test loss: 0.0031979036446039876\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04645742759770817\n",
      "Average test loss: 0.00311477783943216\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04619023773405287\n",
      "Average test loss: 0.003185366699885991\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04612571229537328\n",
      "Average test loss: 0.0030570234252760807\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04666484829121166\n",
      "Average test loss: 0.0030404136160181628\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04617151390843921\n",
      "Average test loss: 0.003481330583906836\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04635268714527289\n",
      "Average test loss: 0.003080591153767374\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04601829047666656\n",
      "Average test loss: 0.0030269140427311263\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04614570980105135\n",
      "Average test loss: 0.0031127786460436054\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04617566307385763\n",
      "Average test loss: 0.003164742017371787\n",
      "Epoch 297/300\n",
      "Average training loss: 0.045952472749683594\n",
      "Average test loss: 0.004183770930808452\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04624543994665146\n",
      "Average test loss: 0.0030328426402476097\n",
      "Epoch 299/300\n",
      "Average training loss: 0.045924783796072004\n",
      "Average test loss: 0.0030790879771941237\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04628786840372615\n",
      "Average test loss: 0.003079432973638177\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6697998311784532\n",
      "Average test loss: 0.007136004538171821\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5657980137401157\n",
      "Average test loss: 0.00920382914526595\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3738559789922502\n",
      "Average test loss: 0.004712158565926883\n",
      "Epoch 4/300\n",
      "Average training loss: 0.28498371775945025\n",
      "Average test loss: 0.004469752228094472\n",
      "Epoch 5/300\n",
      "Average training loss: 0.22966539563073052\n",
      "Average test loss: 0.004507542214045922\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1988419784704844\n",
      "Average test loss: 0.0041370596471760005\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1671993166340722\n",
      "Average test loss: 0.004757800496286816\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15393130099773408\n",
      "Average test loss: 0.003985132818213767\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13881255945894452\n",
      "Average test loss: 0.0036374647008876005\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12681610112720065\n",
      "Average test loss: 0.0033948017025573385\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11991946304506726\n",
      "Average test loss: 0.004010538379765219\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1109025041129854\n",
      "Average test loss: 0.007303314246237278\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10549699634975858\n",
      "Average test loss: 0.003181270429140164\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10090780936347114\n",
      "Average test loss: 0.014258435912430286\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0947908705804083\n",
      "Average test loss: 0.004477796856313944\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09090348884132174\n",
      "Average test loss: 0.0034802322963045705\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08694037886460622\n",
      "Average test loss: 0.003555539680437909\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0862349326842361\n",
      "Average test loss: 0.0029938016749090617\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07931425787342919\n",
      "Average test loss: 0.002559769156699379\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07784624712003602\n",
      "Average test loss: 0.0027316984797103538\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07622134388155408\n",
      "Average test loss: 0.002963006666344073\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07348402192195257\n",
      "Average test loss: 0.0026339473188337352\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06938387012150553\n",
      "Average test loss: 0.002775128167329563\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06716755775610606\n",
      "Average test loss: 0.0031468624654743408\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06486150622367859\n",
      "Average test loss: 0.002589941412831346\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06366902540127437\n",
      "Average test loss: 0.0026137429593751827\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06194401725795534\n",
      "Average test loss: 0.002833327965500454\n",
      "Epoch 28/300\n",
      "Average training loss: 0.058943748070134055\n",
      "Average test loss: 0.002762555146589875\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05860742211341858\n",
      "Average test loss: 0.002365564917317695\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05706404846575525\n",
      "Average test loss: 0.0023790933045869074\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05481959956553247\n",
      "Average test loss: 0.0027822060183518463\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05468616396188736\n",
      "Average test loss: 0.0023396220542490484\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05312702559431394\n",
      "Average test loss: 0.0025752946399152277\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05323751770456632\n",
      "Average test loss: 0.003135721307868759\n",
      "Epoch 35/300\n",
      "Average training loss: 0.051379026343425116\n",
      "Average test loss: 0.002265330055521594\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05024172638853391\n",
      "Average test loss: 0.0022562260737435686\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05073586652345127\n",
      "Average test loss: 0.00251230748804907\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04970006534788344\n",
      "Average test loss: 0.0027121083076215454\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04851474966936641\n",
      "Average test loss: 0.002685925271362066\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04912047805388769\n",
      "Average test loss: 0.0022307753487386638\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04832741674780846\n",
      "Average test loss: 0.002454921559120218\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04764086518022749\n",
      "Average test loss: 0.002284912286326289\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04721435288257069\n",
      "Average test loss: 0.0021915027902772027\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04632612160510487\n",
      "Average test loss: 0.002181236673767368\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0465749537481202\n",
      "Average test loss: 0.0024085711484981907\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04554660941163699\n",
      "Average test loss: 0.0023229625954603156\n",
      "Epoch 47/300\n",
      "Average training loss: 0.045624698950184714\n",
      "Average test loss: 0.002225104657519195\n",
      "Epoch 48/300\n",
      "Average training loss: 0.044908753633499145\n",
      "Average test loss: 0.0022181186336610054\n",
      "Epoch 49/300\n",
      "Average training loss: 0.045169840208358236\n",
      "Average test loss: 0.0024810758938805925\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0443847677608331\n",
      "Average test loss: 0.002095820209839278\n",
      "Epoch 51/300\n",
      "Average training loss: 0.044787719355689155\n",
      "Average test loss: 0.0022259112192938727\n",
      "Epoch 52/300\n",
      "Average training loss: 0.043830571817027195\n",
      "Average test loss: 0.0025643245824095275\n",
      "Epoch 53/300\n",
      "Average training loss: 0.044246137251456576\n",
      "Average test loss: 0.002200435823036565\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04346699768967099\n",
      "Average test loss: 0.002243366373909844\n",
      "Epoch 55/300\n",
      "Average training loss: 0.044232669681310656\n",
      "Average test loss: 0.0022175886165350677\n",
      "Epoch 56/300\n",
      "Average training loss: 0.043266887684663136\n",
      "Average test loss: 0.0021755911647859546\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04287524205115106\n",
      "Average test loss: 0.002157248278458913\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04255605517162217\n",
      "Average test loss: 0.002319362013704247\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0427131050394641\n",
      "Average test loss: 0.0022187900993352137\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04238416372074021\n",
      "Average test loss: 0.0020494317300617693\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04187524688243866\n",
      "Average test loss: 0.002068194109015167\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04330644106202655\n",
      "Average test loss: 0.0024097264918188253\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04215137095914947\n",
      "Average test loss: 0.002319468988105655\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04179785100950135\n",
      "Average test loss: 0.002174290631380346\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04129281769196192\n",
      "Average test loss: 0.0027712884572231107\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04125924784607357\n",
      "Average test loss: 0.002782047843767537\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04198055048121346\n",
      "Average test loss: 0.0022071178913530376\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04305088137586911\n",
      "Average test loss: 0.0021327010558711158\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04203469411697652\n",
      "Average test loss: 0.0029923564592997235\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04107437080807156\n",
      "Average test loss: 0.002336841186508536\n",
      "Epoch 71/300\n",
      "Average training loss: 0.040613991452587976\n",
      "Average test loss: 0.0022187109115637013\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04090156592097547\n",
      "Average test loss: 0.0024153428619934454\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04098378218544854\n",
      "Average test loss: 0.0027470712806615563\n",
      "Epoch 74/300\n",
      "Average training loss: 0.041332854943142996\n",
      "Average test loss: 0.0021747671578907306\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0405844758119848\n",
      "Average test loss: 0.002157252232854565\n",
      "Epoch 76/300\n",
      "Average training loss: 0.041301605138513775\n",
      "Average test loss: 0.002081807070929143\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04011643348468674\n",
      "Average test loss: 0.002031601709831092\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039892544405327905\n",
      "Average test loss: 0.002154185349949532\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04047000332011117\n",
      "Average test loss: 0.0020223374244653517\n",
      "Epoch 80/300\n",
      "Average training loss: 0.039981368690729144\n",
      "Average test loss: 0.00217619124862055\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04061788455148538\n",
      "Average test loss: 0.0022175523799119723\n",
      "Epoch 82/300\n",
      "Average training loss: 0.040081345023380385\n",
      "Average test loss: 0.002191117309965193\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04032225720087687\n",
      "Average test loss: 0.0022714525647461415\n",
      "Epoch 84/300\n",
      "Average training loss: 0.040154984346694414\n",
      "Average test loss: 0.0021520413476973773\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03966707274317741\n",
      "Average test loss: 0.002015608709719446\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03929633421699206\n",
      "Average test loss: 0.001981386703128616\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03921784181230598\n",
      "Average test loss: 0.0021755886228962077\n",
      "Epoch 88/300\n",
      "Average training loss: 0.039632962831192546\n",
      "Average test loss: 0.0020766087665946946\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03933145829041799\n",
      "Average test loss: 0.00198799057242771\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03961984035703871\n",
      "Average test loss: 0.0023239727887428467\n",
      "Epoch 91/300\n",
      "Average training loss: 0.039281307693984774\n",
      "Average test loss: 0.0020057243435747094\n",
      "Epoch 92/300\n",
      "Average training loss: 0.039347131659587226\n",
      "Average test loss: 0.002123909912899964\n",
      "Epoch 93/300\n",
      "Average training loss: 0.038769902186261286\n",
      "Average test loss: 0.0020079533306674824\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03925899174478319\n",
      "Average test loss: 0.0019624265537907678\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03882296172446675\n",
      "Average test loss: 0.0020575868555655083\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03884483524494701\n",
      "Average test loss: 0.001999787894698481\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03897535502579477\n",
      "Average test loss: 0.0020717342355185083\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03891752605968051\n",
      "Average test loss: 0.0028064699375794995\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03872359000974231\n",
      "Average test loss: 0.0020139732639201813\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03837466741932763\n",
      "Average test loss: 0.0019646817549235293\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03833175520764457\n",
      "Average test loss: 0.0022367625468307072\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03849462604853842\n",
      "Average test loss: 0.0023458751315871876\n",
      "Epoch 103/300\n",
      "Average training loss: 0.038608901037110226\n",
      "Average test loss: 0.0022826895152943004\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03913695754276381\n",
      "Average test loss: 0.002162114414593412\n",
      "Epoch 105/300\n",
      "Average training loss: 0.038766672313213346\n",
      "Average test loss: 0.0019782889648858044\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03813188490933842\n",
      "Average test loss: 0.002252080571320322\n",
      "Epoch 107/300\n",
      "Average training loss: 0.038955588453345825\n",
      "Average test loss: 0.0020687352878352005\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03838105927904447\n",
      "Average test loss: 0.0019724677343749336\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03836722022957272\n",
      "Average test loss: 0.002261581189619998\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03797607697712051\n",
      "Average test loss: 0.002215325822846757\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03827549421456125\n",
      "Average test loss: 0.0019976730938586922\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03753673774169551\n",
      "Average test loss: 0.002039162972735034\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03793251647882991\n",
      "Average test loss: 0.0019320472706523208\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03833536470929782\n",
      "Average test loss: 0.002090037053450942\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03780790311760373\n",
      "Average test loss: 0.0023959605453742873\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03838035962647862\n",
      "Average test loss: 0.0020877848712520467\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03828242224289311\n",
      "Average test loss: 0.0020020969570096994\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03763972798652119\n",
      "Average test loss: 0.002011300070832173\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03748786126242744\n",
      "Average test loss: 0.00204118129859368\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03743127021359073\n",
      "Average test loss: 0.0019966673759950534\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03725927061835925\n",
      "Average test loss: 0.0020625053379270766\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03789216148191028\n",
      "Average test loss: 0.0027003964742438663\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03771335541539722\n",
      "Average test loss: 0.001977297867337863\n",
      "Epoch 124/300\n",
      "Average training loss: 0.037337233742078146\n",
      "Average test loss: 0.0021909514589028224\n",
      "Epoch 125/300\n",
      "Average training loss: 0.037691365384393265\n",
      "Average test loss: 0.0020932843266054987\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03741157963871956\n",
      "Average test loss: 0.0020064854154156313\n",
      "Epoch 127/300\n",
      "Average training loss: 0.037851492275794346\n",
      "Average test loss: 0.0020528113081430396\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03707384055190616\n",
      "Average test loss: 0.0020836272032724486\n",
      "Epoch 129/300\n",
      "Average training loss: 0.037383328845103585\n",
      "Average test loss: 0.002056916189690431\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03732683457599746\n",
      "Average test loss: 0.0020768425592945683\n",
      "Epoch 131/300\n",
      "Average training loss: 0.037044043716457156\n",
      "Average test loss: 0.0020778500966520774\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0369962897648414\n",
      "Average test loss: 0.0024644506524006526\n",
      "Epoch 133/300\n",
      "Average training loss: 0.036971106585529115\n",
      "Average test loss: 0.0020264812147037852\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03686423366599613\n",
      "Average test loss: 0.0026114975698292254\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03721878976457649\n",
      "Average test loss: 0.0023794590750088294\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03699915772676468\n",
      "Average test loss: 0.003545107631219758\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03668568093743589\n",
      "Average test loss: 0.002029244642911686\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03700513996349441\n",
      "Average test loss: 0.00219312916468415\n",
      "Epoch 139/300\n",
      "Average training loss: 0.036530927237537175\n",
      "Average test loss: 0.0019886722440520923\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03635647377371788\n",
      "Average test loss: 0.016467286759780514\n",
      "Epoch 141/300\n",
      "Average training loss: 0.037825143926673466\n",
      "Average test loss: 0.002448557237163186\n",
      "Epoch 142/300\n",
      "Average training loss: 0.036863250623146694\n",
      "Average test loss: 0.0020304144106598363\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03663521016968621\n",
      "Average test loss: 0.0020440248420668973\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03631083813640806\n",
      "Average test loss: 0.002015608473887874\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03642972090509203\n",
      "Average test loss: 0.0020539836866988077\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03696206402447488\n",
      "Average test loss: 0.0031002255411197743\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03632785898447037\n",
      "Average test loss: 0.002004660068700711\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03654938002096282\n",
      "Average test loss: 0.002046695043022434\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03625973812407918\n",
      "Average test loss: 0.0020282795573067333\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03642649351888233\n",
      "Average test loss: 0.0020809723599296477\n",
      "Epoch 151/300\n",
      "Average training loss: 0.036251420381996365\n",
      "Average test loss: 0.0021444164990550942\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03633646567993694\n",
      "Average test loss: 0.0023580463566920825\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0363472732702891\n",
      "Average test loss: 0.0022440542715291183\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03613385468390253\n",
      "Average test loss: 0.00196256494615227\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03667318224244648\n",
      "Average test loss: 0.0020054853630976544\n",
      "Epoch 156/300\n",
      "Average training loss: 0.037012414948807824\n",
      "Average test loss: 0.002060856898832652\n",
      "Epoch 157/300\n",
      "Average training loss: 0.036657248980469175\n",
      "Average test loss: 0.002051323235862785\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03585929071240955\n",
      "Average test loss: 0.00193054925215741\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03595092306700018\n",
      "Average test loss: 0.01498338855140739\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03611334741446707\n",
      "Average test loss: 0.001979586904558043\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0361466902229521\n",
      "Average test loss: 0.001957783988573485\n",
      "Epoch 162/300\n",
      "Average training loss: 0.036500109331475364\n",
      "Average test loss: 0.0021902564983401034\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03587161809868283\n",
      "Average test loss: 0.002022567708252205\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03592090994119644\n",
      "Average test loss: 0.0019623171037269965\n",
      "Epoch 165/300\n",
      "Average training loss: 0.036021775881449385\n",
      "Average test loss: 0.0022192276127429474\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03591212801966402\n",
      "Average test loss: 0.001960616543371644\n",
      "Epoch 167/300\n",
      "Average training loss: 0.035649979763560824\n",
      "Average test loss: 0.002238208638918069\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03612040353152487\n",
      "Average test loss: 0.0019180639361341795\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03593009578022692\n",
      "Average test loss: 0.0019145546141597959\n",
      "Epoch 170/300\n",
      "Average training loss: 0.038621140602562165\n",
      "Average test loss: 0.001983714294412898\n",
      "Epoch 171/300\n",
      "Average training loss: 0.035886308398511674\n",
      "Average test loss: 0.00207220787430803\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03642286940084563\n",
      "Average test loss: 0.0022508836595548525\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03557446008755101\n",
      "Average test loss: 0.002460981205933624\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03643192921744452\n",
      "Average test loss: 0.0020400923828904826\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03575059782796436\n",
      "Average test loss: 0.0021841521782593595\n",
      "Epoch 176/300\n",
      "Average training loss: 0.035782284317745106\n",
      "Average test loss: 0.0019873052965849636\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0358622495730718\n",
      "Average test loss: 0.0019193944375341138\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03558687198824353\n",
      "Average test loss: 0.0019189950171858072\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0360141879287031\n",
      "Average test loss: 0.0109577259214388\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03600239254037539\n",
      "Average test loss: 0.0030388010268410045\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03565480080081357\n",
      "Average test loss: 0.002052259133197367\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03529349844985538\n",
      "Average test loss: 0.0021474788108219703\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03521928760740492\n",
      "Average test loss: 0.0019644483540000187\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03556643414828512\n",
      "Average test loss: 0.0020239361096173526\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03559130020605193\n",
      "Average test loss: 0.001959035434242752\n",
      "Epoch 186/300\n",
      "Average training loss: 0.036356243077251646\n",
      "Average test loss: 0.0019542531705357964\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03559145047267278\n",
      "Average test loss: 0.0018868955478279128\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0355754469037056\n",
      "Average test loss: 0.0019315076985706886\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03556338902645641\n",
      "Average test loss: 0.0022925757850623795\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03555177440245946\n",
      "Average test loss: 0.002048090238745014\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03533984155125088\n",
      "Average test loss: 0.003126643136764566\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0352316576557027\n",
      "Average test loss: 0.0020422189137380984\n",
      "Epoch 193/300\n",
      "Average training loss: 0.034987300554911296\n",
      "Average test loss: 0.0019022180856102043\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03535344521204631\n",
      "Average test loss: 0.001885750946899255\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03535595698489083\n",
      "Average test loss: 0.002161264507513907\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03542105173402362\n",
      "Average test loss: 0.0019476102257354392\n",
      "Epoch 197/300\n",
      "Average training loss: 0.035255634152226975\n",
      "Average test loss: 0.002048615643547641\n",
      "Epoch 198/300\n",
      "Average training loss: 0.034800843990511365\n",
      "Average test loss: 0.002036678676286505\n",
      "Epoch 199/300\n",
      "Average training loss: 0.035358609331978694\n",
      "Average test loss: 0.0021775025930255653\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03516387939453125\n",
      "Average test loss: 0.0021197954591156707\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03609625435206625\n",
      "Average test loss: 0.002145097848234905\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03482635384135776\n",
      "Average test loss: 0.005314469729032782\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03492653799719281\n",
      "Average test loss: 0.002037887676256812\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03493776026037004\n",
      "Average test loss: 0.001994349526655343\n",
      "Epoch 205/300\n",
      "Average training loss: 0.035072520166635514\n",
      "Average test loss: 0.0019689526698655553\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0349013326631652\n",
      "Average test loss: 0.0020757830476181376\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03569749761041668\n",
      "Average test loss: 0.002645891957088477\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03545421135094431\n",
      "Average test loss: 0.0019183308631181718\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03497804715567165\n",
      "Average test loss: 0.001941072556293673\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0350074959927135\n",
      "Average test loss: 0.0019840761356883577\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03486676436331537\n",
      "Average test loss: 0.0019691197122964593\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03477603968150086\n",
      "Average test loss: 0.0020737547802014483\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03492742917107211\n",
      "Average test loss: 0.0020735299568623304\n",
      "Epoch 214/300\n",
      "Average training loss: 0.034911403892768755\n",
      "Average test loss: 0.001982622422898809\n",
      "Epoch 215/300\n",
      "Average training loss: 0.034485237490799694\n",
      "Average test loss: 0.002035454238454501\n",
      "Epoch 216/300\n",
      "Average training loss: 0.034461505459414586\n",
      "Average test loss: 0.001955916633622514\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03454665223426289\n",
      "Average test loss: 0.0022556740039338667\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03477951445513301\n",
      "Average test loss: 0.002138630916364491\n",
      "Epoch 219/300\n",
      "Average training loss: 0.034701499346229764\n",
      "Average test loss: 0.0021052326717310482\n",
      "Epoch 220/300\n",
      "Average training loss: 0.034744729770554435\n",
      "Average test loss: 0.0025071406707995467\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03528849090801345\n",
      "Average test loss: 0.0019316872158605192\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03486088537838724\n",
      "Average test loss: 0.0019303692820378476\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0346712943845325\n",
      "Average test loss: 0.0019725847145956425\n",
      "Epoch 224/300\n",
      "Average training loss: 0.034279678278499176\n",
      "Average test loss: 0.0020440719727840687\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03449703222016493\n",
      "Average test loss: 0.0021022624387923216\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03435059003035228\n",
      "Average test loss: 0.0019178715534508227\n",
      "Epoch 227/300\n",
      "Average training loss: 0.034339420361651316\n",
      "Average test loss: 0.0021833730410370563\n",
      "Epoch 228/300\n",
      "Average training loss: 0.034240582529041504\n",
      "Average test loss: 0.0020116427816036673\n",
      "Epoch 229/300\n",
      "Average training loss: 0.034429846798380216\n",
      "Average test loss: 0.0020011614486575126\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03463933663566907\n",
      "Average test loss: 0.001974346971553233\n",
      "Epoch 231/300\n",
      "Average training loss: 0.034468043986294\n",
      "Average test loss: 0.0019407863238205512\n",
      "Epoch 232/300\n",
      "Average training loss: 0.035600298868285286\n",
      "Average test loss: 0.0018961976900075874\n",
      "Epoch 233/300\n",
      "Average training loss: 0.034703211632039814\n",
      "Average test loss: 0.002249249905968706\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03451348152425554\n",
      "Average test loss: 0.0018967396130578386\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03431397718191147\n",
      "Average test loss: 0.0019418855537143018\n",
      "Epoch 236/300\n",
      "Average training loss: 0.034454864892694684\n",
      "Average test loss: 0.0019250245846083596\n",
      "Epoch 237/300\n",
      "Average training loss: 0.034169278944532075\n",
      "Average test loss: 0.0020655173282656406\n",
      "Epoch 238/300\n",
      "Average training loss: 0.034528356426292\n",
      "Average test loss: 0.0020106842294335364\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03435681498381827\n",
      "Average test loss: 0.0019150523385033011\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03449184082614051\n",
      "Average test loss: 0.001969244279588262\n",
      "Epoch 241/300\n",
      "Average training loss: 0.034157884745134245\n",
      "Average test loss: 0.0019760650707822706\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03418363510734505\n",
      "Average test loss: 0.002028080720040533\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03392095918456713\n",
      "Average test loss: 0.002058705484494567\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03421079145703051\n",
      "Average test loss: 0.0024660705654985376\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03503268701500363\n",
      "Average test loss: 0.0020660703780336514\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03413918601142035\n",
      "Average test loss: 0.002056498503105508\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03438030075033506\n",
      "Average test loss: 0.002032070607257386\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03410784863597817\n",
      "Average test loss: 0.0020134808484775324\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03427700684302383\n",
      "Average test loss: 0.0021233132001426485\n",
      "Epoch 250/300\n",
      "Average training loss: 0.033772577752669654\n",
      "Average test loss: 0.0020680660971750817\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03464928725030687\n",
      "Average test loss: 0.0019217925286955304\n",
      "Epoch 252/300\n",
      "Average training loss: 0.034126300166050595\n",
      "Average test loss: 0.002144129409765204\n",
      "Epoch 253/300\n",
      "Average training loss: 0.034131332473622426\n",
      "Average test loss: 0.0019965202719904483\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03450617511736022\n",
      "Average test loss: 0.003926386437896225\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03373558321429623\n",
      "Average test loss: 0.03714225778563155\n",
      "Epoch 256/300\n",
      "Average training loss: 0.033892165874441466\n",
      "Average test loss: 0.0021408193970306054\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03442227845390638\n",
      "Average test loss: 0.0020195584655221966\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03357289238108529\n",
      "Average test loss: 0.001971556027316385\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03376231777667999\n",
      "Average test loss: 0.0020463438394169014\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03402828206949764\n",
      "Average test loss: 0.0021181671782914135\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03432822713918156\n",
      "Average test loss: 0.001958917297422886\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03382902104324765\n",
      "Average test loss: 0.016752101028958955\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034118130620982914\n",
      "Average test loss: 0.0021405464261770248\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03399397570722633\n",
      "Average test loss: 0.0019049090636480185\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03419947954018911\n",
      "Average test loss: 0.0019374640226985018\n",
      "Epoch 266/300\n",
      "Average training loss: 0.033878031498856015\n",
      "Average test loss: 0.0019239519314012594\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03359947977794541\n",
      "Average test loss: 0.0019323854538508588\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03394682915674316\n",
      "Average test loss: 0.0022099300507042144\n",
      "Epoch 269/300\n",
      "Average training loss: 0.033860251131984924\n",
      "Average test loss: 0.0021898592803627254\n",
      "Epoch 270/300\n",
      "Average training loss: 0.033575321594874065\n",
      "Average test loss: 0.001980327971176141\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03355749833583832\n",
      "Average test loss: 0.0019020618576970365\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03357429511513975\n",
      "Average test loss: 0.002222445279877219\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0336808358364635\n",
      "Average test loss: 0.0020297717646592193\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03356535219815042\n",
      "Average test loss: 0.002042385107216736\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03355400946570767\n",
      "Average test loss: 0.0020934722206244864\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03380495237973001\n",
      "Average test loss: 0.0019965123947088916\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03383530078331629\n",
      "Average test loss: 0.0020558677214301295\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03380483524998029\n",
      "Average test loss: 0.001968012463715341\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03349225555857022\n",
      "Average test loss: 0.002838108647304277\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0341788822611173\n",
      "Average test loss: 0.001988431385821766\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03386382672521803\n",
      "Average test loss: 0.0019484519124444988\n",
      "Epoch 282/300\n",
      "Average training loss: 0.033338849080933465\n",
      "Average test loss: 0.0036431589594317806\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03352076226472855\n",
      "Average test loss: 0.0019207507483661175\n",
      "Epoch 284/300\n",
      "Average training loss: 0.033244938323895135\n",
      "Average test loss: 0.0019489369723531935\n",
      "Epoch 285/300\n",
      "Average training loss: 0.033575263435641926\n",
      "Average test loss: 0.0020000045930759773\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03357008715470632\n",
      "Average test loss: 0.0020028492514457966\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03366799274252521\n",
      "Average test loss: 0.0021269579593920047\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03329885597527027\n",
      "Average test loss: 0.0020074782409808703\n",
      "Epoch 289/300\n",
      "Average training loss: 0.033543458281291856\n",
      "Average test loss: 0.0020854940982535483\n",
      "Epoch 290/300\n",
      "Average training loss: 0.033614682194259435\n",
      "Average test loss: 0.0021779253151681687\n",
      "Epoch 291/300\n",
      "Average training loss: 0.033594364805354014\n",
      "Average test loss: 0.001941950579587784\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03350591773788134\n",
      "Average test loss: 0.00201868380068077\n",
      "Epoch 293/300\n",
      "Average training loss: 0.033294542365603974\n",
      "Average test loss: 0.0019692906278909908\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03350531614157889\n",
      "Average test loss: 0.0019438392453723485\n",
      "Epoch 295/300\n",
      "Average training loss: 0.033740933613644705\n",
      "Average test loss: 0.00197597357372029\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03379278132319451\n",
      "Average test loss: 0.001927775569881002\n",
      "Epoch 297/300\n",
      "Average training loss: 0.033451899704005986\n",
      "Average test loss: 0.002087683431390259\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0332567337916957\n",
      "Average test loss: 0.001979218072930558\n",
      "Epoch 299/300\n",
      "Average training loss: 0.033128016776508756\n",
      "Average test loss: 0.001973491539971696\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03348063979049524\n",
      "Average test loss: 0.001998497923836112\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.3027694977654352\n",
      "Average test loss: 0.007125290152099398\n",
      "Epoch 2/300\n",
      "Average training loss: 0.3958422716193729\n",
      "Average test loss: 0.00987027114050256\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2624282897843255\n",
      "Average test loss: 0.0045614317564500705\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1977275530497233\n",
      "Average test loss: 0.005005196641302771\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1663131046295166\n",
      "Average test loss: 0.004378799606735507\n",
      "Epoch 6/300\n",
      "Average training loss: 0.14060124126407836\n",
      "Average test loss: 0.003724997225734923\n",
      "Epoch 7/300\n",
      "Average training loss: 0.12438249841001299\n",
      "Average test loss: 0.003971914012812906\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1130176045232349\n",
      "Average test loss: 0.002924748350555698\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10443249230252372\n",
      "Average test loss: 0.003437796374368999\n",
      "Epoch 10/300\n",
      "Average training loss: 0.09645852099524604\n",
      "Average test loss: 0.012107757002943092\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09069707569148805\n",
      "Average test loss: 0.002496376351142923\n",
      "Epoch 12/300\n",
      "Average training loss: 0.08550555037789874\n",
      "Average test loss: 0.00258957246856557\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08211539879110125\n",
      "Average test loss: 0.0026532694173769817\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07748527503675885\n",
      "Average test loss: 0.002904279840696189\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07439321235153411\n",
      "Average test loss: 0.0026135853779398733\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07025046827726894\n",
      "Average test loss: 0.0020442512618998685\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06588474566075538\n",
      "Average test loss: 0.0024690367177956636\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06587119281291962\n",
      "Average test loss: 0.0031973914408849344\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0624798878133297\n",
      "Average test loss: 0.0022002054336998198\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05861527625719706\n",
      "Average test loss: 0.0020223301691520545\n",
      "Epoch 21/300\n",
      "Average training loss: 0.058337660315963956\n",
      "Average test loss: 0.0029821156391666996\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05504612099130948\n",
      "Average test loss: 0.0018017215441084571\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05285535894499885\n",
      "Average test loss: 0.002088764939457178\n",
      "Epoch 24/300\n",
      "Average training loss: 0.050849262913068134\n",
      "Average test loss: 0.0017341656637274556\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05176866448587841\n",
      "Average test loss: 0.0017546587041061786\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04715546368890339\n",
      "Average test loss: 0.0018615770780791839\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04702635701994101\n",
      "Average test loss: 0.001697509203106165\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04492751572860612\n",
      "Average test loss: 0.0016953577072256142\n",
      "Epoch 29/300\n",
      "Average training loss: 0.043430582450495823\n",
      "Average test loss: 0.0016022095045902663\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04752434668607182\n",
      "Average test loss: 0.0016945260951502456\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0436772384610441\n",
      "Average test loss: 0.0019483801919139095\n",
      "Epoch 32/300\n",
      "Average training loss: 0.041256254689561\n",
      "Average test loss: 0.0017098546371691757\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04088944951030943\n",
      "Average test loss: 0.0016548933369211025\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04035865441461404\n",
      "Average test loss: 0.001548616376808948\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04082201041943497\n",
      "Average test loss: 0.0018135480166723331\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03903317936592632\n",
      "Average test loss: 0.0016288386163198286\n",
      "Epoch 37/300\n",
      "Average training loss: 0.038456006318330765\n",
      "Average test loss: 0.0016693452242761851\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0385115168955591\n",
      "Average test loss: 0.0015756067332501212\n",
      "Epoch 39/300\n",
      "Average training loss: 0.038351856158839334\n",
      "Average test loss: 0.0016777340148886044\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04209312779042456\n",
      "Average test loss: 0.001564696822522415\n",
      "Epoch 41/300\n",
      "Average training loss: 0.037207177350918455\n",
      "Average test loss: 0.0019327979435523352\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03705418995022774\n",
      "Average test loss: 0.0015794412933496965\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03708886198533906\n",
      "Average test loss: 0.0015734032331448462\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03611530058913761\n",
      "Average test loss: 0.001663050409199463\n",
      "Epoch 45/300\n",
      "Average training loss: 0.035926314685079785\n",
      "Average test loss: 0.0015421232966085275\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03581081389718586\n",
      "Average test loss: 0.001571227694137229\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03523621560798751\n",
      "Average test loss: 0.001620036327900986\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0349473773141702\n",
      "Average test loss: 0.002452033796451158\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03585288172629145\n",
      "Average test loss: 0.001929545226506889\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03674460271663136\n",
      "Average test loss: 0.0017873202173246277\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03428333998388714\n",
      "Average test loss: 0.0017746568887184063\n",
      "Epoch 52/300\n",
      "Average training loss: 0.033889168365134135\n",
      "Average test loss: 0.0015732124634087086\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03458200104369057\n",
      "Average test loss: 0.0016206718486630254\n",
      "Epoch 54/300\n",
      "Average training loss: 0.033655513771706155\n",
      "Average test loss: 0.001598350399484237\n",
      "Epoch 55/300\n",
      "Average training loss: 0.033394941222336556\n",
      "Average test loss: 0.001535234935581684\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03381385169095463\n",
      "Average test loss: 0.0015701723648235202\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03373176513115565\n",
      "Average test loss: 0.0014258793259246483\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03304652120338546\n",
      "Average test loss: 0.0015884617818519473\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03361985606617398\n",
      "Average test loss: 0.0016516743248535528\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03276018654969003\n",
      "Average test loss: 0.001564861731396781\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03279476228687498\n",
      "Average test loss: 0.001545417830761936\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03247437394658725\n",
      "Average test loss: 0.0016280374877258308\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03213300070828862\n",
      "Average test loss: 0.0014451101685149803\n",
      "Epoch 64/300\n",
      "Average training loss: 0.031960979024569194\n",
      "Average test loss: 0.0016018708099921545\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03198313093516562\n",
      "Average test loss: 0.0017799881932636101\n",
      "Epoch 66/300\n",
      "Average training loss: 0.035105020175377526\n",
      "Average test loss: 0.0014228279546110167\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03158951242764791\n",
      "Average test loss: 0.00145981037347681\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03182429857055346\n",
      "Average test loss: 0.0017593672991626792\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03156107957495583\n",
      "Average test loss: 0.0014076761638538704\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03138949158291022\n",
      "Average test loss: 0.1505407760474417\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03170343062281609\n",
      "Average test loss: 0.0015299645852194063\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03149698864751392\n",
      "Average test loss: 0.0013770260734276639\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0313277927554316\n",
      "Average test loss: 0.001511483822328349\n",
      "Epoch 74/300\n",
      "Average training loss: 0.031325146931740974\n",
      "Average test loss: 0.001397418653488987\n",
      "Epoch 75/300\n",
      "Average training loss: 0.030841766385568514\n",
      "Average test loss: 0.0018510931303931607\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03129872369435099\n",
      "Average test loss: 0.0015134003991261124\n",
      "Epoch 77/300\n",
      "Average training loss: 0.030668778023786016\n",
      "Average test loss: 0.001622515949834552\n",
      "Epoch 78/300\n",
      "Average training loss: 0.030980046537187365\n",
      "Average test loss: 0.0015791696237607135\n",
      "Epoch 79/300\n",
      "Average training loss: 0.030791474027766122\n",
      "Average test loss: 0.0015198304088682765\n",
      "Epoch 80/300\n",
      "Average training loss: 0.030867821156978607\n",
      "Average test loss: 0.0014472124375299448\n",
      "Epoch 81/300\n",
      "Average training loss: 0.031267975866794584\n",
      "Average test loss: 0.0013959492924105789\n",
      "Epoch 82/300\n",
      "Average training loss: 0.031081651783651777\n",
      "Average test loss: 0.0013821199889191324\n",
      "Epoch 83/300\n",
      "Average training loss: 0.030262774373094242\n",
      "Average test loss: 0.001502137260718478\n",
      "Epoch 84/300\n",
      "Average training loss: 0.030135341071420244\n",
      "Average test loss: 0.0015305765169776149\n",
      "Epoch 85/300\n",
      "Average training loss: 0.030618722463647524\n",
      "Average test loss: 0.0014110089848852819\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03016526586479611\n",
      "Average test loss: 0.0016425778158009052\n",
      "Epoch 87/300\n",
      "Average training loss: 0.031356857041517895\n",
      "Average test loss: 0.0015978708814622627\n",
      "Epoch 88/300\n",
      "Average training loss: 0.029957497772243286\n",
      "Average test loss: 0.0019637130604436\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03017490971750683\n",
      "Average test loss: 0.0014842409341492588\n",
      "Epoch 90/300\n",
      "Average training loss: 0.029719610719217193\n",
      "Average test loss: 0.0015834808964282274\n",
      "Epoch 91/300\n",
      "Average training loss: 0.029910432606935503\n",
      "Average test loss: 0.0014126907876796193\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02994517371389601\n",
      "Average test loss: 0.0013221985951272978\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02973929313321908\n",
      "Average test loss: 0.0014516637719546755\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03103550862107012\n",
      "Average test loss: 0.00159818359474755\n",
      "Epoch 95/300\n",
      "Average training loss: 0.029464785458313093\n",
      "Average test loss: 0.0014748886067213283\n",
      "Epoch 96/300\n",
      "Average training loss: 0.029375202012558776\n",
      "Average test loss: 0.001518815808619062\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02950383724603388\n",
      "Average test loss: 0.0015247863437980414\n",
      "Epoch 98/300\n",
      "Average training loss: 0.030253214415576723\n",
      "Average test loss: 0.001388172065011329\n",
      "Epoch 99/300\n",
      "Average training loss: 0.029415868666436938\n",
      "Average test loss: 0.0014944993547267385\n",
      "Epoch 100/300\n",
      "Average training loss: 0.029819437281952965\n",
      "Average test loss: 0.0014566756119537684\n",
      "Epoch 101/300\n",
      "Average training loss: 0.029371023987730345\n",
      "Average test loss: 0.001394087036864625\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02934939128657182\n",
      "Average test loss: 0.001655503976572719\n",
      "Epoch 103/300\n",
      "Average training loss: 0.029625835872358747\n",
      "Average test loss: 0.0015100624800349276\n",
      "Epoch 104/300\n",
      "Average training loss: 0.029140625478492842\n",
      "Average test loss: 0.001519019107023875\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02955567151473628\n",
      "Average test loss: 0.001557124219627844\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0294886358992921\n",
      "Average test loss: 0.0017125924631125396\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02902165469692813\n",
      "Average test loss: 0.001357956283622318\n",
      "Epoch 108/300\n",
      "Average training loss: 0.029471391436126496\n",
      "Average test loss: 0.0013815020175857676\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029002344861626624\n",
      "Average test loss: 0.001435375034602152\n",
      "Epoch 110/300\n",
      "Average training loss: 0.028952304375668366\n",
      "Average test loss: 0.0014906940410017138\n",
      "Epoch 111/300\n",
      "Average training loss: 0.029246995664305157\n",
      "Average test loss: 0.0014452683637953467\n",
      "Epoch 112/300\n",
      "Average training loss: 0.028775779676106242\n",
      "Average test loss: 0.0014518003405796158\n",
      "Epoch 113/300\n",
      "Average training loss: 0.029018344117535486\n",
      "Average test loss: 0.001462908534022669\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028689570112360847\n",
      "Average test loss: 0.001352984883977721\n",
      "Epoch 115/300\n",
      "Average training loss: 0.029364099004202417\n",
      "Average test loss: 0.0014578549245682856\n",
      "Epoch 116/300\n",
      "Average training loss: 0.028554455899529988\n",
      "Average test loss: 0.0013771537712050808\n",
      "Epoch 117/300\n",
      "Average training loss: 0.029036081847217347\n",
      "Average test loss: 0.0013862501741904352\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02886120971871747\n",
      "Average test loss: 0.0015239551019751362\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0285837647103601\n",
      "Average test loss: 0.0014416373481767044\n",
      "Epoch 120/300\n",
      "Average training loss: 0.028691291285885705\n",
      "Average test loss: 0.001420815366320312\n",
      "Epoch 121/300\n",
      "Average training loss: 0.028574255850580002\n",
      "Average test loss: 0.001423948536730475\n",
      "Epoch 122/300\n",
      "Average training loss: 0.028260554962688023\n",
      "Average test loss: 0.0016363537705813844\n",
      "Epoch 123/300\n",
      "Average training loss: 0.029175658404827116\n",
      "Average test loss: 0.001329468136239383\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02829935675859451\n",
      "Average test loss: 0.0015105215610108442\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028352692173586953\n",
      "Average test loss: 0.0015357587900426653\n",
      "Epoch 126/300\n",
      "Average training loss: 0.028396590100394355\n",
      "Average test loss: 0.001425211881287396\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02836142502559556\n",
      "Average test loss: 0.0015457253926951025\n",
      "Epoch 128/300\n",
      "Average training loss: 0.028078491101662318\n",
      "Average test loss: 0.0014547751470365459\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0287929973486397\n",
      "Average test loss: 0.0016754656072912944\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028547752862175306\n",
      "Average test loss: 0.001346958000626829\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02815088459352652\n",
      "Average test loss: 0.0013386549818226034\n",
      "Epoch 132/300\n",
      "Average training loss: 0.028193594126237762\n",
      "Average test loss: 0.0015334774913887182\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028036456901166174\n",
      "Average test loss: 0.0016536672921954759\n",
      "Epoch 134/300\n",
      "Average training loss: 0.028040612646275095\n",
      "Average test loss: 0.0015302075727118387\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028016938204566636\n",
      "Average test loss: 0.0014172454483600127\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02796318361825413\n",
      "Average test loss: 0.001384264754752318\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0282864890611834\n",
      "Average test loss: 0.0013338702999883228\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02813027469813824\n",
      "Average test loss: 0.001637486902376016\n",
      "Epoch 139/300\n",
      "Average training loss: 0.027960960171288914\n",
      "Average test loss: 0.0013673475378503403\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02864734045498901\n",
      "Average test loss: 0.0013487267572846677\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02814817502432399\n",
      "Average test loss: 0.0017122374609526661\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02859544545246495\n",
      "Average test loss: 0.0013257814990873967\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0276205818619993\n",
      "Average test loss: 0.0013384919838151997\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028080639375580682\n",
      "Average test loss: 0.0012903829184878204\n",
      "Epoch 145/300\n",
      "Average training loss: 0.027653459252582654\n",
      "Average test loss: 0.001300903400251021\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027866528065668212\n",
      "Average test loss: 0.0013673327435842818\n",
      "Epoch 147/300\n",
      "Average training loss: 0.027664014953706\n",
      "Average test loss: 0.0013380930633801553\n",
      "Epoch 148/300\n",
      "Average training loss: 0.028157359353370135\n",
      "Average test loss: 0.001337215857890745\n",
      "Epoch 149/300\n",
      "Average training loss: 0.027715994793507788\n",
      "Average test loss: 0.0015624094059069951\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02806073844432831\n",
      "Average test loss: 0.0023458048913420904\n",
      "Epoch 151/300\n",
      "Average training loss: 0.028111909752090773\n",
      "Average test loss: 0.0013460391749524408\n",
      "Epoch 152/300\n",
      "Average training loss: 0.027655916758709483\n",
      "Average test loss: 0.0012940544626779027\n",
      "Epoch 153/300\n",
      "Average training loss: 0.028599096124370893\n",
      "Average test loss: 0.0012967471226842868\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027490943569276068\n",
      "Average test loss: 0.0014085475963850815\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02745922998752859\n",
      "Average test loss: 0.0014062166379557716\n",
      "Epoch 156/300\n",
      "Average training loss: 0.027703739863302972\n",
      "Average test loss: 0.0013016631139649284\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02741515265405178\n",
      "Average test loss: 0.0015720553527482682\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02751064744260576\n",
      "Average test loss: 0.001360150804122289\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027526364437407917\n",
      "Average test loss: 0.0013026776707006826\n",
      "Epoch 160/300\n",
      "Average training loss: 0.027928562061654196\n",
      "Average test loss: 0.0013772219827191698\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02714119039972623\n",
      "Average test loss: 0.001319652761850092\n",
      "Epoch 162/300\n",
      "Average training loss: 0.027550560790631506\n",
      "Average test loss: 0.0014003947246819734\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027280383924643197\n",
      "Average test loss: 0.001536426670021481\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02761844970120324\n",
      "Average test loss: 0.0012933670829774605\n",
      "Epoch 165/300\n",
      "Average training loss: 0.027255970411830477\n",
      "Average test loss: 0.0013540217720179094\n",
      "Epoch 166/300\n",
      "Average training loss: 0.027246504977345466\n",
      "Average test loss: 0.0013422522670071986\n",
      "Epoch 167/300\n",
      "Average training loss: 0.027287441886133617\n",
      "Average test loss: 0.0018539328986985817\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02706678390999635\n",
      "Average test loss: 0.001326643356639478\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02729786191880703\n",
      "Average test loss: 0.0015879804278827376\n",
      "Epoch 170/300\n",
      "Average training loss: 0.027431674798329673\n",
      "Average test loss: 0.0017149289102397032\n",
      "Epoch 171/300\n",
      "Average training loss: 0.027814445558521483\n",
      "Average test loss: 0.003067916890192363\n",
      "Epoch 172/300\n",
      "Average training loss: 0.029438640063007674\n",
      "Average test loss: 0.0014426270020711754\n",
      "Epoch 173/300\n",
      "Average training loss: 0.026906945917341444\n",
      "Average test loss: 0.001374408446252346\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026825146385365064\n",
      "Average test loss: 0.0013291289060790505\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02681083862317933\n",
      "Average test loss: 0.0012944673855478565\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026898951154616144\n",
      "Average test loss: 0.0014006589834267894\n",
      "Epoch 177/300\n",
      "Average training loss: 0.027221312097377246\n",
      "Average test loss: 0.0013936540208136041\n",
      "Epoch 178/300\n",
      "Average training loss: 0.027284550643629498\n",
      "Average test loss: 0.0013096418573728039\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02658817793097761\n",
      "Average test loss: 0.001333834663964808\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026698636907670232\n",
      "Average test loss: 0.0013870583987898298\n",
      "Epoch 181/300\n",
      "Average training loss: 0.027538582305113475\n",
      "Average test loss: 0.0015829416785596146\n",
      "Epoch 182/300\n",
      "Average training loss: 0.027265243873000146\n",
      "Average test loss: 0.0013037663247022365\n",
      "Epoch 183/300\n",
      "Average training loss: 0.027111456683940358\n",
      "Average test loss: 0.0014678825809516841\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026883019528455204\n",
      "Average test loss: 0.0015601503511683808\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027082545154624514\n",
      "Average test loss: 0.0017700902255665925\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026754333150055674\n",
      "Average test loss: 0.0013511680248710844\n",
      "Epoch 187/300\n",
      "Average training loss: 0.027028339359495376\n",
      "Average test loss: 0.0013443213939252827\n",
      "Epoch 188/300\n",
      "Average training loss: 0.026507821450630825\n",
      "Average test loss: 0.0014031831135766374\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026872075870633127\n",
      "Average test loss: 0.0013733427171698875\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026982901997036405\n",
      "Average test loss: 0.0013786456439023216\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027033927862842876\n",
      "Average test loss: 0.0013182833184384637\n",
      "Epoch 192/300\n",
      "Average training loss: 0.026783809625440174\n",
      "Average test loss: 0.0014178448721973433\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026937263207303153\n",
      "Average test loss: 0.0013371926539887985\n",
      "Epoch 194/300\n",
      "Average training loss: 0.026542528730299737\n",
      "Average test loss: 0.0013889315608474942\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026687825363543298\n",
      "Average test loss: 0.0013292483922301068\n",
      "Epoch 196/300\n",
      "Average training loss: 0.026644327032897208\n",
      "Average test loss: 0.002017990982470413\n",
      "Epoch 197/300\n",
      "Average training loss: 0.027526203622420628\n",
      "Average test loss: 0.0012964641831608283\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02661545947690805\n",
      "Average test loss: 0.0013164143971581427\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02642141729924414\n",
      "Average test loss: 0.0012782255362512338\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026617278940147825\n",
      "Average test loss: 0.0013803109146861567\n",
      "Epoch 201/300\n",
      "Average training loss: 0.026494301944971085\n",
      "Average test loss: 0.0012777920873421763\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02769949806895521\n",
      "Average test loss: 0.0015848599736475281\n",
      "Epoch 203/300\n",
      "Average training loss: 0.026566118127769895\n",
      "Average test loss: 0.0013706477997410629\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02699325956073072\n",
      "Average test loss: 0.0012975235031917691\n",
      "Epoch 205/300\n",
      "Average training loss: 0.026385153462489448\n",
      "Average test loss: 0.0014329389086407093\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02652095973988374\n",
      "Average test loss: 0.0014681086047656005\n",
      "Epoch 207/300\n",
      "Average training loss: 0.026579970495568383\n",
      "Average test loss: 0.0013542221972408393\n",
      "Epoch 208/300\n",
      "Average training loss: 0.026478334221574994\n",
      "Average test loss: 0.0013774441628096005\n",
      "Epoch 209/300\n",
      "Average training loss: 0.026635431869162455\n",
      "Average test loss: 0.0013581806073586146\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026744407885604435\n",
      "Average test loss: 0.0013655840628263023\n",
      "Epoch 211/300\n",
      "Average training loss: 0.026347102481457923\n",
      "Average test loss: 0.0013809521166193816\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026415199877487288\n",
      "Average test loss: 0.001335078489035368\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026342480790283944\n",
      "Average test loss: 0.0014867291496031814\n",
      "Epoch 214/300\n",
      "Average training loss: 0.027539416457215946\n",
      "Average test loss: 0.001307864209740526\n",
      "Epoch 215/300\n",
      "Average training loss: 0.026222512607773144\n",
      "Average test loss: 0.0012740728726817502\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02606976573003663\n",
      "Average test loss: 0.001334567055830525\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02652687226401435\n",
      "Average test loss: 0.001611067822927402\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026331536326143476\n",
      "Average test loss: 0.0016241426140897804\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02624965044359366\n",
      "Average test loss: 0.0013967491120306982\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0266550503058566\n",
      "Average test loss: 0.0014129734515315956\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026279836053649586\n",
      "Average test loss: 0.0013743171864706609\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026280993360612126\n",
      "Average test loss: 0.001279648525433408\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0259534360104137\n",
      "Average test loss: 0.0013620667020893759\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0261125341206789\n",
      "Average test loss: 0.0015188016293363439\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026088873750633665\n",
      "Average test loss: 0.0013020235652931862\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02651868274145656\n",
      "Average test loss: 0.0013240189163221253\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026325765904453065\n",
      "Average test loss: 0.0013809255590765842\n",
      "Epoch 228/300\n",
      "Average training loss: 0.026175268929865624\n",
      "Average test loss: 0.0013934811655845908\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026439115290840465\n",
      "Average test loss: 0.0013360352194350627\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026054801820052993\n",
      "Average test loss: 0.0013479428249928687\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026108761856953305\n",
      "Average test loss: 0.0013879583964331282\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02618655710419019\n",
      "Average test loss: 0.0013258050937826435\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026211326057712238\n",
      "Average test loss: 0.0013517473076159756\n",
      "Epoch 234/300\n",
      "Average training loss: 0.025975981389482816\n",
      "Average test loss: 0.0013238329558322827\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02622965493467119\n",
      "Average test loss: 0.001309573013542427\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02697305958966414\n",
      "Average test loss: 0.001369866965752509\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026093380777372253\n",
      "Average test loss: 0.001373224427147458\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026220205328530734\n",
      "Average test loss: 0.0012723920290461845\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025871581291158994\n",
      "Average test loss: 0.0014071420480807621\n",
      "Epoch 240/300\n",
      "Average training loss: 0.025991136910186875\n",
      "Average test loss: 0.001441533763685988\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026176786795258523\n",
      "Average test loss: 0.001308298838428325\n",
      "Epoch 242/300\n",
      "Average training loss: 0.025782586499220796\n",
      "Average test loss: 0.0013991159536461864\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026814432432254155\n",
      "Average test loss: 0.001386443144745297\n",
      "Epoch 244/300\n",
      "Average training loss: 0.025782243301471074\n",
      "Average test loss: 0.0012704968143047559\n",
      "Epoch 245/300\n",
      "Average training loss: 0.026182875532242987\n",
      "Average test loss: 0.0013701375311033594\n",
      "Epoch 246/300\n",
      "Average training loss: 0.025815302706427044\n",
      "Average test loss: 0.0012751404153597023\n",
      "Epoch 247/300\n",
      "Average training loss: 0.025889513863457572\n",
      "Average test loss: 0.0013630785608353714\n",
      "Epoch 248/300\n",
      "Average training loss: 0.025992953547173077\n",
      "Average test loss: 0.001319292660906083\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0260282071414921\n",
      "Average test loss: 0.0013865698189992044\n",
      "Epoch 250/300\n",
      "Average training loss: 0.025880226547519366\n",
      "Average test loss: 0.0012967440753968225\n",
      "Epoch 251/300\n",
      "Average training loss: 0.025848040012849703\n",
      "Average test loss: 0.0013258017461953893\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02600803589158588\n",
      "Average test loss: 0.001255820591106183\n",
      "Epoch 253/300\n",
      "Average training loss: 0.025785757232043478\n",
      "Average test loss: 0.001280517832479543\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02600916216439671\n",
      "Average test loss: 0.0015939478176749415\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025613057047128678\n",
      "Average test loss: 0.0013515481458355983\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0257938139239947\n",
      "Average test loss: 0.0016443602725242576\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025957769804530673\n",
      "Average test loss: 0.0013049893598589633\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025651384598679012\n",
      "Average test loss: 0.00132350558316749\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02570741056568093\n",
      "Average test loss: 0.0014378476645797492\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02570092682374848\n",
      "Average test loss: 0.0013687038140164482\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026535041325622136\n",
      "Average test loss: 0.0014974953840590185\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025778013941314484\n",
      "Average test loss: 0.0013430401790990598\n",
      "Epoch 263/300\n",
      "Average training loss: 0.026380234816008145\n",
      "Average test loss: 0.0012946276072826651\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025566379765669504\n",
      "Average test loss: 0.001370411875554257\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02561147854808304\n",
      "Average test loss: 0.001489210148031513\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02566900269024902\n",
      "Average test loss: 0.0013048175448655255\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02551226987855302\n",
      "Average test loss: 0.0013410827772588365\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02572875699400902\n",
      "Average test loss: 0.0019795553242373796\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025765465249617894\n",
      "Average test loss: 0.0013860170490418871\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02551273525920179\n",
      "Average test loss: 0.001370182535248912\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025828989999161826\n",
      "Average test loss: 0.0013088409594363636\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02555625245637364\n",
      "Average test loss: 0.001528530054518746\n",
      "Epoch 273/300\n",
      "Average training loss: 0.025698456618520948\n",
      "Average test loss: 0.0017165709394547674\n",
      "Epoch 274/300\n",
      "Average training loss: 0.025642031583521102\n",
      "Average test loss: 0.0013644518182199035\n",
      "Epoch 275/300\n",
      "Average training loss: 0.025815160300996568\n",
      "Average test loss: 0.0015438006514062485\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02540370621283849\n",
      "Average test loss: 0.00136652432464891\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02585318133400546\n",
      "Average test loss: 0.0015112194069143799\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02559475771420532\n",
      "Average test loss: 0.001270827204713391\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025697933172186216\n",
      "Average test loss: 0.001304203786369827\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025692807230684494\n",
      "Average test loss: 0.0013974071568292048\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025346510706676376\n",
      "Average test loss: 0.0013439271866033475\n",
      "Epoch 282/300\n",
      "Average training loss: 0.025339402677284347\n",
      "Average test loss: 0.0013735730708059337\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02558612641195456\n",
      "Average test loss: 0.0013562283715129726\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026002237170934677\n",
      "Average test loss: 0.001293053862845732\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02542047984732522\n",
      "Average test loss: 0.0013242941497721606\n",
      "Epoch 286/300\n",
      "Average training loss: 0.025873543515801428\n",
      "Average test loss: 0.0013195023867819044\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025269906292359035\n",
      "Average test loss: 0.001292048525893026\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02567371935976876\n",
      "Average test loss: 0.0012763381951178114\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02518404036263625\n",
      "Average test loss: 0.0014048094707023767\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02543925694624583\n",
      "Average test loss: 0.0015956516884681252\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02572176962594191\n",
      "Average test loss: 0.0012768936487328674\n",
      "Epoch 292/300\n",
      "Average training loss: 0.025279971895946395\n",
      "Average test loss: 0.0013855538909426995\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025458426540096602\n",
      "Average test loss: 0.0013486160136138398\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025704604713453188\n",
      "Average test loss: 0.0013260546299732393\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02556640135248502\n",
      "Average test loss: 0.0013143221487084196\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025150423831409877\n",
      "Average test loss: 0.001287215471888582\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025340921762916777\n",
      "Average test loss: 0.0013101330688223244\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025533314454886647\n",
      "Average test loss: 0.0014636603575199842\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025672885908020866\n",
      "Average test loss: 0.0013529407570345535\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025369187464316687\n",
      "Average test loss: 0.0013202164685353636\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_No_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 20.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.09\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.43\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.99\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.15\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.72\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.73\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.74\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.22\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.39\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.54\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.63\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.73\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.25\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.68\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.35\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.72\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.67\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.89\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.50\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.96\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.05\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.68\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.56\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.68\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.352560437520346\n",
      "Average test loss: 0.014856123535169496\n",
      "Epoch 2/300\n",
      "Average training loss: 8.93895070054796\n",
      "Average test loss: 3640.3772151220624\n",
      "Epoch 3/300\n",
      "Average training loss: 8.359578707377116\n",
      "Average test loss: 0.010272356229523818\n",
      "Epoch 4/300\n",
      "Average training loss: 7.954726485782199\n",
      "Average test loss: 474.4447354723679\n",
      "Epoch 5/300\n",
      "Average training loss: 7.617920617845323\n",
      "Average test loss: 2.4588573711439965\n",
      "Epoch 6/300\n",
      "Average training loss: 7.325577413770888\n",
      "Average test loss: 4.608387209326029\n",
      "Epoch 7/300\n",
      "Average training loss: 7.0603167775472\n",
      "Average test loss: 1995.7807840131388\n",
      "Epoch 8/300\n",
      "Average training loss: 6.8168908309936525\n",
      "Average test loss: 4843.132585701267\n",
      "Epoch 9/300\n",
      "Average training loss: 6.590393706003825\n",
      "Average test loss: 0.011727124406231774\n",
      "Epoch 10/300\n",
      "Average training loss: 6.369628987206353\n",
      "Average test loss: 0.012212613692714109\n",
      "Epoch 11/300\n",
      "Average training loss: 6.163257616678874\n",
      "Average test loss: 0.009899840554015504\n",
      "Epoch 12/300\n",
      "Average training loss: 5.968358129713271\n",
      "Average test loss: 0.018050683192908763\n",
      "Epoch 13/300\n",
      "Average training loss: 5.778568897671169\n",
      "Average test loss: 0.3984876874163747\n",
      "Epoch 14/300\n",
      "Average training loss: 5.600529025183784\n",
      "Average test loss: 0.008357456567386786\n",
      "Epoch 15/300\n",
      "Average training loss: 5.434128975338406\n",
      "Average test loss: 0.10799483560522398\n",
      "Epoch 16/300\n",
      "Average training loss: 5.273449910905626\n",
      "Average test loss: 0.007588326433466541\n",
      "Epoch 17/300\n",
      "Average training loss: 5.10529045147366\n",
      "Average training loss: 4.936439731174045\n",
      "Average test loss: 0.007400435680316554\n",
      "Epoch 19/300\n",
      "Average training loss: 4.771033487531874\n",
      "Average test loss: 0.011303871278133657\n",
      "Epoch 20/300\n",
      "Average training loss: 4.608335434807671\n",
      "Average test loss: 0.007003317552308242\n",
      "Epoch 21/300\n",
      "Average training loss: 4.449385238223606\n",
      "Average test loss: 0.007588940998746289\n",
      "Epoch 22/300\n",
      "Average training loss: 4.284592103322347\n",
      "Average test loss: 0.009990923097150193\n",
      "Epoch 23/300\n",
      "Average training loss: 4.1179463456471765\n",
      "Average test loss: 0.009258667602307267\n",
      "Epoch 24/300\n",
      "Average training loss: 3.949103013144599\n",
      "Average test loss: 0.007007770708037747\n",
      "Epoch 25/300\n",
      "Average training loss: 3.7739540763431125\n",
      "Average test loss: 0.0070431767317155995\n",
      "Epoch 26/300\n",
      "Average training loss: 3.598977002673679\n",
      "Average test loss: 0.006571651830441422\n",
      "Epoch 27/300\n",
      "Average training loss: 3.4270289652082657\n",
      "Average test loss: 0.006342203251189656\n",
      "Epoch 28/300\n",
      "Average training loss: 3.2535608745151094\n",
      "Average test loss: 0.006574105365408791\n",
      "Epoch 29/300\n",
      "Average training loss: 3.080508355034722\n",
      "Average test loss: 0.006311969359301859\n",
      "Epoch 30/300\n",
      "Average training loss: 2.9099094409942627\n",
      "Average test loss: 0.006099858836995231\n",
      "Epoch 31/300\n",
      "Average training loss: 2.740172932730781\n",
      "Average test loss: 0.006309345984624491\n",
      "Epoch 32/300\n",
      "Average training loss: 2.5793763650258383\n",
      "Average test loss: 0.0060650145817134115\n",
      "Epoch 33/300\n",
      "Average training loss: 2.421142515818278\n",
      "Average test loss: 0.006058709492286046\n",
      "Epoch 34/300\n",
      "Average training loss: 2.2681442337036133\n",
      "Average test loss: 0.020304836003316774\n",
      "Epoch 35/300\n",
      "Average training loss: 2.115824090321859\n",
      "Average test loss: 0.006186874265058173\n",
      "Epoch 36/300\n",
      "Average training loss: 1.9748437910079957\n",
      "Average test loss: 0.005890293894128667\n",
      "Epoch 37/300\n",
      "Average training loss: 1.832244513935513\n",
      "Average test loss: 0.0058805386655860475\n",
      "Epoch 38/300\n",
      "Average training loss: 1.6893730552461412\n",
      "Average test loss: 0.006006851560125748\n",
      "Epoch 39/300\n",
      "Average training loss: 1.5455046158896553\n",
      "Average test loss: 0.006484441875169674\n",
      "Epoch 40/300\n",
      "Average training loss: 1.4064021581013997\n",
      "Average test loss: 0.005925859241022004\n",
      "Epoch 41/300\n",
      "Average training loss: 1.2729201915529038\n",
      "Average test loss: 0.006309351792765989\n",
      "Epoch 42/300\n",
      "Average training loss: 1.1509022490183511\n",
      "Average test loss: 0.0060838618464767935\n",
      "Epoch 43/300\n",
      "Average training loss: 1.0289650893211364\n",
      "Average test loss: 0.0058883813338147266\n",
      "Epoch 44/300\n",
      "Average training loss: 0.9273985033035278\n",
      "Average test loss: 0.006640122284077936\n",
      "Epoch 45/300\n",
      "Average training loss: 0.8347604159779018\n",
      "Average test loss: 0.0061982118942671355\n",
      "Epoch 46/300\n",
      "Average training loss: 0.7538816798528035\n",
      "Average test loss: 0.005930724278092384\n",
      "Epoch 47/300\n",
      "Average training loss: 0.6837808510992263\n",
      "Average test loss: 0.005935007866885927\n",
      "Epoch 48/300\n",
      "Average training loss: 0.6204543256759644\n",
      "Average test loss: 0.006419353167629904\n",
      "Epoch 49/300\n",
      "Average training loss: 0.5646282703081766\n",
      "Average test loss: 0.032508884424964586\n",
      "Epoch 50/300\n",
      "Average training loss: 0.5108062196837532\n",
      "Average test loss: 0.005932283620039622\n",
      "Epoch 51/300\n",
      "Average training loss: 0.4675722926457723\n",
      "Average test loss: 0.005715711579968532\n",
      "Epoch 52/300\n",
      "Average training loss: 0.4341705222659641\n",
      "Average test loss: 0.005894342618683974\n",
      "Epoch 53/300\n",
      "Average training loss: 0.40128685895601907\n",
      "Average test loss: 0.005679632222900788\n",
      "Epoch 54/300\n",
      "Average training loss: 0.3731185093190935\n",
      "Average test loss: 0.00562859161860413\n",
      "Epoch 55/300\n",
      "Average training loss: 0.3516124915811751\n",
      "Average test loss: 0.00558753389492631\n",
      "Epoch 56/300\n",
      "Average training loss: 0.33371938916047417\n",
      "Average test loss: 0.0060182675160467625\n",
      "Epoch 57/300\n",
      "Average training loss: 0.3149687768750721\n",
      "Average test loss: 0.005564461307807102\n",
      "Epoch 58/300\n",
      "Average training loss: 0.301392812649409\n",
      "Average test loss: 0.006142502773553133\n",
      "Epoch 59/300\n",
      "Average training loss: 0.2875625163184272\n",
      "Average test loss: 0.005832123702598942\n",
      "Epoch 60/300\n",
      "Average training loss: 0.275309389617708\n",
      "Average test loss: 0.006088971579447389\n",
      "Epoch 61/300\n",
      "Average training loss: 0.26478243446350097\n",
      "Average test loss: 0.0054691094333926835\n",
      "Epoch 62/300\n",
      "Average training loss: 0.26226831003030143\n",
      "Average test loss: 0.005607978925936752\n",
      "Epoch 63/300\n",
      "Average training loss: 0.2499613687992096\n",
      "Average test loss: 0.005590401666031943\n",
      "Epoch 64/300\n",
      "Average training loss: 0.24587657761573792\n",
      "Average test loss: 0.005516767938931783\n",
      "Epoch 65/300\n",
      "Average training loss: 0.24042231163713668\n",
      "Average test loss: 0.005532705400553015\n",
      "Epoch 66/300\n",
      "Average training loss: 0.2353946626053916\n",
      "Average test loss: 0.005820152850614654\n",
      "Epoch 67/300\n",
      "Average training loss: 0.2303059151702457\n",
      "Average test loss: 0.005811928120752176\n",
      "Epoch 68/300\n",
      "Average training loss: 0.2256770041121377\n",
      "Average test loss: 0.006267222854826186\n",
      "Epoch 69/300\n",
      "Average training loss: 0.22078989923000336\n",
      "Average test loss: 0.005510913500769271\n",
      "Epoch 70/300\n",
      "Average training loss: 0.21988172616561255\n",
      "Average test loss: 0.005474121794518497\n",
      "Epoch 71/300\n",
      "Average training loss: 0.21729559327496423\n",
      "Average test loss: 0.005835187098632256\n",
      "Epoch 72/300\n",
      "Average training loss: 0.21486982258160908\n",
      "Average test loss: 0.005524666894641188\n",
      "Epoch 73/300\n",
      "Average training loss: 0.21209307600392235\n",
      "Average test loss: 0.005618981961988741\n",
      "Epoch 74/300\n",
      "Average training loss: 0.2077288847764333\n",
      "Average test loss: 0.005530812273422877\n",
      "Epoch 75/300\n",
      "Average training loss: 0.20640683330429924\n",
      "Average test loss: 0.005568416244867775\n",
      "Epoch 76/300\n",
      "Average training loss: 0.20704699065950183\n",
      "Average test loss: 0.005535076208826569\n",
      "Epoch 77/300\n",
      "Average training loss: 0.20466655939155154\n",
      "Average test loss: 0.005973634992622667\n",
      "Epoch 78/300\n",
      "Average training loss: 0.20166725288497078\n",
      "Average test loss: 0.00530284850506319\n",
      "Epoch 79/300\n",
      "Average training loss: 0.19826113204161325\n",
      "Average test loss: 0.005587854260371791\n",
      "Epoch 80/300\n",
      "Average training loss: 0.19644801074928708\n",
      "Average test loss: 0.005482459732227855\n",
      "Epoch 81/300\n",
      "Average training loss: 0.19659109081162346\n",
      "Average test loss: 0.006327722834423184\n",
      "Epoch 82/300\n",
      "Average training loss: 0.1950351687669754\n",
      "Average test loss: 0.0058749046255317\n",
      "Epoch 83/300\n",
      "Average training loss: 0.19013560296429527\n",
      "Average test loss: 0.005456077806858553\n",
      "Epoch 84/300\n",
      "Average training loss: 0.18949366091357336\n",
      "Average test loss: 0.009143075588676665\n",
      "Epoch 85/300\n",
      "Average training loss: 0.18967166623804305\n",
      "Average test loss: 0.005572162616584036\n",
      "Epoch 86/300\n",
      "Average training loss: 0.18647429000006782\n",
      "Average test loss: 0.0053850791789591314\n",
      "Epoch 87/300\n",
      "Average training loss: 0.1866396554046207\n",
      "Average test loss: 0.005758249793201685\n",
      "Epoch 88/300\n",
      "Average training loss: 0.18458601807223426\n",
      "Average test loss: 0.005586601706842581\n",
      "Epoch 89/300\n",
      "Average training loss: 0.18313364301787483\n",
      "Average test loss: 0.005456853883961836\n",
      "Epoch 90/300\n",
      "Average training loss: 0.18254889607429503\n",
      "Average test loss: 0.005576508302655485\n",
      "Epoch 91/300\n",
      "Average training loss: 0.18106941752963596\n",
      "Average test loss: 0.005776444162345595\n",
      "Epoch 92/300\n",
      "Average training loss: 0.18144427722030215\n",
      "Average test loss: 0.005260521900529663\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1800328774584664\n",
      "Average test loss: 0.005521506448586782\n",
      "Epoch 94/300\n",
      "Average training loss: 0.17823704177803462\n",
      "Average test loss: 0.005276119388639927\n",
      "Epoch 95/300\n",
      "Average training loss: 0.1790001932779948\n",
      "Average test loss: 0.00858768237961663\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1771096338563495\n",
      "Average test loss: 0.005658452574577597\n",
      "Epoch 97/300\n",
      "Average training loss: 0.17631652014785343\n",
      "Average test loss: 0.005412949544274145\n",
      "Epoch 98/300\n",
      "Average training loss: 0.175424497101042\n",
      "Average test loss: 0.005246939781639311\n",
      "Epoch 99/300\n",
      "Average training loss: 0.17530085967646705\n",
      "Average test loss: 0.006390224788337946\n",
      "Epoch 100/300\n",
      "Average training loss: 0.17764360757668812\n",
      "Average test loss: 0.0055790070473319955\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1746857169866562\n",
      "Average test loss: 0.005333225929488738\n",
      "Epoch 102/300\n",
      "Average training loss: 0.17240284746223025\n",
      "Average test loss: 0.00570447867239515\n",
      "Epoch 103/300\n",
      "Average training loss: 0.17233048919174407\n",
      "Average test loss: 0.005515124518010351\n",
      "Epoch 104/300\n",
      "Average training loss: 0.17332889901267157\n",
      "Average test loss: 0.02057384422918161\n",
      "Epoch 105/300\n",
      "Average training loss: 0.17468936622142792\n",
      "Average test loss: 0.00542013553240233\n",
      "Epoch 106/300\n",
      "Average training loss: 0.17191943196455636\n",
      "Average test loss: 0.005352825378792153\n",
      "Epoch 107/300\n",
      "Average training loss: 0.17116817723380195\n",
      "Average test loss: 0.005579868074506522\n",
      "Epoch 108/300\n",
      "Average training loss: 0.16932315754890442\n",
      "Average test loss: 0.005332802322589689\n",
      "Epoch 109/300\n",
      "Average training loss: 0.16915154895517562\n",
      "Average test loss: 0.005809475819269816\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1709322290023168\n",
      "Average test loss: 0.005388937142988046\n",
      "Epoch 111/300\n",
      "Average training loss: 0.16964342251088885\n",
      "Average test loss: 0.005401198399563631\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1692163528667556\n",
      "Average test loss: 0.005396233850469192\n",
      "Epoch 113/300\n",
      "Average training loss: 0.1682676082584593\n",
      "Average test loss: 0.005471527379833989\n",
      "Epoch 114/300\n",
      "Average training loss: 0.16890391908751592\n",
      "Average test loss: 0.005386583473947313\n",
      "Epoch 115/300\n",
      "Average training loss: 0.1670582571691937\n",
      "Average test loss: 0.005360536434998115\n",
      "Epoch 116/300\n",
      "Average training loss: 0.16807097499900395\n",
      "Average test loss: 0.005714308542509873\n",
      "Epoch 117/300\n",
      "Average training loss: 0.16794227847788068\n",
      "Average test loss: 0.005303011593305402\n",
      "Epoch 118/300\n",
      "Average training loss: 0.16658321028285555\n",
      "Average test loss: 0.0059145770503415\n",
      "Epoch 119/300\n",
      "Average training loss: 0.16619854219754537\n",
      "Average test loss: 0.005640492686380943\n",
      "Epoch 120/300\n",
      "Average training loss: 0.17011086024178398\n",
      "Average test loss: 0.005370762083265516\n",
      "Epoch 121/300\n",
      "Average training loss: 0.16610321711169349\n",
      "Average test loss: 0.005185573396583398\n",
      "Epoch 122/300\n",
      "Average training loss: 0.16422698732217153\n",
      "Average test loss: 0.0054993250966072086\n",
      "Epoch 123/300\n",
      "Average training loss: 0.16592414712905884\n",
      "Average test loss: 0.005321278063787354\n",
      "Epoch 124/300\n",
      "Average training loss: 0.163887410097652\n",
      "Average test loss: 0.0063710481267836355\n",
      "Epoch 125/300\n",
      "Average training loss: 0.16684118378162385\n",
      "Average test loss: 0.005453611545264721\n",
      "Epoch 126/300\n",
      "Average training loss: 0.16453443172242907\n",
      "Average test loss: 0.0057512267149156995\n",
      "Epoch 127/300\n",
      "Average training loss: 0.16305781500869326\n",
      "Average test loss: 0.005334840918373731\n",
      "Epoch 128/300\n",
      "Average training loss: 0.16233783937162824\n",
      "Average test loss: 0.005428503599017858\n",
      "Epoch 129/300\n",
      "Average training loss: 0.16270278260442947\n",
      "Average test loss: 0.005565165190647045\n",
      "Epoch 130/300\n",
      "Average training loss: 0.16229756751325394\n",
      "Average test loss: 0.0054771135699831774\n",
      "Epoch 131/300\n",
      "Average training loss: 0.16278288560443455\n",
      "Average test loss: 0.005230743545211023\n",
      "Epoch 132/300\n",
      "Average training loss: 0.1616451274951299\n",
      "Average test loss: 0.0058173166509303785\n",
      "Epoch 133/300\n",
      "Average training loss: 0.16192194718784755\n",
      "Average test loss: 0.005304183946301539\n",
      "Epoch 134/300\n",
      "Average training loss: 0.17029048428270552\n",
      "Average test loss: 0.0053306497488584785\n",
      "Epoch 135/300\n",
      "Average training loss: 0.16471020282639398\n",
      "Average test loss: 0.005393200375967556\n",
      "Epoch 136/300\n",
      "Average training loss: 0.16092329155074225\n",
      "Average test loss: 0.005308388558940755\n",
      "Epoch 137/300\n",
      "Average training loss: 0.16397763678762647\n",
      "Average test loss: 0.00541535808559921\n",
      "Epoch 138/300\n",
      "Average training loss: 0.1597955303589503\n",
      "Average test loss: 0.005225410158435503\n",
      "Epoch 139/300\n",
      "Average training loss: 0.16082660431332058\n",
      "Average test loss: 0.005346461256345113\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1599546584222052\n",
      "Average test loss: 0.005627653668738073\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1603066913684209\n",
      "Average test loss: 0.005406515563113822\n",
      "Epoch 142/300\n",
      "Average training loss: 0.16565075740549298\n",
      "Average test loss: 0.006164192075530688\n",
      "Epoch 143/300\n",
      "Average training loss: 0.1604927706056171\n",
      "Average test loss: 0.005400585450232029\n",
      "Epoch 144/300\n",
      "Average training loss: 0.15846169070402782\n",
      "Average test loss: 0.006997865201284488\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1590978427330653\n",
      "Average test loss: 0.005427879941960176\n",
      "Epoch 146/300\n",
      "Average training loss: 0.1588442669974433\n",
      "Average test loss: 0.0056251783786962425\n",
      "Epoch 147/300\n",
      "Average training loss: 0.15974830414189234\n",
      "Average test loss: 0.00554594674706459\n",
      "Epoch 148/300\n",
      "Average training loss: 0.15828905118836298\n",
      "Average test loss: 0.005553539923909638\n",
      "Epoch 149/300\n",
      "Average training loss: 0.15757487882508173\n",
      "Average test loss: 0.0052481737658381464\n",
      "Epoch 150/300\n",
      "Average training loss: 0.15986736192968157\n",
      "Average test loss: 0.005450142720507251\n",
      "Epoch 151/300\n",
      "Average training loss: 0.1590804068379932\n",
      "Average test loss: 0.005430695210066107\n",
      "Epoch 152/300\n",
      "Average training loss: 0.15711241606871287\n",
      "Average test loss: 0.005569775443110201\n",
      "Epoch 153/300\n",
      "Average training loss: 0.15743709097305933\n",
      "Average test loss: 0.005437065069874128\n",
      "Epoch 154/300\n",
      "Average training loss: 0.15666215048895943\n",
      "Average test loss: 0.005381733277191718\n",
      "Epoch 155/300\n",
      "Average training loss: 0.15741773815949758\n",
      "Average test loss: 0.006730590316570468\n",
      "Epoch 156/300\n",
      "Average training loss: 0.15663903991381328\n",
      "Average test loss: 0.005425171636872821\n",
      "Epoch 157/300\n",
      "Average training loss: 0.15720661722289192\n",
      "Average test loss: 0.005360792353335354\n",
      "Epoch 158/300\n",
      "Average training loss: 0.15546420822540918\n",
      "Average test loss: 0.005384790970219506\n",
      "Epoch 159/300\n",
      "Average training loss: 0.15518431814511616\n",
      "Average test loss: 0.005396117327941789\n",
      "Epoch 160/300\n",
      "Average training loss: 0.15693584382534026\n",
      "Average test loss: 0.005328990020478765\n",
      "Epoch 161/300\n",
      "Average training loss: 0.15613110935688018\n",
      "Average test loss: 0.005676158826798201\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1540236877931489\n",
      "Average test loss: 0.005454873004721271\n",
      "Epoch 163/300\n",
      "Average training loss: 0.15581573095586565\n",
      "Average test loss: 0.005395145645986\n",
      "Epoch 164/300\n",
      "Average training loss: 0.1553812999063068\n",
      "Average test loss: 0.0052711600789593326\n",
      "Epoch 165/300\n",
      "Average training loss: 0.15403493330213758\n",
      "Average test loss: 0.005385605640295479\n",
      "Epoch 166/300\n",
      "Average training loss: 0.15519553207026587\n",
      "Average test loss: 0.005411528003712495\n",
      "Epoch 167/300\n",
      "Average training loss: 0.15425540749231975\n",
      "Average test loss: 0.0056100074818564785\n",
      "Epoch 168/300\n",
      "Average training loss: 0.15456936430931092\n",
      "Average test loss: 0.005737978814790646\n",
      "Epoch 169/300\n",
      "Average training loss: 0.15286551540427737\n",
      "Average test loss: 0.005550063129100535\n",
      "Epoch 170/300\n",
      "Average training loss: 0.15356785380840302\n",
      "Average test loss: 0.0052434226392457884\n",
      "Epoch 171/300\n",
      "Average training loss: 0.1528271959092882\n",
      "Average test loss: 0.005855504017737176\n",
      "Epoch 172/300\n",
      "Average training loss: 0.15321475434303283\n",
      "Average test loss: 0.00558678814686007\n",
      "Epoch 173/300\n",
      "Average training loss: 0.15444491830137042\n",
      "Average test loss: 0.005472218376480871\n",
      "Epoch 174/300\n",
      "Average training loss: 0.1528665859301885\n",
      "Average test loss: 0.005352352167376214\n",
      "Epoch 175/300\n",
      "Average training loss: 0.1520497901969486\n",
      "Average test loss: 0.005510012432518933\n",
      "Epoch 176/300\n",
      "Average training loss: 0.1541075040102005\n",
      "Average test loss: 0.005306038822150892\n",
      "Epoch 177/300\n",
      "Average training loss: 0.15240823719236585\n",
      "Average test loss: 0.005420379208607806\n",
      "Epoch 178/300\n",
      "Average training loss: 0.15208071138461432\n",
      "Average test loss: 0.005586579474724001\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1516093048254649\n",
      "Average test loss: 0.005196462271114191\n",
      "Epoch 180/300\n",
      "Average training loss: 0.15313341856002807\n",
      "Average test loss: 0.006707306491418017\n",
      "Epoch 181/300\n",
      "Average training loss: 0.15455762244595422\n",
      "Average test loss: 0.005313881690303485\n",
      "Epoch 182/300\n",
      "Average training loss: 0.15487746555937662\n",
      "Average test loss: 0.005384640627023246\n",
      "Epoch 183/300\n",
      "Average training loss: 0.1507513370513916\n",
      "Average test loss: 0.005210673309862614\n",
      "Epoch 184/300\n",
      "Average training loss: 0.15165721513165367\n",
      "Average test loss: 0.005258400195174747\n",
      "Epoch 185/300\n",
      "Average training loss: 0.15121479566891988\n",
      "Average test loss: 0.005802127066999674\n",
      "Epoch 186/300\n",
      "Average training loss: 0.15070511289437613\n",
      "Average test loss: 0.005318676476677259\n",
      "Epoch 187/300\n",
      "Average training loss: 0.14990960289372338\n",
      "Average test loss: 0.00525318133044574\n",
      "Epoch 188/300\n",
      "Average training loss: 0.14986145039399465\n",
      "Average test loss: 0.005680084993234939\n",
      "Epoch 189/300\n",
      "Average training loss: 0.15178749149375492\n",
      "Average test loss: 0.005724871883789698\n",
      "Epoch 190/300\n",
      "Average training loss: 0.14994592361980014\n",
      "Average test loss: 0.00532147853448987\n",
      "Epoch 191/300\n",
      "Average training loss: 0.1492133516934183\n",
      "Average test loss: 0.005271177950418658\n",
      "Epoch 192/300\n",
      "Average training loss: 0.14944582796096803\n",
      "Average test loss: 0.005424278632634216\n",
      "Epoch 193/300\n",
      "Average training loss: 0.15694634875324037\n",
      "Average test loss: 0.005241387756748332\n",
      "Epoch 194/300\n",
      "Average training loss: 0.15502981392542522\n",
      "Average test loss: 0.005118634223937988\n",
      "Epoch 195/300\n",
      "Average training loss: 0.14861996830834281\n",
      "Average test loss: 0.005419380452069971\n",
      "Epoch 196/300\n",
      "Average training loss: 0.1482076613108317\n",
      "Average test loss: 0.005338953227218654\n",
      "Epoch 197/300\n",
      "Average training loss: 0.1498683689435323\n",
      "Average test loss: 0.005316690220187108\n",
      "Epoch 198/300\n",
      "Average training loss: 0.14892386991447873\n",
      "Average test loss: 0.005562290166401201\n",
      "Epoch 199/300\n",
      "Average training loss: 0.1510290708674325\n",
      "Average test loss: 0.006781758406509956\n",
      "Epoch 200/300\n",
      "Average training loss: 0.14910482087400226\n",
      "Average test loss: 0.005267918660408921\n",
      "Epoch 201/300\n",
      "Average training loss: 0.14880795809957717\n",
      "Average test loss: 0.0056753046446376375\n",
      "Epoch 202/300\n",
      "Average training loss: 0.1479840645260281\n",
      "Average test loss: 0.0053607746749702425\n",
      "Epoch 203/300\n",
      "Average training loss: 0.15238770047823588\n",
      "Average test loss: 0.005299307783030801\n",
      "Epoch 204/300\n",
      "Average training loss: 0.14813989253838858\n",
      "Average test loss: 0.005634377164973153\n",
      "Epoch 205/300\n",
      "Average training loss: 0.1472879816558626\n",
      "Average test loss: 0.0059380575194954875\n",
      "Epoch 206/300\n",
      "Average training loss: 0.14763062373797098\n",
      "Average test loss: 0.00520644735565616\n",
      "Epoch 207/300\n",
      "Average training loss: 0.14826834568712446\n",
      "Average test loss: 0.006882997450729211\n",
      "Epoch 208/300\n",
      "Average training loss: 0.15364429441425537\n",
      "Average test loss: 0.005594666539794869\n",
      "Epoch 209/300\n",
      "Average training loss: 0.14698424577713012\n",
      "Average test loss: 0.005610003363548053\n",
      "Epoch 210/300\n",
      "Average training loss: 0.14678113262520895\n",
      "Average test loss: 0.005681369774871403\n",
      "Epoch 211/300\n",
      "Average training loss: 0.14786653435230254\n",
      "Average test loss: 0.01394139195813073\n",
      "Epoch 212/300\n",
      "Average training loss: 0.14701707530021668\n",
      "Average test loss: 0.00625016458498107\n",
      "Epoch 213/300\n",
      "Average training loss: 0.16649595420890384\n",
      "Average test loss: 0.0052270431187417775\n",
      "Epoch 214/300\n",
      "Average training loss: 0.16233539161417218\n",
      "Average test loss: 0.005435150284320116\n",
      "Epoch 215/300\n",
      "Average training loss: 0.15179823336336348\n",
      "Average test loss: 0.0057057140585449004\n",
      "Epoch 216/300\n",
      "Average training loss: 0.14832734674215317\n",
      "Average test loss: 0.005626703173336056\n",
      "Epoch 217/300\n",
      "Average training loss: 0.1479822126030922\n",
      "Average test loss: 0.00638868408360415\n",
      "Epoch 218/300\n",
      "Average training loss: 0.14821781239244672\n",
      "Average test loss: 0.007424018479055828\n",
      "Epoch 219/300\n",
      "Average training loss: 0.14647382748126983\n",
      "Average test loss: 0.0053192869221998585\n",
      "Epoch 220/300\n",
      "Average training loss: 0.1462870553864373\n",
      "Average test loss: 0.005397203991396559\n",
      "Epoch 221/300\n",
      "Average training loss: 0.14579054640399086\n",
      "Average test loss: 0.010480897382729584\n",
      "Epoch 222/300\n",
      "Average training loss: 0.1470594538781378\n",
      "Average test loss: 0.0052368960053556495\n",
      "Epoch 223/300\n",
      "Average training loss: 0.1456206317676438\n",
      "Average test loss: 0.007272540528741148\n",
      "Epoch 224/300\n",
      "Average training loss: 0.14589176269372303\n",
      "Average test loss: 0.005488723330199718\n",
      "Epoch 225/300\n",
      "Average training loss: 0.14514303926626843\n",
      "Average test loss: 0.005619218780762619\n",
      "Epoch 226/300\n",
      "Average training loss: 0.14573031277126736\n",
      "Average test loss: 0.005522446556637684\n",
      "Epoch 227/300\n",
      "Average training loss: 0.14548332630263436\n",
      "Average test loss: 0.00543279576756888\n",
      "Epoch 228/300\n",
      "Average training loss: 0.15462820188866722\n",
      "Average test loss: 0.005275213731866744\n",
      "Epoch 229/300\n",
      "Average training loss: 0.14526802553070917\n",
      "Average test loss: 0.005420348850803243\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1450377196205987\n",
      "Average test loss: 0.005289963851786322\n",
      "Epoch 231/300\n",
      "Average training loss: 0.14418788181410896\n",
      "Average test loss: 0.005149825481904877\n",
      "Epoch 232/300\n",
      "Average training loss: 0.14739470875263214\n",
      "Average test loss: 0.005201798733737733\n",
      "Epoch 233/300\n",
      "Average training loss: 0.1439415397644043\n",
      "Average test loss: 0.005365000881668594\n",
      "Epoch 234/300\n",
      "Average training loss: 0.14415563512510723\n",
      "Average test loss: 0.005883927525538537\n",
      "Epoch 235/300\n",
      "Average training loss: 0.14450893939203685\n",
      "Average test loss: 0.005269134601371156\n",
      "Epoch 236/300\n",
      "Average training loss: 0.1440862950351503\n",
      "Average test loss: 0.005192482078861859\n",
      "Epoch 237/300\n",
      "Average training loss: 0.14394470869170295\n",
      "Average test loss: 0.005699083435866567\n",
      "Epoch 238/300\n",
      "Average training loss: 0.1448414793809255\n",
      "Average test loss: 0.005297142394300964\n",
      "Epoch 239/300\n",
      "Average training loss: 0.14353935021824307\n",
      "Average test loss: 0.005444080116020308\n",
      "Epoch 240/300\n",
      "Average training loss: 0.14411134816540613\n",
      "Average test loss: 0.005303004802515109\n",
      "Epoch 241/300\n",
      "Average training loss: 0.14311393465598424\n",
      "Average test loss: 0.006380209805650843\n",
      "Epoch 242/300\n",
      "Average training loss: 0.1438864052693049\n",
      "Average test loss: 0.005601949803324209\n",
      "Epoch 243/300\n",
      "Average training loss: 0.14367246533764733\n",
      "Average test loss: 0.005308137510385778\n",
      "Epoch 244/300\n",
      "Average training loss: 0.14343129723601872\n",
      "Average test loss: 0.005204249877482653\n",
      "Epoch 245/300\n",
      "Average training loss: 0.14235443217224544\n",
      "Average test loss: 0.00581163087570005\n",
      "Epoch 246/300\n",
      "Average training loss: 0.14333610745271047\n",
      "Average test loss: 0.006069347498731481\n",
      "Epoch 247/300\n",
      "Average training loss: 0.14228771264023252\n",
      "Average test loss: 0.005319794193738037\n",
      "Epoch 248/300\n",
      "Average training loss: 0.14514530506398943\n",
      "Average test loss: 0.007585978748897711\n",
      "Epoch 249/300\n",
      "Average training loss: 0.14274529185560014\n",
      "Average test loss: 0.0053340878072712155\n",
      "Epoch 250/300\n",
      "Average training loss: 0.14591002158323924\n",
      "Average test loss: 0.005443547508782811\n",
      "Epoch 251/300\n",
      "Average training loss: 0.14218972488244375\n",
      "Average test loss: 0.00547719132900238\n",
      "Epoch 252/300\n",
      "Average training loss: 0.14139669230249194\n",
      "Average test loss: 0.0053201152380141946\n",
      "Epoch 253/300\n",
      "Average training loss: 0.14322606980800628\n",
      "Average test loss: 0.0060701756982339755\n",
      "Epoch 254/300\n",
      "Average training loss: 0.1427293546795845\n",
      "Average test loss: 0.005528214109440645\n",
      "Epoch 255/300\n",
      "Average training loss: 0.1414023736053043\n",
      "Average test loss: 0.005215280658048061\n",
      "Epoch 256/300\n",
      "Average training loss: 0.14247272786829207\n",
      "Average test loss: 0.005389104241712226\n",
      "Epoch 257/300\n",
      "Average training loss: 0.14149487034479777\n",
      "Average test loss: 0.005364377250067062\n",
      "Epoch 258/300\n",
      "Average training loss: 0.14092703738477494\n",
      "Average test loss: 0.005893810862261388\n",
      "Epoch 259/300\n",
      "Average training loss: 0.14074977909194097\n",
      "Average test loss: 0.0053187280827098424\n",
      "Epoch 260/300\n",
      "Average training loss: 0.14200777628686692\n",
      "Average test loss: 0.005556938954111602\n",
      "Epoch 261/300\n",
      "Average training loss: 0.1416444120672014\n",
      "Average test loss: 0.0055231081839236945\n",
      "Epoch 262/300\n",
      "Average training loss: 0.1412905402580897\n",
      "Average test loss: 0.005675286151468754\n",
      "Epoch 263/300\n",
      "Average training loss: 0.14108111126555337\n",
      "Average test loss: 0.005382718241877026\n",
      "Epoch 264/300\n",
      "Average training loss: 0.1413030522796843\n",
      "Average test loss: 0.00556826733963357\n",
      "Epoch 265/300\n",
      "Average training loss: 0.14243961873319413\n",
      "Average test loss: 0.005457811582005687\n",
      "Epoch 266/300\n",
      "Average training loss: 0.13986330292622248\n",
      "Average test loss: 0.005301295949767033\n",
      "Epoch 267/300\n",
      "Average training loss: 0.13996410904990303\n",
      "Average test loss: 0.0055212738683654205\n",
      "Epoch 268/300\n",
      "Average training loss: 0.14033835687902238\n",
      "Average test loss: 0.00568879830547505\n",
      "Epoch 269/300\n",
      "Average training loss: 0.14352387843529382\n",
      "Average test loss: 0.005573022872209549\n",
      "Epoch 270/300\n",
      "Average training loss: 0.1393088066511684\n",
      "Average test loss: 0.0063721339599125916\n",
      "Epoch 271/300\n",
      "Average training loss: 0.1401040928496255\n",
      "Average test loss: 0.005709931597527531\n",
      "Epoch 272/300\n",
      "Average training loss: 0.14067295511563618\n",
      "Average test loss: 0.005299815645441413\n",
      "Epoch 273/300\n",
      "Average training loss: 0.13974827082951863\n",
      "Average test loss: 0.005323179848492145\n",
      "Epoch 274/300\n",
      "Average training loss: 0.13978074059883752\n",
      "Average test loss: 0.005554180093523529\n",
      "Epoch 275/300\n",
      "Average training loss: 0.1391093582643403\n",
      "Average test loss: 0.005542392942640516\n",
      "Epoch 276/300\n",
      "Average training loss: 0.1395412007437812\n",
      "Average test loss: 0.005568435609754589\n",
      "Epoch 277/300\n",
      "Average training loss: 0.14168893015384673\n",
      "Average test loss: 0.005253526499701871\n",
      "Epoch 278/300\n",
      "Average training loss: 0.13921900146537358\n",
      "Average test loss: 0.0054696055497560235\n",
      "Epoch 279/300\n",
      "Average training loss: 0.13894303656286663\n",
      "Average test loss: 0.005408115229672856\n",
      "Epoch 280/300\n",
      "Average training loss: 0.1399118217892117\n",
      "Average test loss: 0.006534457535793384\n",
      "Epoch 281/300\n",
      "Average training loss: 0.13840424778726365\n",
      "Average test loss: 0.005440175521704886\n",
      "Epoch 282/300\n",
      "Average training loss: 0.13861985713905758\n",
      "Average test loss: 0.005298124901950359\n",
      "Epoch 283/300\n",
      "Average training loss: 0.14010950542820824\n",
      "Average test loss: 0.005577966866807805\n",
      "Epoch 284/300\n",
      "Average training loss: 0.13855794132418103\n",
      "Average test loss: 0.005455509693672259\n",
      "Epoch 285/300\n",
      "Average training loss: 0.13875995867119895\n",
      "Average test loss: 0.005412736095488071\n",
      "Epoch 286/300\n",
      "Average training loss: 0.13807277292675443\n",
      "Average test loss: 0.028379198395543627\n",
      "Epoch 287/300\n",
      "Average training loss: 0.13996761054462856\n",
      "Average test loss: 0.005603845965945058\n",
      "Epoch 288/300\n",
      "Average training loss: 0.13744349516100354\n",
      "Average test loss: 0.005253092942552434\n",
      "Epoch 289/300\n",
      "Average training loss: 0.13822905244429906\n",
      "Average test loss: 0.005858876112020678\n",
      "Epoch 290/300\n",
      "Average training loss: 0.13881010799275503\n",
      "Average test loss: 0.0054454074510269694\n",
      "Epoch 291/300\n",
      "Average training loss: 0.13794764602184295\n",
      "Average test loss: 0.005479883172031906\n",
      "Epoch 292/300\n",
      "Average training loss: 0.1369310144583384\n",
      "Average test loss: 0.005405468967225817\n",
      "Epoch 293/300\n",
      "Average training loss: 0.13744603097438812\n",
      "Average test loss: 0.005305025520424048\n",
      "Epoch 294/300\n",
      "Average training loss: 0.13810806337330075\n",
      "Average test loss: 0.005302602921095159\n",
      "Epoch 295/300\n",
      "Average training loss: 0.13778104355600146\n",
      "Average test loss: 0.005417910209960407\n",
      "Epoch 296/300\n",
      "Average training loss: 0.1378415084415012\n",
      "Average test loss: 0.00539745081257489\n",
      "Epoch 297/300\n",
      "Average training loss: 0.1383214767244127\n",
      "Average test loss: 0.005364463418722152\n",
      "Epoch 298/300\n",
      "Average training loss: 0.1381834020614624\n",
      "Average test loss: 0.005347010998676221\n",
      "Epoch 299/300\n",
      "Average training loss: 0.1370331311888165\n",
      "Average test loss: 0.005696373803748025\n",
      "Epoch 300/300\n",
      "Average training loss: 0.13896457526418898\n",
      "Average test loss: 0.005394331886950467\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.195082506815593\n",
      "Average test loss: 16726.301562259654\n",
      "Epoch 2/300\n",
      "Average training loss: 8.637917172749837\n",
      "Average test loss: 13.909244821870493\n",
      "Epoch 3/300\n",
      "Average training loss: 8.117745426601834\n",
      "Average test loss: 0.01380349864396784\n",
      "Epoch 4/300\n",
      "Average training loss: 7.760781886630588\n",
      "Average test loss: 1057.5856617246213\n",
      "Epoch 5/300\n",
      "Average training loss: 7.480111202663846\n",
      "Average test loss: 0.005849287652307087\n",
      "Epoch 6/300\n",
      "Average training loss: 7.234458185407851\n",
      "Average test loss: 0.012243680155111684\n",
      "Epoch 7/300\n",
      "Average training loss: 7.01881407462226\n",
      "Average test loss: 0.010363901651567883\n",
      "Epoch 8/300\n",
      "Average training loss: 6.809339680565728\n",
      "Average test loss: 7.268322111758921\n",
      "Epoch 9/300\n",
      "Average training loss: 6.602465552012125\n",
      "Average test loss: 0.005754951929052671\n",
      "Epoch 10/300\n",
      "Average training loss: 6.387366936577691\n",
      "Average test loss: 0.053621221051861845\n",
      "Epoch 11/300\n",
      "Average training loss: 6.165551252153184\n",
      "Average test loss: 0.00819578786359893\n",
      "Epoch 12/300\n",
      "Average training loss: 5.941680908626981\n",
      "Average test loss: 0.005356988044662608\n",
      "Epoch 13/300\n",
      "Average training loss: 5.712519259558784\n",
      "Average test loss: 0.0051531181484460835\n",
      "Epoch 14/300\n",
      "Average training loss: 5.479597355312771\n",
      "Average test loss: 0.005761216409090493\n",
      "Epoch 15/300\n",
      "Average training loss: 5.2367282252841525\n",
      "Average test loss: 0.004866420903553565\n",
      "Epoch 16/300\n",
      "Average training loss: 4.990351757473416\n",
      "Average test loss: 0.0653137702267203\n",
      "Epoch 17/300\n",
      "Average training loss: 4.7473270280626085\n",
      "Average test loss: 0.007819962586793634\n",
      "Epoch 18/300\n",
      "Average training loss: 4.509135495927599\n",
      "Average test loss: 0.005173765897336933\n",
      "Epoch 19/300\n",
      "Average training loss: 4.269116046905517\n",
      "Average test loss: 0.007017326970895132\n",
      "Epoch 20/300\n",
      "Average training loss: 4.0309564726087785\n",
      "Average test loss: 0.004337110933123363\n",
      "Epoch 21/300\n",
      "Average training loss: 3.7881357027689617\n",
      "Average test loss: 0.004771271172083086\n",
      "Epoch 22/300\n",
      "Average training loss: 3.5414628014034695\n",
      "Average test loss: 0.0038696196193082465\n",
      "Epoch 23/300\n",
      "Average training loss: 3.291543298297458\n",
      "Average test loss: 0.004328319090935919\n",
      "Epoch 24/300\n",
      "Average training loss: 3.054140851550632\n",
      "Average test loss: 0.0036967810398588576\n",
      "Epoch 25/300\n",
      "Average training loss: 2.8136675022972955\n",
      "Average test loss: 0.003953908191372951\n",
      "Epoch 26/300\n",
      "Average training loss: 2.5879671221839056\n",
      "Average test loss: 0.0039151089890963504\n",
      "Epoch 27/300\n",
      "Average training loss: 2.3644789146847196\n",
      "Average test loss: 0.004965941807462109\n",
      "Epoch 28/300\n",
      "Average training loss: 2.1461620315975614\n",
      "Average test loss: 0.004363582460002766\n",
      "Epoch 29/300\n",
      "Average training loss: 1.9349960618548923\n",
      "Average test loss: 0.0036791396290063857\n",
      "Epoch 30/300\n",
      "Average training loss: 1.7408578304714626\n",
      "Average test loss: 0.003545888503599498\n",
      "Epoch 31/300\n",
      "Average training loss: 1.5585745233959623\n",
      "Average test loss: 0.004402708255789346\n",
      "Epoch 32/300\n",
      "Average training loss: 1.3899472881952921\n",
      "Average test loss: 0.0034934846152447994\n",
      "Epoch 33/300\n",
      "Average training loss: 1.2402136490080091\n",
      "Average test loss: 0.004865429773926735\n",
      "Epoch 34/300\n",
      "Average training loss: 1.0970919396082561\n",
      "Average test loss: 0.003681253701862362\n",
      "Epoch 35/300\n",
      "Average training loss: 0.9758317604594761\n",
      "Average test loss: 0.0034871069757888714\n",
      "Epoch 36/300\n",
      "Average training loss: 0.8580346119668749\n",
      "Average test loss: 0.0035678499374124737\n",
      "Epoch 37/300\n",
      "Average training loss: 0.767144237836202\n",
      "Average test loss: 0.0036514007823748723\n",
      "Epoch 38/300\n",
      "Average training loss: 0.6863546971744962\n",
      "Average test loss: 0.0033995818793773652\n",
      "Epoch 39/300\n",
      "Average training loss: 0.6146010121239556\n",
      "Average test loss: 0.0038458801350659795\n",
      "Epoch 40/300\n",
      "Average training loss: 0.5510347672568428\n",
      "Average test loss: 0.00348309967170159\n",
      "Epoch 41/300\n",
      "Average training loss: 0.49566573323143853\n",
      "Average test loss: 0.0044760267651743355\n",
      "Epoch 42/300\n",
      "Average training loss: 0.44258337092399597\n",
      "Average test loss: 0.0035388588565919135\n",
      "Epoch 43/300\n",
      "Average training loss: 0.40009274199273853\n",
      "Average test loss: 0.0035279487607379756\n",
      "Epoch 44/300\n",
      "Average training loss: 0.36414616791407267\n",
      "Average test loss: 0.0035353790720303855\n",
      "Epoch 45/300\n",
      "Average training loss: 0.3337379115687476\n",
      "Average test loss: 0.0033143261495149796\n",
      "Epoch 46/300\n",
      "Average training loss: 0.3071684254010518\n",
      "Average test loss: 0.0034627230506804256\n",
      "Epoch 47/300\n",
      "Average training loss: 0.28342911557356515\n",
      "Average test loss: 0.0034413156890206868\n",
      "Epoch 48/300\n",
      "Average training loss: 0.26174404033025106\n",
      "Average test loss: 0.0033504457502729363\n",
      "Epoch 49/300\n",
      "Average training loss: 0.2431506468984816\n",
      "Average test loss: 0.003339057433936331\n",
      "Epoch 50/300\n",
      "Average training loss: 0.23118457270993126\n",
      "Average test loss: 0.0035239878774931035\n",
      "Epoch 51/300\n",
      "Average training loss: 0.21558454081747266\n",
      "Average test loss: 0.003308038569572899\n",
      "Epoch 52/300\n",
      "Average training loss: 0.20260117389096155\n",
      "Average test loss: 0.0034221209099309312\n",
      "Epoch 53/300\n",
      "Average training loss: 0.19568878022829692\n",
      "Average test loss: 0.0034158994687928093\n",
      "Epoch 54/300\n",
      "Average training loss: 0.18688228584660424\n",
      "Average test loss: 0.0032625756470693484\n",
      "Epoch 55/300\n",
      "Average training loss: 0.18405050807529025\n",
      "Average test loss: 0.0033272666446864606\n",
      "Epoch 56/300\n",
      "Average training loss: 0.17534242187605964\n",
      "Average test loss: 0.00331865997860829\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1704300020005968\n",
      "Average test loss: 0.0033751420186211665\n",
      "Epoch 58/300\n",
      "Average training loss: 0.16865504529741077\n",
      "Average test loss: 0.0033180870049529607\n",
      "Epoch 59/300\n",
      "Average training loss: 0.16044106760289933\n",
      "Average test loss: 0.003195154920220375\n",
      "Epoch 60/300\n",
      "Average training loss: 0.15765647575590347\n",
      "Average test loss: 0.0032914566327300336\n",
      "Epoch 61/300\n",
      "Average training loss: 0.15550083963076275\n",
      "Average test loss: 0.00329240155675345\n",
      "Epoch 62/300\n",
      "Average training loss: 0.14909266414907243\n",
      "Average test loss: 0.0033506722044613626\n",
      "Epoch 63/300\n",
      "Average training loss: 0.14895313880178662\n",
      "Average test loss: 0.003349382121943765\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1449747659895155\n",
      "Average test loss: 0.0033236992802057\n",
      "Epoch 65/300\n",
      "Average training loss: 0.14117976068125832\n",
      "Average test loss: 0.0032081322436117465\n",
      "Epoch 66/300\n",
      "Average training loss: 0.14053789773252276\n",
      "Average test loss: 0.008727978221244282\n",
      "Epoch 67/300\n",
      "Average training loss: 0.14173284403483072\n",
      "Average test loss: 0.0032689756657928227\n",
      "Epoch 68/300\n",
      "Average training loss: 0.13587938770982955\n",
      "Average test loss: 0.0031822517058915562\n",
      "Epoch 69/300\n",
      "Average training loss: 0.13168999988502927\n",
      "Average test loss: 0.003240712497590317\n",
      "Epoch 70/300\n",
      "Average training loss: 0.13322216383616128\n",
      "Average test loss: 0.003175715818276836\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1284226423766878\n",
      "Average test loss: 0.003267188717611134\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1263130608929528\n",
      "Average test loss: 0.003320042690055238\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1251650898721483\n",
      "Average test loss: 0.0033669006501634915\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12344705255826315\n",
      "Average test loss: 0.0031511466356201304\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12248563810189565\n",
      "Average test loss: 0.003211094964295626\n",
      "Epoch 76/300\n",
      "Average training loss: 0.12045544289880329\n",
      "Average test loss: 0.0033265099335047935\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11963555081685384\n",
      "Average test loss: 0.003174080916369955\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11952664799822701\n",
      "Average test loss: 0.0031243998553189965\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11696616679430008\n",
      "Average test loss: 0.003154439530438847\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11905435538623069\n",
      "Average test loss: 0.0033952618047801983\n",
      "Epoch 81/300\n",
      "Average training loss: 0.1148874946170383\n",
      "Average test loss: 0.0032048129166165986\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11461997071239684\n",
      "Average test loss: 0.003810246906346745\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11269733731614219\n",
      "Average test loss: 0.0034301616930299335\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11298208679093255\n",
      "Average test loss: 0.0032341283867135646\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11356492298841477\n",
      "Average test loss: 0.0032495227998329533\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11160941383573744\n",
      "Average test loss: 0.03531677677647935\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11016026669740676\n",
      "Average test loss: 0.0037333486133979427\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11002546754810545\n",
      "Average test loss: 0.0030766600260718003\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10969031338559257\n",
      "Average test loss: 0.003896297202962968\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10961566607819663\n",
      "Average test loss: 0.003300005588473545\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10775471033652624\n",
      "Average test loss: 0.003062555273373922\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10848123686843449\n",
      "Average test loss: 0.0032366743282311494\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10759487837553024\n",
      "Average test loss: 0.0030286707969175443\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10989806934860018\n",
      "Average test loss: 0.00322235414882501\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10699840311540498\n",
      "Average test loss: 0.003285906247380707\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10714057548178567\n",
      "Average test loss: 0.0034051024404664833\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1057396303349071\n",
      "Average test loss: 0.0033966238871216774\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10625328268607458\n",
      "Average test loss: 0.0033203130457550288\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1233067404097981\n",
      "Average test loss: 0.003178030749368999\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10800241560406156\n",
      "Average test loss: 0.003075007219695383\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1086983839670817\n",
      "Average test loss: 0.003205619388156467\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10617703029844495\n",
      "Average test loss: 0.0034149691578414707\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10452543697092269\n",
      "Average test loss: 0.0030263078034751946\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10422408221165338\n",
      "Average test loss: 0.003260391143047147\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10685494367281596\n",
      "Average test loss: 0.0031525347992363904\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10470836710267596\n",
      "Average test loss: 0.003610555129332675\n",
      "Epoch 107/300\n",
      "Average training loss: 0.1041492240495152\n",
      "Average test loss: 0.0033540071816080146\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10588986309038269\n",
      "Average test loss: 0.003533816097821626\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10709701665904788\n",
      "Average test loss: 0.003095005179444949\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10384177160925336\n",
      "Average test loss: 0.0032357705142349005\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10265597110324436\n",
      "Average test loss: 0.003132769051939249\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10415179887082841\n",
      "Average test loss: 0.04803114321165615\n",
      "Epoch 113/300\n",
      "Average training loss: 0.12809166487058005\n",
      "Average test loss: 0.0034332497554520765\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10835463837782541\n",
      "Average test loss: 0.002971426075738337\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10318025265799628\n",
      "Average test loss: 0.0031157498156858814\n",
      "Epoch 116/300\n",
      "Average training loss: 0.1027611426115036\n",
      "Average test loss: 0.003004554290738371\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10352711449729071\n",
      "Average test loss: 0.0031509526500271426\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10174920376473003\n",
      "Average test loss: 0.0034939861924697955\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10203709403011534\n",
      "Average test loss: 0.004658786932213439\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10228200239605374\n",
      "Average test loss: 0.003044721061156856\n",
      "Epoch 121/300\n",
      "Average training loss: 0.101570878525575\n",
      "Average test loss: 0.002987227711205681\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10240194131268396\n",
      "Average test loss: 0.0031943330326014096\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10309801479842928\n",
      "Average test loss: 0.003130536271466149\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10018821668624878\n",
      "Average test loss: 0.003088254514046841\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10070391367541419\n",
      "Average test loss: 0.003089616296398971\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10105464647213618\n",
      "Average test loss: 0.003045943385404017\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10242941475576825\n",
      "Average test loss: 0.003070445786954628\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10063000441922082\n",
      "Average test loss: 0.0032855356393588914\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10098839957515399\n",
      "Average test loss: 0.0032303909046782387\n",
      "Epoch 130/300\n",
      "Average training loss: 0.1010140467815929\n",
      "Average test loss: 0.0034695006793157923\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09967846149206161\n",
      "Average test loss: 0.003038867769555913\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09893105510208342\n",
      "Average test loss: 0.003114386964796318\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10013189525736703\n",
      "Average test loss: 0.003193846010706491\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10365312505430646\n",
      "Average test loss: 0.003233734049109949\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09934134571419821\n",
      "Average test loss: 0.003422325987782743\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11002328620354335\n",
      "Average test loss: 0.00304222012249132\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09979504835605621\n",
      "Average test loss: 0.003177562323295408\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09889264520671633\n",
      "Average test loss: 0.0030742384551299944\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09836239603492948\n",
      "Average test loss: 0.003159836698323488\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09811515428622564\n",
      "Average test loss: 0.0035212329692310756\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09840087725056543\n",
      "Average test loss: 0.0029709500219259\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10002519611848726\n",
      "Average test loss: 0.003271492769734727\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09776147911945979\n",
      "Average test loss: 0.0030316543171389236\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10039912368191613\n",
      "Average test loss: 0.0030775836892426014\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09811546395222347\n",
      "Average test loss: 0.0030187887187219328\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09896927339500851\n",
      "Average test loss: 0.0034436726052727963\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09791225093603134\n",
      "Average test loss: 0.0030873590865068967\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09924526523219214\n",
      "Average test loss: 0.0029831962844149933\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09720445709096061\n",
      "Average test loss: 0.003293940409190125\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09786004242632124\n",
      "Average test loss: 0.003919201442350944\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0978672151433097\n",
      "Average test loss: 0.0034197530618144405\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09743331403864755\n",
      "Average test loss: 0.003912439284225305\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10184675237867567\n",
      "Average test loss: 0.0030800475434710583\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09616529070337614\n",
      "Average test loss: 0.0031186511744227675\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09657864897780948\n",
      "Average test loss: 0.003063704504321019\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09713511903418436\n",
      "Average test loss: 0.0030767957402600184\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09610324662923812\n",
      "Average test loss: 0.0032981455330219536\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09667889515558879\n",
      "Average test loss: 0.0031085778799735835\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09705307620101505\n",
      "Average test loss: 0.00305152394870917\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09556794229480955\n",
      "Average test loss: 0.003678712405471338\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09610697988006804\n",
      "Average test loss: 0.0036723893415182827\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09642392661174139\n",
      "Average test loss: 0.0030669287248618073\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09503981081644694\n",
      "Average test loss: 0.003001261040361391\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09747718463341395\n",
      "Average test loss: 0.003124784490921431\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0968568968574206\n",
      "Average test loss: 0.0031332972575392986\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09522529548406601\n",
      "Average test loss: 0.0029348588331292074\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09484661955303617\n",
      "Average test loss: 0.002958710052486923\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09558195833365123\n",
      "Average test loss: 0.0031141697332883874\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09503980440563625\n",
      "Average test loss: 0.003294972006024586\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09746808863348431\n",
      "Average test loss: 0.0029423704627487396\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09402130907111697\n",
      "Average test loss: 0.0031689524890647996\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09456827114688025\n",
      "Average test loss: 0.0029852035817586713\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09487763644920455\n",
      "Average test loss: 0.0033198325054513084\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09388652147187126\n",
      "Average test loss: 0.0031134513614492283\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09333151276244057\n",
      "Average test loss: 0.0041279868456638525\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09635354175832536\n",
      "Average test loss: 0.003049696503621009\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09406094188160366\n",
      "Average test loss: 0.0030825041642205584\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09885218694474962\n",
      "Average test loss: 0.0031527455736779504\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09316634979844093\n",
      "Average test loss: 0.002992095000627968\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09419171817435158\n",
      "Average test loss: 0.0028960050257543723\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09457367069853677\n",
      "Average test loss: 0.003137886363805996\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09469427908791436\n",
      "Average test loss: 0.003739709629780716\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0929769093990326\n",
      "Average test loss: 0.0031972555571960076\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0939852083656523\n",
      "Average test loss: 0.003240456340213617\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09698480606079102\n",
      "Average test loss: 0.0031284145886699356\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0939650386042065\n",
      "Average test loss: 0.0030797928563422628\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09277302068471908\n",
      "Average test loss: 0.0030802178918901417\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09315644510587057\n",
      "Average test loss: 0.003070518748006887\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09227916912237803\n",
      "Average test loss: 0.003087633881924881\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0916676746805509\n",
      "Average test loss: 0.0031059085476315682\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10066942615641487\n",
      "Average test loss: 0.002969553608240353\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09230399844381544\n",
      "Average test loss: 0.0030098228740195435\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09221645767158933\n",
      "Average test loss: 0.0031722569368365736\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09289485216140747\n",
      "Average test loss: 0.003002384891319606\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09462739134497113\n",
      "Average test loss: 0.00303303822916415\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09199757613076104\n",
      "Average test loss: 0.002952816090856989\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09194122302532196\n",
      "Average test loss: 0.003422297863082753\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09294449228710598\n",
      "Average test loss: 0.0030442808044867384\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09318049629529317\n",
      "Average test loss: 0.003445017879828811\n",
      "Epoch 200/300\n",
      "Average training loss: 0.092615329960982\n",
      "Average test loss: 0.009367988273087475\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09075328230857849\n",
      "Average test loss: 0.0032532595290491978\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09162123622496923\n",
      "Average test loss: 0.004049975959377157\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09117805647187763\n",
      "Average test loss: 0.002984061874449253\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09176483254300223\n",
      "Average test loss: 0.0037040239514576066\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09131627044412825\n",
      "Average test loss: 0.0030641784216794703\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0919892445868916\n",
      "Average test loss: 0.002903274490394526\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0912282967434989\n",
      "Average test loss: 0.0029613728628804284\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09118482636743122\n",
      "Average test loss: 0.0030941543190015687\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09110635437568029\n",
      "Average test loss: 0.0029511552206757997\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09060513223542108\n",
      "Average test loss: 0.0029310350846499206\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0906005458301968\n",
      "Average test loss: 0.0030021097666273516\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09135076977478133\n",
      "Average test loss: 0.003040130059338278\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09043508597877291\n",
      "Average test loss: 0.0032425258851920564\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0904563878443506\n",
      "Average test loss: 0.0031326935102956163\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09057182858718767\n",
      "Average test loss: 0.0030457171098225645\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09143318399455812\n",
      "Average test loss: 0.0029735965703924496\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09014016348785825\n",
      "Average test loss: 0.003112286145798862\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09000084855821397\n",
      "Average test loss: 0.0033383408755891853\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09046182350979912\n",
      "Average test loss: 0.0029481599614438084\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09207604953315524\n",
      "Average test loss: 0.0032609561503761345\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08900906536976497\n",
      "Average test loss: 0.002966388256599506\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08999695575899548\n",
      "Average test loss: 0.003326690317235059\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08956384444236755\n",
      "Average test loss: 0.0032141596285833254\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08957056158781052\n",
      "Average test loss: 0.0036563044540170167\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08975952736536662\n",
      "Average test loss: 0.0030362505048720373\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08975018339686923\n",
      "Average test loss: 0.003045673619127936\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08885517468717363\n",
      "Average test loss: 0.004342932550443543\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09007908285988701\n",
      "Average test loss: 0.0035001423702471788\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08915138319465848\n",
      "Average test loss: 0.0029132977695100837\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08834203590949376\n",
      "Average test loss: 0.003543938321371873\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09028373531831635\n",
      "Average test loss: 0.0031287363133289748\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08876497589879566\n",
      "Average test loss: 0.0034173117257240747\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0919776389532619\n",
      "Average test loss: 0.0030745107914424605\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08852342675129572\n",
      "Average test loss: 0.002974711824208498\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09175549110439088\n",
      "Average test loss: 0.0030298944283276797\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08890451835261451\n",
      "Average test loss: 0.0029348004311323165\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0881004014743699\n",
      "Average test loss: 0.0031511905238860182\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08886806599961387\n",
      "Average test loss: 0.002940647112619546\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08825589356819788\n",
      "Average test loss: 0.0030012379924042355\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08883111740483178\n",
      "Average test loss: 0.003036049124888248\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08826553081803852\n",
      "Average test loss: 0.0032273126331468422\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08700931726561652\n",
      "Average test loss: 0.003001337606459856\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08736272470156352\n",
      "Average test loss: 0.002976507763067881\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08787518899970584\n",
      "Average test loss: 0.0029976653723667065\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08831591296195984\n",
      "Average test loss: 0.0029962530785964596\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08759624854723612\n",
      "Average test loss: 0.003978549587643809\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08756950345966551\n",
      "Average test loss: 0.002954139395099547\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0882994408806165\n",
      "Average test loss: 0.0030150901840792763\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0902145778172546\n",
      "Average test loss: 0.0030561414977742568\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08760048821899626\n",
      "Average test loss: 0.0030868122946057053\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0877152918378512\n",
      "Average test loss: 0.0029624989804708296\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08780827097760306\n",
      "Average test loss: 0.0030325104066481192\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08821707432137595\n",
      "Average test loss: 0.002979560485109687\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0868211988011996\n",
      "Average test loss: 0.0029481243333882756\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08754829822646247\n",
      "Average test loss: 0.0029957598580254447\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0874647573961152\n",
      "Average test loss: 0.00310267356203662\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08738079025348028\n",
      "Average test loss: 0.0031505188201036718\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08700393993986978\n",
      "Average test loss: 0.004247208208259609\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08757604061894947\n",
      "Average test loss: 0.0029909490801187026\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08675650193293889\n",
      "Average test loss: 0.0031052029283924233\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08769908953375287\n",
      "Average test loss: 0.004627733038945331\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08575571431716283\n",
      "Average test loss: 0.0030448307821320164\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08643422581752142\n",
      "Average test loss: 0.003322036160673532\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0869307100309266\n",
      "Average test loss: 0.0037444893556336564\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08686657202243805\n",
      "Average test loss: 0.0030570596715228425\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08608695356713401\n",
      "Average test loss: 0.002980593911268645\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08731762029065027\n",
      "Average test loss: 0.0032302891018076074\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08641535841756397\n",
      "Average test loss: 0.003225668938209613\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08578326729271148\n",
      "Average test loss: 0.0031228584282928045\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08569198391834895\n",
      "Average test loss: 0.003166858715729581\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08626416180531184\n",
      "Average test loss: 0.0031965725639214117\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08578009712033802\n",
      "Average test loss: 0.0029673439872761566\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08571170620785819\n",
      "Average test loss: 0.0030706650896204844\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08588892869154612\n",
      "Average test loss: 0.0030890988481955396\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08583815292517344\n",
      "Average test loss: 0.0030136529575619433\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08588771858480242\n",
      "Average test loss: 0.003034377699510919\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08532381341523594\n",
      "Average test loss: 0.002998746346268389\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08693288944164912\n",
      "Average test loss: 0.003164310960099101\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08632459722624884\n",
      "Average test loss: 0.002928155267921587\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08533231649796168\n",
      "Average test loss: 0.0029667915370729233\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08491804535521401\n",
      "Average test loss: 0.0030867803446534606\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0862427881691191\n",
      "Average test loss: 0.0030273371752765443\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08461111106475194\n",
      "Average test loss: 0.0029416896344886885\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08453875221808752\n",
      "Average test loss: 0.0030016507597433197\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08605891329050064\n",
      "Average test loss: 0.0029910939666959973\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0851216222776307\n",
      "Average test loss: 0.003033363196377953\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0899197180668513\n",
      "Average test loss: 0.0029271220850447814\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08588349215189615\n",
      "Average test loss: 0.002919005643369423\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08403956076171663\n",
      "Average test loss: 0.0030912981101622185\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08434835029972924\n",
      "Average test loss: 0.003411581105656094\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08549386118517982\n",
      "Average test loss: 0.0032415156635559267\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08472428294022878\n",
      "Average test loss: 0.002960092153193222\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08446977412700653\n",
      "Average test loss: 0.0030023733214992617\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08599370574951172\n",
      "Average test loss: 0.004015400041515628\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08422852785057491\n",
      "Average test loss: 0.0030843270416888928\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08427516172329584\n",
      "Average test loss: 0.003436966612107224\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08496105501386854\n",
      "Average test loss: 0.003085756133413977\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08504079826010598\n",
      "Average test loss: 0.003274952254568537\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08405294364028507\n",
      "Average test loss: 0.0030424027947915924\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08453755365477668\n",
      "Average test loss: 0.0029526983872056006\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.670446882459853\n",
      "Average test loss: 0.01047293803592523\n",
      "Epoch 2/300\n",
      "Average training loss: 8.53530774307251\n",
      "Average test loss: 10.185930161370171\n",
      "Epoch 3/300\n",
      "Average training loss: 7.494183185153537\n",
      "Average test loss: 0.006792045488125748\n",
      "Epoch 4/300\n",
      "Average training loss: 6.822188478257921\n",
      "Average test loss: 0.007252156179812219\n",
      "Epoch 5/300\n",
      "Average training loss: 6.364687789493137\n",
      "Average test loss: 65.28425392134984\n",
      "Epoch 6/300\n",
      "Average training loss: 6.004256732516819\n",
      "Average test loss: 0.009318765425847636\n",
      "Epoch 7/300\n",
      "Average training loss: 5.673724313524034\n",
      "Average test loss: 0.012989568234110871\n",
      "Epoch 8/300\n",
      "Average training loss: 5.379944966210259\n",
      "Average test loss: 0.006690953826738728\n",
      "Epoch 9/300\n",
      "Average training loss: 5.116589667426215\n",
      "Average test loss: 0.008953531393574344\n",
      "Epoch 10/300\n",
      "Average training loss: 4.863686319139268\n",
      "Average test loss: 0.0034062156033598714\n",
      "Epoch 11/300\n",
      "Average training loss: 4.61981774520874\n",
      "Average test loss: 0.003339254366647866\n",
      "Epoch 12/300\n",
      "Average training loss: 4.373007600996229\n",
      "Average test loss: 0.004015720430140694\n",
      "Epoch 13/300\n",
      "Average training loss: 4.129122138977051\n",
      "Average test loss: 0.005145736945999993\n",
      "Epoch 14/300\n",
      "Average training loss: 3.880400818718804\n",
      "Average test loss: 0.003922098154822986\n",
      "Epoch 15/300\n",
      "Average training loss: 3.631613644917806\n",
      "Average test loss: 0.003586766961961985\n",
      "Epoch 16/300\n",
      "Average training loss: 3.3887701880137127\n",
      "Average test loss: 0.0027327715814527537\n",
      "Epoch 17/300\n",
      "Average training loss: 3.1308686531914605\n",
      "Average test loss: 0.003756591099831793\n",
      "Epoch 18/300\n",
      "Average training loss: 2.884772756576538\n",
      "Average test loss: 0.0035580039330654675\n",
      "Epoch 19/300\n",
      "Average training loss: 2.636807341893514\n",
      "Average test loss: 0.002608328909302751\n",
      "Epoch 20/300\n",
      "Average training loss: 2.4053594063652888\n",
      "Average test loss: 0.0029928571292095712\n",
      "Epoch 21/300\n",
      "Average training loss: 2.1865204897986517\n",
      "Average test loss: 0.0027035551224317816\n",
      "Epoch 22/300\n",
      "Average training loss: 1.9757055888705783\n",
      "Average test loss: 0.002681488132311238\n",
      "Epoch 23/300\n",
      "Average training loss: 1.7741835731930202\n",
      "Average test loss: 0.0027526780012995006\n",
      "Epoch 24/300\n",
      "Average training loss: 1.5717289093865288\n",
      "Average test loss: 0.003376400782623225\n",
      "Epoch 25/300\n",
      "Average training loss: 1.3867362792756823\n",
      "Average test loss: 0.002451610279786918\n",
      "Epoch 26/300\n",
      "Average training loss: 1.2109563145107693\n",
      "Average test loss: 0.0024470393367939523\n",
      "Epoch 27/300\n",
      "Average training loss: 1.0493585166931152\n",
      "Average test loss: 0.0025178138344652125\n",
      "Epoch 28/300\n",
      "Average training loss: 0.904824950059255\n",
      "Average test loss: 0.0026880904781735604\n",
      "Epoch 29/300\n",
      "Average training loss: 0.7767696918911404\n",
      "Average test loss: 0.002561250244163805\n",
      "Epoch 30/300\n",
      "Average training loss: 0.6693625917434692\n",
      "Average test loss: 0.002442120686794321\n",
      "Epoch 31/300\n",
      "Average training loss: 0.5871612389882406\n",
      "Average test loss: 0.0033247289549973276\n",
      "Epoch 32/300\n",
      "Average training loss: 0.5169528663423326\n",
      "Average test loss: 0.0023516440447419883\n",
      "Epoch 33/300\n",
      "Average training loss: 0.462175611866845\n",
      "Average test loss: 0.002602519705063767\n",
      "Epoch 34/300\n",
      "Average training loss: 0.41177526680628457\n",
      "Average test loss: 0.002238822313129074\n",
      "Epoch 35/300\n",
      "Average training loss: 0.3695443551540375\n",
      "Average test loss: 0.0022679437653472024\n",
      "Epoch 36/300\n",
      "Average training loss: 0.329844513575236\n",
      "Average test loss: 0.0023405500238554344\n",
      "Epoch 37/300\n",
      "Average training loss: 0.30141498404079015\n",
      "Average test loss: 0.0023455418875027033\n",
      "Epoch 38/300\n",
      "Average training loss: 0.271524648586909\n",
      "Average test loss: 0.0022478224933147432\n",
      "Epoch 39/300\n",
      "Average training loss: 0.2482883269124561\n",
      "Average test loss: 0.00241562232023312\n",
      "Epoch 40/300\n",
      "Average training loss: 0.22679415015379586\n",
      "Average test loss: 0.00249538357473082\n",
      "Epoch 41/300\n",
      "Average training loss: 0.20894679559601678\n",
      "Average test loss: 0.002204600704109503\n",
      "Epoch 42/300\n",
      "Average training loss: 0.19246937318642934\n",
      "Average test loss: 0.002323033773029844\n",
      "Epoch 43/300\n",
      "Average training loss: 0.18246900391578674\n",
      "Average test loss: 0.0021642253138124943\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1702666313648224\n",
      "Average test loss: 0.0026386121430744727\n",
      "Epoch 45/300\n",
      "Average training loss: 0.16483484607272678\n",
      "Average test loss: 0.0022910136638416186\n",
      "Epoch 46/300\n",
      "Average training loss: 0.14965782389375898\n",
      "Average test loss: 0.0021152196203668913\n",
      "Epoch 47/300\n",
      "Average training loss: 0.14750493803289202\n",
      "Average test loss: 0.0023862700580308833\n",
      "Epoch 48/300\n",
      "Average training loss: 0.13917334127426148\n",
      "Average test loss: 0.0021600757197787366\n",
      "Epoch 49/300\n",
      "Average training loss: 0.13572152905993992\n",
      "Average test loss: 0.002078478550952342\n",
      "Epoch 50/300\n",
      "Average training loss: 0.13188527222474417\n",
      "Average test loss: 0.002256573475897312\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12646676954958175\n",
      "Average test loss: 0.0021661841163618696\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12032336041662428\n",
      "Average test loss: 0.0020947617834640875\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11854584037595325\n",
      "Average test loss: 0.0021007697651576666\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11405940271748437\n",
      "Average test loss: 0.0021224393254766862\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1116389345990287\n",
      "Average test loss: 0.002166723594069481\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10711386151446237\n",
      "Average test loss: 0.002260423676421245\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10524869851933585\n",
      "Average test loss: 0.0024389095353997414\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10465842659605874\n",
      "Average test loss: 0.002282334097557598\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10168145019478268\n",
      "Average test loss: 0.002067050874647167\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09887791594531802\n",
      "Average test loss: 0.0020804844522434806\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09799576931529574\n",
      "Average test loss: 0.002168871789963709\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09579047673940659\n",
      "Average test loss: 0.0020102717789510886\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09443780769904454\n",
      "Average test loss: 0.0021856628545663423\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09225370841887262\n",
      "Average test loss: 0.002217749301240676\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09262119586600198\n",
      "Average test loss: 0.0020198877522101004\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08915063491794799\n",
      "Average test loss: 0.002077509222138259\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08662157340182199\n",
      "Average test loss: 0.0020780530156981614\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08698302869664298\n",
      "Average test loss: 0.002420062594736616\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08483066378037135\n",
      "Average test loss: 0.0022866758541721438\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08621540207333035\n",
      "Average test loss: 0.002372190272021625\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08381524284680685\n",
      "Average test loss: 0.0022239246947897803\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08718538080983691\n",
      "Average test loss: 0.0021043413046540485\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0818375005390909\n",
      "Average test loss: 0.002086481944554382\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07976566954453786\n",
      "Average test loss: 0.002049128618505266\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08192893838220172\n",
      "Average test loss: 0.00203164945418636\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08044253414869308\n",
      "Average test loss: 0.0020003684892629585\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07954742394553291\n",
      "Average test loss: 0.0025513743342210848\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07985635322332382\n",
      "Average test loss: 0.0022825763122075135\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0777170213063558\n",
      "Average test loss: 0.002176813887225257\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07741559332609177\n",
      "Average test loss: 0.0020141586281566155\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08013573002152972\n",
      "Average test loss: 0.0021404288998908465\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0782592430445883\n",
      "Average test loss: 0.0020730133084580302\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07924126425054338\n",
      "Average test loss: 0.002050663963995046\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07663984645075268\n",
      "Average test loss: 0.0019735231773099966\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0756785641113917\n",
      "Average test loss: 0.0021953197272701398\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07787380033400323\n",
      "Average test loss: 0.0021668642884534266\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07751057427459293\n",
      "Average test loss: 0.002055583500717249\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07535502191384634\n",
      "Average test loss: 0.00196851336893936\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0753519306116634\n",
      "Average test loss: 0.002034353734718429\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07954322091076109\n",
      "Average test loss: 0.0021805020064736406\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0746074548429913\n",
      "Average test loss: 0.0019657386319918767\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07420749594767888\n",
      "Average test loss: 0.0022601294212250245\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07890626469916767\n",
      "Average test loss: 0.0020072329323738815\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10723980595668157\n",
      "Average test loss: 0.0020052141412678693\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08355071855253643\n",
      "Average test loss: 0.001969391810397307\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07796425166394975\n",
      "Average test loss: 0.002004694594691197\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07644846733411154\n",
      "Average test loss: 0.0021858069490020473\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07791375178760952\n",
      "Average test loss: 0.0020470492036806214\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0742112973994679\n",
      "Average test loss: 0.0020597504565699233\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07374899808565775\n",
      "Average test loss: 0.001934088646951649\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07416738098859788\n",
      "Average test loss: 0.002250397262887822\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07990879557530085\n",
      "Average test loss: 0.0019694579901794592\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07425174736976624\n",
      "Average test loss: 0.002123161400978764\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07228004711866379\n",
      "Average test loss: 0.0019394206546453966\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07270496587620841\n",
      "Average test loss: 0.0019683037768635486\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07288873869180679\n",
      "Average test loss: 0.0019627599166706206\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0728219374914964\n",
      "Average test loss: 0.0019908475464002953\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07210988979207145\n",
      "Average test loss: 0.0020203704059951837\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07579744915829764\n",
      "Average test loss: 0.001954183200374246\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0715055281188753\n",
      "Average test loss: 0.0020364267046873766\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07187574232949151\n",
      "Average test loss: 0.0023392339758574964\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07755388895008299\n",
      "Average test loss: 0.002068121013128095\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0723028094470501\n",
      "Average test loss: 0.001922149890826808\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07115874777237574\n",
      "Average test loss: 0.001957877457866238\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07206985422637728\n",
      "Average test loss: 0.0019663171470165252\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07145643213722441\n",
      "Average test loss: 0.001959871746185753\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07295198907454808\n",
      "Average test loss: 0.0022675946158253486\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07580793076091343\n",
      "Average test loss: 0.001987274383298225\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07022678744130664\n",
      "Average test loss: 0.0020400204840633606\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07532154856125514\n",
      "Average test loss: 0.0020843881648033857\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06956530169314808\n",
      "Average test loss: 0.0024326955791976715\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0699087791244189\n",
      "Average test loss: 0.0025919989823467203\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07139867228931851\n",
      "Average test loss: 0.0019658431459425225\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06985555898149809\n",
      "Average test loss: 0.0020950055436955556\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0700890627503395\n",
      "Average test loss: 0.002010547929339939\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07444792835911115\n",
      "Average test loss: 0.0020985944082753524\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07640153043137657\n",
      "Average test loss: 0.0019085696457574765\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07411520874831412\n",
      "Average test loss: 0.002072081529431873\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07085922431614664\n",
      "Average test loss: 0.0019350967544855342\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06921544669402971\n",
      "Average test loss: 0.0020264292052015664\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07005215498142772\n",
      "Average test loss: 0.0019175866580464773\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06978280573752191\n",
      "Average test loss: 0.001967223856391178\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06909189145101441\n",
      "Average test loss: 0.001990569719009929\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07535332960221502\n",
      "Average test loss: 0.0022058174149650668\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07043316671583387\n",
      "Average test loss: 0.0019231613081776434\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06939871455563439\n",
      "Average test loss: 0.0020096487299435667\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06885357113348113\n",
      "Average test loss: 0.001959444896524979\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0694082417289416\n",
      "Average test loss: 0.0022412124613506927\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06879897644784716\n",
      "Average test loss: 0.0019178989372319645\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06840479875935448\n",
      "Average test loss: 0.002176819548010826\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06821003044313854\n",
      "Average test loss: 0.001938221547856099\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06877863643897904\n",
      "Average test loss: 0.00200745928970476\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06813211932447222\n",
      "Average test loss: 0.001866129737554325\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0701744269033273\n",
      "Average test loss: 0.0019331221226602793\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06973615656296413\n",
      "Average test loss: 0.0019003098633968167\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06892422651913431\n",
      "Average test loss: 0.0019064793859918912\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06839395092924436\n",
      "Average test loss: 0.0019148428802274995\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06759593407313029\n",
      "Average test loss: 0.001964700058516529\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0682892479730977\n",
      "Average test loss: 0.0020220602196123864\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06745923861530093\n",
      "Average test loss: 0.00195915520687898\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07068787600596746\n",
      "Average test loss: 0.0018955625602975488\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06783410810099708\n",
      "Average test loss: 0.0019275913031564819\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06676358143819704\n",
      "Average test loss: 0.0025075953675227033\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0711439292728901\n",
      "Average test loss: 0.0018979377191927697\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0697596912185351\n",
      "Average test loss: 0.001983592361211777\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06736759084463119\n",
      "Average test loss: 0.0019194160671904682\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06690799853536818\n",
      "Average test loss: 0.0018976950590602227\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0670485671195719\n",
      "Average test loss: 0.0019163653747075135\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06743810218572617\n",
      "Average test loss: 0.0019148823050782085\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06705867758393287\n",
      "Average test loss: 0.0018868183232843875\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06571345979472001\n",
      "Average test loss: 0.0019606172493141557\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06820686224434111\n",
      "Average test loss: 0.0019690046511176558\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06585540847645865\n",
      "Average test loss: 0.001956353923305869\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06634837505883641\n",
      "Average test loss: 0.0020376243584064972\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06697095559206274\n",
      "Average test loss: 0.002043528842843241\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07469615829322074\n",
      "Average test loss: 0.002209453672170639\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06890994808740086\n",
      "Average test loss: 0.0021847992050978873\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0673067006667455\n",
      "Average test loss: 0.0021509982825567327\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06670532569289207\n",
      "Average test loss: 0.0022803946773832045\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0739605967534913\n",
      "Average test loss: 0.0019836298630365895\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0668767979575528\n",
      "Average test loss: 0.002015850544182791\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06595236391822497\n",
      "Average test loss: 0.0019317641141307024\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0653009119828542\n",
      "Average test loss: 0.0022969565416375796\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06808241874641842\n",
      "Average test loss: 0.0019542936026636097\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06558047460185157\n",
      "Average test loss: 0.0020297173145744536\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06499639673696624\n",
      "Average test loss: 0.0021338613462737865\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0685896734793981\n",
      "Average test loss: 0.0019031929686251614\n",
      "Epoch 178/300\n",
      "Average training loss: 0.070125828931729\n",
      "Average test loss: 0.0019512973028338618\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06677708097630077\n",
      "Average test loss: 0.001901872483185596\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0709540674024158\n",
      "Average test loss: 0.002212027560505602\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06635391575760312\n",
      "Average test loss: 0.0018964960345377526\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06689933486117257\n",
      "Average test loss: 0.0019816792898087036\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06839642680353589\n",
      "Average test loss: 0.0018954305909574033\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06620355992184745\n",
      "Average test loss: 0.0019640876989190778\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06637917103701167\n",
      "Average test loss: 0.0018830728608494004\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06627053721083535\n",
      "Average test loss: 0.001959227869908015\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06716370322306951\n",
      "Average test loss: 0.009049944294409619\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06610659434398015\n",
      "Average test loss: 0.0021002830821606846\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06534303300579389\n",
      "Average test loss: 0.0020987115046009423\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06553092502554257\n",
      "Average test loss: 0.0018833905822700924\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06603282658921347\n",
      "Average test loss: 0.0018893703170534638\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06585374789105522\n",
      "Average test loss: 0.0018560654868682227\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0655641851839092\n",
      "Average test loss: 0.001944423302801119\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06456728215350045\n",
      "Average test loss: 0.0019201939391593138\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06471711739897729\n",
      "Average test loss: 0.001988419521910449\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06437827173868815\n",
      "Average test loss: 0.0018708263501111004\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06431387407249875\n",
      "Average test loss: 0.001949684420807494\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06746940247880089\n",
      "Average test loss: 0.0021242308577315676\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06499524114860429\n",
      "Average test loss: 0.0021255035946766533\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06395780240495999\n",
      "Average test loss: 0.0027341155070397588\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06418602978520924\n",
      "Average test loss: 0.0018537612594664097\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06454361048671935\n",
      "Average test loss: 0.0018886394147864646\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06466188158922725\n",
      "Average test loss: 0.001890608434461885\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0743938396109475\n",
      "Average test loss: 0.0018642930538497037\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0659376088447041\n",
      "Average test loss: 0.00194028512098723\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06526362043619156\n",
      "Average test loss: 0.002124202644866374\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06361015431086223\n",
      "Average test loss: 0.002021711321754588\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06468036701613003\n",
      "Average test loss: 0.002148625205167466\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06482608506083488\n",
      "Average test loss: 0.0036011546237601174\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07020003682043817\n",
      "Average test loss: 0.0019154386605239576\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06268271649546094\n",
      "Average test loss: 0.0019407094665285613\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06355916334523096\n",
      "Average test loss: 0.0022980325050238107\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06523918309476641\n",
      "Average test loss: 0.004107185863579313\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08117993688914511\n",
      "Average test loss: 0.0018727091176228392\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06352246701055103\n",
      "Average test loss: 0.002028915859758854\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06277141899863879\n",
      "Average test loss: 0.0018689906290835804\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06373814286457168\n",
      "Average test loss: 0.001849370603967044\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06405048151479827\n",
      "Average test loss: 0.0018306271929500832\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06524126147230466\n",
      "Average test loss: 0.0018887779615405532\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0647409096095297\n",
      "Average test loss: 0.0018355854149493906\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06373550614714622\n",
      "Average test loss: 0.0020022543138927883\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06576753603749805\n",
      "Average test loss: 0.0019133371408614848\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06328712238205804\n",
      "Average test loss: 0.0018412174549367693\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06395887243085438\n",
      "Average test loss: 0.002376881763856444\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06393863354457749\n",
      "Average test loss: 0.0018774372939434317\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0699453594022327\n",
      "Average test loss: 0.0019337288769375946\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0641393889553017\n",
      "Average test loss: 0.0019104987523621982\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06338368530405893\n",
      "Average test loss: 0.0019404217099977864\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06257601970765325\n",
      "Average test loss: 0.0020076988611577285\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06325356485446294\n",
      "Average test loss: 0.001997266257906126\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06266536544428931\n",
      "Average test loss: 0.001880200410882632\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06334088563587931\n",
      "Average test loss: 0.0019048806290245719\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06253993141651154\n",
      "Average test loss: 0.0018873789494650232\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06518332332041528\n",
      "Average test loss: 0.001941973917807142\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06379248438278834\n",
      "Average test loss: 0.0018550434379527967\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06306671418415176\n",
      "Average test loss: 0.002432068139211171\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06378821517361535\n",
      "Average test loss: 0.0018710002208956415\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06252054490645727\n",
      "Average test loss: 0.002010864215178622\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06378668512569534\n",
      "Average test loss: 0.001895695017443763\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06523822569847107\n",
      "Average test loss: 0.0018781949285831716\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06226251199179225\n",
      "Average test loss: 0.0018778327562742763\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06468264633582697\n",
      "Average test loss: 0.0019514065355890327\n",
      "Epoch 243/300\n",
      "Average training loss: 0.062206306245591904\n",
      "Average test loss: 0.0018784097414463758\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06374528239832984\n",
      "Average test loss: 0.0019039680674258206\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06253057109647327\n",
      "Average test loss: 0.0019475477573772271\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0625127581357956\n",
      "Average test loss: 0.0018613204511089458\n",
      "Epoch 247/300\n",
      "Average training loss: 0.061320081608162984\n",
      "Average test loss: 0.00188821447847618\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06224799803561634\n",
      "Average test loss: 0.001970011246494121\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0624333330127928\n",
      "Average test loss: 0.0020086233464794026\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06142035277353393\n",
      "Average test loss: 0.0019066439498629836\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06700205985042784\n",
      "Average test loss: 0.001845327945633067\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06308467934528987\n",
      "Average test loss: 0.0018501534957645668\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0624119771487183\n",
      "Average test loss: 0.0019039143396334516\n",
      "Epoch 254/300\n",
      "Average training loss: 0.061560375107659236\n",
      "Average test loss: 0.0020059571113023493\n",
      "Epoch 255/300\n",
      "Average training loss: 0.061148123416635725\n",
      "Average test loss: 0.0018417709500839312\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06114382259713279\n",
      "Average test loss: 0.0019021756992571883\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06273609579934014\n",
      "Average test loss: 0.0019755027208270296\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06441149385439025\n",
      "Average test loss: 0.0020300347165515026\n",
      "Epoch 259/300\n",
      "Average training loss: 0.060954620672596826\n",
      "Average test loss: 0.0020976486158453757\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06149398536483447\n",
      "Average test loss: 0.0038011402442223495\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06122004934151967\n",
      "Average test loss: 0.0021168258528535566\n",
      "Epoch 262/300\n",
      "Average training loss: 0.062374764677551056\n",
      "Average test loss: 0.0019003410175856617\n",
      "Epoch 263/300\n",
      "Average training loss: 0.060601550728082654\n",
      "Average test loss: 0.0018957462544656463\n",
      "Epoch 264/300\n",
      "Average training loss: 0.061698738863070805\n",
      "Average test loss: 0.0019048852828434772\n",
      "Epoch 265/300\n",
      "Average training loss: 0.068807196944952\n",
      "Average test loss: 0.0024298566449433563\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06870556649896833\n",
      "Average test loss: 0.0018204720697055261\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06483664419915941\n",
      "Average test loss: 0.0018484658072185185\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06309841946760814\n",
      "Average test loss: 0.0018819047227087947\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0617929358250565\n",
      "Average test loss: 0.002086775477147765\n",
      "Epoch 270/300\n",
      "Average training loss: 0.061032985862758425\n",
      "Average test loss: 0.001929756781189806\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0605358411471049\n",
      "Average test loss: 0.0019061445349620447\n",
      "Epoch 272/300\n",
      "Average training loss: 0.061271359615855746\n",
      "Average test loss: 0.00192774531741937\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0661646904150645\n",
      "Average test loss: 0.0018563943693621291\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0607576143278016\n",
      "Average test loss: 0.0018462055740464064\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06060985401272774\n",
      "Average test loss: 0.0019072850251363384\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06163855379157596\n",
      "Average test loss: 0.0020153251959838803\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06003409966164165\n",
      "Average test loss: 0.0018860850687035255\n",
      "Epoch 278/300\n",
      "Average training loss: 0.061365586857000984\n",
      "Average test loss: 0.0019971968858606285\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06075554830498166\n",
      "Average test loss: 0.0019547356574071777\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06026694103413158\n",
      "Average test loss: 0.0018475548499781223\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06129273344741927\n",
      "Average test loss: 0.0019627339177661473\n",
      "Epoch 282/300\n",
      "Average training loss: 0.060295007563299606\n",
      "Average test loss: 0.001988012970218228\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06357102624906434\n",
      "Average test loss: 0.001935463532184561\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0697075388001071\n",
      "Average test loss: 0.0018522637805177106\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06420536172058847\n",
      "Average test loss: 0.0018756124106132322\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06216117352578375\n",
      "Average test loss: 0.0022475644182413817\n",
      "Epoch 287/300\n",
      "Average training loss: 0.061040738476647274\n",
      "Average test loss: 0.0020686432637481225\n",
      "Epoch 288/300\n",
      "Average training loss: 0.061137831979327734\n",
      "Average test loss: 0.0018740984706415071\n",
      "Epoch 289/300\n",
      "Average training loss: 0.060094677279392876\n",
      "Average test loss: 0.0020224731161983477\n",
      "Epoch 290/300\n",
      "Average training loss: 0.061696415589915385\n",
      "Average test loss: 0.0019805590423444906\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06053899923298094\n",
      "Average test loss: 0.0020911464397278096\n",
      "Epoch 292/300\n",
      "Average training loss: 0.060417077660560606\n",
      "Average test loss: 0.00198484288321601\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0601448594364855\n",
      "Average test loss: 0.0019729426286907658\n",
      "Epoch 294/300\n",
      "Average training loss: 0.059705636531114575\n",
      "Average test loss: 0.0024109036658580105\n",
      "Epoch 295/300\n",
      "Average training loss: 0.059943905648257995\n",
      "Average test loss: 0.0018572626266007623\n",
      "Epoch 296/300\n",
      "Average training loss: 0.061570627205901673\n",
      "Average test loss: 0.0018664772232683997\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05987793466117647\n",
      "Average test loss: 0.0018785744893054168\n",
      "Epoch 298/300\n",
      "Average training loss: 0.059632341391510434\n",
      "Average test loss: 0.0018841092723111312\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0599020124607616\n",
      "Average test loss: 0.001906869953705205\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06019090073969629\n",
      "Average test loss: 0.0019718465230738123\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.9724553290473095\n",
      "Average test loss: 0.7189235497348838\n",
      "Epoch 2/300\n",
      "Average training loss: 6.014329715304904\n",
      "Average test loss: 0.004560062113735411\n",
      "Epoch 3/300\n",
      "Average training loss: 5.376580635918511\n",
      "Average test loss: 0.004146019798806972\n",
      "Epoch 4/300\n",
      "Average training loss: 4.90959906217787\n",
      "Average test loss: 0.0036705898499737183\n",
      "Epoch 5/300\n",
      "Average training loss: 4.507071607377794\n",
      "Average test loss: 0.006660449869930744\n",
      "Epoch 6/300\n",
      "Average training loss: 4.153003309885661\n",
      "Average test loss: 0.00530427943666776\n",
      "Epoch 7/300\n",
      "Average training loss: 3.8255597356160482\n",
      "Average test loss: 0.0035782308160430855\n",
      "Epoch 8/300\n",
      "Average training loss: 3.508815493689643\n",
      "Average test loss: 0.0042926996122631764\n",
      "Epoch 9/300\n",
      "Average training loss: 3.201967264811198\n",
      "Average test loss: 0.0034200014703803594\n",
      "Epoch 10/300\n",
      "Average training loss: 2.910346965577867\n",
      "Average test loss: 0.0031622750425918233\n",
      "Epoch 11/300\n",
      "Average training loss: 2.621967293633355\n",
      "Average test loss: 0.0026447797200332087\n",
      "Epoch 12/300\n",
      "Average training loss: 2.341116189956665\n",
      "Average test loss: 0.002202671421898736\n",
      "Epoch 13/300\n",
      "Average training loss: 2.091438914404975\n",
      "Average test loss: 0.002300641429093149\n",
      "Epoch 14/300\n",
      "Average training loss: 1.8544713259802925\n",
      "Average test loss: 0.0022069430714473127\n",
      "Epoch 15/300\n",
      "Average training loss: 1.6481896372901068\n",
      "Average test loss: 0.002563535978189773\n",
      "Epoch 16/300\n",
      "Average training loss: 1.4548479764726427\n",
      "Average test loss: 0.0024075259033383596\n",
      "Epoch 17/300\n",
      "Average training loss: 1.2884173437754314\n",
      "Average test loss: 0.0020555943065426415\n",
      "Epoch 18/300\n",
      "Average training loss: 1.1300158193376328\n",
      "Average test loss: 0.0019043612310455905\n",
      "Epoch 19/300\n",
      "Average training loss: 0.9872871555752224\n",
      "Average test loss: 0.0022496596549948056\n",
      "Epoch 20/300\n",
      "Average training loss: 0.8650006550682916\n",
      "Average test loss: 0.0018383697917064032\n",
      "Epoch 21/300\n",
      "Average training loss: 0.7541943490770128\n",
      "Average test loss: 0.0017738864179700612\n",
      "Epoch 22/300\n",
      "Average training loss: 0.6559115436342028\n",
      "Average test loss: 0.001989706794834799\n",
      "Epoch 23/300\n",
      "Average training loss: 0.5731396950085957\n",
      "Average test loss: 0.0020746036395430565\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5000224305788676\n",
      "Average test loss: 0.0017049712458004555\n",
      "Epoch 25/300\n",
      "Average training loss: 0.4410511775281694\n",
      "Average test loss: 0.0017944683656096459\n",
      "Epoch 26/300\n",
      "Average training loss: 0.38468726767434014\n",
      "Average test loss: 0.0017577978880662058\n",
      "Epoch 27/300\n",
      "Average training loss: 0.34230477725134956\n",
      "Average test loss: 0.0020716677539878423\n",
      "Epoch 28/300\n",
      "Average training loss: 0.30687381807963054\n",
      "Average test loss: 0.0017105549595111773\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2729666153589884\n",
      "Average test loss: 0.0016358419296642144\n",
      "Epoch 30/300\n",
      "Average training loss: 0.24651820475525327\n",
      "Average test loss: 0.0018080228683021334\n",
      "Epoch 31/300\n",
      "Average training loss: 0.22064021465513442\n",
      "Average test loss: 0.0019703053538170125\n",
      "Epoch 32/300\n",
      "Average training loss: 0.19948271339469487\n",
      "Average test loss: 0.0020515426277286477\n",
      "Epoch 33/300\n",
      "Average training loss: 0.18188173729843563\n",
      "Average test loss: 0.0016034002787330085\n",
      "Epoch 34/300\n",
      "Average training loss: 0.167762282093366\n",
      "Average test loss: 0.0015922370019058386\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15594167351722718\n",
      "Average test loss: 0.0015904508433822128\n",
      "Epoch 36/300\n",
      "Average training loss: 0.14801852130889892\n",
      "Average test loss: 0.0016094961814168427\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1398298832707935\n",
      "Average test loss: 0.0016137607755760351\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1307514136367374\n",
      "Average test loss: 0.001723272733597292\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12318846215142144\n",
      "Average test loss: 0.0014917176278928916\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11868410164780087\n",
      "Average test loss: 0.0019866984884978995\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11323358680142297\n",
      "Average test loss: 0.0015480958265769813\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10875846435626348\n",
      "Average test loss: 0.002155364842671487\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10445032690631019\n",
      "Average test loss: 0.0015701017221435905\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09928194802337223\n",
      "Average test loss: 0.0017513291674355665\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10169385539823109\n",
      "Average test loss: 0.0018786105196923017\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09450235381391313\n",
      "Average test loss: 0.0018565144876742528\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09294155475828382\n",
      "Average test loss: 0.0015607013604086307\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09136543707052866\n",
      "Average test loss: 0.002006148724506299\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08601393075784047\n",
      "Average test loss: 0.0015627917734285196\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08284408993853463\n",
      "Average test loss: 0.0017998350320590868\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08065978923108842\n",
      "Average test loss: 0.001635764669937392\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08073929109176\n",
      "Average test loss: 0.00417199404992991\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07885684930947091\n",
      "Average test loss: 0.0020368113671946856\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07651118655668365\n",
      "Average test loss: 0.0014939200015117724\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07492176416185167\n",
      "Average test loss: 0.001872242351476517\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07399112509687741\n",
      "Average test loss: 0.001493304757711788\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07066486726866827\n",
      "Average test loss: 0.00482846194298731\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06839824210604031\n",
      "Average test loss: 0.0017740388977237874\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07102157696088156\n",
      "Average test loss: 0.0013933699445074631\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06898602827390035\n",
      "Average test loss: 0.0015514926631003617\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0659858292804824\n",
      "Average test loss: 0.0015361338225710723\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06833317533797688\n",
      "Average test loss: 0.0016531195897195074\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06508746918704775\n",
      "Average test loss: 0.0014040882754036122\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06428962966468599\n",
      "Average test loss: 0.0014601286398246884\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06936119871669345\n",
      "Average test loss: 0.0014571334102915392\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06613716058929761\n",
      "Average test loss: 0.001586386586746408\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0648323893878195\n",
      "Average test loss: 0.0015289040418962638\n",
      "Epoch 68/300\n",
      "Average training loss: 0.062458977709213895\n",
      "Average test loss: 0.001506516047546433\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0616086711982886\n",
      "Average test loss: 0.001381407882604334\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06586800502406226\n",
      "Average test loss: 0.0016697765288667546\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06702989762028058\n",
      "Average test loss: 0.0013844589062242045\n",
      "Epoch 72/300\n",
      "Average training loss: 0.061159283290306725\n",
      "Average test loss: 0.0014839795277350478\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06020124085744222\n",
      "Average test loss: 0.0016663199316503273\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06055245357751846\n",
      "Average test loss: 0.0015689513094516265\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06010672578215599\n",
      "Average test loss: 0.0014626861632698112\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06005378333065245\n",
      "Average test loss: 0.0014460509652789268\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06544945777787102\n",
      "Average test loss: 0.0014964070438096921\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05731910900274913\n",
      "Average test loss: 0.0014196490230452684\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06060677688320478\n",
      "Average test loss: 0.0014692206855656373\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05751193082994885\n",
      "Average test loss: 0.0015198901287383504\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05814297332697445\n",
      "Average test loss: 0.0013985301474523212\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06982045116027197\n",
      "Average test loss: 0.0013888582188843026\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06779355978965759\n",
      "Average test loss: 0.0015780731632063787\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05921626381410493\n",
      "Average test loss: 0.0015454091337612935\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0566310214665201\n",
      "Average test loss: 0.0013991880611413056\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05743193704883257\n",
      "Average test loss: 0.0014491395811136397\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05740713307923741\n",
      "Average test loss: 0.0015235713424368038\n",
      "Epoch 88/300\n",
      "Average training loss: 0.055715546295046804\n",
      "Average test loss: 0.0017036451996407574\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05657070454955101\n",
      "Average test loss: 0.0013969870708468888\n",
      "Epoch 90/300\n",
      "Average training loss: 0.058516516569587916\n",
      "Average test loss: 0.0013342658754231201\n",
      "Epoch 91/300\n",
      "Average training loss: 0.056455842273102866\n",
      "Average test loss: 0.0015610302071500984\n",
      "Epoch 92/300\n",
      "Average training loss: 0.057032593982087244\n",
      "Average test loss: 0.00156603152025491\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05568627741601732\n",
      "Average test loss: 0.0013811067982783748\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05701801061961386\n",
      "Average test loss: 0.0013765788109352192\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05547432284884982\n",
      "Average test loss: 0.001429254843129052\n",
      "Epoch 96/300\n",
      "Average training loss: 0.054428597334358425\n",
      "Average test loss: 0.0017963886933608187\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05391752040717337\n",
      "Average test loss: 0.0014221232764215933\n",
      "Epoch 98/300\n",
      "Average training loss: 0.054864524887667764\n",
      "Average test loss: 0.0013681972464546562\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05418637627032068\n",
      "Average test loss: 0.0013413781573375066\n",
      "Epoch 100/300\n",
      "Average training loss: 0.054954294082191255\n",
      "Average test loss: 0.0012973643115514683\n",
      "Epoch 101/300\n",
      "Average training loss: 0.055143329686588714\n",
      "Average test loss: 0.0014649060990454422\n",
      "Epoch 102/300\n",
      "Average training loss: 0.054949730436007184\n",
      "Average test loss: 0.0020527769331302906\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05439688189824422\n",
      "Average test loss: 0.0015412897113710642\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05277343773510721\n",
      "Average test loss: 0.0013386997094170915\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05280488469865587\n",
      "Average test loss: 0.0015167957949969505\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05347229843007194\n",
      "Average test loss: 0.0013465285688224766\n",
      "Epoch 107/300\n",
      "Average training loss: 0.056357285433345373\n",
      "Average test loss: 0.0013453189535583886\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05407050179110633\n",
      "Average test loss: 0.0014951129040370384\n",
      "Epoch 109/300\n",
      "Average training loss: 0.052822936134205925\n",
      "Average test loss: 0.001362489556376305\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05421385583281517\n",
      "Average test loss: 0.0013304315466019841\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05238702780339453\n",
      "Average test loss: 0.0015327742625441817\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05587193061576949\n",
      "Average test loss: 0.0014373554638069538\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05256787058048778\n",
      "Average test loss: 0.0014200150553757946\n",
      "Epoch 114/300\n",
      "Average training loss: 0.052082387258609135\n",
      "Average test loss: 0.00139102981508606\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05405016239153014\n",
      "Average test loss: 0.0013236220974682106\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05174399220281177\n",
      "Average test loss: 0.001394498715073698\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05135396079553498\n",
      "Average test loss: 0.0014097687030831972\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05205415725376871\n",
      "Average test loss: 0.001290250441680352\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05146220768822564\n",
      "Average test loss: 0.0015426387258080973\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05381864446732733\n",
      "Average test loss: 0.0014535327648950947\n",
      "Epoch 121/300\n",
      "Average training loss: 0.051055986523628236\n",
      "Average test loss: 0.0016092138736405305\n",
      "Epoch 122/300\n",
      "Average training loss: 0.053573272158702216\n",
      "Average test loss: 0.005448919360422425\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05720860605107413\n",
      "Average test loss: 0.0013343051328427262\n",
      "Epoch 124/300\n",
      "Average training loss: 0.050613489740424684\n",
      "Average test loss: 0.0013866399164415067\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05173015114665031\n",
      "Average test loss: 0.0015579983093258407\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05048135483927197\n",
      "Average test loss: 0.0013073301228384176\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05066824909713533\n",
      "Average test loss: 0.0014353368429260121\n",
      "Epoch 128/300\n",
      "Average training loss: 0.052785122825039756\n",
      "Average test loss: 0.0013928416555023027\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05058570965131124\n",
      "Average test loss: 0.0013956544783173335\n",
      "Epoch 130/300\n",
      "Average training loss: 0.050583088295327296\n",
      "Average test loss: 0.001453407513598601\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05145049213369687\n",
      "Average test loss: 0.0015021582036796543\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05069666449891196\n",
      "Average test loss: 0.0013098418388722672\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04920034630762206\n",
      "Average test loss: 0.0013398443715025982\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04978244626853201\n",
      "Average test loss: 0.0013611478117398089\n",
      "Epoch 135/300\n",
      "Average training loss: 0.053404665827751156\n",
      "Average test loss: 0.0012814657000514368\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0515913179119428\n",
      "Average test loss: 0.001277774743942751\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05069013529353671\n",
      "Average test loss: 0.0013159769816944997\n",
      "Epoch 138/300\n",
      "Average training loss: 0.049965603187680244\n",
      "Average test loss: 0.0013451464861217472\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0494775455892086\n",
      "Average test loss: 0.0014264613806994424\n",
      "Epoch 140/300\n",
      "Average training loss: 0.050051465706692805\n",
      "Average test loss: 0.0013801438361406326\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04994274675183826\n",
      "Average test loss: 0.0013301489294196168\n",
      "Epoch 142/300\n",
      "Average training loss: 0.049674250721931455\n",
      "Average test loss: 0.004713585704772009\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04980865881509251\n",
      "Average test loss: 0.0012979438107771177\n",
      "Epoch 144/300\n",
      "Average training loss: 0.050221761289570066\n",
      "Average test loss: 0.001391084555329548\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05090171534485287\n",
      "Average test loss: 0.001335143441800028\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0493301526274946\n",
      "Average test loss: 0.00131464142859396\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05210207326213519\n",
      "Average test loss: 0.0012735459647244876\n",
      "Epoch 148/300\n",
      "Average training loss: 0.049975858350594836\n",
      "Average test loss: 0.0013592505926887195\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05042067549626032\n",
      "Average test loss: 0.0013040246169807182\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04845090775026215\n",
      "Average test loss: 0.001281628180295229\n",
      "Epoch 151/300\n",
      "Average training loss: 0.049359777162472404\n",
      "Average test loss: 0.0014831390406729447\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04975493227442106\n",
      "Average test loss: 0.0012940046143614583\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04868978240754869\n",
      "Average test loss: 0.001389178153851794\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04895952369769414\n",
      "Average test loss: 0.0014193034897972312\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04937138702140914\n",
      "Average test loss: 0.0013592699433987339\n",
      "Epoch 156/300\n",
      "Average training loss: 0.049210048394070734\n",
      "Average test loss: 0.0012704666893308362\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0483560416566001\n",
      "Average test loss: 0.0013253406886425284\n",
      "Epoch 158/300\n",
      "Average training loss: 0.052359011822276647\n",
      "Average test loss: 0.0013836345880602796\n",
      "Epoch 159/300\n",
      "Average training loss: 0.048211708048979444\n",
      "Average test loss: 0.001491157054176761\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05123940580421024\n",
      "Average test loss: 0.0013605933325986066\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04856087904009554\n",
      "Average test loss: 0.0013795720637879438\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05139149934384558\n",
      "Average test loss: 0.0014759314018819066\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04961397890912162\n",
      "Average test loss: 0.0013155130636360911\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04811396954456965\n",
      "Average test loss: 0.0013970189877889224\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050875288201702965\n",
      "Average test loss: 0.0013087319608570801\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04784523232446777\n",
      "Average test loss: 0.0020859525988085404\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05033179290095965\n",
      "Average test loss: 0.0012682203677379423\n",
      "Epoch 168/300\n",
      "Average training loss: 0.047960138658682504\n",
      "Average test loss: 0.0012804584135818814\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04825408654411634\n",
      "Average test loss: 0.001284404140069253\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04729455172353321\n",
      "Average test loss: 0.0013251973634792699\n",
      "Epoch 171/300\n",
      "Average training loss: 0.047829471521907384\n",
      "Average test loss: 0.001810007979420738\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04827700444393688\n",
      "Average test loss: 0.0013574273510732584\n",
      "Epoch 173/300\n",
      "Average training loss: 0.047587466726700466\n",
      "Average test loss: 0.0013007208885004124\n",
      "Epoch 174/300\n",
      "Average training loss: 0.047278985493712955\n",
      "Average test loss: 0.0012650469239387247\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04783127806915177\n",
      "Average test loss: 0.0013534597509230176\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04753288335601489\n",
      "Average test loss: 0.0012765206720473038\n",
      "Epoch 177/300\n",
      "Average training loss: 0.051666534533103305\n",
      "Average test loss: 0.001405899049093326\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04967011783520381\n",
      "Average test loss: 0.0012730027500333057\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04869159072306421\n",
      "Average test loss: 0.001357646047241158\n",
      "Epoch 180/300\n",
      "Average training loss: 0.048026043073998555\n",
      "Average test loss: 0.0012928256882975499\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04760592806008127\n",
      "Average test loss: 0.001456116809302734\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04817516554726495\n",
      "Average test loss: 0.0012533533876347874\n",
      "Epoch 183/300\n",
      "Average training loss: 0.047026138759321635\n",
      "Average test loss: 0.0013965737804149587\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04736635827687052\n",
      "Average test loss: 0.0013337561968300078\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04784770375490189\n",
      "Average test loss: 0.0014179455490989817\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04682539389199681\n",
      "Average test loss: 0.0015229363807787498\n",
      "Epoch 187/300\n",
      "Average training loss: 0.047926690343353485\n",
      "Average test loss: 0.0013262024077379869\n",
      "Epoch 188/300\n",
      "Average training loss: 0.046368077748351624\n",
      "Average test loss: 0.0014562679157695837\n",
      "Epoch 189/300\n",
      "Average training loss: 0.046930742795268696\n",
      "Average test loss: 0.0017316702153119777\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04830112196339501\n",
      "Average test loss: 0.0013939503216081196\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04781049011482133\n",
      "Average test loss: 0.0013147632576939133\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04766551171077622\n",
      "Average test loss: 0.0014742659450405174\n",
      "Epoch 193/300\n",
      "Average training loss: 0.046793262836005954\n",
      "Average test loss: 0.0014295324713198675\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04631087875035074\n",
      "Average test loss: 0.0016715320232841703\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04586624519858096\n",
      "Average test loss: 0.0013166456657151381\n",
      "Epoch 196/300\n",
      "Average training loss: 0.046943296833170785\n",
      "Average test loss: 0.0015509171143898532\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04923874658677313\n",
      "Average test loss: 0.001279851351554195\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04832681059837341\n",
      "Average test loss: 0.0012744868027253283\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04776956279079119\n",
      "Average test loss: 0.0013005542215994662\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04645655625396305\n",
      "Average test loss: 0.0013474119392534096\n",
      "Epoch 201/300\n",
      "Average training loss: 0.047858707795540495\n",
      "Average test loss: 0.001248287279676232\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04957427276836501\n",
      "Average test loss: 0.0014999412273367246\n",
      "Epoch 203/300\n",
      "Average training loss: 0.046318352998958694\n",
      "Average test loss: 0.0012999588832155698\n",
      "Epoch 204/300\n",
      "Average training loss: 0.046312301056252586\n",
      "Average test loss: 0.0013213831215269035\n",
      "Epoch 205/300\n",
      "Average training loss: 0.046620764788654116\n",
      "Average test loss: 0.0013234499090144204\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04709360746211476\n",
      "Average test loss: 0.0014185954751964245\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04600489580300119\n",
      "Average test loss: 0.001276744059028311\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04613460469577048\n",
      "Average test loss: 0.00128933277576127\n",
      "Epoch 209/300\n",
      "Average training loss: 0.046068122665087384\n",
      "Average test loss: 0.0012417782979706924\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04559783019953304\n",
      "Average test loss: 0.0014692843852357731\n",
      "Epoch 211/300\n",
      "Average training loss: 0.045990521010425355\n",
      "Average test loss: 0.0014904842120595277\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04652659546666675\n",
      "Average test loss: 0.0013361604421709974\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04653398233320978\n",
      "Average test loss: 0.001427595806825492\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04629042318463326\n",
      "Average test loss: 0.0012582647379280791\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04512727389732996\n",
      "Average test loss: 0.0012741096202387578\n",
      "Epoch 216/300\n",
      "Average training loss: 0.046601699348953037\n",
      "Average test loss: 0.0013096286085330778\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04647115384538968\n",
      "Average test loss: 0.001318401547976666\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04732964653770129\n",
      "Average test loss: 0.0012689832039177419\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04532644445697467\n",
      "Average test loss: 0.0012905631766964992\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04612338377535343\n",
      "Average test loss: 0.0017268260560101933\n",
      "Epoch 221/300\n",
      "Average training loss: 0.046618548042244384\n",
      "Average test loss: 0.0012527356918694244\n",
      "Epoch 222/300\n",
      "Average training loss: 0.045510019759337106\n",
      "Average test loss: 0.0014176081037148834\n",
      "Epoch 223/300\n",
      "Average training loss: 0.046424340973297756\n",
      "Average test loss: 0.0013060008987991346\n",
      "Epoch 224/300\n",
      "Average training loss: 0.045673828174670535\n",
      "Average test loss: 0.001762014248408377\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04556292470627361\n",
      "Average test loss: 0.001736064364719722\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04515069572461976\n",
      "Average test loss: 0.0013253647917881607\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04486116715603405\n",
      "Average test loss: 0.0012437666891039246\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04709946780072318\n",
      "Average test loss: 0.001275700421589944\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04596570080849859\n",
      "Average test loss: 0.001410468796475066\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04552618181871043\n",
      "Average test loss: 0.0012880378387764924\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04468591601318783\n",
      "Average test loss: 0.0013687589996390873\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0462446870373355\n",
      "Average test loss: 0.0012637947157232298\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04519883107145627\n",
      "Average test loss: 0.0012784522923951348\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0453746852543619\n",
      "Average test loss: 0.001363414661751853\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04520558688044548\n",
      "Average test loss: 0.0016196373011916876\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04496530344088872\n",
      "Average test loss: 0.0016072323768296175\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04523545348644257\n",
      "Average test loss: 0.0012800907535064552\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04511329925722546\n",
      "Average test loss: 0.00144964657485899\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04593205673661497\n",
      "Average test loss: 0.001260535720632308\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04534575153721703\n",
      "Average test loss: 0.0012752240300178527\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04479994296530883\n",
      "Average test loss: 0.001420161278711425\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04462265297108226\n",
      "Average test loss: 0.0013328645348341928\n",
      "Epoch 243/300\n",
      "Average training loss: 0.044387419717179404\n",
      "Average test loss: 0.0013270255533150502\n",
      "Epoch 244/300\n",
      "Average training loss: 0.044849068873458436\n",
      "Average test loss: 0.0013751397697358495\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04967165291971631\n",
      "Average test loss: 0.0013231143382533143\n",
      "Epoch 246/300\n",
      "Average training loss: 0.045192685772975284\n",
      "Average test loss: 0.0015240463364041513\n",
      "Epoch 247/300\n",
      "Average training loss: 0.045366424805588196\n",
      "Average test loss: 0.0013329397288875447\n",
      "Epoch 248/300\n",
      "Average training loss: 0.045234437247117364\n",
      "Average test loss: 0.001288727348877324\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04495424813694424\n",
      "Average test loss: 0.0013937947254420982\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04510033498538865\n",
      "Average test loss: 0.0014566527034880386\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0445296422276232\n",
      "Average test loss: 0.0012857413397481044\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04502186886138386\n",
      "Average test loss: 0.0012299041566956373\n",
      "Epoch 253/300\n",
      "Average training loss: 0.044644667890336776\n",
      "Average test loss: 0.0013963978459230729\n",
      "Epoch 254/300\n",
      "Average training loss: 0.044214693867497976\n",
      "Average test loss: 0.0013392102361346285\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04486738405625026\n",
      "Average test loss: 0.001254448480800622\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04406499903731876\n",
      "Average test loss: 0.001392470132973459\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04497610113355849\n",
      "Average test loss: 0.0012871314082812104\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04447049195567767\n",
      "Average test loss: 0.0016606268139763012\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04441820800469981\n",
      "Average test loss: 0.0017139516728412773\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04466269571251339\n",
      "Average test loss: 0.0012726708428106375\n",
      "Epoch 261/300\n",
      "Average training loss: 0.044859903275966645\n",
      "Average test loss: 0.0012294604506136641\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04456366190976567\n",
      "Average test loss: 0.0012482015125246512\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04459980636172824\n",
      "Average test loss: 0.0012472897748359375\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04547632888952891\n",
      "Average test loss: 0.0012283650330371326\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04408472921119796\n",
      "Average test loss: 0.0014918187183017532\n",
      "Epoch 266/300\n",
      "Average training loss: 0.044204940574036705\n",
      "Average test loss: 0.0014021162275845806\n",
      "Epoch 267/300\n",
      "Average training loss: 0.045456798566712274\n",
      "Average test loss: 0.0012582285601852668\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04372527108920945\n",
      "Average test loss: 0.0013006601931734218\n",
      "Epoch 269/300\n",
      "Average training loss: 0.044107754982180065\n",
      "Average test loss: 0.0015713499896228313\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04410909904374016\n",
      "Average test loss: 0.0014438010858785775\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04462003840340508\n",
      "Average test loss: 0.0012262956835329532\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04478981869750553\n",
      "Average test loss: 0.0012512106020520959\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04424888962176111\n",
      "Average test loss: 0.0016908327501474155\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04425643820895089\n",
      "Average test loss: 0.0012640474237915543\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04415962972574764\n",
      "Average test loss: 0.0014358988713680041\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0432160447968377\n",
      "Average test loss: 0.0012396239122996727\n",
      "Epoch 277/300\n",
      "Average training loss: 0.043847126556767355\n",
      "Average test loss: 0.0012247147486131225\n",
      "Epoch 278/300\n",
      "Average training loss: 0.043983613209591974\n",
      "Average test loss: 0.001367922883066866\n",
      "Epoch 279/300\n",
      "Average training loss: 0.043754944231775075\n",
      "Average test loss: 0.0012703822470373578\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04394754802021715\n",
      "Average test loss: 0.0013261318057775496\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04319722143477864\n",
      "Average test loss: 0.0013975504139024351\n",
      "Epoch 282/300\n",
      "Average training loss: 0.043797255701488916\n",
      "Average test loss: 0.001379028171249148\n",
      "Epoch 283/300\n",
      "Average training loss: 0.044880578885475794\n",
      "Average test loss: 0.0014386810573438803\n",
      "Epoch 284/300\n",
      "Average training loss: 0.044646409554613964\n",
      "Average test loss: 0.0015797999118351274\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04667061986856991\n",
      "Average test loss: 0.0013351967947350608\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0440293652051025\n",
      "Average test loss: 0.0012810025449014372\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04419639473160108\n",
      "Average test loss: 0.0012450421475287941\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0456912877327866\n",
      "Average test loss: 0.0012902158338369595\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04328900373644299\n",
      "Average test loss: 0.001266208741089536\n",
      "Epoch 290/300\n",
      "Average training loss: 0.043424227509233686\n",
      "Average test loss: 0.0013028116601829728\n",
      "Epoch 291/300\n",
      "Average training loss: 0.043506129547953604\n",
      "Average test loss: 0.0013521004870740904\n",
      "Epoch 292/300\n",
      "Average training loss: 0.043168720099661086\n",
      "Average test loss: 0.0012860039584338665\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04346644261148241\n",
      "Average test loss: 0.0013441292590772112\n",
      "Epoch 294/300\n",
      "Average training loss: 0.043863061699602336\n",
      "Average test loss: 0.001311451612247361\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04441400244169765\n",
      "Average test loss: 0.0013872312180077035\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04299261049760712\n",
      "Average test loss: 0.0012427875000155634\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04323776105377409\n",
      "Average test loss: 0.0012863845420587395\n",
      "Epoch 298/300\n",
      "Average training loss: 0.043463901413811575\n",
      "Average test loss: 0.0014187290476014216\n",
      "Epoch 299/300\n",
      "Average training loss: 0.043764116081926555\n",
      "Average test loss: 0.0012474522743788031\n",
      "Epoch 300/300\n",
      "Average training loss: 0.043579410400655536\n",
      "Average test loss: 0.0012785867211512394\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_No_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.74\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.41\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.28\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.70\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.68\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.86\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.20\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.28\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.38\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.37\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.41\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.80\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.82\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.98\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.84\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.88\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.96\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.32\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.26\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.15\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.42\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.78\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.55\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.43\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.42\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.83\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.01\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.32\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.19\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.22\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.49\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.84\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.70\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.05\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.27\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.54\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.51\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.71\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.57\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.08\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.53\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.94\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.48\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.39\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.52\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.53\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.74\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.81\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.94\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.05\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
