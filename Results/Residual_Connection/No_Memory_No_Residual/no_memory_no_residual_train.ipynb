{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.PGD_Network.PGD import PGD\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14106526245011225\n",
      "Average test loss: 0.01156786440561215\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06489006615347333\n",
      "Average test loss: 0.019004538108905157\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06045732507440779\n",
      "Average test loss: 0.00985056170158916\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05826850797732671\n",
      "Average test loss: 0.009530746683478355\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05568554176555739\n",
      "Average test loss: 0.010488323995636569\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05325416010287073\n",
      "Average test loss: 0.01009189753068818\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05208356853326162\n",
      "Average test loss: 0.01036539326608181\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05048946090539296\n",
      "Average test loss: 0.030581016979283757\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04977813705801964\n",
      "Average test loss: 0.01094391548881928\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04860679568184747\n",
      "Average test loss: 0.017488500240776274\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04719269042213758\n",
      "Average test loss: 0.008315489387346638\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0465817228588793\n",
      "Average test loss: 0.010215503918627898\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04562737859951125\n",
      "Average test loss: 0.010634927740527524\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04471610180868043\n",
      "Average test loss: 0.007993082054787212\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04417709844311078\n",
      "Average test loss: 0.012012003064155579\n",
      "Epoch 16/300\n",
      "Average training loss: 0.043754877348740895\n",
      "Average test loss: 0.011930392058359253\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04313851610819499\n",
      "Average test loss: 0.007919105817874273\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04320680007338524\n",
      "Average test loss: 0.008758896381490761\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04350755387875769\n",
      "Average test loss: 0.007500350979881154\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04227428811126285\n",
      "Average test loss: 0.007519767680929767\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0415068289273315\n",
      "Average test loss: 0.008672827098932532\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04148550415039062\n",
      "Average test loss: 0.011654991475244363\n",
      "Epoch 23/300\n",
      "Average training loss: 0.040971231682433024\n",
      "Average test loss: 0.008299510041872661\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04050233476029502\n",
      "Average test loss: 0.008821106815503703\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04014585374130143\n",
      "Average test loss: 0.00934439323676957\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03974659619397587\n",
      "Average test loss: 0.0073506236697236695\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0401465759889947\n",
      "Average test loss: 0.00857965596848064\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03935516465041373\n",
      "Average test loss: 0.007538159048391713\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03910670635435316\n",
      "Average test loss: 0.007560458118716876\n",
      "Epoch 30/300\n",
      "Average training loss: 0.038943477352460225\n",
      "Average test loss: 0.007289426157457961\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03873840245604515\n",
      "Average test loss: 0.007463502381824785\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03892315337393019\n",
      "Average test loss: 0.00789698469142119\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03815213026603063\n",
      "Average test loss: 0.0070226996232652\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0381284865240256\n",
      "Average test loss: 0.006845247549729215\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03818995043304232\n",
      "Average test loss: 0.0072020562510523535\n",
      "Epoch 36/300\n",
      "Average training loss: 0.037798389178183345\n",
      "Average test loss: 0.00705739969925748\n",
      "Epoch 37/300\n",
      "Average training loss: 0.037513727406660716\n",
      "Average test loss: 0.006840061800761355\n",
      "Epoch 38/300\n",
      "Average training loss: 0.037165887349181706\n",
      "Average test loss: 0.007525833488338523\n",
      "Epoch 39/300\n",
      "Average training loss: 0.037172077036566206\n",
      "Average test loss: 0.0068321474240057994\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03701378148463037\n",
      "Average test loss: 0.00684925903711054\n",
      "Epoch 41/300\n",
      "Average training loss: 0.037027529587348305\n",
      "Average test loss: 0.007545563721408447\n",
      "Epoch 42/300\n",
      "Average training loss: 0.036810501323805915\n",
      "Average test loss: 0.007187413298835357\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03643415832519531\n",
      "Average test loss: 0.0067649032291438845\n",
      "Epoch 44/300\n",
      "Average training loss: 0.036510771185159684\n",
      "Average test loss: 0.00714827392084731\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03633481437133418\n",
      "Average test loss: 0.006970311016672188\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03634746004475488\n",
      "Average test loss: 0.007277224902063608\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03626917556590504\n",
      "Average test loss: 0.006625163459115558\n",
      "Epoch 48/300\n",
      "Average training loss: 0.036081390710340604\n",
      "Average test loss: 0.0066100457894305385\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0359427126629485\n",
      "Average test loss: 0.0070718359094527035\n",
      "Epoch 50/300\n",
      "Average training loss: 0.035767724017302195\n",
      "Average test loss: 0.007193661985297998\n",
      "Epoch 51/300\n",
      "Average training loss: 0.035787448310189776\n",
      "Average test loss: 0.0070479173432621695\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03574221539828512\n",
      "Average test loss: 0.006747504515780343\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03566403020090527\n",
      "Average test loss: 0.0069043623871273465\n",
      "Epoch 54/300\n",
      "Average training loss: 0.035233621140321095\n",
      "Average test loss: 0.006477373972328173\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03530613179339303\n",
      "Average test loss: 0.008798951676322355\n",
      "Epoch 56/300\n",
      "Average training loss: 0.035662043624454076\n",
      "Average test loss: 0.007037662013123433\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03504544754657481\n",
      "Average test loss: 0.006802809037682083\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03512053373952707\n",
      "Average test loss: 0.007601801654530896\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03492995505531629\n",
      "Average test loss: 0.007006249224146207\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03482071166071627\n",
      "Average test loss: 0.006484544092582332\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03484490188293987\n",
      "Average test loss: 0.006489340745326546\n",
      "Epoch 62/300\n",
      "Average training loss: 0.034647715896368024\n",
      "Average test loss: 0.006988162479880784\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03466532889670796\n",
      "Average test loss: 0.006603249568492174\n",
      "Epoch 64/300\n",
      "Average training loss: 0.034504708904359076\n",
      "Average test loss: 0.00684254892791311\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03450744986202982\n",
      "Average test loss: 0.0065920788654022745\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03450721659594112\n",
      "Average test loss: 0.006490429074400001\n",
      "Epoch 67/300\n",
      "Average training loss: 0.034417957918511495\n",
      "Average test loss: 0.006529170149730311\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03427688046627574\n",
      "Average test loss: 0.006895898532950216\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03421545892291599\n",
      "Average test loss: 0.0063598132158319155\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03438854308591949\n",
      "Average test loss: 0.006377353417376677\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03400365753968557\n",
      "Average test loss: 0.006542277183383703\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03398904656370481\n",
      "Average test loss: 0.006380139643119441\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03409598328669866\n",
      "Average test loss: 0.0062888135661681496\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03387522976928287\n",
      "Average test loss: 0.006339722872608238\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03389159039490753\n",
      "Average test loss: 0.006219713132000632\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03389451855421066\n",
      "Average test loss: 0.0068499566291769344\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03385399971736802\n",
      "Average test loss: 0.006297187897894118\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03369412448339992\n",
      "Average test loss: 0.00631893512316876\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03368635646502177\n",
      "Average test loss: 0.006241506088938978\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03361289017068015\n",
      "Average test loss: 0.006646357525967889\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03356974146432347\n",
      "Average test loss: 0.006986162877745099\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03368846225076252\n",
      "Average test loss: 0.006786164183997446\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03344669085409906\n",
      "Average test loss: 0.006290988117456436\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03341987177067333\n",
      "Average test loss: 0.0072583561560346024\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03344188349776798\n",
      "Average test loss: 0.006283867897258865\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03335624121957355\n",
      "Average test loss: 0.006470048343969716\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03331760765446557\n",
      "Average test loss: 0.007023012439823813\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033360535982582304\n",
      "Average test loss: 0.0067645180130170455\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03326437901788288\n",
      "Average test loss: 0.006312383381856812\n",
      "Epoch 90/300\n",
      "Average training loss: 0.033346566574441065\n",
      "Average test loss: 0.0062322448848022355\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03305210819178157\n",
      "Average test loss: 0.006157391503867176\n",
      "Epoch 92/300\n",
      "Average training loss: 0.033099423769447536\n",
      "Average test loss: 0.006305036708712578\n",
      "Epoch 93/300\n",
      "Average training loss: 0.033098161551687454\n",
      "Average test loss: 0.006211863328185347\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03308322807484203\n",
      "Average test loss: 0.006474283224178685\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03309438216686249\n",
      "Average test loss: 0.008272392787867122\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03296547100610203\n",
      "Average test loss: 0.006192776377830241\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03296663619081179\n",
      "Average test loss: 0.006287410508427355\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03290708939234416\n",
      "Average test loss: 0.006632029028402434\n",
      "Epoch 99/300\n",
      "Average training loss: 0.032977587822410795\n",
      "Average test loss: 0.006233901212198867\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03288614044586818\n",
      "Average test loss: 0.006319803144782782\n",
      "Epoch 101/300\n",
      "Average training loss: 0.032771674530373676\n",
      "Average test loss: 0.0062082227932082285\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03279204370246993\n",
      "Average test loss: 0.006212851827343305\n",
      "Epoch 103/300\n",
      "Average training loss: 0.032800869665212104\n",
      "Average test loss: 0.006621565757526292\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03272252856360541\n",
      "Average test loss: 0.006271257674114572\n",
      "Epoch 105/300\n",
      "Average training loss: 0.032567646589544086\n",
      "Average test loss: 0.006368341697586907\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03267714824113581\n",
      "Average test loss: 0.0066140783387753695\n",
      "Epoch 107/300\n",
      "Average training loss: 0.032667320314380854\n",
      "Average test loss: 0.006191777119206057\n",
      "Epoch 108/300\n",
      "Average training loss: 0.032536785076061885\n",
      "Average test loss: 0.006485078659736448\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03254267370700836\n",
      "Average test loss: 0.006395739306178358\n",
      "Epoch 110/300\n",
      "Average training loss: 0.032463307776384884\n",
      "Average test loss: 0.006330550882965327\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03246015650696225\n",
      "Average test loss: 0.006485197448482116\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03246897487839063\n",
      "Average test loss: 0.006453222317828073\n",
      "Epoch 113/300\n",
      "Average training loss: 0.032527341009842024\n",
      "Average test loss: 0.00636027051239378\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03242974197367827\n",
      "Average test loss: 0.00635710322442982\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03235765926208761\n",
      "Average test loss: 0.006325397627635135\n",
      "Epoch 116/300\n",
      "Average training loss: 0.032343589074081844\n",
      "Average test loss: 0.006327364246878359\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0323025522198942\n",
      "Average test loss: 0.006174546393462353\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03233145726058218\n",
      "Average test loss: 0.0066241265601581995\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03244285358322991\n",
      "Average test loss: 0.007489024819599258\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0323385148247083\n",
      "Average test loss: 0.00631847684416506\n",
      "Epoch 121/300\n",
      "Average training loss: 0.032163556393649845\n",
      "Average test loss: 0.006368728893084659\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03217154642773999\n",
      "Average test loss: 0.006162844013422728\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03214768850141102\n",
      "Average test loss: 0.0063561672738028895\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03219844835168786\n",
      "Average test loss: 0.006383076775405142\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0321125408411026\n",
      "Average test loss: 0.006185354035761622\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032109846012459864\n",
      "Average test loss: 0.006148425258696079\n",
      "Epoch 127/300\n",
      "Average training loss: 0.032105495121743946\n",
      "Average test loss: 0.00623799765896466\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03204021801551183\n",
      "Average test loss: 0.006142368184609546\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03200861956013574\n",
      "Average test loss: 0.006361712864703602\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03198990026447508\n",
      "Average test loss: 0.006295254254920615\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031945320987039144\n",
      "Average test loss: 0.006547600607491202\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03194234133594566\n",
      "Average test loss: 0.006355432002494732\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03188116558558411\n",
      "Average test loss: 0.00624818396071593\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03191427515281571\n",
      "Average test loss: 0.0061949389465153215\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031906011856264536\n",
      "Average test loss: 0.006188885795159472\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03191294586658478\n",
      "Average test loss: 0.0062854332182970315\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03184409024318059\n",
      "Average test loss: 0.00626921786285109\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03170859209365315\n",
      "Average test loss: 0.006481665934125583\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03179251867367162\n",
      "Average test loss: 0.0061659797587328485\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03177016390032238\n",
      "Average test loss: 0.006305752652800745\n",
      "Epoch 141/300\n",
      "Average training loss: 0.031787532433867456\n",
      "Average test loss: 0.006697604837930865\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03174690903557671\n",
      "Average test loss: 0.006155321598880821\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031599431680308446\n",
      "Average test loss: 0.006210198005868329\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03168335097697046\n",
      "Average test loss: 0.006233248967677355\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03168700272507138\n",
      "Average test loss: 0.006315198481082916\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03166291493177414\n",
      "Average test loss: 0.006275604094689091\n",
      "Epoch 147/300\n",
      "Average training loss: 0.031636439171102315\n",
      "Average test loss: 0.006213873188114828\n",
      "Epoch 148/300\n",
      "Average training loss: 0.031543822397788364\n",
      "Average test loss: 0.006177387529777156\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0315622926056385\n",
      "Average test loss: 0.006258254243267907\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031592397961351604\n",
      "Average test loss: 0.0062297519209484255\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03157600266072485\n",
      "Average test loss: 0.00626269538121091\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03148827524648772\n",
      "Average test loss: 0.006641404407719771\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031581096337901224\n",
      "Average test loss: 0.006339941643178463\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031418408325976796\n",
      "Average test loss: 0.006136578974624475\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03143472205930286\n",
      "Average test loss: 0.0064442983642220494\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03141864855918619\n",
      "Average test loss: 0.006398068147401015\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03139453489912881\n",
      "Average test loss: 0.0063370628083745635\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031464983069234426\n",
      "Average test loss: 0.0061939769453472565\n",
      "Epoch 159/300\n",
      "Average training loss: 0.031365202162000866\n",
      "Average test loss: 0.007255753074255255\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031330433911747405\n",
      "Average test loss: 0.006249947314874994\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03135187910993894\n",
      "Average test loss: 0.006162501580599281\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03144832581281662\n",
      "Average test loss: 0.006304416566673252\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031222676830159293\n",
      "Average test loss: 0.006352365035149786\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03130515393945906\n",
      "Average test loss: 0.006962811771366331\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031223790476719537\n",
      "Average test loss: 0.006398297204739518\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031257122092776826\n",
      "Average test loss: 0.006306447632196877\n",
      "Epoch 167/300\n",
      "Average training loss: 0.031302346269289655\n",
      "Average test loss: 0.006917203672230244\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03126100182533264\n",
      "Average test loss: 0.006228270274897417\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03126175297300021\n",
      "Average test loss: 0.006216825627618366\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03117118356625239\n",
      "Average test loss: 0.006131969226317273\n",
      "Epoch 171/300\n",
      "Average training loss: 0.031212119898862307\n",
      "Average test loss: 0.008519944603244463\n",
      "Epoch 172/300\n",
      "Average training loss: 0.031146588423185878\n",
      "Average test loss: 0.006291078541013929\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03116203044189347\n",
      "Average test loss: 0.006270833265036344\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03115235571232107\n",
      "Average test loss: 0.0062843771684500905\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03117103390892347\n",
      "Average test loss: 0.006155966641174422\n",
      "Epoch 176/300\n",
      "Average training loss: 0.031092608544561598\n",
      "Average test loss: 0.006430318057951\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031110015438662635\n",
      "Average test loss: 0.0065138215286036335\n",
      "Epoch 178/300\n",
      "Average training loss: 0.031056436588366827\n",
      "Average test loss: 0.00634489458385441\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031083951191769706\n",
      "Average test loss: 0.006214102483457989\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03099813097053104\n",
      "Average test loss: 0.006319601266334454\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03101863801148203\n",
      "Average test loss: 0.006358274393197563\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03099405195646816\n",
      "Average test loss: 0.0072767871680359045\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030948873046371672\n",
      "Average test loss: 0.006676039677527216\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030993480026721955\n",
      "Average test loss: 0.006192132388965951\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03097484571735064\n",
      "Average test loss: 0.006152792312618759\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03096169437799189\n",
      "Average test loss: 0.0062724061001920035\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03091000502804915\n",
      "Average test loss: 0.00625982417869899\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030888748799761136\n",
      "Average test loss: 0.006550727100008064\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03097671701841884\n",
      "Average test loss: 0.006267225897974438\n",
      "Epoch 190/300\n",
      "Average training loss: 0.030874955733617145\n",
      "Average test loss: 0.006266237281676796\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030914597200022802\n",
      "Average test loss: 0.0065130905827714336\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030867629206842846\n",
      "Average test loss: 0.00621320861702164\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03080579457680384\n",
      "Average test loss: 0.006693567699856229\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03084310118854046\n",
      "Average test loss: 0.006274616729054186\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030846068784594536\n",
      "Average test loss: 0.006494691268023518\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03085226867761877\n",
      "Average test loss: 0.006553634272267421\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030800561259190243\n",
      "Average test loss: 0.006676895389954249\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030701927615536584\n",
      "Average test loss: 0.006278196375403139\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03076635412375132\n",
      "Average test loss: 0.006603515990906292\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03083312177989218\n",
      "Average test loss: 0.006410600457340479\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03082796816693412\n",
      "Average test loss: 0.006255464517407947\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030694559031062656\n",
      "Average test loss: 0.0063022996733586\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03069001488553153\n",
      "Average test loss: 0.006270179489834441\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030734648512469397\n",
      "Average test loss: 0.006524094564219316\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03068368070986536\n",
      "Average test loss: 0.006413495203687085\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030690011302630108\n",
      "Average test loss: 0.006217633624457651\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030650357297725146\n",
      "Average test loss: 0.006215606180744039\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03057682853937149\n",
      "Average test loss: 0.006345550080554353\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03061235422392686\n",
      "Average test loss: 0.006258481110549635\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0305958990537458\n",
      "Average test loss: 0.006550707272771332\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030662798096736273\n",
      "Average test loss: 0.006591340221050713\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030620973265833323\n",
      "Average test loss: 0.00770299743115902\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030625682406955294\n",
      "Average test loss: 0.006249363578855992\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030535676919751696\n",
      "Average test loss: 0.006222878142984377\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030525274793306988\n",
      "Average test loss: 0.006557467881590128\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030539362281560897\n",
      "Average test loss: 0.006196688520825572\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030527893778350617\n",
      "Average test loss: 0.00659905627866586\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03048127820591132\n",
      "Average test loss: 0.006413557729787297\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030491857479015987\n",
      "Average test loss: 0.0071708534720043345\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03046388883392016\n",
      "Average test loss: 0.006199518030716313\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030444144846664533\n",
      "Average test loss: 0.006278299762970872\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03048782392177317\n",
      "Average test loss: 0.0063257328449851935\n",
      "Epoch 223/300\n",
      "Average training loss: 0.030483081272906727\n",
      "Average test loss: 0.006213708703716596\n",
      "Epoch 224/300\n",
      "Average training loss: 0.030447936119304762\n",
      "Average test loss: 0.006258677174233728\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03048166646560033\n",
      "Average test loss: 0.006459760308265686\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03037944702141815\n",
      "Average test loss: 0.006306655473179287\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030434551103247536\n",
      "Average test loss: 0.006304857219258944\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030392539030975767\n",
      "Average test loss: 0.006451848708921008\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030291108124785953\n",
      "Average test loss: 0.006587368517286247\n",
      "Epoch 230/300\n",
      "Average training loss: 0.030346132912569577\n",
      "Average test loss: 0.006391153147651089\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030397771982683074\n",
      "Average test loss: 0.006556133766141203\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030371682758132616\n",
      "Average test loss: 0.006317427122758495\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030301448070340686\n",
      "Average test loss: 0.00649339176962773\n",
      "Epoch 234/300\n",
      "Average training loss: 0.030284031285179987\n",
      "Average test loss: 0.00632585995644331\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030368254939715067\n",
      "Average test loss: 0.006534293853574329\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030281901799970205\n",
      "Average test loss: 0.0063674987943636045\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030292018857267167\n",
      "Average test loss: 0.006390664171013567\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03023531151480145\n",
      "Average test loss: 0.006402912921375698\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0303128562271595\n",
      "Average test loss: 0.006340055459903346\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030263083792395062\n",
      "Average test loss: 0.006373944226238462\n",
      "Epoch 241/300\n",
      "Average training loss: 0.030202736112806532\n",
      "Average test loss: 0.006562488870488273\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030196175899770524\n",
      "Average test loss: 0.0063589426076246635\n",
      "Epoch 243/300\n",
      "Average training loss: 0.030274403946267233\n",
      "Average test loss: 0.006174829104294379\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030174954204095734\n",
      "Average test loss: 0.0063253328034447295\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03017052751945125\n",
      "Average test loss: 0.006294042970157332\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03014704480436113\n",
      "Average test loss: 0.006338799136380355\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030173644800980885\n",
      "Average test loss: 0.0067724325855573015\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030150418394141728\n",
      "Average test loss: 0.0062376075602240035\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030162779258357152\n",
      "Average test loss: 0.006564377128663991\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03014917454454634\n",
      "Average test loss: 0.006319494124501944\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03016679832339287\n",
      "Average test loss: 0.0062161938096914025\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03011636371910572\n",
      "Average test loss: 0.0066746305425961815\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030034573720561134\n",
      "Average test loss: 0.006454977748294672\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03009109539124701\n",
      "Average test loss: 0.00645599707795514\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03007741349107689\n",
      "Average test loss: 0.006395087230536673\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03007713392045763\n",
      "Average test loss: 0.006478636436992221\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03004357616437806\n",
      "Average test loss: 0.006405190964125925\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030144762247800827\n",
      "Average test loss: 0.006283820373316606\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03007162974609269\n",
      "Average test loss: 0.00626165793567068\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03001417777604527\n",
      "Average test loss: 0.00673364350779189\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03004082713690069\n",
      "Average test loss: 0.006278388876054022\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030029024809598924\n",
      "Average test loss: 0.0062944264867239525\n",
      "Epoch 263/300\n",
      "Average training loss: 0.029981388794051277\n",
      "Average test loss: 0.006258854238523378\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02995173495180077\n",
      "Average test loss: 0.006533077920890517\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02992750385072496\n",
      "Average test loss: 0.006288651404695378\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030045027080509396\n",
      "Average test loss: 0.00630021891494592\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029949032243755128\n",
      "Average test loss: 0.006439427300459809\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029942644458678034\n",
      "Average test loss: 0.006237966982440816\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02998651901549763\n",
      "Average test loss: 0.006548846652110417\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029955746448702282\n",
      "Average test loss: 0.006392298280778859\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029989146997531257\n",
      "Average test loss: 0.006418939458413256\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029863710002766716\n",
      "Average test loss: 0.0063545525744557385\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029993027369181316\n",
      "Average test loss: 0.006313063092529774\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02985778073966503\n",
      "Average test loss: 0.006395987095518245\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029793008206619155\n",
      "Average test loss: 0.006610101494110293\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029916730811198552\n",
      "Average test loss: 0.006314061714543236\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029973204294840496\n",
      "Average test loss: 0.006379339205308093\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02987583570347892\n",
      "Average test loss: 0.006514118845678038\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02988782038788001\n",
      "Average test loss: 0.006443387004650301\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029884856548574236\n",
      "Average test loss: 0.006319863768087493\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02982105670703782\n",
      "Average test loss: 0.006282151487966379\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029871748030185698\n",
      "Average test loss: 0.006295065235553517\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02988411710659663\n",
      "Average test loss: 0.006257217831909656\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02979781296849251\n",
      "Average test loss: 0.006332918172909154\n",
      "Epoch 285/300\n",
      "Average training loss: 0.029857393885652225\n",
      "Average test loss: 0.006315663094735808\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029818209454417228\n",
      "Average test loss: 0.006414321438305908\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02976028870873981\n",
      "Average test loss: 0.006304805767205026\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029848748513393933\n",
      "Average test loss: 0.006639826580882072\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029779584997230105\n",
      "Average test loss: 0.006319066918558544\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02974683930973212\n",
      "Average test loss: 0.006201652037186755\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029796590675910315\n",
      "Average test loss: 0.006238352150552803\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02976117052965694\n",
      "Average test loss: 0.006567367341783312\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029641657158732414\n",
      "Average test loss: 0.00700736167281866\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02969635015560521\n",
      "Average test loss: 0.006430843076358239\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029790918827056884\n",
      "Average test loss: 0.0064187331597010296\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029686011670364273\n",
      "Average test loss: 0.006314520984888077\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029698537942436005\n",
      "Average test loss: 0.006377574216574431\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02969477179977629\n",
      "Average test loss: 0.00627208612445328\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029709471431043414\n",
      "Average test loss: 0.006449362321032418\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029682441441549197\n",
      "Average test loss: 0.006369769166741106\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12507898056175973\n",
      "Average test loss: 0.010985560807916853\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05274229166573948\n",
      "Average test loss: 0.009342380445036623\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04847659631239044\n",
      "Average test loss: 0.00714373882404632\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04621465567416615\n",
      "Average test loss: 0.009324484604928228\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04407686818639437\n",
      "Average test loss: 0.006302444662898779\n",
      "Epoch 6/300\n",
      "Average training loss: 0.041689561775989\n",
      "Average test loss: 0.006357304236541191\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04060984814498159\n",
      "Average test loss: 0.005943441845062706\n",
      "Epoch 8/300\n",
      "Average training loss: 0.038894048187467785\n",
      "Average test loss: 0.006125625224163135\n",
      "Epoch 9/300\n",
      "Average training loss: 0.037585941221978926\n",
      "Average test loss: 0.006722776741203335\n",
      "Epoch 10/300\n",
      "Average training loss: 0.036601910849412285\n",
      "Average test loss: 0.010141637932923105\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03546891294585334\n",
      "Average test loss: 0.005345164740251171\n",
      "Epoch 12/300\n",
      "Average training loss: 0.034730460978216596\n",
      "Average test loss: 0.005255258401648866\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0335342493057251\n",
      "Average test loss: 0.006145742363399929\n",
      "Epoch 14/300\n",
      "Average training loss: 0.033163401597075995\n",
      "Average test loss: 0.007721165508859687\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03279745682742861\n",
      "Average test loss: 0.005099796753376722\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03176031513512135\n",
      "Average test loss: 0.005639952364067237\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03135770442088445\n",
      "Average test loss: 0.005937205168522066\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030731715048352877\n",
      "Average test loss: 0.004887429874804285\n",
      "Epoch 19/300\n",
      "Average training loss: 0.030570607675446403\n",
      "Average test loss: 0.00504444018461638\n",
      "Epoch 20/300\n",
      "Average training loss: 0.030003761963711843\n",
      "Average test loss: 0.005507581618511014\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02972031015985542\n",
      "Average test loss: 0.005187233350343174\n",
      "Epoch 22/300\n",
      "Average training loss: 0.029296175124744573\n",
      "Average test loss: 0.005136017448786232\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029071787317593894\n",
      "Average test loss: 0.004498506223989857\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02905290518866645\n",
      "Average test loss: 0.004736799335314168\n",
      "Epoch 25/300\n",
      "Average training loss: 0.028488081971804302\n",
      "Average test loss: 0.004899834225161208\n",
      "Epoch 26/300\n",
      "Average training loss: 0.028367799152930577\n",
      "Average test loss: 0.005835932784610324\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02837509923014376\n",
      "Average test loss: 0.006960693177249696\n",
      "Epoch 28/300\n",
      "Average training loss: 0.027798548014627562\n",
      "Average test loss: 0.004431504957791832\n",
      "Epoch 29/300\n",
      "Average training loss: 0.027669716861512927\n",
      "Average test loss: 0.004942706836594476\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02752581018043889\n",
      "Average test loss: 0.0047108423631224364\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02731985461877452\n",
      "Average test loss: 0.004554482205460469\n",
      "Epoch 32/300\n",
      "Average training loss: 0.027140210792422296\n",
      "Average test loss: 0.0044019034248259335\n",
      "Epoch 33/300\n",
      "Average training loss: 0.027002751396762\n",
      "Average test loss: 0.004531589935430222\n",
      "Epoch 34/300\n",
      "Average training loss: 0.026839116550154155\n",
      "Average test loss: 0.004381335927794377\n",
      "Epoch 35/300\n",
      "Average training loss: 0.026864872500300407\n",
      "Average test loss: 0.004519121885920564\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02647543857826127\n",
      "Average test loss: 0.004360222875658009\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026372912862234647\n",
      "Average test loss: 0.004602922846666641\n",
      "Epoch 38/300\n",
      "Average training loss: 0.026363244980573654\n",
      "Average test loss: 0.004986434100816647\n",
      "Epoch 39/300\n",
      "Average training loss: 0.026119618652595412\n",
      "Average test loss: 0.0049713027688364185\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026083253564106092\n",
      "Average test loss: 0.00460506591697534\n",
      "Epoch 41/300\n",
      "Average training loss: 0.025824179962277413\n",
      "Average test loss: 0.004203352416762047\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02588527263700962\n",
      "Average test loss: 0.004243883515811629\n",
      "Epoch 43/300\n",
      "Average training loss: 0.025681805905368595\n",
      "Average test loss: 0.004572425439125962\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025679082118802602\n",
      "Average test loss: 0.004401836254116562\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02548201729191674\n",
      "Average test loss: 0.004489295795973804\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02544716195596589\n",
      "Average test loss: 0.004430475200836857\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025437428302235072\n",
      "Average test loss: 0.004187054628713264\n",
      "Epoch 48/300\n",
      "Average training loss: 0.025207546552022297\n",
      "Average test loss: 0.0040291805364605455\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02521840612590313\n",
      "Average test loss: 0.004005162819392151\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025259629600577885\n",
      "Average test loss: 0.003966704952220122\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025175237110919423\n",
      "Average test loss: 0.004180082737985585\n",
      "Epoch 52/300\n",
      "Average training loss: 0.024953833328353034\n",
      "Average test loss: 0.004100419604116016\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02488905545241303\n",
      "Average test loss: 0.0043856209717277024\n",
      "Epoch 54/300\n",
      "Average training loss: 0.024944714580972988\n",
      "Average test loss: 0.004973162918455071\n",
      "Epoch 55/300\n",
      "Average training loss: 0.024914159392317135\n",
      "Average test loss: 0.004551819068897102\n",
      "Epoch 56/300\n",
      "Average training loss: 0.024786289286282327\n",
      "Average test loss: 0.003978771667306622\n",
      "Epoch 57/300\n",
      "Average training loss: 0.024715636259979672\n",
      "Average test loss: 0.003973866135709815\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02481634648144245\n",
      "Average test loss: 0.003993572624400258\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024559420961472723\n",
      "Average test loss: 0.004364439032971859\n",
      "Epoch 60/300\n",
      "Average training loss: 0.024693893808457587\n",
      "Average test loss: 0.004117626138652364\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024444300338625907\n",
      "Average test loss: 0.003969317650215493\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02443742677403821\n",
      "Average test loss: 0.003982150509125657\n",
      "Epoch 63/300\n",
      "Average training loss: 0.024403330975108675\n",
      "Average test loss: 0.005051902005241977\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024272342556052737\n",
      "Average test loss: 0.004161284072945515\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024341341346502305\n",
      "Average test loss: 0.004740006004356675\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024345999351806112\n",
      "Average test loss: 0.00403159301314089\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0242085713479254\n",
      "Average test loss: 0.004043294036553966\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02430568461285697\n",
      "Average test loss: 0.003882904448029068\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024137306989894972\n",
      "Average test loss: 0.003934124666783545\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024115481567051674\n",
      "Average test loss: 0.004041439805593756\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024064146551820965\n",
      "Average test loss: 0.004287977219041851\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02423720234963629\n",
      "Average test loss: 0.003973511713246504\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024017097468177477\n",
      "Average test loss: 0.004239406487594048\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02409408336877823\n",
      "Average test loss: 0.003846089510867993\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023960002513395414\n",
      "Average test loss: 0.0039097681302163335\n",
      "Epoch 76/300\n",
      "Average training loss: 0.023865817884604137\n",
      "Average test loss: 0.003987591349416309\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023898082786136202\n",
      "Average test loss: 0.0039052018467336893\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023768930721614095\n",
      "Average test loss: 0.0039993982286088995\n",
      "Epoch 79/300\n",
      "Average training loss: 0.023819838868247137\n",
      "Average test loss: 0.004001715995578302\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023902803558442327\n",
      "Average test loss: 0.0038557605068716737\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023781569732560053\n",
      "Average test loss: 0.004060120786850651\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023646223114596472\n",
      "Average test loss: 0.0037995541240606044\n",
      "Epoch 83/300\n",
      "Average training loss: 0.023636756650275655\n",
      "Average test loss: 0.004309147345523039\n",
      "Epoch 84/300\n",
      "Average training loss: 0.023655777040455076\n",
      "Average test loss: 0.004460361066998707\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02381671429011557\n",
      "Average test loss: 0.0038621240167154205\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02350085362129741\n",
      "Average test loss: 0.003853916538672315\n",
      "Epoch 87/300\n",
      "Average training loss: 0.023529668784803816\n",
      "Average test loss: 0.00398639282087485\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02355876462492678\n",
      "Average test loss: 0.0039999023458609975\n",
      "Epoch 89/300\n",
      "Average training loss: 0.023480788232551682\n",
      "Average test loss: 0.0038493408833940824\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023511731922626497\n",
      "Average test loss: 0.004126634166472488\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02355490357014868\n",
      "Average test loss: 0.0038376030830873384\n",
      "Epoch 92/300\n",
      "Average training loss: 0.023405735939741136\n",
      "Average test loss: 0.004006034003363715\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023408046070072387\n",
      "Average test loss: 0.0042599951198531525\n",
      "Epoch 94/300\n",
      "Average training loss: 0.023274689909484652\n",
      "Average test loss: 0.0038374944354097047\n",
      "Epoch 95/300\n",
      "Average training loss: 0.023426988032129076\n",
      "Average test loss: 0.004267300877720118\n",
      "Epoch 96/300\n",
      "Average training loss: 0.023294010288185542\n",
      "Average test loss: 0.004050135232715143\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02336336022449864\n",
      "Average test loss: 0.003896688501454062\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023294719283779464\n",
      "Average test loss: 0.0038909589455773433\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023323250465922884\n",
      "Average test loss: 0.0040367453282492026\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0233230289965868\n",
      "Average test loss: 0.003963017639600568\n",
      "Epoch 101/300\n",
      "Average training loss: 0.023185272988345888\n",
      "Average test loss: 0.00449365401847495\n",
      "Epoch 102/300\n",
      "Average training loss: 0.023285999468631215\n",
      "Average test loss: 0.0038295001482797995\n",
      "Epoch 103/300\n",
      "Average training loss: 0.023205151327782208\n",
      "Average test loss: 0.003991089880880382\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023160464839802847\n",
      "Average test loss: 0.003950354555621744\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02312965807484256\n",
      "Average test loss: 0.004063193531499969\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023184892055061128\n",
      "Average test loss: 0.004196806917173995\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023061531249019836\n",
      "Average test loss: 0.0038575420079545843\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02306604919499821\n",
      "Average test loss: 0.00407218530960381\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023158023936880958\n",
      "Average test loss: 0.0038784570193125143\n",
      "Epoch 110/300\n",
      "Average training loss: 0.022968503351012866\n",
      "Average test loss: 0.003899332152886523\n",
      "Epoch 111/300\n",
      "Average training loss: 0.023013351221879322\n",
      "Average test loss: 0.003924185006154908\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02297924429923296\n",
      "Average test loss: 0.003937739788658089\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023061096550689802\n",
      "Average test loss: 0.004001659633798732\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023011052341924774\n",
      "Average test loss: 0.0037400796653495895\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023062758305006557\n",
      "Average test loss: 0.0038193648652070098\n",
      "Epoch 116/300\n",
      "Average training loss: 0.022911134012871318\n",
      "Average test loss: 0.0038440730422735214\n",
      "Epoch 117/300\n",
      "Average training loss: 0.022896039504143928\n",
      "Average test loss: 0.004242823273357418\n",
      "Epoch 118/300\n",
      "Average training loss: 0.022936620396044518\n",
      "Average test loss: 0.0037629717576007048\n",
      "Epoch 119/300\n",
      "Average training loss: 0.022874051249689525\n",
      "Average test loss: 0.0039042064253654745\n",
      "Epoch 120/300\n",
      "Average training loss: 0.022874711588025093\n",
      "Average test loss: 0.004639660543865627\n",
      "Epoch 121/300\n",
      "Average training loss: 0.022864857017993928\n",
      "Average test loss: 0.003921714268003901\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022831623640325333\n",
      "Average test loss: 0.004163329908417331\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022851134793625938\n",
      "Average test loss: 0.003926891632378101\n",
      "Epoch 124/300\n",
      "Average training loss: 0.022754460086425145\n",
      "Average test loss: 0.003809607555468877\n",
      "Epoch 125/300\n",
      "Average training loss: 0.022883080878191523\n",
      "Average test loss: 0.003893221525061462\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02275530228184329\n",
      "Average test loss: 0.004100209867788686\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02266786645849546\n",
      "Average test loss: 0.00379849436506629\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02279878102739652\n",
      "Average test loss: 0.003921231260937122\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0227181246512466\n",
      "Average test loss: 0.0038010167317051027\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02278268031279246\n",
      "Average test loss: 0.0039644595949600145\n",
      "Epoch 131/300\n",
      "Average training loss: 0.022668732196092604\n",
      "Average test loss: 0.004105952622161971\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02272717590464486\n",
      "Average test loss: 0.003980395201800598\n",
      "Epoch 133/300\n",
      "Average training loss: 0.022653796629773244\n",
      "Average test loss: 0.0039634561300691635\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0226107261363003\n",
      "Average test loss: 0.004053884635368983\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02268233560356829\n",
      "Average test loss: 0.003758063346354498\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02264439249369833\n",
      "Average test loss: 0.0040992065394918125\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02261048231687811\n",
      "Average test loss: 0.003795841342045201\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022570750269624924\n",
      "Average test loss: 0.0037820040854728886\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022585777880416977\n",
      "Average test loss: 0.003928447844150165\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022579297499524222\n",
      "Average test loss: 0.0037881377389033634\n",
      "Epoch 141/300\n",
      "Average training loss: 0.022510421986381212\n",
      "Average test loss: 0.004173139527440071\n",
      "Epoch 142/300\n",
      "Average training loss: 0.022605230988727677\n",
      "Average test loss: 0.004520904088599814\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022491878100567395\n",
      "Average test loss: 0.004019138804326455\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022457462709811\n",
      "Average test loss: 0.003838044936251309\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022496075972914694\n",
      "Average test loss: 0.004633768892536561\n",
      "Epoch 146/300\n",
      "Average training loss: 0.022591810645328628\n",
      "Average test loss: 0.0038172762853403886\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022485522106289863\n",
      "Average test loss: 0.004363920548102922\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022472120793329345\n",
      "Average test loss: 0.003876817793895801\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022523856861723795\n",
      "Average test loss: 0.0039572945050895215\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022352067129479514\n",
      "Average test loss: 0.003799525668223699\n",
      "Epoch 151/300\n",
      "Average training loss: 0.022425991545120875\n",
      "Average test loss: 0.003834230316388938\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02236981727845139\n",
      "Average test loss: 0.003765054497867823\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022304743068085775\n",
      "Average test loss: 0.0037392315440293814\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02243051467835903\n",
      "Average test loss: 0.0037486784921752082\n",
      "Epoch 155/300\n",
      "Average training loss: 0.022371140296260517\n",
      "Average test loss: 0.003886798278739055\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022318055848280588\n",
      "Average test loss: 0.003929953290563491\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022392574813630845\n",
      "Average test loss: 0.0037641550799210866\n",
      "Epoch 158/300\n",
      "Average training loss: 0.022316163279943995\n",
      "Average test loss: 0.004154075641598967\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02234508952167299\n",
      "Average test loss: 0.003928450122475624\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02228435060216321\n",
      "Average test loss: 0.0039797220076951715\n",
      "Epoch 161/300\n",
      "Average training loss: 0.022285312501920593\n",
      "Average test loss: 0.003838592747847239\n",
      "Epoch 162/300\n",
      "Average training loss: 0.022278994608256553\n",
      "Average test loss: 0.003887564388828145\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022331335523062282\n",
      "Average test loss: 0.003852498296027382\n",
      "Epoch 164/300\n",
      "Average training loss: 0.022280950312813123\n",
      "Average test loss: 0.003984114363272157\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02224588228099876\n",
      "Average test loss: 0.0040043331904129845\n",
      "Epoch 166/300\n",
      "Average training loss: 0.022154687714245586\n",
      "Average test loss: 0.004205613744962547\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02226068888604641\n",
      "Average test loss: 0.0038851441006279653\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022220188612739247\n",
      "Average test loss: 0.003912287034508255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022173446169330015\n",
      "Average test loss: 0.004032484818249941\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022196064543392922\n",
      "Average test loss: 0.0038981974580221705\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022216449808743266\n",
      "Average test loss: 0.003826115452374021\n",
      "Epoch 172/300\n",
      "Average training loss: 0.022148782347639402\n",
      "Average test loss: 0.003990962829647793\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02220573020643658\n",
      "Average test loss: 0.003893177450324098\n",
      "Epoch 174/300\n",
      "Average training loss: 0.022107435196638107\n",
      "Average test loss: 0.0038588869124650954\n",
      "Epoch 175/300\n",
      "Average training loss: 0.022147125161356397\n",
      "Average test loss: 0.0037541622954110304\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02216343563132816\n",
      "Average test loss: 0.0038000192269682883\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022105343436201415\n",
      "Average test loss: 0.0038069311630808643\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022094557392928336\n",
      "Average test loss: 0.0038251983837948905\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022091759676734605\n",
      "Average test loss: 0.0040744435938282145\n",
      "Epoch 180/300\n",
      "Average training loss: 0.022150064549512332\n",
      "Average test loss: 0.0039556559717489614\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02217081479728222\n",
      "Average test loss: 0.003949058622121811\n",
      "Epoch 182/300\n",
      "Average training loss: 0.022115146926707692\n",
      "Average test loss: 0.004233747829579645\n",
      "Epoch 183/300\n",
      "Average training loss: 0.022084244698286057\n",
      "Average test loss: 0.004106549739009804\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022048530081907908\n",
      "Average test loss: 0.003767183249195417\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02200728078848786\n",
      "Average test loss: 0.003982824028780063\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02210346520609326\n",
      "Average test loss: 0.003989351811922259\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02198929463989205\n",
      "Average test loss: 0.0040516777338667044\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02195868212978045\n",
      "Average test loss: 0.003852464666797055\n",
      "Epoch 189/300\n",
      "Average training loss: 0.022024544833434953\n",
      "Average test loss: 0.00391793516671492\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021960057617889512\n",
      "Average test loss: 0.004039786971691582\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021961528677079414\n",
      "Average test loss: 0.004166991034729613\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02199878779715962\n",
      "Average test loss: 0.0037497357895804775\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02203067974332306\n",
      "Average test loss: 0.003789930806391769\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022064480642477673\n",
      "Average test loss: 0.00415964318646325\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02197638237807486\n",
      "Average test loss: 0.003882084825800525\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021954303226537176\n",
      "Average test loss: 0.003878094941791561\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021957312362889447\n",
      "Average test loss: 0.0037804384140504733\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021916701032055747\n",
      "Average test loss: 0.003796522457566526\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0218749926785628\n",
      "Average test loss: 0.004111015473802884\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021818792487184206\n",
      "Average test loss: 0.003852990560233593\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0219386500277453\n",
      "Average test loss: 0.00395116061800056\n",
      "Epoch 202/300\n",
      "Average training loss: 0.021885670718219546\n",
      "Average test loss: 0.0037703169191049203\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021973437117205726\n",
      "Average test loss: 0.003983358282595873\n",
      "Epoch 204/300\n",
      "Average training loss: 0.021873876374628808\n",
      "Average test loss: 0.004388889405462477\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021927226160963376\n",
      "Average test loss: 0.0038786111901410753\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021998592699567478\n",
      "Average test loss: 0.004087087442891465\n",
      "Epoch 207/300\n",
      "Average training loss: 0.021844064889682664\n",
      "Average test loss: 0.0037780264077915088\n",
      "Epoch 208/300\n",
      "Average training loss: 0.021772018858128125\n",
      "Average test loss: 0.003812561075720522\n",
      "Epoch 209/300\n",
      "Average training loss: 0.021860872661074\n",
      "Average test loss: 0.004161028722922008\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021815431343184578\n",
      "Average test loss: 0.00404891827950875\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021797036651108\n",
      "Average test loss: 0.0038914798224965732\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02172663565311167\n",
      "Average test loss: 0.003799134162151151\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02182564229435391\n",
      "Average test loss: 0.003946130057175954\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021838606918851533\n",
      "Average test loss: 0.004148945877949397\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02178042588300175\n",
      "Average test loss: 0.003882302985423141\n",
      "Epoch 216/300\n",
      "Average training loss: 0.021772868149810366\n",
      "Average test loss: 0.003860506201369895\n",
      "Epoch 217/300\n",
      "Average training loss: 0.021784095120098854\n",
      "Average test loss: 0.0038649170328345565\n",
      "Epoch 218/300\n",
      "Average training loss: 0.021790041821698348\n",
      "Average test loss: 0.003961026189020938\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02172833412223392\n",
      "Average test loss: 0.004130763140403562\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021690067066086664\n",
      "Average test loss: 0.0038793615765041773\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021718716757165062\n",
      "Average test loss: 0.003774956048776706\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02172562419374784\n",
      "Average test loss: 0.00396612859186199\n",
      "Epoch 223/300\n",
      "Average training loss: 0.021791976264781424\n",
      "Average test loss: 0.003827103879923622\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02172680939733982\n",
      "Average test loss: 0.003802936621424225\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021756511469682056\n",
      "Average test loss: 0.004162339942736758\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021639552002151806\n",
      "Average test loss: 0.00384177652394606\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021709203771418994\n",
      "Average test loss: 0.004146574203338888\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021777860720952353\n",
      "Average test loss: 0.003775610600908597\n",
      "Epoch 229/300\n",
      "Average training loss: 0.021713651155432067\n",
      "Average test loss: 0.004034026715076632\n",
      "Epoch 230/300\n",
      "Average training loss: 0.021644727046291033\n",
      "Average test loss: 0.0038164938017725943\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02168645021236605\n",
      "Average test loss: 0.0038495504727794065\n",
      "Epoch 232/300\n",
      "Average training loss: 0.021685302754243217\n",
      "Average test loss: 0.0037824580390006302\n",
      "Epoch 233/300\n",
      "Average training loss: 0.021652703609731462\n",
      "Average test loss: 0.003886078230622742\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021599421166711385\n",
      "Average test loss: 0.003929163637260596\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02165259906484021\n",
      "Average test loss: 0.00447339092919396\n",
      "Epoch 236/300\n",
      "Average training loss: 0.021597466632723807\n",
      "Average test loss: 0.003887702187316285\n",
      "Epoch 237/300\n",
      "Average training loss: 0.021612957941161262\n",
      "Average test loss: 0.0038396609675967032\n",
      "Epoch 238/300\n",
      "Average training loss: 0.021632395966185464\n",
      "Average test loss: 0.003896041098982096\n",
      "Epoch 239/300\n",
      "Average training loss: 0.021561532540453805\n",
      "Average test loss: 0.0038917529835469193\n",
      "Epoch 240/300\n",
      "Average training loss: 0.021616166843308342\n",
      "Average test loss: 0.003830835627184974\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02156194583574931\n",
      "Average test loss: 0.0038247379466063445\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021602776918146344\n",
      "Average test loss: 0.003873313885182142\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02155634800924195\n",
      "Average test loss: 0.003909753040100138\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02157990071343051\n",
      "Average test loss: 0.00393600438742174\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021614910976754293\n",
      "Average test loss: 0.003884164217652546\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021539468847215176\n",
      "Average test loss: 0.003873852212395933\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02152944267955091\n",
      "Average test loss: 0.004024437434350451\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02154469878309303\n",
      "Average test loss: 0.003883071549029814\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021578416503138013\n",
      "Average test loss: 0.0038963244340072074\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02152199632426103\n",
      "Average test loss: 0.00390266286705931\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021549187406897544\n",
      "Average test loss: 0.0038399740633451277\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02150457450416353\n",
      "Average test loss: 0.0038426416334178715\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021495558936562804\n",
      "Average test loss: 0.003934534769298302\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021482495034734407\n",
      "Average test loss: 0.0038350938227441574\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02150056416955259\n",
      "Average test loss: 0.0038050866540935303\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021466765214999517\n",
      "Average test loss: 0.004053129986756378\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021453688331776195\n",
      "Average test loss: 0.003792988391800059\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021546921976738506\n",
      "Average test loss: 0.00399249728437927\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02144807401796182\n",
      "Average test loss: 0.0038585182740870447\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021485406724943056\n",
      "Average test loss: 0.003868437775100271\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021456981124149427\n",
      "Average test loss: 0.0051810666343404185\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0214571800082922\n",
      "Average test loss: 0.003929677121134268\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02152107746567991\n",
      "Average test loss: 0.003933343075008856\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02140271398425102\n",
      "Average test loss: 0.0043330646132429445\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021435349355141322\n",
      "Average test loss: 0.004114827900504073\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02143397399948703\n",
      "Average test loss: 0.0038483356249829135\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021431949295931393\n",
      "Average test loss: 0.003915238261222839\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02138777797586388\n",
      "Average test loss: 0.00429544759914279\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021411044917172856\n",
      "Average test loss: 0.003922653908530871\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021379548327790366\n",
      "Average test loss: 0.0041827779962784715\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021367235193649928\n",
      "Average test loss: 0.0038968260942233933\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021384475588798524\n",
      "Average test loss: 0.003795377489593294\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021375003332893053\n",
      "Average test loss: 0.0038331592679023744\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02145397657321559\n",
      "Average test loss: 0.0038483041529026297\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02134446359342999\n",
      "Average test loss: 0.0039061177565405765\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021336372945043775\n",
      "Average test loss: 0.0038893712024307913\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02136209176811907\n",
      "Average test loss: 0.003890403099772003\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021356347827447785\n",
      "Average test loss: 0.003854693326685164\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02134997521671984\n",
      "Average test loss: 0.0038683021308647263\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021436145250995953\n",
      "Average test loss: 0.003886605949865447\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021332888376381663\n",
      "Average test loss: 0.003808712575170729\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02139043474694093\n",
      "Average test loss: 0.003806418641159932\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021302540585398676\n",
      "Average test loss: 0.0039650308618115054\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02125432324740622\n",
      "Average test loss: 0.0038025532590432298\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021350068522824183\n",
      "Average test loss: 0.0038081641209622226\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021319546068708103\n",
      "Average test loss: 0.003792485105494658\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0212901089058982\n",
      "Average test loss: 0.003867593130303754\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02126910104188654\n",
      "Average test loss: 0.003879872679710388\n",
      "Epoch 289/300\n",
      "Average training loss: 0.021316679515772395\n",
      "Average test loss: 0.0038659644205537107\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021311301516162024\n",
      "Average test loss: 0.019269993162817425\n",
      "Epoch 291/300\n",
      "Average training loss: 0.021472338184714317\n",
      "Average test loss: 0.003951494408357474\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02126718840168582\n",
      "Average test loss: 0.0038538722147544225\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021255431065956752\n",
      "Average test loss: 0.004472720230619113\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02128439748618338\n",
      "Average test loss: 0.0038480132673349644\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02122747014794085\n",
      "Average test loss: 0.003814565521561437\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021289074260327553\n",
      "Average test loss: 0.004176989674154255\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021244142848584387\n",
      "Average test loss: 0.003922215219794048\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021239237374729582\n",
      "Average test loss: 0.003959086860012677\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021268051478597852\n",
      "Average test loss: 0.004642911356356409\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021217082558406725\n",
      "Average test loss: 0.00392762932409015\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11123582391606437\n",
      "Average test loss: 0.009504338995450072\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04557604556282362\n",
      "Average test loss: 0.007667451564636495\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04083377746078703\n",
      "Average test loss: 0.0054416600701709585\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03857678168680933\n",
      "Average test loss: 0.006740244974692663\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03661028568612205\n",
      "Average test loss: 0.005816825501206848\n",
      "Epoch 6/300\n",
      "Average training loss: 0.033415748748514384\n",
      "Average test loss: 0.006548626289185551\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03251285803152455\n",
      "Average test loss: 0.004853754657424159\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03079439705941412\n",
      "Average test loss: 0.005252937448314495\n",
      "Epoch 9/300\n",
      "Average training loss: 0.029461996323532528\n",
      "Average test loss: 0.004698680540753735\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028456657846768697\n",
      "Average test loss: 0.005554442720280753\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027786374050709935\n",
      "Average test loss: 0.007058137613866065\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026957578195465935\n",
      "Average test loss: 0.007202740090588728\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026019724057780372\n",
      "Average test loss: 0.0037066303497801227\n",
      "Epoch 14/300\n",
      "Average training loss: 0.025376284799642032\n",
      "Average test loss: 0.0042941199822558295\n",
      "Epoch 15/300\n",
      "Average training loss: 0.025478444937202666\n",
      "Average test loss: 0.004188854381235109\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02461695673233933\n",
      "Average test loss: 0.004149444945984416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024278565438257325\n",
      "Average test loss: 0.003446515732962224\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024178351644012663\n",
      "Average test loss: 0.003543708892332183\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02380689413183265\n",
      "Average test loss: 0.010174471776518557\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02342758057680395\n",
      "Average test loss: 0.0040189641726513705\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02314958171877596\n",
      "Average test loss: 0.0041333091714315946\n",
      "Epoch 22/300\n",
      "Average training loss: 0.022925585822926628\n",
      "Average test loss: 0.0034097327699677813\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02271400360100799\n",
      "Average test loss: 0.0038369655112425486\n",
      "Epoch 24/300\n",
      "Average training loss: 0.022464647483494547\n",
      "Average test loss: 0.003328495261983739\n",
      "Epoch 25/300\n",
      "Average training loss: 0.022067373709546194\n",
      "Average test loss: 0.003706044772018989\n",
      "Epoch 26/300\n",
      "Average training loss: 0.022377267233199542\n",
      "Average test loss: 0.003327888768994146\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021829689712988005\n",
      "Average test loss: 0.0033038161916451323\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02178625164429347\n",
      "Average test loss: 0.0035009840147362816\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02170540001160569\n",
      "Average test loss: 0.0033230383892854055\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02146792072719998\n",
      "Average test loss: 0.0031855171310404937\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021209060298071968\n",
      "Average test loss: 0.003184198100119829\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021022048716743787\n",
      "Average test loss: 0.0030503867539680668\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021274286894334686\n",
      "Average test loss: 0.0031971612053198948\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021098620859285197\n",
      "Average test loss: 0.0033025707221693464\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020899402484297752\n",
      "Average test loss: 0.0038154662582609387\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020832724993427594\n",
      "Average test loss: 0.003872564173820946\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020818563921584023\n",
      "Average test loss: 0.002973458586881558\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020796102495657075\n",
      "Average test loss: 0.0031574127887272173\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020706357330083847\n",
      "Average test loss: 0.003154726198150052\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020416966151860026\n",
      "Average test loss: 0.009542008372230662\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02044167702479495\n",
      "Average test loss: 0.0030126976422551605\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020352228990859457\n",
      "Average test loss: 0.003135445763874385\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020371398934059672\n",
      "Average test loss: 0.0029341365384558836\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02024981815285153\n",
      "Average test loss: 0.0036393021694901915\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020251344932450187\n",
      "Average test loss: 0.0037501670002109476\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02017645702428288\n",
      "Average test loss: 0.002817945081533657\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02011688441865974\n",
      "Average test loss: 0.0031978278470536073\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019841384440660477\n",
      "Average test loss: 0.0030375208099269204\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020003062105841105\n",
      "Average test loss: 0.0028756145553456413\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019956111995710266\n",
      "Average test loss: 0.002882904736325145\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01989932247830762\n",
      "Average test loss: 0.0031547534908685417\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019869352196653683\n",
      "Average test loss: 0.0028479714228047265\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019839556207259496\n",
      "Average test loss: 0.002832795041716761\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01979901184969478\n",
      "Average test loss: 0.0028038516843484507\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01972481358713574\n",
      "Average test loss: 0.002851986899971962\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019568353691034848\n",
      "Average test loss: 0.0027846338231530456\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01957325605220265\n",
      "Average test loss: 0.00280332576814625\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01963064234620995\n",
      "Average test loss: 0.0029881636657648615\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019492789001928437\n",
      "Average test loss: 0.0028664502476652464\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01949968509044912\n",
      "Average test loss: 0.002843272171086735\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019466233329640495\n",
      "Average test loss: 0.0029618520974698994\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01937942405210601\n",
      "Average test loss: 0.0030817959056132368\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01952151877515846\n",
      "Average test loss: 0.0028980268587668737\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01940464668141471\n",
      "Average test loss: 0.00287424469490846\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01934665388862292\n",
      "Average test loss: 0.02665103414737516\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019217177198992835\n",
      "Average test loss: 0.003102621341124177\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019257353241244952\n",
      "Average test loss: 0.002863502882627977\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019145397184623613\n",
      "Average test loss: 0.002982473192943467\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019244873215754826\n",
      "Average test loss: 0.0028317621428933406\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019206096819705434\n",
      "Average test loss: 0.004453045916226175\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019198616863952744\n",
      "Average test loss: 0.002980230456839005\n",
      "Epoch 72/300\n",
      "Average training loss: 0.018981316336327127\n",
      "Average test loss: 0.002863217172937261\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019079008898801274\n",
      "Average test loss: 0.002816661981244882\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018948320855696997\n",
      "Average test loss: 0.0028129656337615517\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019040971709622278\n",
      "Average test loss: 0.002840000558851494\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018987598617871602\n",
      "Average test loss: 0.0028180458042770624\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019106328407095537\n",
      "Average test loss: 0.002824379609276851\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018897175730930433\n",
      "Average test loss: 0.00949838934880164\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018919057870904605\n",
      "Average test loss: 0.00330210919243594\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018845212143328453\n",
      "Average test loss: 0.0028052005097270014\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018789138524068726\n",
      "Average test loss: 0.002751780528575182\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018992902722623613\n",
      "Average test loss: 0.0027492595842729013\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01893423685762617\n",
      "Average test loss: 0.0027742593011094462\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018676282551553514\n",
      "Average test loss: 0.0028864719259242217\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01891245622601774\n",
      "Average test loss: 0.0029542409165038003\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018757645840446154\n",
      "Average test loss: 0.002833970525715914\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0186272772004207\n",
      "Average test loss: 0.0027836885396391154\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01877158864339193\n",
      "Average test loss: 0.002867152916267514\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01858960464100043\n",
      "Average test loss: 0.0034389501112616723\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01871546087000105\n",
      "Average test loss: 0.0030184065161479845\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018638834731446372\n",
      "Average test loss: 0.00291322120030721\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018566412248545223\n",
      "Average test loss: 0.0027643377520143986\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018657391549812424\n",
      "Average test loss: 0.00281740388336281\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018586649816897182\n",
      "Average test loss: 0.002785667160111997\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01852668905258179\n",
      "Average test loss: 0.0027895832902027503\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018605476970473926\n",
      "Average test loss: 0.0029070080411103035\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018445202563371923\n",
      "Average test loss: 0.002695957162313991\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01848699792391724\n",
      "Average test loss: 0.0027167911490218505\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01850030618078179\n",
      "Average test loss: 0.0027129557997816137\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01852957380314668\n",
      "Average test loss: 0.0027570859315908616\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018450726276470555\n",
      "Average test loss: 0.004002454203450017\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018414670128789213\n",
      "Average test loss: 0.002744002310766114\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018406013051668803\n",
      "Average test loss: 0.0028091039467189045\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018403522128860157\n",
      "Average test loss: 0.00567904840865069\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01831324735780557\n",
      "Average test loss: 0.0026869362882441946\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018376767516964012\n",
      "Average test loss: 0.0028936297108108798\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01836487265759044\n",
      "Average test loss: 0.0028682161350217132\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01835621525347233\n",
      "Average test loss: 0.002675589431387683\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018281788741548857\n",
      "Average test loss: 0.0028130775801837446\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018310749100314248\n",
      "Average test loss: 0.0027351516109580796\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018338399397002325\n",
      "Average test loss: 0.003111644533980224\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018275757172869313\n",
      "Average test loss: 0.008160960778387056\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018351386988328562\n",
      "Average test loss: 0.0028108223531809117\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018258528149790235\n",
      "Average test loss: 0.0026665729474690224\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01817389137794574\n",
      "Average test loss: 0.002739248795227872\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018293596542543835\n",
      "Average test loss: 0.002906674933205876\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018160905978745884\n",
      "Average test loss: 0.0030245044680519235\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01813358979423841\n",
      "Average test loss: 0.0033937589128812153\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018214344206783507\n",
      "Average test loss: 0.0027310309594290125\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01816871941751904\n",
      "Average test loss: 0.0028103611527217757\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01816481531659762\n",
      "Average test loss: 0.002711024671792984\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01810189138021734\n",
      "Average test loss: 0.0027737419735640286\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018068176830808323\n",
      "Average test loss: 0.0028054321099900536\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018124503173761897\n",
      "Average test loss: 0.002717542114564114\n",
      "Epoch 125/300\n",
      "Average training loss: 0.018040006278289687\n",
      "Average test loss: 0.002791128183197644\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018070194176501696\n",
      "Average test loss: 0.002668958098731107\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01798437586095598\n",
      "Average test loss: 0.002802313289812042\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018118571023974153\n",
      "Average test loss: 0.002727052415824599\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018047006626923878\n",
      "Average test loss: 0.0031849667244694298\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017966055969397227\n",
      "Average test loss: 0.002669633851490087\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01799652756253878\n",
      "Average test loss: 0.0027697633161313006\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018025712591078547\n",
      "Average test loss: 0.0028099454496469762\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018034141977628072\n",
      "Average test loss: 0.002938598581900199\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017967064147194227\n",
      "Average test loss: 0.0030537914698943497\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017943190202116967\n",
      "Average test loss: 0.0031952470706568823\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018076095398929383\n",
      "Average test loss: 0.0038459935809175175\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017924180964628854\n",
      "Average test loss: 0.002652406093560987\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017911744325525232\n",
      "Average test loss: 0.0026551206504098245\n",
      "Epoch 139/300\n",
      "Average training loss: 0.018077906823820537\n",
      "Average test loss: 0.007393260853158103\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01789616586930222\n",
      "Average test loss: 0.002782340572112136\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017934907977779708\n",
      "Average test loss: 0.002730063203929199\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017874734229511684\n",
      "Average test loss: 0.009092064317315816\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017901043261918756\n",
      "Average test loss: 0.002681742899533775\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017767078301972813\n",
      "Average test loss: 0.0028404086826162205\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017827202709184753\n",
      "Average test loss: 0.0027422047346416446\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01778337026387453\n",
      "Average test loss: 0.002979132770664162\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017759837962687014\n",
      "Average test loss: 0.003305203983146283\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017816841111414964\n",
      "Average test loss: 0.002861331710902353\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017777470278243224\n",
      "Average test loss: 0.0027204571916825244\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01784234725766712\n",
      "Average test loss: 0.0027300047636446027\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017907314356002544\n",
      "Average test loss: 0.002809985707824429\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01776860945423444\n",
      "Average test loss: 0.002672371209288637\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017765757833917935\n",
      "Average test loss: 0.002695372010788156\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017696611039340497\n",
      "Average test loss: 0.0027052459102123974\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01777337252597014\n",
      "Average test loss: 0.002646418841348754\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017737900357279512\n",
      "Average test loss: 0.0029960172788964378\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017711215568913353\n",
      "Average test loss: 0.002826795324579709\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017704185399744245\n",
      "Average test loss: 0.003090840848369731\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01770235381854905\n",
      "Average test loss: 0.003530147599677245\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01778151975820462\n",
      "Average test loss: 0.002939398331774606\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017640598232547443\n",
      "Average test loss: 0.002754517696383927\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01770418416791492\n",
      "Average test loss: 0.0027133545523716344\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017694145609935125\n",
      "Average test loss: 0.002737364870392614\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017647024433645936\n",
      "Average test loss: 0.00277798709521691\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017740156321061983\n",
      "Average test loss: 0.0029225753910011717\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017616574050651656\n",
      "Average test loss: 0.0031633718862301772\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01767209664152728\n",
      "Average test loss: 0.002815732089802623\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017640189259416527\n",
      "Average test loss: 0.002816431257356372\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017533073446816868\n",
      "Average test loss: 0.010147545108364688\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017605733593304952\n",
      "Average test loss: 0.002712518209384547\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017638512346479628\n",
      "Average test loss: 0.0027537773449180856\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017502446292175188\n",
      "Average test loss: 0.0036899681476255256\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017634990591141913\n",
      "Average test loss: 0.0027330989816950427\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017542282073034182\n",
      "Average test loss: 0.002710903050791886\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017549542049566904\n",
      "Average test loss: 0.0026311424518417983\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017617983960443073\n",
      "Average test loss: 0.002743563517100281\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017543375690778098\n",
      "Average test loss: 0.0036578925705204406\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01751396351555983\n",
      "Average test loss: 0.0026393459979444744\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017629900180631215\n",
      "Average test loss: 0.0026722413740224308\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01757006548013952\n",
      "Average test loss: 0.0027167498621468744\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0175063273343775\n",
      "Average test loss: 0.002873672441062\n",
      "Epoch 183/300\n",
      "Average training loss: 0.017512000783450072\n",
      "Average test loss: 0.002790137056675222\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01744932232466009\n",
      "Average test loss: 0.0027302202954888346\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01747200957602925\n",
      "Average test loss: 0.0028858168948855665\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01744096771876017\n",
      "Average test loss: 0.002694092761518227\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01749447918931643\n",
      "Average test loss: 0.0027150510936561558\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017454216197133065\n",
      "Average test loss: 0.005760835772587194\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01744993189473947\n",
      "Average test loss: 0.0027812837217417027\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01746099286278089\n",
      "Average test loss: 0.002955450435893403\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01740014436758227\n",
      "Average test loss: 0.002736439566852318\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017450027232368787\n",
      "Average test loss: 0.0027473919635845555\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017438669265972245\n",
      "Average test loss: 0.0029709661598834726\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01745463867733876\n",
      "Average test loss: 0.002688289208130704\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017454678879843817\n",
      "Average test loss: 0.0027529108037965167\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017362621249424087\n",
      "Average test loss: 0.002691604476215111\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017406042619711824\n",
      "Average test loss: 0.002764371694996953\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017327788945701385\n",
      "Average test loss: 0.0029201125454985434\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017356331813666557\n",
      "Average test loss: 0.0029322733100917604\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017410061203771168\n",
      "Average test loss: 0.003011931528647741\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017360429990622733\n",
      "Average test loss: 0.002715043246642583\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017390558734536172\n",
      "Average test loss: 0.0030932713308268124\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017398959125081697\n",
      "Average test loss: 0.0027411698359582157\n",
      "Epoch 204/300\n",
      "Average training loss: 0.017289100605580542\n",
      "Average test loss: 0.0027583745214053326\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017343520436022018\n",
      "Average test loss: 0.0027175868054231007\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017228954914543362\n",
      "Average test loss: 0.002705799978536864\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01733596994313929\n",
      "Average test loss: 0.00277003667751948\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01727601301835643\n",
      "Average test loss: 0.0027413538201815552\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017297192096710205\n",
      "Average test loss: 0.002783088592191537\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01739726818104585\n",
      "Average test loss: 0.0034285904094576835\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01731165683435069\n",
      "Average test loss: 0.0029151706590006747\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01724156028860145\n",
      "Average test loss: 0.0027430276448527973\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017237752540243997\n",
      "Average test loss: 0.0027187336145175827\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01729601288172934\n",
      "Average test loss: 0.0026951517787658506\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017344617419772678\n",
      "Average test loss: 0.0027740810089227227\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01723140863577525\n",
      "Average test loss: 0.002723903785770138\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017195870263708963\n",
      "Average test loss: 0.002717062484679951\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01766387060450183\n",
      "Average test loss: 0.003894303817508949\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01725133872197734\n",
      "Average test loss: 0.002808155085477564\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017189887104762925\n",
      "Average test loss: 0.00273234682261116\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017234794682926602\n",
      "Average test loss: 0.0026586819966841076\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017213528846700986\n",
      "Average test loss: 0.0027020460491379104\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017280466158356933\n",
      "Average test loss: 0.003134348993293113\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017147154182195663\n",
      "Average test loss: 0.0026875748882691066\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017271669876244334\n",
      "Average test loss: 0.0031474054443339505\n",
      "Epoch 226/300\n",
      "Average training loss: 0.017235428917739125\n",
      "Average test loss: 0.002703968879663282\n",
      "Epoch 227/300\n",
      "Average training loss: 0.017165777641865942\n",
      "Average test loss: 0.0029839919147392115\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017199707258078786\n",
      "Average test loss: 0.0028969534474114576\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01714005432360702\n",
      "Average test loss: 0.0027274585598044926\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01745485298666689\n",
      "Average test loss: 0.0027372054557005566\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017155929442081188\n",
      "Average test loss: 0.0028230711604572004\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01712570363779863\n",
      "Average test loss: 0.008922512375232247\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017189996947844823\n",
      "Average test loss: 0.0027369100387311645\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01717938695847988\n",
      "Average test loss: 0.002795923603905572\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017142978664901522\n",
      "Average test loss: 0.0027597450638810793\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017033503401610587\n",
      "Average test loss: 0.005026732900283403\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01712831723690033\n",
      "Average test loss: 0.002734099133561055\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01708770165261295\n",
      "Average test loss: 0.002936011384965645\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01716398967968093\n",
      "Average test loss: 0.00289174387893743\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0171691223581632\n",
      "Average test loss: 0.002884404073572821\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01712065752595663\n",
      "Average test loss: 0.0027143181045022275\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017033526790638763\n",
      "Average test loss: 0.002729643450636003\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01706513715783755\n",
      "Average test loss: 0.0028365119265185462\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017191788945760993\n",
      "Average test loss: 0.002675499335345295\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017087461882167393\n",
      "Average test loss: 0.0027852526302966805\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017037930250167848\n",
      "Average test loss: 0.002640070726060205\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017159877508878707\n",
      "Average test loss: 0.0028241903425918684\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017075457433031665\n",
      "Average test loss: 0.0028595899308307303\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017031705699033207\n",
      "Average test loss: 0.002778817747357405\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01700888494650523\n",
      "Average test loss: 0.002758324921545055\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017006996277305814\n",
      "Average test loss: 0.0027898873546057276\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01702595031592581\n",
      "Average test loss: 0.00266063382041951\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017055890573395623\n",
      "Average test loss: 0.0026938508016367754\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01701759901146094\n",
      "Average test loss: 0.00295197437186208\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017017558394206895\n",
      "Average test loss: 0.0027776236256791486\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017077078783677684\n",
      "Average test loss: 0.0027884863580887515\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016985139527254636\n",
      "Average test loss: 0.0028723334479663107\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017019349256323445\n",
      "Average test loss: 0.0027886889001561537\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01702659014198515\n",
      "Average test loss: 0.008471632508767975\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01698113262653351\n",
      "Average test loss: 0.002987571318116453\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016986028431190386\n",
      "Average test loss: 0.0027374803086535797\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016986312597990036\n",
      "Average test loss: 0.002739659399415056\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016967156015336514\n",
      "Average test loss: 0.0027633225772943763\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01696779312027825\n",
      "Average test loss: 0.0028106716486314934\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01705502340859837\n",
      "Average test loss: 0.002709408675216966\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01696841656251086\n",
      "Average test loss: 0.0027732331920415163\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016983178548514843\n",
      "Average test loss: 0.01522703113531073\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01695292211572329\n",
      "Average test loss: 0.002675257837606801\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016948107473552226\n",
      "Average test loss: 0.002769996929499838\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01692550630867481\n",
      "Average test loss: 0.0030918968206064567\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016942969045705265\n",
      "Average test loss: 0.002684406412558423\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016926381369431812\n",
      "Average test loss: 0.0028140680924471882\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016910735486282243\n",
      "Average test loss: 0.002773429670681556\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016935750545726883\n",
      "Average test loss: 0.0026883593090913363\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016897915691137314\n",
      "Average test loss: 0.002764171368959877\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016911630450023547\n",
      "Average test loss: 0.002758571673391594\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016889429825875494\n",
      "Average test loss: 0.002750389378103945\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016922950569126342\n",
      "Average test loss: 0.00277985858068698\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016887110413776503\n",
      "Average test loss: 0.002678825086914003\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016897035367786886\n",
      "Average test loss: 0.0027308064771609172\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016861133696304428\n",
      "Average test loss: 0.002680388062778446\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01686352355943786\n",
      "Average test loss: 0.0028538636337551806\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01688608380158742\n",
      "Average test loss: 0.0027354627850775916\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016884041830897333\n",
      "Average test loss: 0.003008518922246165\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01684364035560025\n",
      "Average test loss: 0.0027229213896724914\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01703774092760351\n",
      "Average test loss: 0.01662646018092831\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016879977530075443\n",
      "Average test loss: 0.003082313692818085\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016841765464180047\n",
      "Average test loss: 0.002807877337767018\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01687960558384657\n",
      "Average test loss: 0.0028310828072329363\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016819749533302255\n",
      "Average test loss: 0.002816762606923779\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01684382090303633\n",
      "Average test loss: 0.002690441500602497\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016830760962433286\n",
      "Average test loss: 0.002738547119208508\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01684306646635135\n",
      "Average test loss: 0.0027753012196885217\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01685314650254117\n",
      "Average test loss: 0.0030481247138231995\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01680022025770611\n",
      "Average test loss: 0.0027136686542588804\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01684440145227644\n",
      "Average test loss: 0.0028727684271418388\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016820763962136373\n",
      "Average test loss: 0.0027903292393311858\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01679138779309061\n",
      "Average test loss: 0.002844173659570515\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016852043201526005\n",
      "Average test loss: 0.0027469343147757982\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01680156119995647\n",
      "Average test loss: 0.002886852805606193\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10371448549959394\n",
      "Average test loss: 0.005290561085773839\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04026293843653467\n",
      "Average test loss: 0.007930463440716266\n",
      "Epoch 3/300\n",
      "Average training loss: 0.036309020866950356\n",
      "Average test loss: 0.010633842526210678\n",
      "Epoch 4/300\n",
      "Average training loss: 0.033449283404482735\n",
      "Average test loss: 0.009797497063875199\n",
      "Epoch 5/300\n",
      "Average training loss: 0.031180576774809097\n",
      "Average test loss: 0.0037831888194713326\n",
      "Epoch 6/300\n",
      "Average training loss: 0.028868750216232406\n",
      "Average test loss: 0.004579989948206477\n",
      "Epoch 7/300\n",
      "Average training loss: 0.027304636786381405\n",
      "Average test loss: 0.005990431446168158\n",
      "Epoch 8/300\n",
      "Average training loss: 0.025831558134820725\n",
      "Average test loss: 0.004228610369066397\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02466931413941913\n",
      "Average test loss: 0.005664550460461113\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02410979385011726\n",
      "Average test loss: 0.004827778382433786\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02292699603239695\n",
      "Average test loss: 0.003289546023019486\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02252315630349848\n",
      "Average test loss: 0.0028893833917876086\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021811911976999707\n",
      "Average test loss: 0.003173406376606888\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02170080440905359\n",
      "Average test loss: 0.0030800230149179695\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020916962358686658\n",
      "Average test loss: 0.0030244726033674344\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020564504964484108\n",
      "Average test loss: 0.002629107547820442\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02039387991776069\n",
      "Average test loss: 0.002686371092995008\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020652400579717425\n",
      "Average test loss: 0.003560984064307478\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01970367602755626\n",
      "Average test loss: 0.003187832201934523\n",
      "Epoch 20/300\n",
      "Average training loss: 0.019437648753325144\n",
      "Average test loss: 0.002650616904720664\n",
      "Epoch 21/300\n",
      "Average training loss: 0.019521417864494853\n",
      "Average test loss: 0.0029353444785293606\n",
      "Epoch 22/300\n",
      "Average training loss: 0.019219757202598782\n",
      "Average test loss: 0.0027156655869136253\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018896627070175278\n",
      "Average test loss: 0.010134658196734057\n",
      "Epoch 24/300\n",
      "Average training loss: 0.018937120194236438\n",
      "Average test loss: 0.0025058992898298635\n",
      "Epoch 25/300\n",
      "Average training loss: 0.018831296008494165\n",
      "Average test loss: 0.0028277855977002116\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01847990612188975\n",
      "Average test loss: 0.0024401754432668288\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01835437272157934\n",
      "Average test loss: 0.002320729464913408\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01812394349442588\n",
      "Average test loss: 0.0024135830134360326\n",
      "Epoch 29/300\n",
      "Average training loss: 0.018134679622120328\n",
      "Average test loss: 0.002528382185432646\n",
      "Epoch 30/300\n",
      "Average training loss: 0.018068495359685684\n",
      "Average test loss: 0.002390739103986157\n",
      "Epoch 31/300\n",
      "Average training loss: 0.018045437840951813\n",
      "Average test loss: 0.002839620133034057\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017826758806904156\n",
      "Average test loss: 0.0023235014350050025\n",
      "Epoch 33/300\n",
      "Average training loss: 0.017710612792935638\n",
      "Average test loss: 0.002641093685809109\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017516299832198354\n",
      "Average test loss: 0.002236678842455149\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01760366051726871\n",
      "Average test loss: 0.0238455457786719\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017534786593582895\n",
      "Average test loss: 0.00241989064671927\n",
      "Epoch 37/300\n",
      "Average training loss: 0.017440543138318592\n",
      "Average test loss: 0.0023888181644595332\n",
      "Epoch 38/300\n",
      "Average training loss: 0.017315460373957953\n",
      "Average test loss: 0.002306251951182882\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017252634211546845\n",
      "Average test loss: 0.0023472477684004438\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01722509358988868\n",
      "Average test loss: 0.002248393478285935\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01719048375553555\n",
      "Average test loss: 0.002443709096767836\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017141825899481773\n",
      "Average test loss: 0.0022962026736802524\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01690288144018915\n",
      "Average test loss: 0.0027366431086427636\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016925404066012964\n",
      "Average test loss: 0.0023846905392905077\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01712019677956899\n",
      "Average test loss: 0.0023480766907127367\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016836944450106885\n",
      "Average test loss: 0.0022820619872460763\n",
      "Epoch 47/300\n",
      "Average training loss: 0.016721568181282943\n",
      "Average test loss: 0.00223732554850479\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016796518688400586\n",
      "Average test loss: 0.00217413287030326\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016640728677312533\n",
      "Average test loss: 0.002258108790860408\n",
      "Epoch 50/300\n",
      "Average training loss: 0.016631470517979727\n",
      "Average test loss: 0.0033114938880834314\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01678044055898984\n",
      "Average test loss: 0.002151598378395041\n",
      "Epoch 52/300\n",
      "Average training loss: 0.016589587195879885\n",
      "Average test loss: 0.002247248395346105\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016498795620269246\n",
      "Average test loss: 0.0024139125231239533\n",
      "Epoch 54/300\n",
      "Average training loss: 0.016547876624597443\n",
      "Average test loss: 0.0023651963983558947\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016386208688219387\n",
      "Average test loss: 0.0023566444737629757\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016603434322608843\n",
      "Average test loss: 0.0033114004246890547\n",
      "Epoch 57/300\n",
      "Average training loss: 0.016636208900147016\n",
      "Average test loss: 0.002532550691316525\n",
      "Epoch 58/300\n",
      "Average training loss: 0.016278994747334057\n",
      "Average test loss: 0.0022621607540382277\n",
      "Epoch 59/300\n",
      "Average training loss: 0.016392342406842442\n",
      "Average test loss: 0.002107543771051698\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016190159876313476\n",
      "Average test loss: 0.0022227214014985497\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01640696633855502\n",
      "Average test loss: 0.0021400726040204368\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01639289431273937\n",
      "Average test loss: 0.0020817171697401337\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016090564496815205\n",
      "Average test loss: 0.0021428466975275014\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01627327601942751\n",
      "Average test loss: 0.002132906136620376\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01611138886710008\n",
      "Average test loss: 0.0026991787447283666\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016092442304723793\n",
      "Average test loss: 0.002390275536932879\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016105680550138154\n",
      "Average test loss: 0.00218449476816588\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01602789391246107\n",
      "Average test loss: 0.0021028261792121663\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016060178390807575\n",
      "Average test loss: 0.002226597728828589\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01602133884363704\n",
      "Average test loss: 0.0021318134238115616\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01603738410770893\n",
      "Average test loss: 0.0023540601651701664\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01599650223635965\n",
      "Average test loss: 0.0022283967778914506\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01590265495909585\n",
      "Average test loss: 0.004299567657212416\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015963664883540736\n",
      "Average test loss: 0.0025411192264614833\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01596191201524602\n",
      "Average test loss: 0.0021675571468141343\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015894800966812506\n",
      "Average test loss: 0.002068486915487382\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015893154036667613\n",
      "Average test loss: 0.0021703815505736404\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01578583927700917\n",
      "Average test loss: 0.0035861583343810505\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015831446894341045\n",
      "Average test loss: 0.0022382733119237753\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015927307188510896\n",
      "Average test loss: 0.0021727842578871384\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015680862840678957\n",
      "Average test loss: 0.002271196491188473\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01580109221322669\n",
      "Average test loss: 0.0020724740996956826\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015602229335241847\n",
      "Average test loss: 0.0022869531836153733\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01578177087671227\n",
      "Average test loss: 0.0022320476547918384\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0158622968883978\n",
      "Average test loss: 0.0032210896611213685\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01571202343867885\n",
      "Average test loss: 0.00242338632999195\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0155959106153912\n",
      "Average test loss: 0.002151134927964045\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015676662292745377\n",
      "Average test loss: 0.0022299956443409125\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015558874698976675\n",
      "Average test loss: 0.0022411725781857966\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015632812104291387\n",
      "Average test loss: 0.0020968172467417186\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015520899405082066\n",
      "Average test loss: 0.0034397878388149872\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015474427518745263\n",
      "Average test loss: 0.002416066435062223\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015498600825667381\n",
      "Average test loss: 0.002109084023990565\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015459077205095026\n",
      "Average test loss: 0.002109677736543947\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01545899560302496\n",
      "Average test loss: 0.0021456234337141116\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015458003622790178\n",
      "Average training loss: 0.015466577917337418\n",
      "Average test loss: 0.0022416277277386852\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015389106915642818\n",
      "Average test loss: 0.002221361697754926\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015499952087799708\n",
      "Average test loss: 0.0021111131813377143\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01535202871925301\n",
      "Average test loss: 0.0022538714315742253\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01538736972047223\n",
      "Average test loss: 0.0031374020980050165\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015461654769877592\n",
      "Average test loss: 0.002461035966459248\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015277098827064038\n",
      "Average test loss: 0.0021167645571339463\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015283608694871267\n",
      "Average test loss: 0.002534371403356393\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015317746135923597\n",
      "Average test loss: 0.0023420339483353828\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015309061924616495\n",
      "Average test loss: 0.002146904192244013\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015251580663853221\n",
      "Average test loss: 0.0022042713484002486\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015316871437761519\n",
      "Average test loss: 0.0022967607418282166\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015260306149721146\n",
      "Average test loss: 0.002119188465178013\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015220653167201413\n",
      "Average test loss: 0.002376978359081679\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015162151967485746\n",
      "Average test loss: 0.002060239918736948\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015184761211276054\n",
      "Average test loss: 0.002110618273106714\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015232169719205962\n",
      "Average test loss: 0.002149101866823104\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015243313364684583\n",
      "Average test loss: 0.0022310827800797093\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01514275343136655\n",
      "Average test loss: 0.002079225564168559\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015173853039741517\n",
      "Average test loss: 0.002114912036392424\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015120440540214379\n",
      "Average test loss: 0.0021252968782145116\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015139633365803295\n",
      "Average test loss: 0.002156977465893659\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015163598779175017\n",
      "Average test loss: 0.0021910715692987043\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015115519927607643\n",
      "Average test loss: 0.002289883737762769\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015008666605585152\n",
      "Average test loss: 0.00302884049858484\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015033573743369845\n",
      "Average test loss: 0.002539620211554898\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01508337978604767\n",
      "Average test loss: 0.002171003481787112\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01518220070666737\n",
      "Average test loss: 0.0022327720642917686\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015046544803513421\n",
      "Average test loss: 0.002225773947727349\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014976694964700275\n",
      "Average test loss: 0.0020735896680918004\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015034457893835174\n",
      "Average test loss: 0.0021129116256617835\n",
      "Epoch 128/300\n",
      "Average training loss: 0.015023178752925661\n",
      "Average test loss: 0.0021640547110388674\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014975189430018266\n",
      "Average test loss: 0.002126555426667134\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014897833362221717\n",
      "Average test loss: 0.0020568868385420903\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015030046326418718\n",
      "Average test loss: 0.0020524690482351515\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01493977197839154\n",
      "Average test loss: 0.002042557344564961\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014977975911564297\n",
      "Average test loss: 0.06375393388999832\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01498180327233341\n",
      "Average test loss: 0.0021418653779352705\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014888490206665463\n",
      "Average test loss: 0.0020112245774103537\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014894329643911786\n",
      "Average test loss: 0.0020582099467929865\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014980402828090721\n",
      "Average test loss: 0.0022547423938910167\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014944591504832109\n",
      "Average test loss: 0.0020632684419138562\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014838589711321725\n",
      "Average test loss: 0.002116232480439875\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014887538717024857\n",
      "Average test loss: 0.00209134880039427\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014873854310148291\n",
      "Average test loss: 0.0021069526674432886\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014797745892571079\n",
      "Average test loss: 0.0021667378971146215\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014951448021663559\n",
      "Average test loss: 0.002085639129496283\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014812941902213626\n",
      "Average test loss: 0.002158336812733776\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014861650746729639\n",
      "Average test loss: 0.002181760563618607\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014786032961888445\n",
      "Average test loss: 0.0024389253051744567\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014754196925295724\n",
      "Average test loss: 0.0021005993417153755\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014812565873894427\n",
      "Average test loss: 0.002075014914282494\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01474135347869661\n",
      "Average test loss: 0.002129314196192556\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01472668413983451\n",
      "Average test loss: 0.0020882338748003048\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014758419206572903\n",
      "Average test loss: 0.00209008247912344\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014740164602796237\n",
      "Average test loss: 0.0021899963799450134\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014859760727319453\n",
      "Average test loss: 0.0022021796481890813\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014719598951439063\n",
      "Average test loss: 0.002090433677451478\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014782513408197298\n",
      "Average test loss: 0.0021231465968820783\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014727874032325216\n",
      "Average test loss: 0.002050220121112135\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014659350541730723\n",
      "Average test loss: 0.002069649408467942\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014691891940931479\n",
      "Average test loss: 0.0019875258795089192\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014659702161947886\n",
      "Average test loss: 0.002023162617244654\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014678772620856761\n",
      "Average test loss: 0.0020361065349231164\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014640208277437422\n",
      "Average test loss: 0.002027425954532292\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014646842740476132\n",
      "Average test loss: 0.0020518198379625876\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01462153937584824\n",
      "Average test loss: 0.001997436918421752\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014633040809796917\n",
      "Average test loss: 0.0026836450793262987\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014651874158117506\n",
      "Average test loss: 0.0019979951021571953\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014587350183063083\n",
      "Average test loss: 0.002474984858185053\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014640210255980491\n",
      "Average test loss: 0.0020588845612688196\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01455239378164212\n",
      "Average test loss: 0.0020843199042396413\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01471054265399774\n",
      "Average test loss: 0.00213394696596596\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01455083225833045\n",
      "Average test loss: 0.002260476092497508\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014538999520242214\n",
      "Average test loss: 0.0020295045280622113\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014545130915111965\n",
      "Average test loss: 0.0020410125572234393\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014554553274479177\n",
      "Average test loss: 0.0019753732447408967\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014549605175024934\n",
      "Average test loss: 0.0020267966143372985\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014514240516556634\n",
      "Average test loss: 0.0022800207568539513\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014510030400421885\n",
      "Average test loss: 0.0021968709541898635\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014534708455204964\n",
      "Average test loss: 0.0021250519250623056\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014524037835498651\n",
      "Average test loss: 0.002246997774475151\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014568207283814747\n",
      "Average test loss: 0.0025351997264143494\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014555676598515775\n",
      "Average test loss: 0.0023158026832259364\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014475364474786652\n",
      "Average test loss: 0.0021259258799254895\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014477575916383002\n",
      "Average test loss: 0.0022400792236957285\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014487881208459535\n",
      "Average test loss: 0.0023238929480107295\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014428194022840924\n",
      "Average test loss: 0.00207274493161175\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014449006811612182\n",
      "Average test loss: 0.002046593568805191\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014453838140600257\n",
      "Average test loss: 0.0021127551938924525\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0145034121233556\n",
      "Average test loss: 0.0019535369959970315\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01440458185142941\n",
      "Average test loss: 0.0020836600094205804\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014404876206484106\n",
      "Average test loss: 0.002182826779368851\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014465767640206548\n",
      "Average test loss: 0.002061427628000577\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014458601586520672\n",
      "Average test loss: 0.0020808817386213274\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014440014589163993\n",
      "Average test loss: 0.0019890997401542133\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014348148686190446\n",
      "Average test loss: 0.0020255304564618403\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014409838513367705\n",
      "Average test loss: 0.0019405739948981338\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014378798074192472\n",
      "Average test loss: 0.001999448210828834\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014317504910959138\n",
      "Average test loss: 0.002730281174596813\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014436951485772927\n",
      "Average test loss: 0.00207214177130825\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014365226629707548\n",
      "Average test loss: 0.0024392118178721933\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01439491897324721\n",
      "Average test loss: 0.0021493708926977385\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014384343133204512\n",
      "Average test loss: 0.0020239572756820257\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014325845460096995\n",
      "Average test loss: 0.0022799658730833067\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014330426244272125\n",
      "Average test loss: 0.00215371538967722\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014330101248290804\n",
      "Average test loss: 0.002596159579232335\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014353354436655839\n",
      "Average test loss: 0.001953233254245586\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01432156568351719\n",
      "Average test loss: 0.0020876348811305232\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014280687672396502\n",
      "Average test loss: 0.0022630219970726307\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014307331993348068\n",
      "Average test loss: 0.001985069859152039\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014303522068593238\n",
      "Average test loss: 0.0020898533205812174\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014423716925912433\n",
      "Average test loss: 0.001967466850144168\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01425045887960328\n",
      "Average test loss: 0.014511122153037124\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01443845047801733\n",
      "Average test loss: 0.002241853412033783\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014244635848535431\n",
      "Average test loss: 0.0019925011824816464\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01427192087057564\n",
      "Average test loss: 0.001965215446634425\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014298520439200931\n",
      "Average test loss: 0.0020561104751088554\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014256403639084763\n",
      "Average test loss: 0.002157937299874094\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014235387321147654\n",
      "Average test loss: 0.0020384738608780835\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014271557981769245\n",
      "Average test loss: 0.001997236803174019\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014274769720104006\n",
      "Average test loss: 0.0020607986067318253\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014210458758804534\n",
      "Average test loss: 0.0020487156996710433\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014261596942941348\n",
      "Average test loss: 0.0019284685825308164\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014226651548511451\n",
      "Average test loss: 0.002376845982339647\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014234970018267632\n",
      "Average test loss: 0.002218917377086149\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014254678845405579\n",
      "Average test loss: 0.0020366610385891466\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014285495622290506\n",
      "Average test loss: 0.0019948741033052404\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014219424443112479\n",
      "Average test loss: 0.0019699210851556724\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014335228501922554\n",
      "Average test loss: 0.00199860819015238\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014181747113664945\n",
      "Average test loss: 0.0020986927058547736\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014212670721941524\n",
      "Average test loss: 0.0020207362825878793\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014194941636588838\n",
      "Average test loss: 0.0020284285368397832\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014192947775953346\n",
      "Average test loss: 0.0020511425356898043\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014178259033295844\n",
      "Average test loss: 0.0019948343631501\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014214744716882706\n",
      "Average test loss: 0.002107200681128436\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01420225326054626\n",
      "Average test loss: 0.002045447495041622\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014210267797940308\n",
      "Average test loss: 0.002163218235803975\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014196902578903568\n",
      "Average test loss: 0.0020074131458790766\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014129729020098845\n",
      "Average test loss: 0.0020044072268323764\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01419102741777897\n",
      "Average test loss: 0.002143475070388781\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014133961069915029\n",
      "Average test loss: 0.0020489419411040015\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01418866577413347\n",
      "Average test loss: 0.002120992658245895\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014131471614042919\n",
      "Average test loss: 0.0019081300326312582\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014126550405389732\n",
      "Average test loss: 0.0021431047378314867\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014135975553757615\n",
      "Average test loss: 0.001981750151142478\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014120036413272223\n",
      "Average test loss: 0.00201683011237118\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014133884481257863\n",
      "Average test loss: 0.002087603700450725\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014102514453232289\n",
      "Average test loss: 0.001962859970724417\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014158718537953165\n",
      "Average test loss: 0.002030915012790097\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014085845919118987\n",
      "Average test loss: 0.002055790995661583\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014052719182438321\n",
      "Average test loss: 0.002198488323017955\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014078721682230632\n",
      "Average test loss: 0.0019708842668268414\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014071294008029832\n",
      "Average test loss: 0.0019508479032665491\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014071485210624006\n",
      "Average test loss: 0.0019268968469566769\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014124456154803434\n",
      "Average test loss: 0.0020261850938614874\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014055070153541035\n",
      "Average test loss: 0.002367240876166357\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01404145057996114\n",
      "Average test loss: 0.001975980019610789\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014033414964046743\n",
      "Average test loss: 0.0020242656163043447\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014098260528511472\n",
      "Average test loss: 0.002071255164013969\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014052975905438265\n",
      "Average test loss: 0.001960683457605127\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014002189027766386\n",
      "Average test loss: 0.0019328982767959435\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014023999072611332\n",
      "Average test loss: 0.002057629748247564\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014124605384137895\n",
      "Average test loss: 0.0020616205605781743\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014090955247481664\n",
      "Average test loss: 0.002092715803119871\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01403326642099354\n",
      "Average test loss: 0.00200003584412237\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014043560172948572\n",
      "Average test loss: 0.001993185301207834\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0140127318981621\n",
      "Average test loss: 0.002344335243312849\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014118430995278888\n",
      "Average test loss: 0.002010124263043205\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013965994972321722\n",
      "Average test loss: 0.0019653457533568143\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013999751022292508\n",
      "Average test loss: 0.001974194665335947\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014010561379293601\n",
      "Average test loss: 0.0021827578970955476\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014019339734481441\n",
      "Average test loss: 0.0019728027590447\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014009302438961134\n",
      "Average test loss: 0.001958278561838799\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013951103754341602\n",
      "Average test loss: 0.002066879961018761\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013943942697511778\n",
      "Average test loss: 0.0020740413655423455\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014023711947931184\n",
      "Average test loss: 0.0020628346706637077\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014012743342253896\n",
      "Average test loss: 0.0019746211170115404\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01392613561782572\n",
      "Average test loss: 0.0019736648806267312\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013952791251242162\n",
      "Average test loss: 0.0020754501154232356\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013982926680809922\n",
      "Average test loss: 0.0019353558539102476\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014036484038664235\n",
      "Average test loss: 0.0021655001308148107\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013919755728708374\n",
      "Average test loss: 0.0020316695853446923\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013961714643571111\n",
      "Average test loss: 0.0019970767641853955\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013935226812958718\n",
      "Average test loss: 0.0019506009693981872\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013931234358913368\n",
      "Average test loss: 0.001970753563567996\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014010177503029506\n",
      "Average test loss: 0.0019431773703545332\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01397411164889733\n",
      "Average test loss: 0.001945862078314854\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013904525373544958\n",
      "Average test loss: 0.0021016854064332116\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01394076322681374\n",
      "Average test loss: 0.0020972894122824074\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01396034407036172\n",
      "Average test loss: 0.010085001232723396\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014284160088333818\n",
      "Average test loss: 0.00210774114024308\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013884170178737906\n",
      "Average test loss: 0.002092055562366214\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01390681576397684\n",
      "Average test loss: 0.002035296944073505\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013885394405159686\n",
      "Average test loss: 0.0021429507217059535\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013942298965321646\n",
      "Average test loss: 0.0031855765144444175\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01392331504656209\n",
      "Average test loss: 0.0022575084041390153\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013929532678590881\n",
      "Average test loss: 0.002086980509468251\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013911640085279942\n",
      "Average test loss: 0.0019647438428364696\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013893818659914865\n",
      "Average test loss: 0.001956333411650525\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014004458814859391\n",
      "Average test loss: 0.0020157896388942995\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013892677198681567\n",
      "Average test loss: 0.0020229219877057605\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013900681189364857\n",
      "Average test loss: 0.002039089761363963\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013854729654888312\n",
      "Average test loss: 0.001976345649713443\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'No_Memory_No_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.13\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.56\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.80\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.59\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.55\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.33\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.6091607366138034\n",
      "Average test loss: 0.0168547436495622\n",
      "Epoch 2/300\n",
      "Average training loss: 1.317989017062717\n",
      "Average test loss: 0.01378881615979804\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9193050871425205\n",
      "Average test loss: 0.9079107015745508\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7178284385469225\n",
      "Average test loss: 0.24709968137078814\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5887193291452196\n",
      "Average test loss: 11.465086097260317\n",
      "Epoch 6/300\n",
      "Average training loss: 0.5030945831669702\n",
      "Average test loss: 6.300297940060497\n",
      "Epoch 7/300\n",
      "Average training loss: 0.4327879217200809\n",
      "Average test loss: 0.010041803370747301\n",
      "Epoch 8/300\n",
      "Average training loss: 0.37654193327162\n",
      "Average test loss: 0.012579170093768173\n",
      "Epoch 9/300\n",
      "Average training loss: 0.34125170273251004\n",
      "Average test loss: 0.019961217428247136\n",
      "Epoch 10/300\n",
      "Average training loss: 0.3063331636587779\n",
      "Average test loss: 0.01358578681697448\n",
      "Epoch 11/300\n",
      "Average training loss: 0.27899042426215276\n",
      "Average test loss: 0.010995103111697567\n",
      "Epoch 12/300\n",
      "Average training loss: 0.2582534053855472\n",
      "Average test loss: 0.012299827687442302\n",
      "Epoch 13/300\n",
      "Average training loss: 0.2407481652100881\n",
      "Average test loss: 0.009183120833088955\n",
      "Epoch 14/300\n",
      "Average training loss: 0.22494651171896193\n",
      "Average test loss: 0.009363216977152559\n",
      "Epoch 15/300\n",
      "Average training loss: 0.21350689702563816\n",
      "Average test loss: 0.011436635235945384\n",
      "Epoch 16/300\n",
      "Average training loss: 0.19861593629254234\n",
      "Average test loss: 0.008971625087161858\n",
      "Epoch 17/300\n",
      "Average training loss: 0.19312903912862142\n",
      "Average test loss: 0.0484341928727097\n",
      "Epoch 18/300\n",
      "Average training loss: 0.18597594372431436\n",
      "Average test loss: 0.00964360422309902\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18026484011279212\n",
      "Average test loss: 0.016497378629942733\n",
      "Epoch 20/300\n",
      "Average training loss: 0.17535963540607027\n",
      "Average test loss: 0.009187701975719796\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1692936563756731\n",
      "Average test loss: 0.0076549061967266935\n",
      "Epoch 22/300\n",
      "Average training loss: 0.16235432921515572\n",
      "Average test loss: 0.007986948625908957\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16252798428800372\n",
      "Average test loss: 0.008687165716042122\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15530498684777153\n",
      "Average test loss: 0.009353565741744306\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1519520690308677\n",
      "Average test loss: 0.013457517933514383\n",
      "Epoch 26/300\n",
      "Average training loss: 0.14835819171534645\n",
      "Average test loss: 0.007716001417901781\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14396666928132376\n",
      "Average test loss: 0.007311825195948283\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14305306878354815\n",
      "Average test loss: 0.007422506745490763\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13888912554581961\n",
      "Average test loss: 0.007140683611234029\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13855617242389254\n",
      "Average test loss: 0.007171752021544509\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13378724449210697\n",
      "Average test loss: 0.01248983930465248\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13267958568202123\n",
      "Average test loss: 0.465903909444809\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1319426026609209\n",
      "Average test loss: 0.007022749932275878\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12774777850839827\n",
      "Average test loss: 0.007566262175639471\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12741719277037514\n",
      "Average test loss: 0.007061605733301905\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12430332329538134\n",
      "Average test loss: 0.007210724370347129\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12447013837761349\n",
      "Average test loss: 0.008272602121035259\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12137325571642982\n",
      "Average test loss: 0.007323830872774124\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12038549073537191\n",
      "Average test loss: 0.007468799409766992\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11892552574475607\n",
      "Average test loss: 0.008421122000863155\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11647234970993466\n",
      "Average test loss: 0.006344551444881492\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11588484434286753\n",
      "Average test loss: 0.006540042633811633\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11568130769994524\n",
      "Average test loss: 0.006866125506659349\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11382652314503988\n",
      "Average test loss: 0.006915562463303407\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1123926959435145\n",
      "Average test loss: 0.0062939608014292185\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11193541866540908\n",
      "Average test loss: 0.006343261565806137\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11035676787296932\n",
      "Average test loss: 0.006571237959381607\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10998383798864153\n",
      "Average test loss: 0.0075513549678855475\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10945653606123394\n",
      "Average test loss: 0.006273882011986441\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10848244233263864\n",
      "Average test loss: 0.0067328352907465566\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10672746787468593\n",
      "Average test loss: 0.006277184587799841\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10614189736710654\n",
      "Average test loss: 0.0061659698424239955\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10555980436007181\n",
      "Average test loss: 0.006383335221144888\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10502509058184094\n",
      "Average test loss: 0.007674393436147107\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10374622599946128\n",
      "Average test loss: 0.006024768485377232\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10316940478483835\n",
      "Average test loss: 0.006731185204452939\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10339689463376998\n",
      "Average test loss: 0.007065330556697316\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10278018879228168\n",
      "Average test loss: 0.007270630114608341\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10065984740522173\n",
      "Average test loss: 0.007186870305902428\n",
      "Epoch 60/300\n",
      "Average training loss: 0.1012424097524749\n",
      "Average test loss: 0.006636920960413085\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1003352481590377\n",
      "Average test loss: 0.005989339626497693\n",
      "Epoch 62/300\n",
      "Average training loss: 0.10026420685317781\n",
      "Average test loss: 0.008845309876733357\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09930007871654299\n",
      "Average test loss: 0.0063979789821638\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09863693762487835\n",
      "Average test loss: 0.006312071235643492\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0983906192779541\n",
      "Average test loss: 0.0061320672382911045\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09742338616318173\n",
      "Average test loss: 0.006038227129520642\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09828109021981557\n",
      "Average test loss: 0.00661463305602471\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09723482936620713\n",
      "Average test loss: 0.0059620588670174285\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09755522970358531\n",
      "Average test loss: 0.007122971255746153\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09641250311666065\n",
      "Average test loss: 0.0059629934949593415\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09555008931292427\n",
      "Average test loss: 0.006491017188049025\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09583657162719303\n",
      "Average test loss: 0.006467365097668436\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09502600904305776\n",
      "Average test loss: 0.007148374696986543\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09522465187311173\n",
      "Average test loss: 0.006539445960273346\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0947505069706175\n",
      "Average test loss: 0.006831507358286116\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0937467852698432\n",
      "Average test loss: 0.007608254940973388\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09386714477009243\n",
      "Average test loss: 0.006135918415255017\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0930007140106625\n",
      "Average test loss: 0.005788762294997771\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09340316139327155\n",
      "Average test loss: 0.0062178421475821075\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09269467292229334\n",
      "Average test loss: 0.005958203477991952\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09217927877108256\n",
      "Average test loss: 0.006168034582088391\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09256296793619792\n",
      "Average test loss: 0.006242282982501719\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09197485697269439\n",
      "Average test loss: 0.006870349533855915\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09248991423845292\n",
      "Average test loss: 0.008623452969309357\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09136741260025236\n",
      "Average test loss: 0.006238003367765082\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09107357195681996\n",
      "Average test loss: 0.005999402775118749\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0911491978002919\n",
      "Average test loss: 0.006372435884343253\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0912234001490805\n",
      "Average test loss: 0.0063757003189788925\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09047542577981948\n",
      "Average test loss: 0.006148443404171202\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09017022248771456\n",
      "Average test loss: 0.006048025657319361\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08998489101727804\n",
      "Average test loss: 0.006094790707445807\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08951516189177831\n",
      "Average test loss: 0.006401772720946206\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08945826182762782\n",
      "Average test loss: 0.006542281918641594\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09058132071627512\n",
      "Average test loss: 0.006327854964882135\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08887661888864305\n",
      "Average test loss: 0.006129960022038884\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08902738228771422\n",
      "Average test loss: 0.006017644051048491\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08882064835230509\n",
      "Average test loss: 0.0062231660021675956\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08904325993193521\n",
      "Average test loss: 0.006074609556131893\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08834234535694123\n",
      "Average test loss: 0.006414729996273915\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0881784579091602\n",
      "Average test loss: 0.005884572639647458\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08767641148964564\n",
      "Average test loss: 0.007147805976784892\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08808337931500541\n",
      "Average test loss: 0.005866540285448234\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0876817267537117\n",
      "Average test loss: 0.006220785048686795\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08832332332266701\n",
      "Average test loss: 0.0060078403457171385\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0875735536813736\n",
      "Average test loss: 0.006296398407469193\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08725346827507019\n",
      "Average test loss: 0.005926311581085125\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08702815921439065\n",
      "Average test loss: 0.006227235385527214\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08716811006599003\n",
      "Average test loss: 0.00654538334823317\n",
      "Epoch 109/300\n",
      "Average training loss: 0.08719844442605973\n",
      "Average test loss: 0.006044728123065498\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08666200612650977\n",
      "Average test loss: 0.006205096567256583\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08616728275020917\n",
      "Average test loss: 0.006138281390898758\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08626119219594532\n",
      "Average test loss: 0.006469113991906246\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08616878772444195\n",
      "Average test loss: 0.006443551497326957\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08601459706491894\n",
      "Average test loss: 0.006027003188100126\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08597173515293333\n",
      "Average test loss: 0.006013006890813509\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08568703498442967\n",
      "Average test loss: 0.0058087839360038435\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08530928838915296\n",
      "Average test loss: 0.006112153343856334\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08534207452668084\n",
      "Average test loss: 0.005812207991878191\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08518579702244865\n",
      "Average test loss: 0.01292179507513841\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08498029866483477\n",
      "Average test loss: 0.006635282189067867\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08553327433268229\n",
      "Average test loss: 0.006278587371524837\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08525141569640901\n",
      "Average test loss: 0.006466061214605967\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08480102739731471\n",
      "Average test loss: 0.00595722969993949\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08462780505418778\n",
      "Average test loss: 0.006095482286893659\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08518316019905937\n",
      "Average test loss: 0.0062005659366647405\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08463044206632508\n",
      "Average test loss: 0.005773064505308866\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08436939347452588\n",
      "Average test loss: 0.006290781166404485\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08433496965302362\n",
      "Average test loss: 0.005920825818346606\n",
      "Epoch 129/300\n",
      "Average training loss: 0.08403955782784356\n",
      "Average test loss: 0.00608001397881243\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08386605642901526\n",
      "Average test loss: 0.007403068107863267\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08444407063060337\n",
      "Average test loss: 0.005854283158977827\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08326140975289875\n",
      "Average test loss: 0.006210164712121089\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08391666741503609\n",
      "Average test loss: 0.005957171623284618\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08360739729801814\n",
      "Average test loss: 0.006191391108350621\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08314200268851386\n",
      "Average test loss: 0.005929217814571328\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08320522901084688\n",
      "Average test loss: 0.006463218301120731\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08331102130148146\n",
      "Average test loss: 0.006080762802726692\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08295970142549938\n",
      "Average test loss: 0.00611756950409876\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08352163985702726\n",
      "Average test loss: 0.005911049980670214\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08265939416488012\n",
      "Average test loss: 0.005913665523131688\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08264678941832648\n",
      "Average test loss: 0.0074254985468255146\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08275686054097281\n",
      "Average test loss: 0.006046036584509744\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0823429187602467\n",
      "Average test loss: 0.006091330635464853\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08267874050802655\n",
      "Average test loss: 0.005941010601818561\n",
      "Epoch 145/300\n",
      "Average training loss: 0.08244943283663855\n",
      "Average test loss: 0.0063496133031116595\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08260703438520431\n",
      "Average test loss: 0.006182337133420838\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08250288516945309\n",
      "Average test loss: 0.006120220652884908\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08278899767663744\n",
      "Average test loss: 0.005768938135355711\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08170613809757762\n",
      "Average test loss: 0.006156907171838814\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08167281997866101\n",
      "Average test loss: 0.006265617633859317\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08221109275023142\n",
      "Average test loss: 0.005923372618854046\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08176180593172709\n",
      "Average test loss: 0.005994986555642552\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08164592880010604\n",
      "Average test loss: 0.005887820165190432\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08178844942649205\n",
      "Average test loss: 0.0058275611930423315\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08150117175446617\n",
      "Average test loss: 0.006297709145893653\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08143681342071957\n",
      "Average test loss: 0.005840162468453249\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08148585672510995\n",
      "Average test loss: 0.005952049406452311\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08105617050992119\n",
      "Average test loss: 0.006182524612380399\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08147757155034277\n",
      "Average test loss: 0.006870753987795777\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08108759198586146\n",
      "Average test loss: 0.005958200969629818\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08118876625431909\n",
      "Average test loss: 0.005857060468859142\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08124832195705838\n",
      "Average test loss: 0.005974014904763963\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08074386171499888\n",
      "Average test loss: 0.009768503039247459\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08060468783643511\n",
      "Average test loss: 0.006149726562201977\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0805710584587521\n",
      "Average test loss: 0.006439243951605426\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08070432131489118\n",
      "Average test loss: 0.0063278540045850805\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08064509192440245\n",
      "Average test loss: 0.0060733355898410085\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08018680418199962\n",
      "Average test loss: 0.005883738348053561\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08072891272438897\n",
      "Average test loss: 0.007214090541005134\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08036278338564767\n",
      "Average test loss: 0.0071450575432843635\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08018056687381532\n",
      "Average test loss: 0.005965512798064285\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08015799309810002\n",
      "Average test loss: 0.005929906152188778\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07999064479271571\n",
      "Average test loss: 0.00671530865256985\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0801453563173612\n",
      "Average test loss: 0.018552750246392358\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08027811574935913\n",
      "Average test loss: 0.0057886229873531396\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07967066052887174\n",
      "Average test loss: 0.007367409463557932\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07963779531584846\n",
      "Average test loss: 0.005794194631692436\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07965360318289863\n",
      "Average test loss: 0.0062005568105313515\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07991156196594239\n",
      "Average test loss: 0.006056553507016765\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07957902862297164\n",
      "Average test loss: 0.006136232649286588\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07953484070301056\n",
      "Average test loss: 0.005843702827476793\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07961825991339154\n",
      "Average test loss: 0.00687457739851541\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07913224379221598\n",
      "Average test loss: 0.005756833292957809\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07940522515111499\n",
      "Average test loss: 0.006918401569128037\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07938951018121508\n",
      "Average test loss: 0.005806808273825381\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0794525971478886\n",
      "Average test loss: 0.00583135274425149\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07961920540862613\n",
      "Average test loss: 0.005854730729427602\n",
      "Epoch 188/300\n",
      "Average training loss: 0.079156548374229\n",
      "Average test loss: 0.00600071874840392\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07888161547316445\n",
      "Average test loss: 0.005811926879816585\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07892572510904736\n",
      "Average test loss: 0.005873718594097429\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0788819131784969\n",
      "Average test loss: 0.006365122439960639\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07885719333754646\n",
      "Average test loss: 0.006245242445833153\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07839008996221754\n",
      "Average test loss: 0.00598260202507178\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0785694402522511\n",
      "Average test loss: 0.005819682384944624\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07898751636346181\n",
      "Average test loss: 0.006003463660677274\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07847155263688829\n",
      "Average test loss: 0.006089400196654929\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0784807539847162\n",
      "Average test loss: 0.0058504847942127125\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07864744705624051\n",
      "Average test loss: 0.0061344562760657735\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07842951501078076\n",
      "Average test loss: 0.0058130029373698764\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07841931670573023\n",
      "Average test loss: 0.005786486407948865\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07840160016881095\n",
      "Average test loss: 0.006001627788775497\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07811574223306444\n",
      "Average test loss: 0.006256613415976366\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07806360256671906\n",
      "Average test loss: 0.0058840342507594165\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07797458579805162\n",
      "Average test loss: 0.005962752093043592\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07786614685588412\n",
      "Average test loss: 0.006353240128192637\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07800093333588706\n",
      "Average test loss: 0.005797196395281289\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07769606519076559\n",
      "Average test loss: 0.005971855629442467\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07800620822774039\n",
      "Average test loss: 0.0060088126804265715\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07765352616707484\n",
      "Average test loss: 0.005799487908060352\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07779150756862428\n",
      "Average test loss: 0.005780714776366949\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07769865392976337\n",
      "Average test loss: 0.007775910964442624\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07809439670377308\n",
      "Average test loss: 0.00681642472371459\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07745268912116686\n",
      "Average test loss: 0.00593735123011801\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07773963620927599\n",
      "Average test loss: 0.005982169192077385\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07739817869663239\n",
      "Average test loss: 0.005843758591761191\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07742753526237275\n",
      "Average test loss: 0.006516530430979199\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07720643821689818\n",
      "Average test loss: 0.005956113388554917\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07711045584413741\n",
      "Average test loss: 0.005821465503424406\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07723759650190672\n",
      "Average test loss: 0.005996000214583344\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07742822542124324\n",
      "Average test loss: 0.006151979991959201\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07765568977594375\n",
      "Average test loss: 0.005994841465519534\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07750130996439192\n",
      "Average test loss: 0.005901123390843471\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07714437334405051\n",
      "Average test loss: 0.006035615758349498\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07695304715633393\n",
      "Average test loss: 0.006948924474832085\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0771333455906974\n",
      "Average test loss: 0.006509259434209929\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07668350235621134\n",
      "Average test loss: 0.007301886519624127\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07668305291069878\n",
      "Average test loss: 0.007196625297268232\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07676180024279489\n",
      "Average test loss: 0.005988170701803432\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07666664780510797\n",
      "Average test loss: 0.006058886899716324\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07635305864943398\n",
      "Average test loss: 0.00608810367849138\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07645343614949121\n",
      "Average test loss: 0.006122867216252618\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07660998138785362\n",
      "Average test loss: 0.005947629563510418\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07665621227357122\n",
      "Average test loss: 0.006954599242657423\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07665735454029507\n",
      "Average test loss: 0.00654898674454954\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07657979363203049\n",
      "Average test loss: 0.005858693826115794\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07651985520124435\n",
      "Average test loss: 0.005797309315659934\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07636957924895817\n",
      "Average test loss: 0.005828636856542693\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0762370827264256\n",
      "Average test loss: 0.005891526196565893\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07609992418024275\n",
      "Average test loss: 0.005701237837473551\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07633868325418897\n",
      "Average test loss: 0.005768336765882042\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07622245480616888\n",
      "Average test loss: 0.006116904528190692\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0760191369785203\n",
      "Average test loss: 0.006431401675360071\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07620801072650485\n",
      "Average test loss: 0.006378849264648226\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07593360456493166\n",
      "Average test loss: 0.005919649712327454\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07588208311796188\n",
      "Average test loss: 0.005819287641594807\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07583293745915096\n",
      "Average test loss: 0.006109861375557052\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07628486087256008\n",
      "Average test loss: 0.006320929815371831\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07598868857489692\n",
      "Average test loss: 0.006161885548382998\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07546140866147147\n",
      "Average test loss: 0.006172559789899322\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07603692440854179\n",
      "Average test loss: 0.00573967422131035\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07611536440584395\n",
      "Average test loss: 0.005940345688412587\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07564764040046268\n",
      "Average test loss: 0.006180970263977846\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07565864535835054\n",
      "Average test loss: 0.009774281770818764\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07543972581624984\n",
      "Average test loss: 0.006152023786058029\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07531414289606943\n",
      "Average test loss: 0.0063809515929056536\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07525230620304743\n",
      "Average test loss: 0.005878463246342208\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07520653761095471\n",
      "Average test loss: 0.005775146498034398\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07567317244741652\n",
      "Average test loss: 0.005775614177187284\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07535924937327702\n",
      "Average test loss: 0.00681651960975594\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07532862340741688\n",
      "Average test loss: 0.006312128663063049\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07514635661575529\n",
      "Average test loss: 0.005843131286816464\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07537328484985563\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'No_Memory_No_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'No_Memory_No_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='PGD', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = PGD(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = PGD(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = PGD(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = PGD(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
