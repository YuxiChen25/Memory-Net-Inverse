{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Nesterov_Network.Nesterov import Nesterov\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14716781975163354\n",
      "Average test loss: 0.011141649240420924\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06556935873296525\n",
      "Average test loss: 0.01991192707584964\n",
      "Epoch 3/300\n",
      "Average training loss: 0.060769728038046096\n",
      "Average test loss: 0.009997374355792999\n",
      "Epoch 4/300\n",
      "Average training loss: 0.058652443120876946\n",
      "Average test loss: 0.009110303069154422\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05615031558275223\n",
      "Average test loss: 0.008999759514298704\n",
      "Epoch 6/300\n",
      "Average training loss: 0.053677908284796606\n",
      "Average test loss: 0.00925846263517936\n",
      "Epoch 7/300\n",
      "Average training loss: 0.053087953474786544\n",
      "Average test loss: 0.008858372744586732\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05110702526569366\n",
      "Average test loss: 0.1519264222515954\n",
      "Epoch 9/300\n",
      "Average training loss: 0.050717637247509426\n",
      "Average test loss: 0.012130486684540908\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04886309303177728\n",
      "Average test loss: 0.01752662971450223\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04802959477239185\n",
      "Average test loss: 0.008762563793195618\n",
      "Epoch 12/300\n",
      "Average training loss: 0.047329150120417274\n",
      "Average test loss: 0.009263635108040439\n",
      "Epoch 13/300\n",
      "Average training loss: 0.046482994662390816\n",
      "Average test loss: 0.010626030485663148\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04510834113094542\n",
      "Average test loss: 0.007568938809964392\n",
      "Epoch 15/300\n",
      "Average training loss: 0.044923202756378386\n",
      "Average test loss: 0.009251295415891541\n",
      "Epoch 16/300\n",
      "Average training loss: 0.044076289296150206\n",
      "Average test loss: 0.008408761905299293\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04341842265592681\n",
      "Average test loss: 0.008104843277070258\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04362916178835763\n",
      "Average test loss: 0.007567028762565719\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04367370641231537\n",
      "Average test loss: 0.007216317631304264\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0426389710770713\n",
      "Average test loss: 0.008176572978496551\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04196872122751342\n",
      "Average test loss: 0.007908848317133055\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04175353717803955\n",
      "Average test loss: 0.011430237387617429\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04132525604632166\n",
      "Average test loss: 0.008798981185588571\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04094350357188119\n",
      "Average test loss: 0.007836356338527467\n",
      "Epoch 25/300\n",
      "Average training loss: 0.040408900009261235\n",
      "Average test loss: 0.008809942969017558\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04025359861387147\n",
      "Average test loss: 0.007895861975434754\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04050614958339267\n",
      "Average test loss: 0.00917740171071556\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039683150632513896\n",
      "Average test loss: 0.013146360860102707\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039548156927029295\n",
      "Average test loss: 0.007258508814705742\n",
      "Epoch 30/300\n",
      "Average training loss: 0.039261607292625636\n",
      "Average test loss: 0.0072348122828536565\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03902566747201813\n",
      "Average test loss: 0.0075703300990992125\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03904797666602665\n",
      "Average test loss: 0.008665004400743378\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03861583522293303\n",
      "Average test loss: 0.0075679507870227095\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03848415967490938\n",
      "Average test loss: 0.007108425070842107\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03838073448671235\n",
      "Average test loss: 0.007109530945205026\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03813512366016706\n",
      "Average test loss: 0.007261304123120175\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0378979793952571\n",
      "Average test loss: 0.006811063228382004\n",
      "Epoch 38/300\n",
      "Average training loss: 0.037498090585072835\n",
      "Average test loss: 0.007463405559460322\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03756508629189597\n",
      "Average test loss: 0.00675691540663441\n",
      "Epoch 40/300\n",
      "Average training loss: 0.037266637586885025\n",
      "Average test loss: 0.007032944865524769\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03731575923495822\n",
      "Average test loss: 0.007062893134438329\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03723110865222083\n",
      "Average test loss: 0.006989047564980057\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03687527633375592\n",
      "Average test loss: 0.006966403148240513\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03702925667828984\n",
      "Average test loss: 0.0074527616351842884\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03685197905202706\n",
      "Average test loss: 0.010352037059764067\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03665038572086228\n",
      "Average test loss: 0.0074195522322422925\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03667146353920301\n",
      "Average test loss: 0.006665640903843774\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0364793301820755\n",
      "Average test loss: 0.006660792035361131\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03635384761956003\n",
      "Average test loss: 0.007456433803670936\n",
      "Epoch 50/300\n",
      "Average training loss: 0.036291827831003404\n",
      "Average test loss: 0.007444104981919129\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03615405318140984\n",
      "Average test loss: 0.006837902116692728\n",
      "Epoch 52/300\n",
      "Average training loss: 0.036109580092959934\n",
      "Average test loss: 0.006994571516083347\n",
      "Epoch 53/300\n",
      "Average training loss: 0.036186677722467316\n",
      "Average test loss: 0.006961726875768767\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03558942651086383\n",
      "Average test loss: 0.0066861535381111835\n",
      "Epoch 55/300\n",
      "Average training loss: 0.035627579513523314\n",
      "Average test loss: 0.007964882847335604\n",
      "Epoch 56/300\n",
      "Average training loss: 0.036146670526928375\n",
      "Average test loss: 0.010190080708099736\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03550746456782023\n",
      "Average test loss: 0.006434167729069789\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03563097607427173\n",
      "Average test loss: 0.0071326031063993775\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03531904073556264\n",
      "Average test loss: 0.006367264029052523\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03518611898687151\n",
      "Average test loss: 0.006860033431400856\n",
      "Epoch 61/300\n",
      "Average training loss: 0.035328459183375044\n",
      "Average test loss: 0.0067677935655746195\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03500190339154667\n",
      "Average test loss: 0.006995169531553984\n",
      "Epoch 63/300\n",
      "Average training loss: 0.035023239543040596\n",
      "Average test loss: 0.006657474762035741\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03495343591603968\n",
      "Average test loss: 0.006702231923739115\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03486809475223224\n",
      "Average test loss: 0.008257986396551132\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03501687455508444\n",
      "Average test loss: 0.006388560523589452\n",
      "Epoch 67/300\n",
      "Average training loss: 0.034852437559101314\n",
      "Average test loss: 0.00686973359146052\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0346675559696224\n",
      "Average test loss: 0.0071487774203221\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03474764615959591\n",
      "Average test loss: 0.006408280136270656\n",
      "Epoch 70/300\n",
      "Average training loss: 0.034805496676100625\n",
      "Average test loss: 0.006473004825827148\n",
      "Epoch 71/300\n",
      "Average training loss: 0.034505793912543194\n",
      "Average test loss: 0.008119106211596065\n",
      "Epoch 72/300\n",
      "Average training loss: 0.034370126095083026\n",
      "Average test loss: 0.00653874280055364\n",
      "Epoch 73/300\n",
      "Average training loss: 0.034515442397859364\n",
      "Average test loss: 0.006322475161817338\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03427044025891357\n",
      "Average test loss: 0.006548015159451299\n",
      "Epoch 75/300\n",
      "Average training loss: 0.034315146174695756\n",
      "Average test loss: 0.0062944621530671915\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03430200389689869\n",
      "Average test loss: 0.006758452037970225\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03423950321475665\n",
      "Average test loss: 0.006298462003469467\n",
      "Epoch 78/300\n",
      "Average training loss: 0.034158496969276\n",
      "Average test loss: 0.006374901420954201\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03403391150964631\n",
      "Average test loss: 0.00642393054233657\n",
      "Epoch 80/300\n",
      "Average training loss: 0.034023882465230096\n",
      "Average test loss: 0.0067516040545370845\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03397466822796398\n",
      "Average test loss: 0.007078502666619089\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034103239625692366\n",
      "Average test loss: 0.006855638816952705\n",
      "Epoch 83/300\n",
      "Average training loss: 0.033828666946954196\n",
      "Average test loss: 0.006321294815589984\n",
      "Epoch 84/300\n",
      "Average training loss: 0.033781012624502185\n",
      "Average test loss: 0.006973431308236387\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03384609552224477\n",
      "Average test loss: 0.0066259200407399075\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033708322872718174\n",
      "Average test loss: 0.007297486926946375\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03368728312849999\n",
      "Average test loss: 0.006797126969115602\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033743791499071654\n",
      "Average test loss: 0.0064649988143808314\n",
      "Epoch 89/300\n",
      "Average training loss: 0.033607876713077224\n",
      "Average test loss: 0.00624704127965702\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03370715106858148\n",
      "Average test loss: 0.0064253466141720615\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03342531653576427\n",
      "Average test loss: 0.00616017317564951\n",
      "Epoch 92/300\n",
      "Average training loss: 0.033553594708442686\n",
      "Average test loss: 0.006431511892626683\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03341067863007387\n",
      "Average test loss: 0.006345659412857559\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03347297590970993\n",
      "Average test loss: 0.006350814902948009\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03342963321010272\n",
      "Average test loss: 0.007823470969994863\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03332093249426948\n",
      "Average test loss: 0.007319885059363312\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0332877828611268\n",
      "Average test loss: 0.00652385441131062\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033303341882096396\n",
      "Average test loss: 0.006640880528423521\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03332093984219763\n",
      "Average test loss: 0.006378718771040439\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03325396429830127\n",
      "Average test loss: 0.006493878714326355\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03312752077314589\n",
      "Average test loss: 0.006322743283377753\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03323131009936333\n",
      "Average test loss: 0.006178323213424947\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03308337803184986\n",
      "Average test loss: 0.006579688408308559\n",
      "Epoch 104/300\n",
      "Average training loss: 0.033161241451899214\n",
      "Average test loss: 0.00935858305543661\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03293588077359729\n",
      "Average test loss: 0.006524419573446114\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03303986261288325\n",
      "Average test loss: 0.006687348274721039\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03291341111063957\n",
      "Average test loss: 0.006229266189038753\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03301141838563813\n",
      "Average test loss: 0.006609209954738617\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03291869094967842\n",
      "Average test loss: 0.006736690726131201\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03280942604608006\n",
      "Average test loss: 0.0063964491672813895\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03282865240176519\n",
      "Average test loss: 0.006627518969691462\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03289733856254154\n",
      "Average test loss: 0.006443179231137037\n",
      "Epoch 113/300\n",
      "Average training loss: 0.032898956659767366\n",
      "Average test loss: 0.00619907829641468\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03272670396003458\n",
      "Average test loss: 0.0065832856910096275\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03269966699348556\n",
      "Average test loss: 0.006283375035143561\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0326165620221032\n",
      "Average test loss: 0.006346438804434406\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03262827742762036\n",
      "Average test loss: 0.006165784988966253\n",
      "Epoch 118/300\n",
      "Average training loss: 0.032649627692169614\n",
      "Average test loss: 0.006246972723760538\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03285473799539937\n",
      "Average test loss: 0.006919819227109353\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032615816411044864\n",
      "Average test loss: 0.006522946767508983\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03254383448428578\n",
      "Average test loss: 0.006361014200581445\n",
      "Epoch 122/300\n",
      "Average training loss: 0.032554179350535076\n",
      "Average test loss: 0.006296362364250753\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03253072277870443\n",
      "Average test loss: 0.00651206437456939\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03247856438656648\n",
      "Average test loss: 0.007624583675629563\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03238397733370463\n",
      "Average test loss: 0.006111931660936938\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03246729125910335\n",
      "Average test loss: 0.006453201171010733\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03245524928304884\n",
      "Average test loss: 0.006126640016833941\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03234066964520348\n",
      "Average test loss: 0.006163633481081989\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0323442682756318\n",
      "Average test loss: 0.006355675975067748\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03229948200451003\n",
      "Average test loss: 0.006342245121796926\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03227087447709508\n",
      "Average test loss: 0.0062673765350547105\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03227381697628233\n",
      "Average test loss: 0.006237774564988083\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0323060717218452\n",
      "Average test loss: 0.006190683744847774\n",
      "Epoch 134/300\n",
      "Average training loss: 0.032216990229156285\n",
      "Average test loss: 0.006471686772174305\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0322473655210601\n",
      "Average test loss: 0.006152487345039845\n",
      "Epoch 136/300\n",
      "Average training loss: 0.032245054446988636\n",
      "Average test loss: 0.006890567755533589\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03209884606467353\n",
      "Average test loss: 0.006482678898506695\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03206888790925344\n",
      "Average test loss: 0.0065792001725898846\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03212094694044855\n",
      "Average test loss: 0.006671655990183353\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03209842013981607\n",
      "Average test loss: 0.006323322153339784\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03209603624376986\n",
      "Average test loss: 0.0076226376249558395\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03210359364251296\n",
      "Average test loss: 0.006144304556979074\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031919142716460756\n",
      "Average test loss: 0.006200650535523891\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03199453836348322\n",
      "Average test loss: 0.006273432263483604\n",
      "Epoch 145/300\n",
      "Average training loss: 0.031925050285127425\n",
      "Average test loss: 0.006250843759212229\n",
      "Epoch 146/300\n",
      "Average training loss: 0.031993112080627016\n",
      "Average test loss: 0.006181205546276437\n",
      "Epoch 147/300\n",
      "Average training loss: 0.032003836664888596\n",
      "Average test loss: 0.006134288749761052\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03184096359875467\n",
      "Average test loss: 0.006183840908937984\n",
      "Epoch 149/300\n",
      "Average training loss: 0.031918767548269694\n",
      "Average test loss: 0.006256958660566144\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03190268644856082\n",
      "Average test loss: 0.006199791791538398\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03189357726938195\n",
      "Average test loss: 0.006318024069484738\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03185700600677066\n",
      "Average test loss: 0.006369069957898723\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03188584329353439\n",
      "Average test loss: 0.006640099146299892\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031716786780291134\n",
      "Average test loss: 0.006073472992827495\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03171328887674544\n",
      "Average test loss: 0.006325640052143071\n",
      "Epoch 156/300\n",
      "Average training loss: 0.031781450874275634\n",
      "Average test loss: 0.0068798474139637415\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03169740484158198\n",
      "Average test loss: 0.006349126939558321\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03182891768548224\n",
      "Average test loss: 0.0061291146278381345\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03183468389842245\n",
      "Average test loss: 0.0073878590597046745\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03167956591811445\n",
      "Average test loss: 0.0063010816098087365\n",
      "Epoch 161/300\n",
      "Average training loss: 0.031658594300349556\n",
      "Average test loss: 0.006235011305246088\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0317661305434174\n",
      "Average test loss: 0.006238168352180057\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0315252724621031\n",
      "Average test loss: 0.006384399440553453\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03161636369592614\n",
      "Average test loss: 0.006777947225918373\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03151750081612004\n",
      "Average test loss: 0.00635363281932142\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031580207222037845\n",
      "Average test loss: 0.006304636990858449\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03158149880998665\n",
      "Average test loss: 0.006410834865023692\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03154778547088305\n",
      "Average test loss: 0.00619907947546906\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03156834403011534\n",
      "Average test loss: 0.006231674069331752\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03153525984949536\n",
      "Average test loss: 0.006663013707018561\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03166533969839414\n",
      "Average test loss: 0.007959684147602982\n",
      "Epoch 172/300\n",
      "Average training loss: 0.031389082507954705\n",
      "Average test loss: 0.0062636517182820374\n",
      "Epoch 173/300\n",
      "Average training loss: 0.031445230753885375\n",
      "Average test loss: 0.006326054599550035\n",
      "Epoch 174/300\n",
      "Average training loss: 0.031533047942651646\n",
      "Average test loss: 0.006280385363433096\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03146834299961726\n",
      "Average test loss: 0.0062189255588584475\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03145455437898636\n",
      "Average test loss: 0.00636422157784303\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031496004359589685\n",
      "Average test loss: 0.006398576351503531\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0313426470624076\n",
      "Average test loss: 0.006329288349383407\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031392947988377676\n",
      "Average test loss: 0.006225513127528959\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03126914417081409\n",
      "Average test loss: 0.00622754588495526\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03138196622994211\n",
      "Average test loss: 0.006553169473591778\n",
      "Epoch 182/300\n",
      "Average training loss: 0.031337679227193195\n",
      "Average test loss: 0.007059277822367019\n",
      "Epoch 183/300\n",
      "Average training loss: 0.031265383478668\n",
      "Average test loss: 0.0065048837024304605\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03132618029912313\n",
      "Average test loss: 0.006171729325420327\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03133720046612951\n",
      "Average test loss: 0.006158810832020309\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03123951893217034\n",
      "Average test loss: 0.006935414573798577\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03123238290515211\n",
      "Average test loss: 0.006329716644353337\n",
      "Epoch 188/300\n",
      "Average training loss: 0.031211450778775744\n",
      "Average test loss: 0.006710651285118527\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03137605556845665\n",
      "Average test loss: 0.00625883878643314\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031146434823671975\n",
      "Average test loss: 0.0061328852669232424\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0312533976468775\n",
      "Average test loss: 0.006207829930716091\n",
      "Epoch 192/300\n",
      "Average training loss: 0.031199345093634392\n",
      "Average test loss: 0.006323331475257874\n",
      "Epoch 193/300\n",
      "Average training loss: 0.031143872804111905\n",
      "Average test loss: 0.006706584904756811\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03115633701947\n",
      "Average test loss: 0.006238837401900027\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031185868077807957\n",
      "Average test loss: 0.006521422698679898\n",
      "Epoch 196/300\n",
      "Average training loss: 0.031168475614653694\n",
      "Average test loss: 0.006287475461347235\n",
      "Epoch 197/300\n",
      "Average training loss: 0.031101407946811783\n",
      "Average test loss: 0.006537228145947059\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0310351718697283\n",
      "Average test loss: 0.006205094367265701\n",
      "Epoch 199/300\n",
      "Average training loss: 0.031112146433856753\n",
      "Average test loss: 0.00660478889859385\n",
      "Epoch 200/300\n",
      "Average training loss: 0.031162196268637976\n",
      "Average test loss: 0.006201341684079833\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031141364379061592\n",
      "Average test loss: 0.006283462632447481\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03106749775674608\n",
      "Average test loss: 0.006394313716226154\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03100462370779779\n",
      "Average test loss: 0.006161204711016681\n",
      "Epoch 204/300\n",
      "Average training loss: 0.031093449029657577\n",
      "Average test loss: 0.006793736469414499\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03100766795873642\n",
      "Average test loss: 0.006155207100427813\n",
      "Epoch 206/300\n",
      "Average training loss: 0.031031899622744986\n",
      "Average test loss: 0.006217991172439522\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030923700536290805\n",
      "Average test loss: 0.006368589104877578\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03092655588189761\n",
      "Average test loss: 0.006312545468823778\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03093593356344435\n",
      "Average test loss: 0.006312483004397816\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03089355090757211\n",
      "Average test loss: 0.0063194620609283445\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030948471797837152\n",
      "Average test loss: 0.006658782241659032\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030993698313832283\n",
      "Average test loss: 0.010455440376367834\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030933617916372086\n",
      "Average test loss: 0.006198811486363411\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030924295521444746\n",
      "Average test loss: 0.006264801729677452\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030848119787043996\n",
      "Average test loss: 0.006257499208052953\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030858492897616493\n",
      "Average test loss: 0.006199095695383019\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030882714865936172\n",
      "Average test loss: 0.006594244860526588\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03080989183485508\n",
      "Average test loss: 0.006463610901186864\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030791826711760627\n",
      "Average test loss: 0.011928455987738238\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03075491378042433\n",
      "Average test loss: 0.006508493757496277\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03079516656531228\n",
      "Average test loss: 0.006304691077106529\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030868467504779496\n",
      "Average test loss: 0.007277609242747228\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03081090572476387\n",
      "Average test loss: 0.006290640936129623\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03075504031115108\n",
      "Average test loss: 0.006163323187579711\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03074568098286788\n",
      "Average test loss: 0.00649822370418244\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03072258460852835\n",
      "Average test loss: 0.006417956343127622\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03076178130176332\n",
      "Average test loss: 0.007281665748192204\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03068763819668028\n",
      "Average test loss: 0.006289688838024934\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030639644553263984\n",
      "Average test loss: 0.006385550907088651\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03065533923771646\n",
      "Average test loss: 0.006383742726097505\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030709171462390158\n",
      "Average test loss: 0.006578362455384599\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03069444639649656\n",
      "Average test loss: 0.006256288934085104\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03065514924459987\n",
      "Average test loss: 0.006182991459137864\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0306386623316341\n",
      "Average test loss: 0.006345053128484222\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030664860818121167\n",
      "Average test loss: 0.006646827802062035\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030569254802332985\n",
      "Average test loss: 0.006407394936515225\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03059431974755393\n",
      "Average test loss: 0.006451601507763068\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030579130348232057\n",
      "Average test loss: 0.0063709115899271435\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030610918432474137\n",
      "Average test loss: 0.006530551618999905\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030513710538546245\n",
      "Average test loss: 0.006432249429325263\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03052429682181941\n",
      "Average test loss: 0.006775086469948292\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030548154873980417\n",
      "Average test loss: 0.006303848761651251\n",
      "Epoch 243/300\n",
      "Average training loss: 0.030559689594639672\n",
      "Average test loss: 0.007995968081057072\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030499312808116276\n",
      "Average test loss: 0.006242470479673809\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030492220731245148\n",
      "Average test loss: 0.006386578670392434\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030414987719721263\n",
      "Average test loss: 0.006243731916778617\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030483118017514545\n",
      "Average test loss: 0.006441776520262162\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03048984455400043\n",
      "Average test loss: 0.006336725834343169\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03045840503440963\n",
      "Average test loss: 0.006350486907694075\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030504545620746084\n",
      "Average test loss: 0.006612180296331644\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030482644826173782\n",
      "Average test loss: 0.006275260298823317\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03037643479141924\n",
      "Average test loss: 0.00866528199447526\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030336682544814215\n",
      "Average test loss: 0.006456492862353723\n",
      "Epoch 254/300\n",
      "Average training loss: 0.030349386304616928\n",
      "Average test loss: 0.006358559632052978\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03038214113480515\n",
      "Average test loss: 0.00640814213703076\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030393284791045718\n",
      "Average test loss: 0.00636580203866793\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030375494168864356\n",
      "Average test loss: 0.006568685852819019\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030481989345616765\n",
      "Average test loss: 0.006579279312656986\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030349952853388255\n",
      "Average test loss: 0.0062226339290953345\n",
      "Epoch 260/300\n",
      "Average training loss: 0.030313211537069746\n",
      "Average test loss: 0.007195944659411907\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030381880929072697\n",
      "Average test loss: 0.006273014532195197\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030293835666444568\n",
      "Average test loss: 0.0062646662568052605\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030322950181033877\n",
      "Average test loss: 0.006230488872362508\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03026140657067299\n",
      "Average test loss: 0.006385056385149558\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030278571160303223\n",
      "Average test loss: 0.0062916871760454445\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03033959951168961\n",
      "Average test loss: 0.006346040673967865\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030273001288374266\n",
      "Average test loss: 0.006340246831377347\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030253667703933187\n",
      "Average test loss: 0.006167072587956985\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03026653195420901\n",
      "Average test loss: 0.00804696972999308\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030220445411072838\n",
      "Average test loss: 0.006315466793460979\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030313530534505844\n",
      "Average test loss: 0.006353807888511155\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03015410245127148\n",
      "Average test loss: 0.006603844712177912\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03033048512869411\n",
      "Average test loss: 0.006465105279452271\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030146609698732695\n",
      "Average test loss: 0.006434741542157199\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030141294962830013\n",
      "Average test loss: 0.006690332670178678\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03023310744596852\n",
      "Average test loss: 0.0062763911307685905\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030213744584057067\n",
      "Average test loss: 0.006255097950084342\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03016494650642077\n",
      "Average test loss: 0.006502293600804276\n",
      "Epoch 279/300\n",
      "Average training loss: 0.030309928718540404\n",
      "Average test loss: 0.006509783906655179\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03018088309466839\n",
      "Average test loss: 0.006524554122653272\n",
      "Epoch 281/300\n",
      "Average training loss: 0.030100166590677366\n",
      "Average test loss: 0.006450135805540614\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030147612866428163\n",
      "Average test loss: 0.00638548228972488\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030153047452370324\n",
      "Average test loss: 0.0063063894088069596\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030141339510679245\n",
      "Average test loss: 0.006395583678450849\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030159127642711003\n",
      "Average test loss: 0.006303479163597027\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03012116563816865\n",
      "Average test loss: 0.0066226843972173\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03010657769607173\n",
      "Average test loss: 0.008234450050526195\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030162953307231267\n",
      "Average test loss: 0.006275997422221634\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030157356417841383\n",
      "Average test loss: 0.006272476555986537\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030054626488023335\n",
      "Average test loss: 0.00618854993664556\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0300778359108501\n",
      "Average test loss: 0.006236779510147041\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030054365320338144\n",
      "Average test loss: 0.007154884322649903\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02996514016389847\n",
      "Average test loss: 0.0069461219492885804\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030043064186970392\n",
      "Average test loss: 0.0062900259780387085\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030069376261697874\n",
      "Average test loss: 0.006358168081276947\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03002218967510594\n",
      "Average test loss: 0.006297878485172987\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029965088246597184\n",
      "Average test loss: 0.006303785070776939\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029994199954801137\n",
      "Average test loss: 0.006289023230887121\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0300872978899214\n",
      "Average test loss: 0.006208985344817241\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02999949674970574\n",
      "Average test loss: 0.006342775963660743\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13250546131862534\n",
      "Average test loss: 0.012195253468222087\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05377941040198008\n",
      "Average test loss: 0.010709126902951135\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04919016491042243\n",
      "Average test loss: 0.0077068141450484596\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04700139437450303\n",
      "Average test loss: 0.008897412307560445\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04490885846482383\n",
      "Average test loss: 0.007168152603010337\n",
      "Epoch 6/300\n",
      "Average training loss: 0.042446419149637225\n",
      "Average test loss: 0.005767915101928843\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04164324035247167\n",
      "Average test loss: 0.005964436399853892\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03977597971757253\n",
      "Average test loss: 0.006274922578699059\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03868254103263219\n",
      "Average test loss: 0.005636835689345996\n",
      "Epoch 10/300\n",
      "Average training loss: 0.037484095205863315\n",
      "Average test loss: 0.0065673654476801555\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03602436468833023\n",
      "Average test loss: 0.005768697162055307\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0355947382317649\n",
      "Average test loss: 0.005621936350233025\n",
      "Epoch 13/300\n",
      "Average training loss: 0.034359346369902295\n",
      "Average test loss: 0.006953802375743786\n",
      "Epoch 14/300\n",
      "Average training loss: 0.033883976257509656\n",
      "Average test loss: 0.005986504616008865\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03345091458161672\n",
      "Average test loss: 0.006304461856683095\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03236283941732512\n",
      "Average test loss: 0.005308954255034526\n",
      "Epoch 17/300\n",
      "Average training loss: 0.031851432694329156\n",
      "Average test loss: 0.005526964777459701\n",
      "Epoch 18/300\n",
      "Average training loss: 0.031193239614367485\n",
      "Average test loss: 0.00504427705746558\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03128995507293277\n",
      "Average test loss: 0.005917737659066916\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03082657426264551\n",
      "Average test loss: 0.005725751321348879\n",
      "Epoch 21/300\n",
      "Average training loss: 0.030259622539083163\n",
      "Average test loss: 0.005042088100479709\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02995030115213659\n",
      "Average test loss: 0.006101805188589626\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029840724544392693\n",
      "Average test loss: 0.005064800767228007\n",
      "Epoch 24/300\n",
      "Average training loss: 0.029966287705633376\n",
      "Average test loss: 0.004615248118837675\n",
      "Epoch 25/300\n",
      "Average training loss: 0.029044430315494536\n",
      "Average test loss: 0.005282181309329139\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02913090381688542\n",
      "Average test loss: 0.006228290262735552\n",
      "Epoch 27/300\n",
      "Average training loss: 0.029254027932882308\n",
      "Average test loss: 0.006190892957978779\n",
      "Epoch 28/300\n",
      "Average training loss: 0.028404334917664528\n",
      "Average test loss: 0.004565578978922632\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02840260159638193\n",
      "Average test loss: 0.005070794345190127\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028480338056882222\n",
      "Average test loss: 0.0049017453599307275\n",
      "Epoch 31/300\n",
      "Average training loss: 0.027977231375045247\n",
      "Average test loss: 0.004608515741717484\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0280046944303645\n",
      "Average test loss: 0.004429957601138287\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02775068289372656\n",
      "Average test loss: 0.004660658836985628\n",
      "Epoch 34/300\n",
      "Average training loss: 0.027437426782316632\n",
      "Average test loss: 0.004553468552728494\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027653571077518993\n",
      "Average test loss: 0.004523184072019325\n",
      "Epoch 36/300\n",
      "Average training loss: 0.027165481977992588\n",
      "Average test loss: 0.004705195713788271\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026964736983180047\n",
      "Average test loss: 0.004898814019229677\n",
      "Epoch 38/300\n",
      "Average training loss: 0.026938966292474005\n",
      "Average test loss: 0.02063859954973062\n",
      "Epoch 39/300\n",
      "Average training loss: 0.026818957759274377\n",
      "Average test loss: 0.005643524563560883\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026676361108819643\n",
      "Average test loss: 0.00507451673100392\n",
      "Epoch 41/300\n",
      "Average training loss: 0.026554059414399995\n",
      "Average test loss: 0.004342987973036038\n",
      "Epoch 42/300\n",
      "Average training loss: 0.026661081857151454\n",
      "Average test loss: 0.006628665364450879\n",
      "Epoch 43/300\n",
      "Average training loss: 0.026354427367448807\n",
      "Average test loss: 0.004779987104651001\n",
      "Epoch 44/300\n",
      "Average training loss: 0.026324819488657846\n",
      "Average test loss: 0.006700266706032886\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02611756312350432\n",
      "Average test loss: 0.005010155872752269\n",
      "Epoch 46/300\n",
      "Average training loss: 0.026152961298823357\n",
      "Average test loss: 0.0047959723331862025\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025959571518831784\n",
      "Average test loss: 0.00422109992057085\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02581095599797037\n",
      "Average test loss: 0.004191550639561481\n",
      "Epoch 49/300\n",
      "Average training loss: 0.025796626589364477\n",
      "Average test loss: 0.006048057816094823\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025967272938953505\n",
      "Average test loss: 0.004123450298276213\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025637908023264672\n",
      "Average test loss: 0.00451325332125028\n",
      "Epoch 52/300\n",
      "Average training loss: 0.025612146076228883\n",
      "Average test loss: 0.004683700459698836\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02540534310705132\n",
      "Average test loss: 0.007736486179547177\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025484211227960057\n",
      "Average test loss: 0.004454200142787562\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025506685352987713\n",
      "Average test loss: 0.004494767900970247\n",
      "Epoch 56/300\n",
      "Average training loss: 0.025294853041569393\n",
      "Average test loss: 0.003908811170193884\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025360657662153244\n",
      "Average test loss: 0.003948875441733334\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02528095888925923\n",
      "Average test loss: 0.004093846393956079\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02513145022590955\n",
      "Average test loss: 0.004223362452454037\n",
      "Epoch 60/300\n",
      "Average training loss: 0.025234222571055093\n",
      "Average test loss: 0.004573893205573162\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024984703567292956\n",
      "Average test loss: 0.004091975308954715\n",
      "Epoch 62/300\n",
      "Average training loss: 0.024981314518385464\n",
      "Average test loss: 0.0041888170544472\n",
      "Epoch 63/300\n",
      "Average training loss: 0.025014617093735272\n",
      "Average test loss: 0.004594370139969719\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024874166135986645\n",
      "Average test loss: 0.004840736192961534\n",
      "Epoch 65/300\n",
      "Average training loss: 0.025094179804126423\n",
      "Average test loss: 0.006460114124748442\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02488748421271642\n",
      "Average test loss: 0.004000786290400558\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024713466150893107\n",
      "Average test loss: 0.004013857886609104\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024846312643753157\n",
      "Average test loss: 0.0040442524728261766\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024637067806389596\n",
      "Average test loss: 0.004027948879533344\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024661424706379574\n",
      "Average test loss: 0.004079070092696283\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024758062220282026\n",
      "Average test loss: 0.004232895084967216\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024804673252834215\n",
      "Average test loss: 0.004784428803457154\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024459242100516955\n",
      "Average test loss: 0.004637044899165631\n",
      "Epoch 74/300\n",
      "Average training loss: 0.024578089952468872\n",
      "Average test loss: 0.004235097807728582\n",
      "Epoch 75/300\n",
      "Average training loss: 0.024464536292685402\n",
      "Average test loss: 0.003914791705500748\n",
      "Epoch 76/300\n",
      "Average training loss: 0.024388440812627473\n",
      "Average test loss: 0.00416266578125457\n",
      "Epoch 77/300\n",
      "Average training loss: 0.024464354857802392\n",
      "Average test loss: 0.0040753096118569376\n",
      "Epoch 78/300\n",
      "Average training loss: 0.024278494480583404\n",
      "Average test loss: 0.004112241080651681\n",
      "Epoch 79/300\n",
      "Average training loss: 0.024387982298930487\n",
      "Average test loss: 0.005338513726161586\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02443270117541154\n",
      "Average test loss: 0.00389291273140245\n",
      "Epoch 81/300\n",
      "Average training loss: 0.024388450355993376\n",
      "Average test loss: 0.004079650288033816\n",
      "Epoch 82/300\n",
      "Average training loss: 0.024123419721921283\n",
      "Average test loss: 0.0038551527849502035\n",
      "Epoch 83/300\n",
      "Average training loss: 0.024041140635808308\n",
      "Average test loss: 0.004459172584944301\n",
      "Epoch 84/300\n",
      "Average training loss: 0.024147162699037127\n",
      "Average test loss: 0.004651749758463767\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02433786246014966\n",
      "Average test loss: 0.004204105499717925\n",
      "Epoch 86/300\n",
      "Average training loss: 0.024052260107464262\n",
      "Average test loss: 0.004268896482470963\n",
      "Epoch 87/300\n",
      "Average training loss: 0.024013400713602702\n",
      "Average test loss: 0.003999567756222354\n",
      "Epoch 88/300\n",
      "Average training loss: 0.024060312158531612\n",
      "Average test loss: 0.004097958879545331\n",
      "Epoch 89/300\n",
      "Average training loss: 0.024027117864953148\n",
      "Average test loss: 0.0039323450198604\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023940048035648136\n",
      "Average test loss: 0.003985846008691523\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02397254716687732\n",
      "Average test loss: 0.004283098727257715\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02396822358171145\n",
      "Average test loss: 0.004400596035851373\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023891693745222358\n",
      "Average test loss: 0.004354260884225369\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02383000701997015\n",
      "Average test loss: 0.00404488445031974\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02389935321940316\n",
      "Average test loss: 0.00506984405592084\n",
      "Epoch 96/300\n",
      "Average training loss: 0.023813184824254777\n",
      "Average test loss: 0.004039723800909188\n",
      "Epoch 97/300\n",
      "Average training loss: 0.023855553762780295\n",
      "Average test loss: 0.004086595268713103\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023801218094097243\n",
      "Average test loss: 0.003963475627203782\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023773700613114568\n",
      "Average test loss: 0.004086990461581283\n",
      "Epoch 100/300\n",
      "Average training loss: 0.023746483503116502\n",
      "Average test loss: 0.004192395062082343\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02367194130520026\n",
      "Average test loss: 0.003971181106236246\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02371992486880885\n",
      "Average test loss: 0.0039827700004809435\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02376889446708891\n",
      "Average test loss: 0.003958071371747388\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023796802308824326\n",
      "Average test loss: 0.004007519039428896\n",
      "Epoch 105/300\n",
      "Average training loss: 0.023542326990101073\n",
      "Average test loss: 0.004047853651146094\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023660321541958386\n",
      "Average test loss: 0.00392397927865386\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023505432699289588\n",
      "Average test loss: 0.004204920077696442\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023513970406519043\n",
      "Average test loss: 0.004206457664362258\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02365228491855992\n",
      "Average test loss: 0.004017761116226514\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023537082579400806\n",
      "Average test loss: 0.003971488378528091\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02343876565992832\n",
      "Average test loss: 0.004179671063398322\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02337009332080682\n",
      "Average test loss: 0.003896494086417887\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023594605525334676\n",
      "Average test loss: 0.004135305157138241\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02356746318605211\n",
      "Average test loss: 0.004045707198066844\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023545041085945236\n",
      "Average test loss: 0.004435862595836322\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02332287736237049\n",
      "Average test loss: 0.0038452825335164864\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02331888280974494\n",
      "Average test loss: 0.0044810867533087734\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023451978431807623\n",
      "Average test loss: 0.0038740263676477803\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023331999644637107\n",
      "Average test loss: 0.003915762967740496\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023412229294578233\n",
      "Average test loss: 0.004315528258681298\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023294290794266596\n",
      "Average test loss: 0.005077814076923662\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023308720861872037\n",
      "Average test loss: 0.0041799929874638715\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023388027704424327\n",
      "Average test loss: 0.0039243941468497115\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023262926833497153\n",
      "Average test loss: 0.004064537829822964\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023226404796044032\n",
      "Average test loss: 0.003883707718923688\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02323091446194384\n",
      "Average test loss: 0.0044640248094995815\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023174722636739413\n",
      "Average test loss: 0.0039046942049430476\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023176331887642544\n",
      "Average test loss: 0.003927916308864951\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023201434100667634\n",
      "Average test loss: 0.0038546168095328743\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02321035254498323\n",
      "Average test loss: 0.00889352655162414\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023100698999232717\n",
      "Average test loss: 0.005365030096636878\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023197902219163048\n",
      "Average test loss: 0.0038459587732536924\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02311427637603548\n",
      "Average test loss: 0.0042911449536267255\n",
      "Epoch 134/300\n",
      "Average training loss: 0.023002731937501165\n",
      "Average test loss: 0.004034352895700269\n",
      "Epoch 135/300\n",
      "Average training loss: 0.023132244237595134\n",
      "Average test loss: 0.004077007943557368\n",
      "Epoch 136/300\n",
      "Average training loss: 0.023103267022305064\n",
      "Average test loss: 0.004410122011270788\n",
      "Epoch 137/300\n",
      "Average training loss: 0.023004014329777823\n",
      "Average test loss: 0.0038635083494914903\n",
      "Epoch 138/300\n",
      "Average training loss: 0.023121340091029802\n",
      "Average test loss: 0.003943883991903729\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022970725307861962\n",
      "Average test loss: 0.00411031764642232\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022961200993922022\n",
      "Average test loss: 0.0038621364645659924\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02296383999950356\n",
      "Average test loss: 0.0042492391727864745\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02303820579747359\n",
      "Average test loss: 0.004771147932857275\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022949326728781066\n",
      "Average test loss: 0.00391026708111167\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022859550045596227\n",
      "Average test loss: 0.003875926593732503\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023040204818050067\n",
      "Average test loss: 0.0048053076362444295\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02299151052865717\n",
      "Average test loss: 0.0039934876167939766\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02290547726220555\n",
      "Average test loss: 0.0040138188139018085\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022881380958689582\n",
      "Average test loss: 0.003931268604265319\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022969155197342238\n",
      "Average test loss: 0.004142197295609448\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022705180644989012\n",
      "Average test loss: 0.00387534970872932\n",
      "Epoch 151/300\n",
      "Average training loss: 0.022961562337146864\n",
      "Average test loss: 0.003986691761554943\n",
      "Epoch 152/300\n",
      "Average training loss: 0.022793130429254637\n",
      "Average test loss: 0.0039037314156691234\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022700099491410785\n",
      "Average test loss: 0.00388458071090281\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022884782504704264\n",
      "Average test loss: 0.004012422760741578\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02273253481255637\n",
      "Average test loss: 0.00387854881832997\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022690843982828987\n",
      "Average test loss: 0.004238474157328407\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022850809676779642\n",
      "Average test loss: 0.00381561454364823\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02269007754822572\n",
      "Average test loss: 0.004913137215707037\n",
      "Epoch 159/300\n",
      "Average training loss: 0.022807677768998676\n",
      "Average test loss: 0.003995360335128175\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02274346460898717\n",
      "Average test loss: 0.00460092791583803\n",
      "Epoch 161/300\n",
      "Average training loss: 0.022786874355541335\n",
      "Average test loss: 0.005594791287763252\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02271649602221118\n",
      "Average test loss: 0.0038657710775732994\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02266864787042141\n",
      "Average test loss: 0.003987824956369069\n",
      "Epoch 164/300\n",
      "Average training loss: 0.022649282950494024\n",
      "Average test loss: 0.003968771827717622\n",
      "Epoch 165/300\n",
      "Average training loss: 0.022612647891872457\n",
      "Average test loss: 0.0039029489778396155\n",
      "Epoch 166/300\n",
      "Average training loss: 0.022657427108950086\n",
      "Average test loss: 0.004097611562659343\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02254827138947116\n",
      "Average test loss: 0.00388517001685169\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022620637539360257\n",
      "Average test loss: 0.003852066976328691\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022560964289638732\n",
      "Average test loss: 0.004008291250715653\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022647265114718013\n",
      "Average test loss: 0.0038548857383430006\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022617338490155008\n",
      "Average test loss: 0.0038416116260406043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.022589111949006715\n",
      "Average test loss: 0.004237726615327928\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0226561365607712\n",
      "Average test loss: 0.004079864310721556\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02254307261440489\n",
      "Average test loss: 0.004130679603873028\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02253170136941804\n",
      "Average test loss: 0.003909159691590402\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02252263799806436\n",
      "Average test loss: 0.00464488890974058\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022489203876919217\n",
      "Average test loss: 0.011072433011399376\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022494964536693363\n",
      "Average test loss: 0.00390730640416344\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022489831684364213\n",
      "Average test loss: 0.003987562494973342\n",
      "Epoch 180/300\n",
      "Average training loss: 0.022561428741448456\n",
      "Average test loss: 0.003996754164083137\n",
      "Epoch 181/300\n",
      "Average training loss: 0.022616399954590534\n",
      "Average test loss: 0.0038687363146907754\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02244333592057228\n",
      "Average test loss: 0.004080063550422589\n",
      "Epoch 183/300\n",
      "Average training loss: 0.022432430798808734\n",
      "Average test loss: 0.004328807150324186\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022491173739234607\n",
      "Average test loss: 0.003867845359982716\n",
      "Epoch 185/300\n",
      "Average training loss: 0.022383636865350935\n",
      "Average test loss: 0.00977421918263038\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02243775118390719\n",
      "Average test loss: 0.0043626188056336505\n",
      "Epoch 187/300\n",
      "Average training loss: 0.022336647644639017\n",
      "Average test loss: 0.004010496281501319\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022309114181333117\n",
      "Average test loss: 0.003933762187966042\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02247134112815062\n",
      "Average test loss: 0.004220772972951333\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02236985698176755\n",
      "Average test loss: 0.004372734183652534\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02228917152186235\n",
      "Average test loss: 0.004185078001684613\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022383142646816043\n",
      "Average test loss: 0.003911867952595154\n",
      "Epoch 193/300\n",
      "Average training loss: 0.022400924060079788\n",
      "Average test loss: 0.0038572328094806938\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022505893508593243\n",
      "Average test loss: 0.004312783990883165\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02237417560319106\n",
      "Average test loss: 0.004028741343153848\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022366774004366662\n",
      "Average test loss: 0.003924406179123455\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022360718844665423\n",
      "Average test loss: 0.004122764664805598\n",
      "Epoch 198/300\n",
      "Average training loss: 0.022213829626639685\n",
      "Average test loss: 0.0039930427500771155\n",
      "Epoch 199/300\n",
      "Average training loss: 0.022252330801553196\n",
      "Average test loss: 0.004391061264193721\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0221848582956526\n",
      "Average test loss: 0.004045233366390069\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022405003059241507\n",
      "Average test loss: 0.004131253185785479\n",
      "Epoch 202/300\n",
      "Average training loss: 0.022267536313997374\n",
      "Average test loss: 0.004032666688991917\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022351289580265683\n",
      "Average test loss: 0.004184190892924865\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02222458894385232\n",
      "Average test loss: 0.004084080358553264\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02226155915028519\n",
      "Average test loss: 0.004013263917424612\n",
      "Epoch 206/300\n",
      "Average training loss: 0.022358832627534866\n",
      "Average test loss: 0.004161669361094634\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022415021868215666\n",
      "Average test loss: 0.004064333465364244\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022099796916047732\n",
      "Average test loss: 0.003980052906192012\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0221945069713725\n",
      "Average test loss: 0.004046456437144015\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02225452156530486\n",
      "Average test loss: 0.0046196964917083585\n",
      "Epoch 211/300\n",
      "Average training loss: 0.022106081089211835\n",
      "Average test loss: 0.004232751097530126\n",
      "Epoch 212/300\n",
      "Average training loss: 0.022096632460753125\n",
      "Average test loss: 0.003792837732575006\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022181895838843453\n",
      "Average test loss: 0.00489752033042411\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02219638707737128\n",
      "Average test loss: 0.004698425523108906\n",
      "Epoch 215/300\n",
      "Average training loss: 0.022173886054091985\n",
      "Average test loss: 0.003992320224642754\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02208753607670466\n",
      "Average test loss: 0.004016427409317758\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022172141667869356\n",
      "Average test loss: 0.003811444350414806\n",
      "Epoch 218/300\n",
      "Average training loss: 0.022151001940998765\n",
      "Average test loss: 0.004048553885064191\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0220909254749616\n",
      "Average test loss: 0.004253929630749755\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02207646542787552\n",
      "Average test loss: 0.0039018732965406446\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02209223074879911\n",
      "Average test loss: 0.003839743946161535\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022070811387565403\n",
      "Average test loss: 0.0039055218106756606\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022176344157920944\n",
      "Average test loss: 0.003855534865624375\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022046241357922554\n",
      "Average test loss: 0.003816776674861709\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022090729273027843\n",
      "Average test loss: 0.006664464307328065\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021991139620542527\n",
      "Average test loss: 0.003984889185676972\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022114122374190224\n",
      "Average test loss: 0.004843868455125226\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02210008404486709\n",
      "Average test loss: 0.0037973204511735172\n",
      "Epoch 229/300\n",
      "Average training loss: 0.022029068378110726\n",
      "Average test loss: 0.004085453649154968\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02201690667039818\n",
      "Average test loss: 0.003964187143991391\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022047382213175298\n",
      "Average test loss: 0.0038837134424183103\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022074139426151912\n",
      "Average test loss: 0.003868478949078255\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022014662659830517\n",
      "Average test loss: 0.0038369895894494323\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021926147972544035\n",
      "Average test loss: 0.0039566681115991535\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02205899306635062\n",
      "Average test loss: 0.004087050498566694\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02192540525396665\n",
      "Average test loss: 0.003920937173689405\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02195830566684405\n",
      "Average test loss: 0.0037981387809332876\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022005594182345604\n",
      "Average test loss: 0.0038991850451048876\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02193584783375263\n",
      "Average test loss: 0.003961125509192546\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02194988029367394\n",
      "Average test loss: 0.0038222549652887714\n",
      "Epoch 241/300\n",
      "Average training loss: 0.021923743918538095\n",
      "Average test loss: 0.004030120365528597\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02191659915447235\n",
      "Average test loss: 0.0038626848666204346\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02190959324936072\n",
      "Average test loss: 0.004059172868521677\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021993024306164847\n",
      "Average test loss: 0.004802374100933472\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02187569710612297\n",
      "Average test loss: 0.003814132875038518\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021864760152995585\n",
      "Average test loss: 0.0038877752386033537\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02192203856839074\n",
      "Average test loss: 0.004054085329174995\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02186072144905726\n",
      "Average test loss: 0.003974137004464865\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021930023077461454\n",
      "Average test loss: 0.003853508248511288\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021890227569474115\n",
      "Average test loss: 0.004325292128241724\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02188654097583559\n",
      "Average test loss: 0.003993971546697948\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0218176877895991\n",
      "Average test loss: 0.0039754246498147645\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02187491754939159\n",
      "Average test loss: 0.004078423743446668\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021811203102270763\n",
      "Average test loss: 0.0039049870177275603\n",
      "Epoch 255/300\n",
      "Average training loss: 0.021901569224066206\n",
      "Average test loss: 0.003965490496406952\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021799861806962226\n",
      "Average test loss: 0.004101399558285872\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021797376410828698\n",
      "Average test loss: 0.00391181294951174\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02192495880358749\n",
      "Average test loss: 0.0040978749559985265\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021803613416022726\n",
      "Average test loss: 0.003993062224239111\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021736405000090598\n",
      "Average test loss: 0.003837935532339745\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021813113820221688\n",
      "Average test loss: 0.0044114115020881094\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02177406285703182\n",
      "Average test loss: 0.004389461352593369\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02191789149575763\n",
      "Average test loss: 0.00413844713423815\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02178253286414676\n",
      "Average test loss: 0.0045818562313086455\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021708491176366806\n",
      "Average test loss: 0.0050466642868187694\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021778060629963875\n",
      "Average test loss: 0.0039035388835602335\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02172320984966225\n",
      "Average test loss: 0.004131463427096606\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021734451161490545\n",
      "Average test loss: 0.00445404848166638\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021769821473293833\n",
      "Average test loss: 0.0038998158135347895\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021708404453264343\n",
      "Average test loss: 0.004451600200186173\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021660970096786816\n",
      "Average test loss: 0.00419574169235097\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02168620493180222\n",
      "Average test loss: 0.0037913804521991145\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021730189207527374\n",
      "Average test loss: 0.0038620621307442586\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021771398329072527\n",
      "Average test loss: 0.003871189436978764\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02164489702714814\n",
      "Average test loss: 0.004397989550398456\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021692948402629957\n",
      "Average test loss: 0.0038073460232052538\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021742046187321343\n",
      "Average test loss: 0.003872520912438631\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021681209976474443\n",
      "Average test loss: 0.003887306377912561\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02170810871819655\n",
      "Average test loss: 0.003982374296006229\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021675011850065654\n",
      "Average test loss: 0.003926758053816027\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021641335614853437\n",
      "Average test loss: 0.00393281953699059\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02179574148191346\n",
      "Average test loss: 0.0038364520085354646\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021600585854715772\n",
      "Average test loss: 0.00406654688095053\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021596600774261687\n",
      "Average test loss: 0.003929423144087196\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021662608068850304\n",
      "Average test loss: 0.0038322387679169574\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021649062944783106\n",
      "Average test loss: 0.0038381523489952087\n",
      "Epoch 287/300\n",
      "Average training loss: 0.021621057021949025\n",
      "Average test loss: 0.004002114064887994\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02158421761294206\n",
      "Average test loss: 0.003884970930715402\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02163157046503491\n",
      "Average test loss: 0.008357799922426542\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021643321428034042\n",
      "Average test loss: 0.005989918514465292\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02177991421189573\n",
      "Average test loss: 0.005215024405883418\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021633055799537233\n",
      "Average test loss: 0.004101319620178806\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021558232266041966\n",
      "Average test loss: 0.004205245789554384\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021515591197543673\n",
      "Average test loss: 0.0038892769399616455\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021634784680273796\n",
      "Average test loss: 0.003906845840728946\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02162832800216145\n",
      "Average test loss: 0.00425227056360907\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02152293978797065\n",
      "Average test loss: 0.0038757080272254015\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02161636977725559\n",
      "Average test loss: 0.0038880574432098203\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021594252932402823\n",
      "Average test loss: 0.003977061673170991\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02151915726562341\n",
      "Average test loss: 0.003972998104575608\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11855898125304117\n",
      "Average test loss: 0.007542951242791282\n",
      "Epoch 2/300\n",
      "Average training loss: 0.046428085009257\n",
      "Average test loss: 0.007289117722875542\n",
      "Epoch 3/300\n",
      "Average training loss: 0.041628019356065325\n",
      "Average test loss: 0.005753804222577148\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03938607246677081\n",
      "Average test loss: 0.006940837180034982\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03771266181601418\n",
      "Average test loss: 0.006148272857897812\n",
      "Epoch 6/300\n",
      "Average training loss: 0.034555688536829415\n",
      "Average test loss: 0.005216170593682262\n",
      "Epoch 7/300\n",
      "Average training loss: 0.033639375173383286\n",
      "Average test loss: 0.005774908528145817\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03191678415404426\n",
      "Average test loss: 0.0047371975371821055\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03052802535560396\n",
      "Average test loss: 0.005199073607308997\n",
      "Epoch 10/300\n",
      "Average training loss: 0.030031604054901333\n",
      "Average test loss: 0.004789340453015434\n",
      "Epoch 11/300\n",
      "Average training loss: 0.028878830171293682\n",
      "Average test loss: 0.007741590647647778\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02797156549493472\n",
      "Average test loss: 0.005617539195550813\n",
      "Epoch 13/300\n",
      "Average training loss: 0.027081221615274747\n",
      "Average test loss: 0.0037757481404890616\n",
      "Epoch 14/300\n",
      "Average training loss: 0.026530501044458813\n",
      "Average test loss: 0.0055558895568052925\n",
      "Epoch 15/300\n",
      "Average training loss: 0.026174399786525304\n",
      "Average test loss: 0.004435710425592131\n",
      "Epoch 16/300\n",
      "Average training loss: 0.025491293064422077\n",
      "Average test loss: 0.004083704758021567\n",
      "Epoch 17/300\n",
      "Average training loss: 0.025107234908474815\n",
      "Average test loss: 0.0034798471722751858\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024947631822692024\n",
      "Average test loss: 0.003389425522337357\n",
      "Epoch 19/300\n",
      "Average training loss: 0.024475597365034952\n",
      "Average test loss: 0.006106964892811245\n",
      "Epoch 20/300\n",
      "Average training loss: 0.024233770394490824\n",
      "Average test loss: 0.003712088517430756\n",
      "Epoch 21/300\n",
      "Average training loss: 0.023876619794302518\n",
      "Average test loss: 0.004634273505045308\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02368904643257459\n",
      "Average test loss: 0.003680299366513888\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02336306064824263\n",
      "Average test loss: 0.0038189773071143364\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02322437909907765\n",
      "Average test loss: 0.0036114048881249294\n",
      "Epoch 25/300\n",
      "Average training loss: 0.022723406963878208\n",
      "Average test loss: 0.004060820467563139\n",
      "Epoch 26/300\n",
      "Average training loss: 0.022998972674210867\n",
      "Average test loss: 0.003932289129743974\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02242825295858913\n",
      "Average test loss: 0.0035898370121916134\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02242768227391773\n",
      "Average test loss: 0.004002405256239904\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02244010741677549\n",
      "Average test loss: 0.003281502383864588\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02204825727807151\n",
      "Average test loss: 0.0032344834974242583\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02188825480474366\n",
      "Average test loss: 0.0031810337127082876\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021665923294093874\n",
      "Average test loss: 0.0033327738551629914\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02193006680905819\n",
      "Average test loss: 0.0032997921084364254\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021822400917609532\n",
      "Average test loss: 0.003660284077541696\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02147021599776215\n",
      "Average test loss: 0.0037103355129559836\n",
      "Epoch 36/300\n",
      "Average training loss: 0.021447626903653146\n",
      "Average test loss: 0.00539565944713023\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021406851660874155\n",
      "Average test loss: 0.0032419237279229696\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0212442098673847\n",
      "Average test loss: 0.0033719035407735244\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021226870758665934\n",
      "Average test loss: 0.0030714529388480715\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02093550145212147\n",
      "Average test loss: 0.0034423495911889606\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02106539779322015\n",
      "Average test loss: 0.003063540102707015\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020927872914406988\n",
      "Average test loss: 0.0030166940738757453\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02093649260368612\n",
      "Average test loss: 0.00308365204392208\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02082906847033236\n",
      "Average test loss: 0.003819888741398851\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020856079694297577\n",
      "Average test loss: 0.0043426639160348305\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020678916225830714\n",
      "Average test loss: 0.003032888320171171\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020702733155753876\n",
      "Average test loss: 0.0032334720740715664\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02039353530936771\n",
      "Average test loss: 0.0031264506928208803\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020382977184322144\n",
      "Average test loss: 0.003156183936115768\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020424153799812\n",
      "Average test loss: 0.00323405974999898\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02041991218758954\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Nesterov_No_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Nesterov_No_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Nesterov_No_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc379985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Nesterov_Network.Nesterov import Nesterov\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04ba292",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa0ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724f609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84a2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e518c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1572802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.14716781975163354\n",
      "Average test loss: 0.011141649240420924\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06556935873296525\n",
      "Average test loss: 0.01991192707584964\n",
      "Epoch 3/300\n",
      "Average training loss: 0.060769728038046096\n",
      "Average test loss: 0.009997374355792999\n",
      "Epoch 4/300\n",
      "Average training loss: 0.058652443120876946\n",
      "Average test loss: 0.009110303069154422\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05615031558275223\n",
      "Average test loss: 0.008999759514298704\n",
      "Epoch 6/300\n",
      "Average training loss: 0.053677908284796606\n",
      "Average test loss: 0.00925846263517936\n",
      "Epoch 7/300\n",
      "Average training loss: 0.053087953474786544\n",
      "Average test loss: 0.008858372744586732\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05110702526569366\n",
      "Average test loss: 0.1519264222515954\n",
      "Epoch 9/300\n",
      "Average training loss: 0.050717637247509426\n",
      "Average test loss: 0.012130486684540908\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04886309303177728\n",
      "Average test loss: 0.01752662971450223\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04802959477239185\n",
      "Average test loss: 0.008762563793195618\n",
      "Epoch 12/300\n",
      "Average training loss: 0.047329150120417274\n",
      "Average test loss: 0.009263635108040439\n",
      "Epoch 13/300\n",
      "Average training loss: 0.046482994662390816\n",
      "Average test loss: 0.010626030485663148\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04510834113094542\n",
      "Average test loss: 0.007568938809964392\n",
      "Epoch 15/300\n",
      "Average training loss: 0.044923202756378386\n",
      "Average test loss: 0.009251295415891541\n",
      "Epoch 16/300\n",
      "Average training loss: 0.044076289296150206\n",
      "Average test loss: 0.008408761905299293\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04341842265592681\n",
      "Average test loss: 0.008104843277070258\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04362916178835763\n",
      "Average test loss: 0.007567028762565719\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04367370641231537\n",
      "Average test loss: 0.007216317631304264\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0426389710770713\n",
      "Average test loss: 0.008176572978496551\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04196872122751342\n",
      "Average test loss: 0.007908848317133055\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04175353717803955\n",
      "Average test loss: 0.011430237387617429\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04132525604632166\n",
      "Average test loss: 0.008798981185588571\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04094350357188119\n",
      "Average test loss: 0.007836356338527467\n",
      "Epoch 25/300\n",
      "Average training loss: 0.040408900009261235\n",
      "Average test loss: 0.008809942969017558\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04025359861387147\n",
      "Average test loss: 0.007895861975434754\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04050614958339267\n",
      "Average test loss: 0.00917740171071556\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039683150632513896\n",
      "Average test loss: 0.013146360860102707\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039548156927029295\n",
      "Average test loss: 0.007258508814705742\n",
      "Epoch 30/300\n",
      "Average training loss: 0.039261607292625636\n",
      "Average test loss: 0.0072348122828536565\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03902566747201813\n",
      "Average test loss: 0.0075703300990992125\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03904797666602665\n",
      "Average test loss: 0.008665004400743378\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03861583522293303\n",
      "Average test loss: 0.0075679507870227095\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03848415967490938\n",
      "Average test loss: 0.007108425070842107\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03838073448671235\n",
      "Average test loss: 0.007109530945205026\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03813512366016706\n",
      "Average test loss: 0.007261304123120175\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0378979793952571\n",
      "Average test loss: 0.006811063228382004\n",
      "Epoch 38/300\n",
      "Average training loss: 0.037498090585072835\n",
      "Average test loss: 0.007463405559460322\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03756508629189597\n",
      "Average test loss: 0.00675691540663441\n",
      "Epoch 40/300\n",
      "Average training loss: 0.037266637586885025\n",
      "Average test loss: 0.007032944865524769\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03731575923495822\n",
      "Average test loss: 0.007062893134438329\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03723110865222083\n",
      "Average test loss: 0.006989047564980057\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03687527633375592\n",
      "Average test loss: 0.006966403148240513\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03702925667828984\n",
      "Average test loss: 0.0074527616351842884\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03685197905202706\n",
      "Average test loss: 0.010352037059764067\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03665038572086228\n",
      "Average test loss: 0.0074195522322422925\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03667146353920301\n",
      "Average test loss: 0.006665640903843774\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0364793301820755\n",
      "Average test loss: 0.006660792035361131\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03635384761956003\n",
      "Average test loss: 0.007456433803670936\n",
      "Epoch 50/300\n",
      "Average training loss: 0.036291827831003404\n",
      "Average test loss: 0.007444104981919129\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03615405318140984\n",
      "Average test loss: 0.006837902116692728\n",
      "Epoch 52/300\n",
      "Average training loss: 0.036109580092959934\n",
      "Average test loss: 0.006994571516083347\n",
      "Epoch 53/300\n",
      "Average training loss: 0.036186677722467316\n",
      "Average test loss: 0.006961726875768767\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03558942651086383\n",
      "Average test loss: 0.0066861535381111835\n",
      "Epoch 55/300\n",
      "Average training loss: 0.035627579513523314\n",
      "Average test loss: 0.007964882847335604\n",
      "Epoch 56/300\n",
      "Average training loss: 0.036146670526928375\n",
      "Average test loss: 0.010190080708099736\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03550746456782023\n",
      "Average test loss: 0.006434167729069789\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03563097607427173\n",
      "Average test loss: 0.0071326031063993775\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03531904073556264\n",
      "Average test loss: 0.006367264029052523\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03518611898687151\n",
      "Average test loss: 0.006860033431400856\n",
      "Epoch 61/300\n",
      "Average training loss: 0.035328459183375044\n",
      "Average test loss: 0.0067677935655746195\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03500190339154667\n",
      "Average test loss: 0.006995169531553984\n",
      "Epoch 63/300\n",
      "Average training loss: 0.035023239543040596\n",
      "Average test loss: 0.006657474762035741\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03495343591603968\n",
      "Average test loss: 0.006702231923739115\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03486809475223224\n",
      "Average test loss: 0.008257986396551132\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03501687455508444\n",
      "Average test loss: 0.006388560523589452\n",
      "Epoch 67/300\n",
      "Average training loss: 0.034852437559101314\n",
      "Average test loss: 0.00686973359146052\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0346675559696224\n",
      "Average test loss: 0.0071487774203221\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03474764615959591\n",
      "Average test loss: 0.006408280136270656\n",
      "Epoch 70/300\n",
      "Average training loss: 0.034805496676100625\n",
      "Average test loss: 0.006473004825827148\n",
      "Epoch 71/300\n",
      "Average training loss: 0.034505793912543194\n",
      "Average test loss: 0.008119106211596065\n",
      "Epoch 72/300\n",
      "Average training loss: 0.034370126095083026\n",
      "Average test loss: 0.00653874280055364\n",
      "Epoch 73/300\n",
      "Average training loss: 0.034515442397859364\n",
      "Average test loss: 0.006322475161817338\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03427044025891357\n",
      "Average test loss: 0.006548015159451299\n",
      "Epoch 75/300\n",
      "Average training loss: 0.034315146174695756\n",
      "Average test loss: 0.0062944621530671915\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03430200389689869\n",
      "Average test loss: 0.006758452037970225\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03423950321475665\n",
      "Average test loss: 0.006298462003469467\n",
      "Epoch 78/300\n",
      "Average training loss: 0.034158496969276\n",
      "Average test loss: 0.006374901420954201\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03403391150964631\n",
      "Average test loss: 0.00642393054233657\n",
      "Epoch 80/300\n",
      "Average training loss: 0.034023882465230096\n",
      "Average test loss: 0.0067516040545370845\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03397466822796398\n",
      "Average test loss: 0.007078502666619089\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034103239625692366\n",
      "Average test loss: 0.006855638816952705\n",
      "Epoch 83/300\n",
      "Average training loss: 0.033828666946954196\n",
      "Average test loss: 0.006321294815589984\n",
      "Epoch 84/300\n",
      "Average training loss: 0.033781012624502185\n",
      "Average test loss: 0.006973431308236387\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03384609552224477\n",
      "Average test loss: 0.0066259200407399075\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033708322872718174\n",
      "Average test loss: 0.007297486926946375\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03368728312849999\n",
      "Average test loss: 0.006797126969115602\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033743791499071654\n",
      "Average test loss: 0.0064649988143808314\n",
      "Epoch 89/300\n",
      "Average training loss: 0.033607876713077224\n",
      "Average test loss: 0.00624704127965702\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03370715106858148\n",
      "Average test loss: 0.0064253466141720615\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03342531653576427\n",
      "Average test loss: 0.00616017317564951\n",
      "Epoch 92/300\n",
      "Average training loss: 0.033553594708442686\n",
      "Average test loss: 0.006431511892626683\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03341067863007387\n",
      "Average test loss: 0.006345659412857559\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03347297590970993\n",
      "Average test loss: 0.006350814902948009\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03342963321010272\n",
      "Average test loss: 0.007823470969994863\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03332093249426948\n",
      "Average test loss: 0.007319885059363312\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0332877828611268\n",
      "Average test loss: 0.00652385441131062\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033303341882096396\n",
      "Average test loss: 0.006640880528423521\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03332093984219763\n",
      "Average test loss: 0.006378718771040439\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03325396429830127\n",
      "Average test loss: 0.006493878714326355\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03312752077314589\n",
      "Average test loss: 0.006322743283377753\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03323131009936333\n",
      "Average test loss: 0.006178323213424947\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03308337803184986\n",
      "Average test loss: 0.006579688408308559\n",
      "Epoch 104/300\n",
      "Average training loss: 0.033161241451899214\n",
      "Average test loss: 0.00935858305543661\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03293588077359729\n",
      "Average test loss: 0.006524419573446114\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03303986261288325\n",
      "Average test loss: 0.006687348274721039\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03291341111063957\n",
      "Average test loss: 0.006229266189038753\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03301141838563813\n",
      "Average test loss: 0.006609209954738617\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03291869094967842\n",
      "Average test loss: 0.006736690726131201\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03280942604608006\n",
      "Average test loss: 0.0063964491672813895\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03282865240176519\n",
      "Average test loss: 0.006627518969691462\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03289733856254154\n",
      "Average test loss: 0.006443179231137037\n",
      "Epoch 113/300\n",
      "Average training loss: 0.032898956659767366\n",
      "Average test loss: 0.00619907829641468\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03272670396003458\n",
      "Average test loss: 0.0065832856910096275\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03269966699348556\n",
      "Average test loss: 0.006283375035143561\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0326165620221032\n",
      "Average test loss: 0.006346438804434406\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03262827742762036\n",
      "Average test loss: 0.006165784988966253\n",
      "Epoch 118/300\n",
      "Average training loss: 0.032649627692169614\n",
      "Average test loss: 0.006246972723760538\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03285473799539937\n",
      "Average test loss: 0.006919819227109353\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032615816411044864\n",
      "Average test loss: 0.006522946767508983\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03254383448428578\n",
      "Average test loss: 0.006361014200581445\n",
      "Epoch 122/300\n",
      "Average training loss: 0.032554179350535076\n",
      "Average test loss: 0.006296362364250753\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03253072277870443\n",
      "Average test loss: 0.00651206437456939\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03247856438656648\n",
      "Average test loss: 0.007624583675629563\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03238397733370463\n",
      "Average test loss: 0.006111931660936938\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03246729125910335\n",
      "Average test loss: 0.006453201171010733\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03245524928304884\n",
      "Average test loss: 0.006126640016833941\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03234066964520348\n",
      "Average test loss: 0.006163633481081989\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0323442682756318\n",
      "Average test loss: 0.006355675975067748\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03229948200451003\n",
      "Average test loss: 0.006342245121796926\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03227087447709508\n",
      "Average test loss: 0.0062673765350547105\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03227381697628233\n",
      "Average test loss: 0.006237774564988083\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0323060717218452\n",
      "Average test loss: 0.006190683744847774\n",
      "Epoch 134/300\n",
      "Average training loss: 0.032216990229156285\n",
      "Average test loss: 0.006471686772174305\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0322473655210601\n",
      "Average test loss: 0.006152487345039845\n",
      "Epoch 136/300\n",
      "Average training loss: 0.032245054446988636\n",
      "Average test loss: 0.006890567755533589\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03209884606467353\n",
      "Average test loss: 0.006482678898506695\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03206888790925344\n",
      "Average test loss: 0.0065792001725898846\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03212094694044855\n",
      "Average test loss: 0.006671655990183353\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03209842013981607\n",
      "Average test loss: 0.006323322153339784\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03209603624376986\n",
      "Average test loss: 0.0076226376249558395\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03210359364251296\n",
      "Average test loss: 0.006144304556979074\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031919142716460756\n",
      "Average test loss: 0.006200650535523891\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03199453836348322\n",
      "Average test loss: 0.006273432263483604\n",
      "Epoch 145/300\n",
      "Average training loss: 0.031925050285127425\n",
      "Average test loss: 0.006250843759212229\n",
      "Epoch 146/300\n",
      "Average training loss: 0.031993112080627016\n",
      "Average test loss: 0.006181205546276437\n",
      "Epoch 147/300\n",
      "Average training loss: 0.032003836664888596\n",
      "Average test loss: 0.006134288749761052\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03184096359875467\n",
      "Average test loss: 0.006183840908937984\n",
      "Epoch 149/300\n",
      "Average training loss: 0.031918767548269694\n",
      "Average test loss: 0.006256958660566144\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03190268644856082\n",
      "Average test loss: 0.006199791791538398\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03189357726938195\n",
      "Average test loss: 0.006318024069484738\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03185700600677066\n",
      "Average test loss: 0.006369069957898723\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03188584329353439\n",
      "Average test loss: 0.006640099146299892\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031716786780291134\n",
      "Average test loss: 0.006073472992827495\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03171328887674544\n",
      "Average test loss: 0.006325640052143071\n",
      "Epoch 156/300\n",
      "Average training loss: 0.031781450874275634\n",
      "Average test loss: 0.0068798474139637415\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03169740484158198\n",
      "Average test loss: 0.006349126939558321\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03182891768548224\n",
      "Average test loss: 0.0061291146278381345\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03183468389842245\n",
      "Average test loss: 0.0073878590597046745\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03167956591811445\n",
      "Average test loss: 0.0063010816098087365\n",
      "Epoch 161/300\n",
      "Average training loss: 0.031658594300349556\n",
      "Average test loss: 0.006235011305246088\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0317661305434174\n",
      "Average test loss: 0.006238168352180057\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0315252724621031\n",
      "Average test loss: 0.006384399440553453\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03161636369592614\n",
      "Average test loss: 0.006777947225918373\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03151750081612004\n",
      "Average test loss: 0.00635363281932142\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031580207222037845\n",
      "Average test loss: 0.006304636990858449\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03158149880998665\n",
      "Average test loss: 0.006410834865023692\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03154778547088305\n",
      "Average test loss: 0.00619907947546906\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03156834403011534\n",
      "Average test loss: 0.006231674069331752\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03153525984949536\n",
      "Average test loss: 0.006663013707018561\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03166533969839414\n",
      "Average test loss: 0.007959684147602982\n",
      "Epoch 172/300\n",
      "Average training loss: 0.031389082507954705\n",
      "Average test loss: 0.0062636517182820374\n",
      "Epoch 173/300\n",
      "Average training loss: 0.031445230753885375\n",
      "Average test loss: 0.006326054599550035\n",
      "Epoch 174/300\n",
      "Average training loss: 0.031533047942651646\n",
      "Average test loss: 0.006280385363433096\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03146834299961726\n",
      "Average test loss: 0.0062189255588584475\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03145455437898636\n",
      "Average test loss: 0.00636422157784303\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031496004359589685\n",
      "Average test loss: 0.006398576351503531\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0313426470624076\n",
      "Average test loss: 0.006329288349383407\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031392947988377676\n",
      "Average test loss: 0.006225513127528959\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03126914417081409\n",
      "Average test loss: 0.00622754588495526\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03138196622994211\n",
      "Average test loss: 0.006553169473591778\n",
      "Epoch 182/300\n",
      "Average training loss: 0.031337679227193195\n",
      "Average test loss: 0.007059277822367019\n",
      "Epoch 183/300\n",
      "Average training loss: 0.031265383478668\n",
      "Average test loss: 0.0065048837024304605\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03132618029912313\n",
      "Average test loss: 0.006171729325420327\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03133720046612951\n",
      "Average test loss: 0.006158810832020309\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03123951893217034\n",
      "Average test loss: 0.006935414573798577\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03123238290515211\n",
      "Average test loss: 0.006329716644353337\n",
      "Epoch 188/300\n",
      "Average training loss: 0.031211450778775744\n",
      "Average test loss: 0.006710651285118527\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03137605556845665\n",
      "Average test loss: 0.00625883878643314\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031146434823671975\n",
      "Average test loss: 0.0061328852669232424\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0312533976468775\n",
      "Average test loss: 0.006207829930716091\n",
      "Epoch 192/300\n",
      "Average training loss: 0.031199345093634392\n",
      "Average test loss: 0.006323331475257874\n",
      "Epoch 193/300\n",
      "Average training loss: 0.031143872804111905\n",
      "Average test loss: 0.006706584904756811\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03115633701947\n",
      "Average test loss: 0.006238837401900027\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031185868077807957\n",
      "Average test loss: 0.006521422698679898\n",
      "Epoch 196/300\n",
      "Average training loss: 0.031168475614653694\n",
      "Average test loss: 0.006287475461347235\n",
      "Epoch 197/300\n",
      "Average training loss: 0.031101407946811783\n",
      "Average test loss: 0.006537228145947059\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0310351718697283\n",
      "Average test loss: 0.006205094367265701\n",
      "Epoch 199/300\n",
      "Average training loss: 0.031112146433856753\n",
      "Average test loss: 0.00660478889859385\n",
      "Epoch 200/300\n",
      "Average training loss: 0.031162196268637976\n",
      "Average test loss: 0.006201341684079833\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031141364379061592\n",
      "Average test loss: 0.006283462632447481\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03106749775674608\n",
      "Average test loss: 0.006394313716226154\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03100462370779779\n",
      "Average test loss: 0.006161204711016681\n",
      "Epoch 204/300\n",
      "Average training loss: 0.031093449029657577\n",
      "Average test loss: 0.006793736469414499\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03100766795873642\n",
      "Average test loss: 0.006155207100427813\n",
      "Epoch 206/300\n",
      "Average training loss: 0.031031899622744986\n",
      "Average test loss: 0.006217991172439522\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030923700536290805\n",
      "Average test loss: 0.006368589104877578\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03092655588189761\n",
      "Average test loss: 0.006312545468823778\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03093593356344435\n",
      "Average test loss: 0.006312483004397816\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03089355090757211\n",
      "Average test loss: 0.0063194620609283445\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030948471797837152\n",
      "Average test loss: 0.006658782241659032\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030993698313832283\n",
      "Average test loss: 0.010455440376367834\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030933617916372086\n",
      "Average test loss: 0.006198811486363411\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030924295521444746\n",
      "Average test loss: 0.006264801729677452\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030848119787043996\n",
      "Average test loss: 0.006257499208052953\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030858492897616493\n",
      "Average test loss: 0.006199095695383019\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030882714865936172\n",
      "Average test loss: 0.006594244860526588\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03080989183485508\n",
      "Average test loss: 0.006463610901186864\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030791826711760627\n",
      "Average test loss: 0.011928455987738238\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03075491378042433\n",
      "Average test loss: 0.006508493757496277\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03079516656531228\n",
      "Average test loss: 0.006304691077106529\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030868467504779496\n",
      "Average test loss: 0.007277609242747228\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03081090572476387\n",
      "Average test loss: 0.006290640936129623\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03075504031115108\n",
      "Average test loss: 0.006163323187579711\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03074568098286788\n",
      "Average test loss: 0.00649822370418244\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03072258460852835\n",
      "Average test loss: 0.006417956343127622\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03076178130176332\n",
      "Average test loss: 0.007281665748192204\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03068763819668028\n",
      "Average test loss: 0.006289688838024934\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030639644553263984\n",
      "Average test loss: 0.006385550907088651\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03065533923771646\n",
      "Average test loss: 0.006383742726097505\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030709171462390158\n",
      "Average test loss: 0.006578362455384599\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03069444639649656\n",
      "Average test loss: 0.006256288934085104\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03065514924459987\n",
      "Average test loss: 0.006182991459137864\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0306386623316341\n",
      "Average test loss: 0.006345053128484222\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030664860818121167\n",
      "Average test loss: 0.006646827802062035\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030569254802332985\n",
      "Average test loss: 0.006407394936515225\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03059431974755393\n",
      "Average test loss: 0.006451601507763068\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030579130348232057\n",
      "Average test loss: 0.0063709115899271435\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030610918432474137\n",
      "Average test loss: 0.006530551618999905\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030513710538546245\n",
      "Average test loss: 0.006432249429325263\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03052429682181941\n",
      "Average test loss: 0.006775086469948292\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030548154873980417\n",
      "Average test loss: 0.006303848761651251\n",
      "Epoch 243/300\n",
      "Average training loss: 0.030559689594639672\n",
      "Average test loss: 0.007995968081057072\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030499312808116276\n",
      "Average test loss: 0.006242470479673809\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030492220731245148\n",
      "Average test loss: 0.006386578670392434\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030414987719721263\n",
      "Average test loss: 0.006243731916778617\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030483118017514545\n",
      "Average test loss: 0.006441776520262162\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03048984455400043\n",
      "Average test loss: 0.006336725834343169\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03045840503440963\n",
      "Average test loss: 0.006350486907694075\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030504545620746084\n",
      "Average test loss: 0.006612180296331644\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030482644826173782\n",
      "Average test loss: 0.006275260298823317\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03037643479141924\n",
      "Average test loss: 0.00866528199447526\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030336682544814215\n",
      "Average test loss: 0.006456492862353723\n",
      "Epoch 254/300\n",
      "Average training loss: 0.030349386304616928\n",
      "Average test loss: 0.006358559632052978\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03038214113480515\n",
      "Average test loss: 0.00640814213703076\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030393284791045718\n",
      "Average test loss: 0.00636580203866793\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030375494168864356\n",
      "Average test loss: 0.006568685852819019\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030481989345616765\n",
      "Average test loss: 0.006579279312656986\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030349952853388255\n",
      "Average test loss: 0.0062226339290953345\n",
      "Epoch 260/300\n",
      "Average training loss: 0.030313211537069746\n",
      "Average test loss: 0.007195944659411907\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030381880929072697\n",
      "Average test loss: 0.006273014532195197\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030293835666444568\n",
      "Average test loss: 0.0062646662568052605\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030322950181033877\n",
      "Average test loss: 0.006230488872362508\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03026140657067299\n",
      "Average test loss: 0.006385056385149558\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030278571160303223\n",
      "Average test loss: 0.0062916871760454445\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03033959951168961\n",
      "Average test loss: 0.006346040673967865\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030273001288374266\n",
      "Average test loss: 0.006340246831377347\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030253667703933187\n",
      "Average test loss: 0.006167072587956985\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03026653195420901\n",
      "Average test loss: 0.00804696972999308\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030220445411072838\n",
      "Average test loss: 0.006315466793460979\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030313530534505844\n",
      "Average test loss: 0.006353807888511155\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03015410245127148\n",
      "Average test loss: 0.006603844712177912\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03033048512869411\n",
      "Average test loss: 0.006465105279452271\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030146609698732695\n",
      "Average test loss: 0.006434741542157199\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030141294962830013\n",
      "Average test loss: 0.006690332670178678\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03023310744596852\n",
      "Average test loss: 0.0062763911307685905\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030213744584057067\n",
      "Average test loss: 0.006255097950084342\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03016494650642077\n",
      "Average test loss: 0.006502293600804276\n",
      "Epoch 279/300\n",
      "Average training loss: 0.030309928718540404\n",
      "Average test loss: 0.006509783906655179\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03018088309466839\n",
      "Average test loss: 0.006524554122653272\n",
      "Epoch 281/300\n",
      "Average training loss: 0.030100166590677366\n",
      "Average test loss: 0.006450135805540614\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030147612866428163\n",
      "Average test loss: 0.00638548228972488\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030153047452370324\n",
      "Average test loss: 0.0063063894088069596\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030141339510679245\n",
      "Average test loss: 0.006395583678450849\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030159127642711003\n",
      "Average test loss: 0.006303479163597027\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03012116563816865\n",
      "Average test loss: 0.0066226843972173\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03010657769607173\n",
      "Average test loss: 0.008234450050526195\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030162953307231267\n",
      "Average test loss: 0.006275997422221634\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030157356417841383\n",
      "Average test loss: 0.006272476555986537\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030054626488023335\n",
      "Average test loss: 0.00618854993664556\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0300778359108501\n",
      "Average test loss: 0.006236779510147041\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030054365320338144\n",
      "Average test loss: 0.007154884322649903\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02996514016389847\n",
      "Average test loss: 0.0069461219492885804\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030043064186970392\n",
      "Average test loss: 0.0062900259780387085\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030069376261697874\n",
      "Average test loss: 0.006358168081276947\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03002218967510594\n",
      "Average test loss: 0.006297878485172987\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029965088246597184\n",
      "Average test loss: 0.006303785070776939\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029994199954801137\n",
      "Average test loss: 0.006289023230887121\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0300872978899214\n",
      "Average test loss: 0.006208985344817241\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02999949674970574\n",
      "Average test loss: 0.006342775963660743\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13250546131862534\n",
      "Average test loss: 0.012195253468222087\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05377941040198008\n",
      "Average test loss: 0.010709126902951135\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04919016491042243\n",
      "Average test loss: 0.0077068141450484596\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04700139437450303\n",
      "Average test loss: 0.008897412307560445\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04490885846482383\n",
      "Average test loss: 0.007168152603010337\n",
      "Epoch 6/300\n",
      "Average training loss: 0.042446419149637225\n",
      "Average test loss: 0.005767915101928843\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04164324035247167\n",
      "Average test loss: 0.005964436399853892\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03977597971757253\n",
      "Average test loss: 0.006274922578699059\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03868254103263219\n",
      "Average test loss: 0.005636835689345996\n",
      "Epoch 10/300\n",
      "Average training loss: 0.037484095205863315\n",
      "Average test loss: 0.0065673654476801555\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03602436468833023\n",
      "Average test loss: 0.005768697162055307\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0355947382317649\n",
      "Average test loss: 0.005621936350233025\n",
      "Epoch 13/300\n",
      "Average training loss: 0.034359346369902295\n",
      "Average test loss: 0.006953802375743786\n",
      "Epoch 14/300\n",
      "Average training loss: 0.033883976257509656\n",
      "Average test loss: 0.005986504616008865\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03345091458161672\n",
      "Average test loss: 0.006304461856683095\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03236283941732512\n",
      "Average test loss: 0.005308954255034526\n",
      "Epoch 17/300\n",
      "Average training loss: 0.031851432694329156\n",
      "Average test loss: 0.005526964777459701\n",
      "Epoch 18/300\n",
      "Average training loss: 0.031193239614367485\n",
      "Average test loss: 0.00504427705746558\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03128995507293277\n",
      "Average test loss: 0.005917737659066916\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03082657426264551\n",
      "Average test loss: 0.005725751321348879\n",
      "Epoch 21/300\n",
      "Average training loss: 0.030259622539083163\n",
      "Average test loss: 0.005042088100479709\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02995030115213659\n",
      "Average test loss: 0.006101805188589626\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029840724544392693\n",
      "Average test loss: 0.005064800767228007\n",
      "Epoch 24/300\n",
      "Average training loss: 0.029966287705633376\n",
      "Average test loss: 0.004615248118837675\n",
      "Epoch 25/300\n",
      "Average training loss: 0.029044430315494536\n",
      "Average test loss: 0.005282181309329139\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02913090381688542\n",
      "Average test loss: 0.006228290262735552\n",
      "Epoch 27/300\n",
      "Average training loss: 0.029254027932882308\n",
      "Average test loss: 0.006190892957978779\n",
      "Epoch 28/300\n",
      "Average training loss: 0.028404334917664528\n",
      "Average test loss: 0.004565578978922632\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02840260159638193\n",
      "Average test loss: 0.005070794345190127\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028480338056882222\n",
      "Average test loss: 0.0049017453599307275\n",
      "Epoch 31/300\n",
      "Average training loss: 0.027977231375045247\n",
      "Average test loss: 0.004608515741717484\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0280046944303645\n",
      "Average test loss: 0.004429957601138287\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02775068289372656\n",
      "Average test loss: 0.004660658836985628\n",
      "Epoch 34/300\n",
      "Average training loss: 0.027437426782316632\n",
      "Average test loss: 0.004553468552728494\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027653571077518993\n",
      "Average test loss: 0.004523184072019325\n",
      "Epoch 36/300\n",
      "Average training loss: 0.027165481977992588\n",
      "Average test loss: 0.004705195713788271\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026964736983180047\n",
      "Average test loss: 0.004898814019229677\n",
      "Epoch 38/300\n",
      "Average training loss: 0.026938966292474005\n",
      "Average test loss: 0.02063859954973062\n",
      "Epoch 39/300\n",
      "Average training loss: 0.026818957759274377\n",
      "Average test loss: 0.005643524563560883\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026676361108819643\n",
      "Average test loss: 0.00507451673100392\n",
      "Epoch 41/300\n",
      "Average training loss: 0.026554059414399995\n",
      "Average test loss: 0.004342987973036038\n",
      "Epoch 42/300\n",
      "Average training loss: 0.026661081857151454\n",
      "Average test loss: 0.006628665364450879\n",
      "Epoch 43/300\n",
      "Average training loss: 0.026354427367448807\n",
      "Average test loss: 0.004779987104651001\n",
      "Epoch 44/300\n",
      "Average training loss: 0.026324819488657846\n",
      "Average test loss: 0.006700266706032886\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02611756312350432\n",
      "Average test loss: 0.005010155872752269\n",
      "Epoch 46/300\n",
      "Average training loss: 0.026152961298823357\n",
      "Average test loss: 0.0047959723331862025\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025959571518831784\n",
      "Average test loss: 0.00422109992057085\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02581095599797037\n",
      "Average test loss: 0.004191550639561481\n",
      "Epoch 49/300\n",
      "Average training loss: 0.025796626589364477\n",
      "Average test loss: 0.006048057816094823\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025967272938953505\n",
      "Average test loss: 0.004123450298276213\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025637908023264672\n",
      "Average test loss: 0.00451325332125028\n",
      "Epoch 52/300\n",
      "Average training loss: 0.025612146076228883\n",
      "Average test loss: 0.004683700459698836\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02540534310705132\n",
      "Average test loss: 0.007736486179547177\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025484211227960057\n",
      "Average test loss: 0.004454200142787562\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025506685352987713\n",
      "Average test loss: 0.004494767900970247\n",
      "Epoch 56/300\n",
      "Average training loss: 0.025294853041569393\n",
      "Average test loss: 0.003908811170193884\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025360657662153244\n",
      "Average test loss: 0.003948875441733334\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02528095888925923\n",
      "Average test loss: 0.004093846393956079\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02513145022590955\n",
      "Average test loss: 0.004223362452454037\n",
      "Epoch 60/300\n",
      "Average training loss: 0.025234222571055093\n",
      "Average test loss: 0.004573893205573162\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024984703567292956\n",
      "Average test loss: 0.004091975308954715\n",
      "Epoch 62/300\n",
      "Average training loss: 0.024981314518385464\n",
      "Average test loss: 0.0041888170544472\n",
      "Epoch 63/300\n",
      "Average training loss: 0.025014617093735272\n",
      "Average test loss: 0.004594370139969719\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024874166135986645\n",
      "Average test loss: 0.004840736192961534\n",
      "Epoch 65/300\n",
      "Average training loss: 0.025094179804126423\n",
      "Average test loss: 0.006460114124748442\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02488748421271642\n",
      "Average test loss: 0.004000786290400558\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024713466150893107\n",
      "Average test loss: 0.004013857886609104\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024846312643753157\n",
      "Average test loss: 0.0040442524728261766\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024637067806389596\n",
      "Average test loss: 0.004027948879533344\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024661424706379574\n",
      "Average test loss: 0.004079070092696283\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024758062220282026\n",
      "Average test loss: 0.004232895084967216\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024804673252834215\n",
      "Average test loss: 0.004784428803457154\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024459242100516955\n",
      "Average test loss: 0.004637044899165631\n",
      "Epoch 74/300\n",
      "Average training loss: 0.024578089952468872\n",
      "Average test loss: 0.004235097807728582\n",
      "Epoch 75/300\n",
      "Average training loss: 0.024464536292685402\n",
      "Average test loss: 0.003914791705500748\n",
      "Epoch 76/300\n",
      "Average training loss: 0.024388440812627473\n",
      "Average test loss: 0.00416266578125457\n",
      "Epoch 77/300\n",
      "Average training loss: 0.024464354857802392\n",
      "Average test loss: 0.0040753096118569376\n",
      "Epoch 78/300\n",
      "Average training loss: 0.024278494480583404\n",
      "Average test loss: 0.004112241080651681\n",
      "Epoch 79/300\n",
      "Average training loss: 0.024387982298930487\n",
      "Average test loss: 0.005338513726161586\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02443270117541154\n",
      "Average test loss: 0.00389291273140245\n",
      "Epoch 81/300\n",
      "Average training loss: 0.024388450355993376\n",
      "Average test loss: 0.004079650288033816\n",
      "Epoch 82/300\n",
      "Average training loss: 0.024123419721921283\n",
      "Average test loss: 0.0038551527849502035\n",
      "Epoch 83/300\n",
      "Average training loss: 0.024041140635808308\n",
      "Average test loss: 0.004459172584944301\n",
      "Epoch 84/300\n",
      "Average training loss: 0.024147162699037127\n",
      "Average test loss: 0.004651749758463767\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02433786246014966\n",
      "Average test loss: 0.004204105499717925\n",
      "Epoch 86/300\n",
      "Average training loss: 0.024052260107464262\n",
      "Average test loss: 0.004268896482470963\n",
      "Epoch 87/300\n",
      "Average training loss: 0.024013400713602702\n",
      "Average test loss: 0.003999567756222354\n",
      "Epoch 88/300\n",
      "Average training loss: 0.024060312158531612\n",
      "Average test loss: 0.004097958879545331\n",
      "Epoch 89/300\n",
      "Average training loss: 0.024027117864953148\n",
      "Average test loss: 0.0039323450198604\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023940048035648136\n",
      "Average test loss: 0.003985846008691523\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02397254716687732\n",
      "Average test loss: 0.004283098727257715\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02396822358171145\n",
      "Average test loss: 0.004400596035851373\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023891693745222358\n",
      "Average test loss: 0.004354260884225369\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02383000701997015\n",
      "Average test loss: 0.00404488445031974\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02389935321940316\n",
      "Average test loss: 0.00506984405592084\n",
      "Epoch 96/300\n",
      "Average training loss: 0.023813184824254777\n",
      "Average test loss: 0.004039723800909188\n",
      "Epoch 97/300\n",
      "Average training loss: 0.023855553762780295\n",
      "Average test loss: 0.004086595268713103\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023801218094097243\n",
      "Average test loss: 0.003963475627203782\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023773700613114568\n",
      "Average test loss: 0.004086990461581283\n",
      "Epoch 100/300\n",
      "Average training loss: 0.023746483503116502\n",
      "Average test loss: 0.004192395062082343\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02367194130520026\n",
      "Average test loss: 0.003971181106236246\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02371992486880885\n",
      "Average test loss: 0.0039827700004809435\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02376889446708891\n",
      "Average test loss: 0.003958071371747388\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023796802308824326\n",
      "Average test loss: 0.004007519039428896\n",
      "Epoch 105/300\n",
      "Average training loss: 0.023542326990101073\n",
      "Average test loss: 0.004047853651146094\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023660321541958386\n",
      "Average test loss: 0.00392397927865386\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023505432699289588\n",
      "Average test loss: 0.004204920077696442\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023513970406519043\n",
      "Average test loss: 0.004206457664362258\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02365228491855992\n",
      "Average test loss: 0.004017761116226514\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023537082579400806\n",
      "Average test loss: 0.003971488378528091\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02343876565992832\n",
      "Average test loss: 0.004179671063398322\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02337009332080682\n",
      "Average test loss: 0.003896494086417887\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023594605525334676\n",
      "Average test loss: 0.004135305157138241\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02356746318605211\n",
      "Average test loss: 0.004045707198066844\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023545041085945236\n",
      "Average test loss: 0.004435862595836322\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02332287736237049\n",
      "Average test loss: 0.0038452825335164864\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02331888280974494\n",
      "Average test loss: 0.0044810867533087734\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023451978431807623\n",
      "Average test loss: 0.0038740263676477803\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023331999644637107\n",
      "Average test loss: 0.003915762967740496\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023412229294578233\n",
      "Average test loss: 0.004315528258681298\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023294290794266596\n",
      "Average test loss: 0.005077814076923662\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023308720861872037\n",
      "Average test loss: 0.0041799929874638715\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023388027704424327\n",
      "Average test loss: 0.0039243941468497115\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023262926833497153\n",
      "Average test loss: 0.004064537829822964\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023226404796044032\n",
      "Average test loss: 0.003883707718923688\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02323091446194384\n",
      "Average test loss: 0.0044640248094995815\n",
      "Epoch 127/300\n",
      "Average training loss: 0.023174722636739413\n",
      "Average test loss: 0.0039046942049430476\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023176331887642544\n",
      "Average test loss: 0.003927916308864951\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023201434100667634\n",
      "Average test loss: 0.0038546168095328743\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02321035254498323\n",
      "Average test loss: 0.00889352655162414\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023100698999232717\n",
      "Average test loss: 0.005365030096636878\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023197902219163048\n",
      "Average test loss: 0.0038459587732536924\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02311427637603548\n",
      "Average test loss: 0.0042911449536267255\n",
      "Epoch 134/300\n",
      "Average training loss: 0.023002731937501165\n",
      "Average test loss: 0.004034352895700269\n",
      "Epoch 135/300\n",
      "Average training loss: 0.023132244237595134\n",
      "Average test loss: 0.004077007943557368\n",
      "Epoch 136/300\n",
      "Average training loss: 0.023103267022305064\n",
      "Average test loss: 0.004410122011270788\n",
      "Epoch 137/300\n",
      "Average training loss: 0.023004014329777823\n",
      "Average test loss: 0.0038635083494914903\n",
      "Epoch 138/300\n",
      "Average training loss: 0.023121340091029802\n",
      "Average test loss: 0.003943883991903729\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022970725307861962\n",
      "Average test loss: 0.00411031764642232\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022961200993922022\n",
      "Average test loss: 0.0038621364645659924\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02296383999950356\n",
      "Average test loss: 0.0042492391727864745\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02303820579747359\n",
      "Average test loss: 0.004771147932857275\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022949326728781066\n",
      "Average test loss: 0.00391026708111167\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022859550045596227\n",
      "Average test loss: 0.003875926593732503\n",
      "Epoch 145/300\n",
      "Average training loss: 0.023040204818050067\n",
      "Average test loss: 0.0048053076362444295\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02299151052865717\n",
      "Average test loss: 0.0039934876167939766\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02290547726220555\n",
      "Average test loss: 0.0040138188139018085\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022881380958689582\n",
      "Average test loss: 0.003931268604265319\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022969155197342238\n",
      "Average test loss: 0.004142197295609448\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022705180644989012\n",
      "Average test loss: 0.00387534970872932\n",
      "Epoch 151/300\n",
      "Average training loss: 0.022961562337146864\n",
      "Average test loss: 0.003986691761554943\n",
      "Epoch 152/300\n",
      "Average training loss: 0.022793130429254637\n",
      "Average test loss: 0.0039037314156691234\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022700099491410785\n",
      "Average test loss: 0.00388458071090281\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022884782504704264\n",
      "Average test loss: 0.004012422760741578\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02273253481255637\n",
      "Average test loss: 0.00387854881832997\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022690843982828987\n",
      "Average test loss: 0.004238474157328407\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022850809676779642\n",
      "Average test loss: 0.00381561454364823\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02269007754822572\n",
      "Average test loss: 0.004913137215707037\n",
      "Epoch 159/300\n",
      "Average training loss: 0.022807677768998676\n",
      "Average test loss: 0.003995360335128175\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02274346460898717\n",
      "Average test loss: 0.00460092791583803\n",
      "Epoch 161/300\n",
      "Average training loss: 0.022786874355541335\n",
      "Average test loss: 0.005594791287763252\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02271649602221118\n",
      "Average test loss: 0.0038657710775732994\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02266864787042141\n",
      "Average test loss: 0.003987824956369069\n",
      "Epoch 164/300\n",
      "Average training loss: 0.022649282950494024\n",
      "Average test loss: 0.003968771827717622\n",
      "Epoch 165/300\n",
      "Average training loss: 0.022612647891872457\n",
      "Average test loss: 0.0039029489778396155\n",
      "Epoch 166/300\n",
      "Average training loss: 0.022657427108950086\n",
      "Average test loss: 0.004097611562659343\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02254827138947116\n",
      "Average test loss: 0.00388517001685169\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022620637539360257\n",
      "Average test loss: 0.003852066976328691\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022560964289638732\n",
      "Average test loss: 0.004008291250715653\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022647265114718013\n",
      "Average test loss: 0.0038548857383430006\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022617338490155008\n",
      "Average test loss: 0.0038416116260406043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.022589111949006715\n",
      "Average test loss: 0.004237726615327928\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0226561365607712\n",
      "Average test loss: 0.004079864310721556\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02254307261440489\n",
      "Average test loss: 0.004130679603873028\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02253170136941804\n",
      "Average test loss: 0.003909159691590402\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02252263799806436\n",
      "Average test loss: 0.00464488890974058\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022489203876919217\n",
      "Average test loss: 0.011072433011399376\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022494964536693363\n",
      "Average test loss: 0.00390730640416344\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022489831684364213\n",
      "Average test loss: 0.003987562494973342\n",
      "Epoch 180/300\n",
      "Average training loss: 0.022561428741448456\n",
      "Average test loss: 0.003996754164083137\n",
      "Epoch 181/300\n",
      "Average training loss: 0.022616399954590534\n",
      "Average test loss: 0.0038687363146907754\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02244333592057228\n",
      "Average test loss: 0.004080063550422589\n",
      "Epoch 183/300\n",
      "Average training loss: 0.022432430798808734\n",
      "Average test loss: 0.004328807150324186\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022491173739234607\n",
      "Average test loss: 0.003867845359982716\n",
      "Epoch 185/300\n",
      "Average training loss: 0.022383636865350935\n",
      "Average test loss: 0.00977421918263038\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02243775118390719\n",
      "Average test loss: 0.0043626188056336505\n",
      "Epoch 187/300\n",
      "Average training loss: 0.022336647644639017\n",
      "Average test loss: 0.004010496281501319\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022309114181333117\n",
      "Average test loss: 0.003933762187966042\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02247134112815062\n",
      "Average test loss: 0.004220772972951333\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02236985698176755\n",
      "Average test loss: 0.004372734183652534\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02228917152186235\n",
      "Average test loss: 0.004185078001684613\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022383142646816043\n",
      "Average test loss: 0.003911867952595154\n",
      "Epoch 193/300\n",
      "Average training loss: 0.022400924060079788\n",
      "Average test loss: 0.0038572328094806938\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022505893508593243\n",
      "Average test loss: 0.004312783990883165\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02237417560319106\n",
      "Average test loss: 0.004028741343153848\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022366774004366662\n",
      "Average test loss: 0.003924406179123455\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022360718844665423\n",
      "Average test loss: 0.004122764664805598\n",
      "Epoch 198/300\n",
      "Average training loss: 0.022213829626639685\n",
      "Average test loss: 0.0039930427500771155\n",
      "Epoch 199/300\n",
      "Average training loss: 0.022252330801553196\n",
      "Average test loss: 0.004391061264193721\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0221848582956526\n",
      "Average test loss: 0.004045233366390069\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022405003059241507\n",
      "Average test loss: 0.004131253185785479\n",
      "Epoch 202/300\n",
      "Average training loss: 0.022267536313997374\n",
      "Average test loss: 0.004032666688991917\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022351289580265683\n",
      "Average test loss: 0.004184190892924865\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02222458894385232\n",
      "Average test loss: 0.004084080358553264\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02226155915028519\n",
      "Average test loss: 0.004013263917424612\n",
      "Epoch 206/300\n",
      "Average training loss: 0.022358832627534866\n",
      "Average test loss: 0.004161669361094634\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022415021868215666\n",
      "Average test loss: 0.004064333465364244\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022099796916047732\n",
      "Average test loss: 0.003980052906192012\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0221945069713725\n",
      "Average test loss: 0.004046456437144015\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02225452156530486\n",
      "Average test loss: 0.0046196964917083585\n",
      "Epoch 211/300\n",
      "Average training loss: 0.022106081089211835\n",
      "Average test loss: 0.004232751097530126\n",
      "Epoch 212/300\n",
      "Average training loss: 0.022096632460753125\n",
      "Average test loss: 0.003792837732575006\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022181895838843453\n",
      "Average test loss: 0.00489752033042411\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02219638707737128\n",
      "Average test loss: 0.004698425523108906\n",
      "Epoch 215/300\n",
      "Average training loss: 0.022173886054091985\n",
      "Average test loss: 0.003992320224642754\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02208753607670466\n",
      "Average test loss: 0.004016427409317758\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022172141667869356\n",
      "Average test loss: 0.003811444350414806\n",
      "Epoch 218/300\n",
      "Average training loss: 0.022151001940998765\n",
      "Average test loss: 0.004048553885064191\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0220909254749616\n",
      "Average test loss: 0.004253929630749755\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02207646542787552\n",
      "Average test loss: 0.0039018732965406446\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02209223074879911\n",
      "Average test loss: 0.003839743946161535\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022070811387565403\n",
      "Average test loss: 0.0039055218106756606\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022176344157920944\n",
      "Average test loss: 0.003855534865624375\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022046241357922554\n",
      "Average test loss: 0.003816776674861709\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022090729273027843\n",
      "Average test loss: 0.006664464307328065\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021991139620542527\n",
      "Average test loss: 0.003984889185676972\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022114122374190224\n",
      "Average test loss: 0.004843868455125226\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02210008404486709\n",
      "Average test loss: 0.0037973204511735172\n",
      "Epoch 229/300\n",
      "Average training loss: 0.022029068378110726\n",
      "Average test loss: 0.004085453649154968\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02201690667039818\n",
      "Average test loss: 0.003964187143991391\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022047382213175298\n",
      "Average test loss: 0.0038837134424183103\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022074139426151912\n",
      "Average test loss: 0.003868478949078255\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022014662659830517\n",
      "Average test loss: 0.0038369895894494323\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021926147972544035\n",
      "Average test loss: 0.0039566681115991535\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02205899306635062\n",
      "Average test loss: 0.004087050498566694\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02192540525396665\n",
      "Average test loss: 0.003920937173689405\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02195830566684405\n",
      "Average test loss: 0.0037981387809332876\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022005594182345604\n",
      "Average test loss: 0.0038991850451048876\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02193584783375263\n",
      "Average test loss: 0.003961125509192546\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02194988029367394\n",
      "Average test loss: 0.0038222549652887714\n",
      "Epoch 241/300\n",
      "Average training loss: 0.021923743918538095\n",
      "Average test loss: 0.004030120365528597\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02191659915447235\n",
      "Average test loss: 0.0038626848666204346\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02190959324936072\n",
      "Average test loss: 0.004059172868521677\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021993024306164847\n",
      "Average test loss: 0.004802374100933472\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02187569710612297\n",
      "Average test loss: 0.003814132875038518\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021864760152995585\n",
      "Average test loss: 0.0038877752386033537\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02192203856839074\n",
      "Average test loss: 0.004054085329174995\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02186072144905726\n",
      "Average test loss: 0.003974137004464865\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021930023077461454\n",
      "Average test loss: 0.003853508248511288\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021890227569474115\n",
      "Average test loss: 0.004325292128241724\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02188654097583559\n",
      "Average test loss: 0.003993971546697948\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0218176877895991\n",
      "Average test loss: 0.0039754246498147645\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02187491754939159\n",
      "Average test loss: 0.004078423743446668\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021811203102270763\n",
      "Average test loss: 0.0039049870177275603\n",
      "Epoch 255/300\n",
      "Average training loss: 0.021901569224066206\n",
      "Average test loss: 0.003965490496406952\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021799861806962226\n",
      "Average test loss: 0.004101399558285872\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021797376410828698\n",
      "Average test loss: 0.00391181294951174\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02192495880358749\n",
      "Average test loss: 0.0040978749559985265\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021803613416022726\n",
      "Average test loss: 0.003993062224239111\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021736405000090598\n",
      "Average test loss: 0.003837935532339745\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021813113820221688\n",
      "Average test loss: 0.0044114115020881094\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02177406285703182\n",
      "Average test loss: 0.004389461352593369\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02191789149575763\n",
      "Average test loss: 0.00413844713423815\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02178253286414676\n",
      "Average test loss: 0.0045818562313086455\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021708491176366806\n",
      "Average test loss: 0.0050466642868187694\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021778060629963875\n",
      "Average test loss: 0.0039035388835602335\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02172320984966225\n",
      "Average test loss: 0.004131463427096606\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021734451161490545\n",
      "Average test loss: 0.00445404848166638\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021769821473293833\n",
      "Average test loss: 0.0038998158135347895\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021708404453264343\n",
      "Average test loss: 0.004451600200186173\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021660970096786816\n",
      "Average test loss: 0.00419574169235097\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02168620493180222\n",
      "Average test loss: 0.0037913804521991145\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021730189207527374\n",
      "Average test loss: 0.0038620621307442586\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021771398329072527\n",
      "Average test loss: 0.003871189436978764\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02164489702714814\n",
      "Average test loss: 0.004397989550398456\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021692948402629957\n",
      "Average test loss: 0.0038073460232052538\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021742046187321343\n",
      "Average test loss: 0.003872520912438631\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021681209976474443\n",
      "Average test loss: 0.003887306377912561\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02170810871819655\n",
      "Average test loss: 0.003982374296006229\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021675011850065654\n",
      "Average test loss: 0.003926758053816027\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021641335614853437\n",
      "Average test loss: 0.00393281953699059\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02179574148191346\n",
      "Average test loss: 0.0038364520085354646\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021600585854715772\n",
      "Average test loss: 0.00406654688095053\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021596600774261687\n",
      "Average test loss: 0.003929423144087196\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021662608068850304\n",
      "Average test loss: 0.0038322387679169574\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021649062944783106\n",
      "Average test loss: 0.0038381523489952087\n",
      "Epoch 287/300\n",
      "Average training loss: 0.021621057021949025\n",
      "Average test loss: 0.004002114064887994\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02158421761294206\n",
      "Average test loss: 0.003884970930715402\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02163157046503491\n",
      "Average test loss: 0.008357799922426542\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021643321428034042\n",
      "Average test loss: 0.005989918514465292\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02177991421189573\n",
      "Average test loss: 0.005215024405883418\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021633055799537233\n",
      "Average test loss: 0.004101319620178806\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021558232266041966\n",
      "Average test loss: 0.004205245789554384\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021515591197543673\n",
      "Average test loss: 0.0038892769399616455\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021634784680273796\n",
      "Average test loss: 0.003906845840728946\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02162832800216145\n",
      "Average test loss: 0.00425227056360907\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02152293978797065\n",
      "Average test loss: 0.0038757080272254015\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02161636977725559\n",
      "Average test loss: 0.0038880574432098203\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021594252932402823\n",
      "Average test loss: 0.003977061673170991\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02151915726562341\n",
      "Average test loss: 0.003972998104575608\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11855898125304117\n",
      "Average test loss: 0.007542951242791282\n",
      "Epoch 2/300\n",
      "Average training loss: 0.046428085009257\n",
      "Average test loss: 0.007289117722875542\n",
      "Epoch 3/300\n",
      "Average training loss: 0.041628019356065325\n",
      "Average test loss: 0.005753804222577148\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03938607246677081\n",
      "Average test loss: 0.006940837180034982\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03771266181601418\n",
      "Average test loss: 0.006148272857897812\n",
      "Epoch 6/300\n",
      "Average training loss: 0.034555688536829415\n",
      "Average test loss: 0.005216170593682262\n",
      "Epoch 7/300\n",
      "Average training loss: 0.033639375173383286\n",
      "Average test loss: 0.005774908528145817\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03191678415404426\n",
      "Average test loss: 0.0047371975371821055\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03052802535560396\n",
      "Average test loss: 0.005199073607308997\n",
      "Epoch 10/300\n",
      "Average training loss: 0.030031604054901333\n",
      "Average test loss: 0.004789340453015434\n",
      "Epoch 11/300\n",
      "Average training loss: 0.028878830171293682\n",
      "Average test loss: 0.007741590647647778\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02797156549493472\n",
      "Average test loss: 0.005617539195550813\n",
      "Epoch 13/300\n",
      "Average training loss: 0.027081221615274747\n",
      "Average test loss: 0.0037757481404890616\n",
      "Epoch 14/300\n",
      "Average training loss: 0.026530501044458813\n",
      "Average test loss: 0.0055558895568052925\n",
      "Epoch 15/300\n",
      "Average training loss: 0.026174399786525304\n",
      "Average test loss: 0.004435710425592131\n",
      "Epoch 16/300\n",
      "Average training loss: 0.025491293064422077\n",
      "Average test loss: 0.004083704758021567\n",
      "Epoch 17/300\n",
      "Average training loss: 0.025107234908474815\n",
      "Average test loss: 0.0034798471722751858\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024947631822692024\n",
      "Average test loss: 0.003389425522337357\n",
      "Epoch 19/300\n",
      "Average training loss: 0.024475597365034952\n",
      "Average test loss: 0.006106964892811245\n",
      "Epoch 20/300\n",
      "Average training loss: 0.024233770394490824\n",
      "Average test loss: 0.003712088517430756\n",
      "Epoch 21/300\n",
      "Average training loss: 0.023876619794302518\n",
      "Average test loss: 0.004634273505045308\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02368904643257459\n",
      "Average test loss: 0.003680299366513888\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02336306064824263\n",
      "Average test loss: 0.0038189773071143364\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02322437909907765\n",
      "Average test loss: 0.0036114048881249294\n",
      "Epoch 25/300\n",
      "Average training loss: 0.022723406963878208\n",
      "Average test loss: 0.004060820467563139\n",
      "Epoch 26/300\n",
      "Average training loss: 0.022998972674210867\n",
      "Average test loss: 0.003932289129743974\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02242825295858913\n",
      "Average test loss: 0.0035898370121916134\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02242768227391773\n",
      "Average test loss: 0.004002405256239904\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02244010741677549\n",
      "Average test loss: 0.003281502383864588\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02204825727807151\n",
      "Average test loss: 0.0032344834974242583\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02188825480474366\n",
      "Average test loss: 0.0031810337127082876\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021665923294093874\n",
      "Average test loss: 0.0033327738551629914\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02193006680905819\n",
      "Average test loss: 0.0032997921084364254\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021822400917609532\n",
      "Average test loss: 0.003660284077541696\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02147021599776215\n",
      "Average test loss: 0.0037103355129559836\n",
      "Epoch 36/300\n",
      "Average training loss: 0.021447626903653146\n",
      "Average test loss: 0.00539565944713023\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021406851660874155\n",
      "Average test loss: 0.0032419237279229696\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0212442098673847\n",
      "Average test loss: 0.0033719035407735244\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021226870758665934\n",
      "Average test loss: 0.0030714529388480715\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02093550145212147\n",
      "Average test loss: 0.0034423495911889606\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02106539779322015\n",
      "Average test loss: 0.003063540102707015\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020927872914406988\n",
      "Average test loss: 0.0030166940738757453\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02093649260368612\n",
      "Average test loss: 0.00308365204392208\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02082906847033236\n",
      "Average test loss: 0.003819888741398851\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020856079694297577\n",
      "Average test loss: 0.0043426639160348305\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020678916225830714\n",
      "Average test loss: 0.003032888320171171\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020702733155753876\n",
      "Average test loss: 0.0032334720740715664\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02039353530936771\n",
      "Average test loss: 0.0031264506928208803\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020382977184322144\n",
      "Average test loss: 0.003156183936115768\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020424153799812\n",
      "Average test loss: 0.00323405974999898\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02041991218758954\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Nesterov_No_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_10_proj5_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj5_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj5_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj5_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a8e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Nesterov_No_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db8a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj15_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj15_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj15_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668d19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Nesterov_No_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Nesterov', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=False, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e52e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = Nesterov(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_20_proj30_model = Nesterov(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_30_proj30_model = Nesterov(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "gauss_40_proj30_model = Nesterov(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=False).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
