{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.ImageDataset import ImageDataset\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1574075043466356\n",
      "Average test loss: 0.010967728753056792\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0628968993028005\n",
      "Average test loss: 0.009528779514961772\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05647332595454322\n",
      "Average test loss: 0.010606511356929938\n",
      "Epoch 4/300\n",
      "Average training loss: 0.053114678104718524\n",
      "Average test loss: 0.008814142848468489\n",
      "Epoch 5/300\n",
      "Average training loss: 0.050792312807506984\n",
      "Average test loss: 0.008183929207424322\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04828707269165251\n",
      "Average test loss: 0.00862255521532562\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0468635076449977\n",
      "Average test loss: 0.008287168747848935\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04547134261661106\n",
      "Average test loss: 0.008100484158015913\n",
      "Epoch 9/300\n",
      "Average training loss: 0.044647974454694324\n",
      "Average test loss: 0.007728543268309699\n",
      "Epoch 10/300\n",
      "Average training loss: 0.043782865398459964\n",
      "Average test loss: 0.008100190644462903\n",
      "Epoch 11/300\n",
      "Average training loss: 0.043053338021039965\n",
      "Average test loss: 0.007276126992785268\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04232675150367949\n",
      "Average test loss: 0.007176679045789771\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04154542495475875\n",
      "Average test loss: 0.007363480387048589\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04103897453347842\n",
      "Average test loss: 0.0069304792554014255\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04051351844933298\n",
      "Average test loss: 0.007300106961694029\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04016960475179884\n",
      "Average test loss: 0.0070760722590817345\n",
      "Epoch 17/300\n",
      "Average training loss: 0.039814756436480414\n",
      "Average test loss: 0.006925983142107725\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03928060471680429\n",
      "Average test loss: 0.006678072769608762\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03889028852184614\n",
      "Average test loss: 0.006735722067869372\n",
      "Epoch 20/300\n",
      "Average training loss: 0.038603794890973306\n",
      "Average test loss: 0.006693466645975908\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03824395355582237\n",
      "Average test loss: 0.006751611501392391\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03797341054346826\n",
      "Average test loss: 0.006478962966137462\n",
      "Epoch 23/300\n",
      "Average training loss: 0.037762915247016485\n",
      "Average test loss: 0.00663288711839252\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03747859632637766\n",
      "Average test loss: 0.006433052023251852\n",
      "Epoch 25/300\n",
      "Average training loss: 0.037161089496480094\n",
      "Average test loss: 0.006602096596111854\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03708988028433588\n",
      "Average test loss: 0.006567127038621241\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0367621493074629\n",
      "Average test loss: 0.006373861781424946\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03649465563562181\n",
      "Average test loss: 0.006440334685146809\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03641027560167842\n",
      "Average test loss: 0.006364928108122614\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03637474990884463\n",
      "Average test loss: 0.00737735460119115\n",
      "Epoch 31/300\n",
      "Average training loss: 0.036058045201831396\n",
      "Average test loss: 0.006424117720375458\n",
      "Epoch 32/300\n",
      "Average training loss: 0.035856786893473734\n",
      "Average test loss: 0.006276755400002003\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03571033489704132\n",
      "Average test loss: 0.006146654058661726\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03559704199267758\n",
      "Average test loss: 0.006328964543011454\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03542511971129311\n",
      "Average test loss: 0.006284201664229234\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03527337863047918\n",
      "Average test loss: 0.00685206188634038\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03520642072955767\n",
      "Average test loss: 0.006068021738280853\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03508727709121174\n",
      "Average test loss: 0.006028232525206275\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0348614314413733\n",
      "Average test loss: 0.006108437692539559\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03478966566092438\n",
      "Average test loss: 0.006256639033142063\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03479777170717716\n",
      "Average test loss: 0.005982637816833125\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03460563309490681\n",
      "Average test loss: 0.006144620465735594\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03456970177756415\n",
      "Average test loss: 0.006160930371946758\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03438413599795765\n",
      "Average test loss: 0.006169738978975349\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03428121736976836\n",
      "Average test loss: 0.006147744696173403\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03422670588890712\n",
      "Average test loss: 0.006011735562649038\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0342511188586553\n",
      "Average test loss: 0.006136737105333143\n",
      "Epoch 48/300\n",
      "Average training loss: 0.034125243501530754\n",
      "Average test loss: 0.006572228131194909\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03387930132614242\n",
      "Average test loss: 0.005975523283912076\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03390792551305559\n",
      "Average test loss: 0.006177567162447505\n",
      "Epoch 51/300\n",
      "Average training loss: 0.033847286803854834\n",
      "Average test loss: 0.006918247249805265\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03375766305625439\n",
      "Average test loss: 0.006052110538714462\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03370672189692656\n",
      "Average test loss: 0.006005944866273138\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03355346485806836\n",
      "Average test loss: 0.005919386471725172\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03354501477215025\n",
      "Average test loss: 0.005912546794033713\n",
      "Epoch 56/300\n",
      "Average training loss: 0.033506168511178755\n",
      "Average test loss: 0.006071383873207702\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03333935774366061\n",
      "Average test loss: 0.005901926111016009\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03332904690172937\n",
      "Average test loss: 0.006073479992441005\n",
      "Epoch 59/300\n",
      "Average training loss: 0.033248306300905014\n",
      "Average test loss: 0.0058689724264873394\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03319955164525244\n",
      "Average test loss: 0.005874605135371288\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03315112462639808\n",
      "Average test loss: 0.006204924303624365\n",
      "Epoch 62/300\n",
      "Average training loss: 0.033111102627383336\n",
      "Average test loss: 0.00619511902746227\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03306091877818108\n",
      "Average test loss: 0.006078158957676755\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03301062040196525\n",
      "Average test loss: 0.006227167534331481\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03310414109296269\n",
      "Average test loss: 0.35816132187843325\n",
      "Epoch 66/300\n",
      "Average training loss: 0.047703195280498926\n",
      "Average test loss: 0.006287556773672502\n",
      "Epoch 67/300\n",
      "Average training loss: 0.035594132656852406\n",
      "Average test loss: 0.0060288475528359415\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03401548972063594\n",
      "Average test loss: 0.006113579747991429\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03333077963855532\n",
      "Average test loss: 0.005913235263278087\n",
      "Epoch 70/300\n",
      "Average training loss: 0.033003991963134874\n",
      "Average test loss: 0.005986075783355369\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03283818136155605\n",
      "Average test loss: 0.0059430438329776125\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03273273605604966\n",
      "Average test loss: 0.006076414611190558\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03277578773763445\n",
      "Average test loss: 0.006011899190644423\n",
      "Epoch 74/300\n",
      "Average training loss: 0.032714586248000464\n",
      "Average test loss: 0.005899298527795407\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03270710775587294\n",
      "Average test loss: 0.0060144415270123215\n",
      "Epoch 76/300\n",
      "Average training loss: 0.032702085316181184\n",
      "Average test loss: 0.005847153096149365\n",
      "Epoch 77/300\n",
      "Average training loss: 0.032608888596296313\n",
      "Average test loss: 0.005949299060636097\n",
      "Epoch 78/300\n",
      "Average training loss: 0.032677590486076144\n",
      "Average test loss: 0.005858421982162528\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03257428048716651\n",
      "Average test loss: 0.0070658885894550215\n",
      "Epoch 80/300\n",
      "Average training loss: 0.032545470141702226\n",
      "Average test loss: 0.006042412798023886\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03256116625335481\n",
      "Average test loss: 0.005995179863439666\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03249947751892938\n",
      "Average test loss: 0.0062835130650136205\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03240900532073445\n",
      "Average test loss: 0.0060851777166956\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03247833447986179\n",
      "Average test loss: 0.005854808445192046\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03241275348265966\n",
      "Average test loss: 0.005903323010438019\n",
      "Epoch 86/300\n",
      "Average training loss: 0.032285635709762575\n",
      "Average test loss: 0.006089229256328609\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03227431412869029\n",
      "Average test loss: 0.0059438905910485324\n",
      "Epoch 88/300\n",
      "Average training loss: 0.032185495111677384\n",
      "Average test loss: 0.005920955434441566\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03221525458825959\n",
      "Average test loss: 0.005907758329891496\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03223204993042681\n",
      "Average test loss: 0.005831888265907764\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03215378383629852\n",
      "Average test loss: 0.006200100653701359\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03204151275422838\n",
      "Average test loss: 0.005828651608692275\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03206989644302262\n",
      "Average test loss: 0.005900236614048481\n",
      "Epoch 94/300\n",
      "Average training loss: 0.032042931225564744\n",
      "Average test loss: 0.005955373877866401\n",
      "Epoch 95/300\n",
      "Average training loss: 0.031994420122769145\n",
      "Average test loss: 0.006164008268465598\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03192411422729492\n",
      "Average test loss: 0.005865259652750359\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03191240269607968\n",
      "Average test loss: 0.00583618233145939\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03186485463877519\n",
      "Average test loss: 0.005905943934702211\n",
      "Epoch 99/300\n",
      "Average training loss: 0.031845108727614084\n",
      "Average test loss: 0.005953284973071681\n",
      "Epoch 100/300\n",
      "Average training loss: 0.031856333255767824\n",
      "Average test loss: 0.005878686716987027\n",
      "Epoch 101/300\n",
      "Average training loss: 0.031797891499267684\n",
      "Average test loss: 0.005914467555781205\n",
      "Epoch 102/300\n",
      "Average training loss: 0.031749039052261244\n",
      "Average test loss: 0.005824232861399651\n",
      "Epoch 103/300\n",
      "Average training loss: 0.031703717067837714\n",
      "Average test loss: 0.005907984169407023\n",
      "Epoch 104/300\n",
      "Average training loss: 0.031684581448634466\n",
      "Average test loss: 0.00583327138175567\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0316477434668276\n",
      "Average test loss: 0.005863940474059847\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03164311368929015\n",
      "Average test loss: 0.005880215793020196\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03160024865302775\n",
      "Average test loss: 0.005802995985994736\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03157760283019807\n",
      "Average test loss: 0.005972799183593856\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03156511993209521\n",
      "Average test loss: 0.006112074041532145\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03155127565397157\n",
      "Average test loss: 0.005821373658047782\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03148072690102789\n",
      "Average test loss: 0.005812722872942686\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03142701675825649\n",
      "Average test loss: 0.005967087556918462\n",
      "Epoch 113/300\n",
      "Average training loss: 0.031420722737908365\n",
      "Average test loss: 0.005949866878489653\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03136155610283216\n",
      "Average test loss: 0.006293647375371721\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03139980509877205\n",
      "Average test loss: 0.005928762778639793\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031333697214722636\n",
      "Average test loss: 0.005847296870003144\n",
      "Epoch 117/300\n",
      "Average training loss: 0.031325546017951435\n",
      "Average test loss: 0.005847501239428917\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03130625490678681\n",
      "Average test loss: 0.005959642303900586\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03123222949107488\n",
      "Average test loss: 0.005952076034413444\n",
      "Epoch 120/300\n",
      "Average training loss: 0.031243527157439124\n",
      "Average test loss: 0.005925444162554211\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03126174498763349\n",
      "Average test loss: 0.00593841481829683\n",
      "Epoch 122/300\n",
      "Average training loss: 0.031230600290828282\n",
      "Average test loss: 0.0058297300284935365\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03118290153476927\n",
      "Average test loss: 0.007542719100498491\n",
      "Epoch 124/300\n",
      "Average training loss: 0.031138197431961695\n",
      "Average test loss: 0.006156284285916223\n",
      "Epoch 125/300\n",
      "Average training loss: 0.031143536768025823\n",
      "Average test loss: 0.006161475079754989\n",
      "Epoch 126/300\n",
      "Average training loss: 0.031091590291923946\n",
      "Average test loss: 0.005961028735670779\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0310884826729695\n",
      "Average test loss: 0.0060676014684140685\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03107861964404583\n",
      "Average test loss: 0.0058362926302684675\n",
      "Epoch 129/300\n",
      "Average training loss: 0.031036909676260417\n",
      "Average test loss: 0.005849423693699969\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03102702287832896\n",
      "Average test loss: 0.005859241648266713\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031018385738134382\n",
      "Average test loss: 0.005838226398246156\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03092771350012885\n",
      "Average test loss: 0.005840575194607178\n",
      "Epoch 133/300\n",
      "Average training loss: 0.030937009887562858\n",
      "Average test loss: 0.006077100029008256\n",
      "Epoch 134/300\n",
      "Average training loss: 0.030972896933555604\n",
      "Average test loss: 0.005774165838956833\n",
      "Epoch 135/300\n",
      "Average training loss: 0.030906948188940683\n",
      "Average test loss: 0.0059003986414108015\n",
      "Epoch 136/300\n",
      "Average training loss: 0.030881210754315058\n",
      "Average test loss: 0.005966161321020788\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03087821905480491\n",
      "Average test loss: 0.005916640006419685\n",
      "Epoch 138/300\n",
      "Average training loss: 0.030870309496919313\n",
      "Average test loss: 0.005769751100904412\n",
      "Epoch 139/300\n",
      "Average training loss: 0.030838774455918205\n",
      "Average test loss: 0.00589143300594555\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03078907391925653\n",
      "Average test loss: 0.0060223554755664535\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0308694545229276\n",
      "Average test loss: 0.005875326172345215\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030818348800142607\n",
      "Average test loss: 0.006026783817137281\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03074846688244078\n",
      "Average test loss: 0.005904014227290948\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03070530487100283\n",
      "Average test loss: 0.006426140316865511\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030735281222396427\n",
      "Average test loss: 0.005941200439714723\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03065482268234094\n",
      "Average test loss: 0.0060395337185925905\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030708141601747937\n",
      "Average test loss: 0.005802638532800807\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03066320951282978\n",
      "Average test loss: 0.006422010497086578\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030658126142289903\n",
      "Average test loss: 0.005837227791133854\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030597630441188812\n",
      "Average test loss: 0.007192994044058852\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030584968792067634\n",
      "Average test loss: 0.00684375622537401\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03060793927974171\n",
      "Average test loss: 0.006491437552703752\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030548907755149735\n",
      "Average test loss: 0.006043954434908099\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030523825075891284\n",
      "Average test loss: 0.005904551054454512\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03051526825957828\n",
      "Average test loss: 0.005892148062172863\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030466586927572887\n",
      "Average test loss: 0.005858412045571539\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030545152240329318\n",
      "Average test loss: 0.005868833960551355\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03047052572667599\n",
      "Average test loss: 0.005874348865614997\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030435344669553967\n",
      "Average test loss: 0.0058593308528264365\n",
      "Epoch 160/300\n",
      "Average training loss: 0.030435936517185635\n",
      "Average test loss: 0.005912241872400046\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030400460857484077\n",
      "Average test loss: 0.005867344304919243\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03034151534239451\n",
      "Average test loss: 0.005837127281145917\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030430284066332712\n",
      "Average test loss: 0.0061322892465525206\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030389629415339892\n",
      "Average test loss: 0.006020722996029589\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030378842292560472\n",
      "Average test loss: 0.00592506729438901\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030280347254541186\n",
      "Average test loss: 0.005854225922375917\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030359024143881266\n",
      "Average test loss: 0.005936686019930575\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030286900877952577\n",
      "Average test loss: 0.005892502822395828\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03024452431499958\n",
      "Average test loss: 0.0060626441352069375\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030269987535145548\n",
      "Average test loss: 0.005914043729917871\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030258578379948935\n",
      "Average test loss: 0.006167403735220432\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03030980241133107\n",
      "Average test loss: 0.005943679420691398\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030255972353948488\n",
      "Average test loss: 0.006103693817638689\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030214420853389636\n",
      "Average test loss: 0.0059510320406407115\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030228587870796523\n",
      "Average test loss: 0.005905668569521772\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030182446069187588\n",
      "Average test loss: 0.006213969451271825\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030149889945983888\n",
      "Average test loss: 0.00605242081069284\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03016020322839419\n",
      "Average test loss: 0.0062680434679819475\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030143501091334554\n",
      "Average test loss: 0.0058585167585147755\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030159705799486902\n",
      "Average test loss: 0.005989364942742719\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03013423423965772\n",
      "Average test loss: 0.0060105849438243444\n",
      "Epoch 182/300\n",
      "Average training loss: 0.030097906798124314\n",
      "Average test loss: 0.006409429784864187\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030067841336131097\n",
      "Average test loss: 0.005856446114472217\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030092178013589647\n",
      "Average test loss: 0.006481632009148598\n",
      "Epoch 185/300\n",
      "Average training loss: 0.030084961089822983\n",
      "Average test loss: 0.0062410967826015415\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03006603548924128\n",
      "Average test loss: 0.005905500290294488\n",
      "Epoch 187/300\n",
      "Average training loss: 0.030044620947705374\n",
      "Average test loss: 0.006081085284137064\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0300512821127971\n",
      "Average test loss: 0.005989183745450444\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030023306839995913\n",
      "Average test loss: 0.006005538379152616\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0299773048410813\n",
      "Average test loss: 0.005878194638424449\n",
      "Epoch 191/300\n",
      "Average training loss: 0.029968582534127767\n",
      "Average test loss: 0.005875056799915102\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030005248486995695\n",
      "Average test loss: 0.006069590911683109\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029993665784597397\n",
      "Average test loss: 0.006475576318800449\n",
      "Epoch 194/300\n",
      "Average training loss: 0.029981829038924643\n",
      "Average test loss: 0.006073086094111204\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029918755523032612\n",
      "Average test loss: 0.006115736288122005\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029909529874722163\n",
      "Average test loss: 0.007157896701660421\n",
      "Epoch 197/300\n",
      "Average training loss: 0.029927988906701408\n",
      "Average test loss: 0.005930923568705717\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029937689907020993\n",
      "Average test loss: 0.005958957956896888\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02990789757337835\n",
      "Average test loss: 0.00622386089960734\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029939316168427467\n",
      "Average test loss: 0.0060558949257764555\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02990344311793645\n",
      "Average test loss: 0.0059421478195322884\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02980769910249445\n",
      "Average test loss: 0.00623602428411444\n",
      "Epoch 203/300\n",
      "Average training loss: 0.029845203765564494\n",
      "Average test loss: 0.006020093983660142\n",
      "Epoch 204/300\n",
      "Average training loss: 0.029840490501787928\n",
      "Average test loss: 0.00598106203849117\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029858133585916626\n",
      "Average test loss: 0.005936126046710544\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029797294898165596\n",
      "Average test loss: 0.005922904368903902\n",
      "Epoch 207/300\n",
      "Average training loss: 0.029778512433171273\n",
      "Average test loss: 0.006105756529503398\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029798986453149053\n",
      "Average test loss: 0.005972307827323675\n",
      "Epoch 209/300\n",
      "Average training loss: 0.029786978357368047\n",
      "Average test loss: 0.006239663558701674\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029746271941396927\n",
      "Average test loss: 0.006003771399044329\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029725005977683596\n",
      "Average test loss: 0.007624528027657005\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029786777291033004\n",
      "Average test loss: 0.00613410966263877\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02970581479701731\n",
      "Average test loss: 0.006098299983474944\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02972614463004801\n",
      "Average test loss: 0.005989409247206317\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029687588441703053\n",
      "Average test loss: 0.006043331443849537\n",
      "Epoch 216/300\n",
      "Average training loss: 0.029720790839857524\n",
      "Average test loss: 0.006127933516684505\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02962706859740946\n",
      "Average test loss: 0.005955104901558823\n",
      "Epoch 218/300\n",
      "Average training loss: 0.029680108166403242\n",
      "Average test loss: 0.006079646586543984\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02972636071178648\n",
      "Average test loss: 0.005867630447364516\n",
      "Epoch 220/300\n",
      "Average training loss: 0.029692053193847337\n",
      "Average test loss: 0.00619395810779598\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029644614881939358\n",
      "Average test loss: 0.006060304523756107\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02960334409442213\n",
      "Average test loss: 0.0060741912298318415\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02965204492211342\n",
      "Average test loss: 0.006047752865072754\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029627968236804008\n",
      "Average test loss: 0.006076504877044095\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02959500198894077\n",
      "Average test loss: 0.005968571177787251\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02965314327345954\n",
      "Average test loss: 0.005972083338432842\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029613367120424908\n",
      "Average test loss: 0.006033869973073403\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029562724138299624\n",
      "Average test loss: 0.005962352291163471\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029547507900330755\n",
      "Average test loss: 0.006083054318196244\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029569589111540052\n",
      "Average test loss: 0.005975514139152235\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029522162104646366\n",
      "Average test loss: 0.005954874792446693\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029535161763429643\n",
      "Average test loss: 0.006078909578422705\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0296240983373589\n",
      "Average test loss: 0.027215345362822215\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02948546584116088\n",
      "Average test loss: 0.005965154632098145\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029532137817806666\n",
      "Average test loss: 0.005891087271687057\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029496889263391494\n",
      "Average test loss: 0.005947300109598372\n",
      "Epoch 237/300\n",
      "Average training loss: 0.029472649266322454\n",
      "Average test loss: 0.006137457892298698\n",
      "Epoch 238/300\n",
      "Average training loss: 0.029571895899044142\n",
      "Average test loss: 0.006249314100378089\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02952882774339782\n",
      "Average test loss: 0.006961438439372513\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029415358947383032\n",
      "Average test loss: 0.006188603488107522\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029457646149728033\n",
      "Average test loss: 0.006547328230407503\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029406302488512462\n",
      "Average test loss: 0.0060545302265220214\n",
      "Epoch 243/300\n",
      "Average training loss: 0.029421526874105135\n",
      "Average test loss: 0.006038750149309635\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02943776616619693\n",
      "Average test loss: 0.006175974785867664\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029394614797499444\n",
      "Average test loss: 0.006265932973474264\n",
      "Epoch 246/300\n",
      "Average training loss: 0.029347861766815187\n",
      "Average test loss: 0.005969517978115214\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029423163692156473\n",
      "Average test loss: 0.006260568351381355\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02937091384165817\n",
      "Average test loss: 0.006147705217202504\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029449645227856106\n",
      "Average test loss: 0.005978486122356521\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02938706919219759\n",
      "Average test loss: 0.006122794602894121\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02935327313674821\n",
      "Average test loss: 0.0060876078839517305\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029407557237479422\n",
      "Average test loss: 0.00602795720886853\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029305364280939102\n",
      "Average test loss: 0.005916214179661539\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029296062529087067\n",
      "Average test loss: 0.005979028681086169\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029304604942599934\n",
      "Average test loss: 0.005964454277522034\n",
      "Epoch 256/300\n",
      "Average training loss: 0.029285543276204005\n",
      "Average test loss: 0.006031956136226654\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029394390675756665\n",
      "Average test loss: 0.008088545685840977\n",
      "Epoch 258/300\n",
      "Average training loss: 0.029332746284703413\n",
      "Average test loss: 0.00619869018263287\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02923570187555419\n",
      "Average test loss: 0.006240641500386927\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029303728034098944\n",
      "Average test loss: 0.005982835236108965\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02930815052323871\n",
      "Average test loss: 0.005891035189645158\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029324135755499203\n",
      "Average test loss: 0.00602284324914217\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02922881187001864\n",
      "Average test loss: 0.007411622581382593\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029357342455122205\n",
      "Average test loss: 0.006244580539978213\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029235352165169187\n",
      "Average test loss: 0.006203286504165993\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02927505610220962\n",
      "Average test loss: 0.006016069865889019\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029224444690677854\n",
      "Average test loss: 0.006002560279435582\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029240259220202763\n",
      "Average test loss: 0.0061600386682483885\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029235521796676846\n",
      "Average test loss: 0.006046784330573347\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029188877181874382\n",
      "Average test loss: 0.006256747137755155\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02921960601872868\n",
      "Average test loss: 0.005962816468129555\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02922807416650984\n",
      "Average test loss: 0.0060600354098197485\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029222446968158088\n",
      "Average test loss: 0.006103486648036374\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029152818078796067\n",
      "Average test loss: 0.005942369201117092\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02918471428917514\n",
      "Average test loss: 0.006166968738039335\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029238005369901655\n",
      "Average test loss: 0.006148301524420579\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029142854664060806\n",
      "Average test loss: 0.006147230967051453\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029110863086250095\n",
      "Average test loss: 0.006327351295285754\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02917872395614783\n",
      "Average test loss: 0.006318016229404344\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02911634858780437\n",
      "Average test loss: 0.006038732757791877\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029076679706573485\n",
      "Average test loss: 0.006011879456126028\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0290816461129321\n",
      "Average test loss: 0.006111544272965855\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029137549499670665\n",
      "Average test loss: 0.006086127195507288\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02910530330075158\n",
      "Average test loss: 0.006043413761589262\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02908835293021467\n",
      "Average test loss: 0.00616868517961767\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02912877680361271\n",
      "Average test loss: 0.006121184503452645\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02909910665286912\n",
      "Average test loss: 0.0070161454524430966\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02914050423602263\n",
      "Average test loss: 0.014164530242482822\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02906834499206808\n",
      "Average test loss: 0.006118367836707168\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029050946025384796\n",
      "Average test loss: 0.006124247249629762\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029051134251885945\n",
      "Average test loss: 0.006163450524624851\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02915411205920908\n",
      "Average test loss: 0.006235237112475766\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029103545564744208\n",
      "Average test loss: 0.006101192292239931\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029074200795756445\n",
      "Average test loss: 0.006339837833825085\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02909330355955495\n",
      "Average test loss: 0.006104780138366752\n",
      "Epoch 296/300\n",
      "Average training loss: 0.28139098700881005\n",
      "Average test loss: 0.017133824229240417\n",
      "Epoch 297/300\n",
      "Average training loss: 0.15073727473947737\n",
      "Average test loss: 0.01082085595279932\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07317252792252435\n",
      "Average test loss: 0.007821868557069036\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05931217514806324\n",
      "Average test loss: 0.007598063070327043\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05179908493161201\n",
      "Average test loss: 0.006918121161146296\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13380455429686441\n",
      "Average test loss: 0.007656647767457697\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04956422819362746\n",
      "Average test loss: 0.0144770102236006\n",
      "Epoch 3/300\n",
      "Average training loss: 0.044315187421109944\n",
      "Average test loss: 0.006563186197645134\n",
      "Epoch 4/300\n",
      "Average training loss: 0.040834923909770116\n",
      "Average test loss: 0.00570548036818703\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03826199558708403\n",
      "Average test loss: 0.005456256176448531\n",
      "Epoch 6/300\n",
      "Average training loss: 0.036381147616439395\n",
      "Average test loss: 0.006718284551882081\n",
      "Epoch 7/300\n",
      "Average training loss: 0.034705011202229394\n",
      "Average test loss: 0.005592264637351036\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03329689355691274\n",
      "Average test loss: 0.005252006353603469\n",
      "Epoch 9/300\n",
      "Average training loss: 0.032497352646456826\n",
      "Average test loss: 0.004787764445361164\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03141598565876484\n",
      "Average test loss: 0.004877434672373864\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03064361348085933\n",
      "Average test loss: 0.004646400534444385\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03001646564073033\n",
      "Average test loss: 0.004619824071311288\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02937902804546886\n",
      "Average test loss: 0.004304823023784492\n",
      "Epoch 14/300\n",
      "Average training loss: 0.028959167761935127\n",
      "Average test loss: 0.004297411562667952\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02839298577275541\n",
      "Average test loss: 0.004350362314739161\n",
      "Epoch 16/300\n",
      "Average training loss: 0.028059951268964343\n",
      "Average test loss: 0.0042037557820892996\n",
      "Epoch 17/300\n",
      "Average training loss: 0.027687276787228054\n",
      "Average test loss: 0.004574623769563105\n",
      "Epoch 18/300\n",
      "Average training loss: 0.027304078267680273\n",
      "Average test loss: 0.004010856974249085\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02702720892843273\n",
      "Average test loss: 0.004292394079474939\n",
      "Epoch 20/300\n",
      "Average training loss: 0.026697550002071593\n",
      "Average test loss: 0.003989470005656282\n",
      "Epoch 21/300\n",
      "Average training loss: 0.026555029814441997\n",
      "Average test loss: 0.0038917982774890132\n",
      "Epoch 22/300\n",
      "Average training loss: 0.026299715037147203\n",
      "Average test loss: 0.003831399576531516\n",
      "Epoch 23/300\n",
      "Average training loss: 0.025987787447041936\n",
      "Average test loss: 0.0038014778102644616\n",
      "Epoch 24/300\n",
      "Average training loss: 0.025838768465651408\n",
      "Average test loss: 0.0037777287126001383\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02564042610923449\n",
      "Average test loss: 0.0037458327093886005\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02543616998857922\n",
      "Average test loss: 0.0037862464781436653\n",
      "Epoch 27/300\n",
      "Average training loss: 0.025276499749885665\n",
      "Average test loss: 0.0037917704201406903\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025197989324728647\n",
      "Average test loss: 0.0042696129416839946\n",
      "Epoch 29/300\n",
      "Average training loss: 0.024957882369558017\n",
      "Average test loss: 0.003855851555450095\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024881849062939485\n",
      "Average test loss: 0.0037919895369559527\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02482530218693945\n",
      "Average test loss: 0.0036048681234113044\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02464195632106728\n",
      "Average test loss: 0.003606278200944265\n",
      "Epoch 33/300\n",
      "Average training loss: 0.024482488047745492\n",
      "Average test loss: 0.003666874488608705\n",
      "Epoch 34/300\n",
      "Average training loss: 0.024429310265514585\n",
      "Average test loss: 0.003687606637262636\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024346665875779258\n",
      "Average test loss: 0.0036733220755640005\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02427035072611438\n",
      "Average test loss: 0.0037828230290777153\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024089931931760577\n",
      "Average test loss: 0.003538546500727534\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02403321221967538\n",
      "Average test loss: 0.003691140559605426\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02393932338224517\n",
      "Average test loss: 0.0035522148998247253\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023908624259961975\n",
      "Average test loss: 0.0036844877385430867\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02386613753437996\n",
      "Average test loss: 0.0035352113739483887\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02380407004058361\n",
      "Average test loss: 0.003458139326630367\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02368804744713836\n",
      "Average test loss: 0.0035347751784655782\n",
      "Epoch 44/300\n",
      "Average training loss: 0.023659506380558012\n",
      "Average test loss: 0.0035912160618851584\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02360120713048511\n",
      "Average test loss: 0.003590193115795652\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023486312945683797\n",
      "Average test loss: 0.003436410894410478\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02348753090368377\n",
      "Average test loss: 0.0035092519701768955\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023416713650027912\n",
      "Average test loss: 0.003566599516198039\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02338160584370295\n",
      "Average test loss: 0.003433383018399278\n",
      "Epoch 50/300\n",
      "Average training loss: 0.023323563640316326\n",
      "Average test loss: 0.0034547742644531858\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023252085955606565\n",
      "Average test loss: 0.0034102617055177687\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02323776572611597\n",
      "Average test loss: 0.0034025700485540763\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023171699699428346\n",
      "Average test loss: 0.0034496029503643514\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023113685404260953\n",
      "Average test loss: 0.0035321873509221607\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023097309683760007\n",
      "Average test loss: 0.003628739521735244\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02307457026342551\n",
      "Average test loss: 0.003913221466458506\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023097467482089996\n",
      "Average test loss: 0.003553704738203022\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02294781558215618\n",
      "Average test loss: 0.003432721201537384\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0229474947684341\n",
      "Average test loss: 0.0033923893964125052\n",
      "Epoch 60/300\n",
      "Average training loss: 0.022874124662743675\n",
      "Average test loss: 0.003421163825525178\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022847162114249336\n",
      "Average test loss: 0.0034280157585938774\n",
      "Epoch 62/300\n",
      "Average training loss: 0.023011706189976798\n",
      "Average test loss: 0.00343621507121457\n",
      "Epoch 63/300\n",
      "Average training loss: 0.022819730134473908\n",
      "Average test loss: 0.0036483806994640163\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02278804709845119\n",
      "Average test loss: 0.0034032219938106008\n",
      "Epoch 65/300\n",
      "Average training loss: 0.022713142133421366\n",
      "Average test loss: 0.003443004232727819\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02271616898311509\n",
      "Average test loss: 0.003391751884793242\n",
      "Epoch 67/300\n",
      "Average training loss: 0.022669149931934144\n",
      "Average test loss: 0.0033883032769792608\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0226557656692134\n",
      "Average test loss: 0.0034313499331474306\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022642456377545993\n",
      "Average test loss: 0.0036882352088060642\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02255779850979646\n",
      "Average test loss: 0.0034407575813432535\n",
      "Epoch 71/300\n",
      "Average training loss: 0.022591828477051525\n",
      "Average test loss: 0.0033978132584856617\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02260252878235446\n",
      "Average test loss: 0.0033519104743997257\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022466466529501808\n",
      "Average test loss: 0.0035937631730404164\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02244817619025707\n",
      "Average test loss: 0.0035069018192589283\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02244247467815876\n",
      "Average test loss: 0.0033799444176256656\n",
      "Epoch 76/300\n",
      "Average training loss: 0.022451589783032737\n",
      "Average test loss: 0.0033760005128052497\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022443686953849264\n",
      "Average test loss: 0.003534988285973668\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0223713531345129\n",
      "Average test loss: 0.003349730657413602\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022401938420202997\n",
      "Average test loss: 0.003395324592375093\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0223271212346024\n",
      "Average test loss: 0.003430266410112381\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02230904640422927\n",
      "Average test loss: 0.0033924829626662866\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022267535386814013\n",
      "Average test loss: 0.003420213762256834\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02231040775610341\n",
      "Average test loss: 0.003348176048033767\n",
      "Epoch 84/300\n",
      "Average training loss: 0.022218626042207083\n",
      "Average test loss: 0.0033627690265162122\n",
      "Epoch 85/300\n",
      "Average training loss: 0.022195704996585845\n",
      "Average test loss: 0.003497123096966081\n",
      "Epoch 86/300\n",
      "Average training loss: 0.022203550143374336\n",
      "Average test loss: 0.0033467560439474054\n",
      "Epoch 87/300\n",
      "Average training loss: 0.022182407471040885\n",
      "Average test loss: 0.0033808310652772584\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022160255572862095\n",
      "Average test loss: 0.0034260956425633694\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02215084481570456\n",
      "Average test loss: 0.0033662015539076594\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022104687415891222\n",
      "Average test loss: 0.0034747044942859146\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02209816132651435\n",
      "Average test loss: 0.0033276578988879917\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022108839962217543\n",
      "Average test loss: 0.0033270510538584656\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022060820662313036\n",
      "Average test loss: 0.0033608678347534604\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02206841938694318\n",
      "Average test loss: 0.004022064769226644\n",
      "Epoch 95/300\n",
      "Average training loss: 0.021984599934683907\n",
      "Average test loss: 0.003444785601976845\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02202045539352629\n",
      "Average test loss: 0.0033634160341074067\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02197323766350746\n",
      "Average test loss: 0.0033960163048985933\n",
      "Epoch 98/300\n",
      "Average training loss: 0.021988918181922702\n",
      "Average test loss: 0.003337400034070015\n",
      "Epoch 99/300\n",
      "Average training loss: 0.021990411412384774\n",
      "Average test loss: 0.0033327519516978\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02190805211663246\n",
      "Average test loss: 0.003371822962123487\n",
      "Epoch 101/300\n",
      "Average training loss: 0.021952952752510708\n",
      "Average test loss: 0.0034303488825551337\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02194758748097552\n",
      "Average test loss: 0.0033354821875691415\n",
      "Epoch 103/300\n",
      "Average training loss: 0.021867668992943234\n",
      "Average test loss: 0.0034772636058429877\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02186003648572498\n",
      "Average test loss: 0.003420841289477216\n",
      "Epoch 105/300\n",
      "Average training loss: 0.021872138597899014\n",
      "Average test loss: 0.003385522506924139\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021864006019300886\n",
      "Average test loss: 0.003373953954420156\n",
      "Epoch 107/300\n",
      "Average training loss: 0.021811228258742228\n",
      "Average test loss: 0.0034836417904330623\n",
      "Epoch 108/300\n",
      "Average training loss: 0.021812549887431993\n",
      "Average test loss: 0.0033284097620182567\n",
      "Epoch 109/300\n",
      "Average training loss: 0.021802029040124682\n",
      "Average test loss: 0.0034106670500089727\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021757764509982534\n",
      "Average test loss: 0.003424416764742798\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02177146901190281\n",
      "Average test loss: 0.0034105684598907827\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02171987152927452\n",
      "Average test loss: 0.0034291965595136084\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021770322521527607\n",
      "Average test loss: 0.003495914365268416\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021713376252187624\n",
      "Average test loss: 0.0036556986924260854\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02173221129675706\n",
      "Average test loss: 0.0033756205265720687\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021703810216652023\n",
      "Average test loss: 0.003364677316405707\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02167817583266232\n",
      "Average test loss: 0.0033920966061866944\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021649623163872294\n",
      "Average test loss: 0.003389898290236791\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021649414451585875\n",
      "Average test loss: 0.0034085156629896827\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02165752558079031\n",
      "Average test loss: 0.003339302409440279\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02161229118704796\n",
      "Average test loss: 0.003392055570665333\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02159420559803645\n",
      "Average test loss: 0.0037051358260214327\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02162322323769331\n",
      "Average test loss: 0.003544630309359895\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021575402026375135\n",
      "Average test loss: 0.0033221666413462826\n",
      "Epoch 125/300\n",
      "Average training loss: 0.021562780102094014\n",
      "Average test loss: 0.0034206847202860645\n",
      "Epoch 126/300\n",
      "Average training loss: 0.021554115818606483\n",
      "Average test loss: 0.003578396302130487\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02156555638048384\n",
      "Average test loss: 0.003636750278994441\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02152146620220608\n",
      "Average test loss: 0.003510307129472494\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02153640006151464\n",
      "Average test loss: 0.0033361080340627166\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021528964981436728\n",
      "Average test loss: 0.0034239988682998553\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02148296439482106\n",
      "Average test loss: 0.0034553701053890917\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021489106769363085\n",
      "Average test loss: 0.0034124958858721788\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02151801627046532\n",
      "Average test loss: 0.003476028071095546\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021463320781787237\n",
      "Average test loss: 0.0033577012249992955\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02143972966902786\n",
      "Average test loss: 0.003523745992117458\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021432201875580682\n",
      "Average test loss: 0.003490430733602908\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021419571306970385\n",
      "Average test loss: 0.0033986141162200105\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021410249234901535\n",
      "Average test loss: 0.003347525996880399\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021411368305484454\n",
      "Average test loss: 0.003633982348566254\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021394648952616585\n",
      "Average test loss: 0.003999057539635234\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021366260134511525\n",
      "Average test loss: 0.0033673855300164887\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021393109689156215\n",
      "Average test loss: 0.3541423552831014\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02151582613421811\n",
      "Average test loss: 0.003330284042076932\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02130529589123196\n",
      "Average test loss: 0.0033356333875821697\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021326074237624806\n",
      "Average test loss: 0.003387736652046442\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021346109966437023\n",
      "Average test loss: 0.00332449320745137\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02131260149843163\n",
      "Average test loss: 0.003338873610521356\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021297959715127945\n",
      "Average test loss: 0.0036155692463119824\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02128624470697509\n",
      "Average test loss: 0.003421062140415112\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021281331572267744\n",
      "Average test loss: 0.0036940314210951327\n",
      "Epoch 151/300\n",
      "Average training loss: 0.021321022561854787\n",
      "Average test loss: 0.0033730193844272033\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021246382024553086\n",
      "Average test loss: 0.0034074727905293307\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021270332812435098\n",
      "Average test loss: 0.0034150995247893864\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02125746060659488\n",
      "Average test loss: 0.003418589959955878\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02121926714314355\n",
      "Average test loss: 0.0033703460954129694\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021219306513667106\n",
      "Average test loss: 0.0035176414563837977\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02123862551152706\n",
      "Average test loss: 0.0034256530010865795\n",
      "Epoch 158/300\n",
      "Average training loss: 0.021181142565276888\n",
      "Average test loss: 0.0033720403040448823\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021245681883560286\n",
      "Average test loss: 0.0033235774797697863\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02123252650267548\n",
      "Average test loss: 0.0034163127069671948\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02116928472287125\n",
      "Average test loss: 0.003440353929168648\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02114600025448534\n",
      "Average test loss: 0.003334345827293065\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02116004477110174\n",
      "Average test loss: 0.0034062716141343118\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021155338649948438\n",
      "Average test loss: 0.003404747701353497\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021140656605362892\n",
      "Average test loss: 0.003992622954977884\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021165976790918244\n",
      "Average test loss: 0.0035108484998345375\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02113761353989442\n",
      "Average test loss: 0.0034634069978362984\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021110249668359757\n",
      "Average test loss: 0.003414633671856589\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02112276750968562\n",
      "Average test loss: 0.003389715450712376\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021130366048879093\n",
      "Average test loss: 0.0033858232440219984\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021079802383979163\n",
      "Average test loss: 0.0033569940177516806\n",
      "Epoch 172/300\n",
      "Average training loss: 0.021078158211376933\n",
      "Average test loss: 0.0034553501355565255\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021101508624851703\n",
      "Average test loss: 0.0034575837175879216\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021091123539540502\n",
      "Average test loss: 0.00345504578244355\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021077303849988512\n",
      "Average test loss: 0.0034514712333265276\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021001730165547794\n",
      "Average test loss: 0.0034166320384376577\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021044256953729523\n",
      "Average test loss: 0.0034810511275298064\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021048431454433334\n",
      "Average test loss: 0.0034569268042428628\n",
      "Epoch 179/300\n",
      "Average training loss: 0.020996903040342862\n",
      "Average test loss: 0.0033857965630789597\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02101930125719971\n",
      "Average test loss: 0.0034105941179311938\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021047960160507095\n",
      "Average test loss: 0.0034387298963136142\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020981137115094396\n",
      "Average test loss: 0.003513615712730421\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020969800232185257\n",
      "Average test loss: 0.0034147078117562667\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021009698615305952\n",
      "Average test loss: 0.0034118979525648886\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020960688812865152\n",
      "Average test loss: 0.003570499051776197\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021009269739190737\n",
      "Average test loss: 0.0034547257241275575\n",
      "Epoch 187/300\n",
      "Average training loss: 0.020950960859656333\n",
      "Average test loss: 0.003566136761258046\n",
      "Epoch 188/300\n",
      "Average training loss: 0.020957688165207704\n",
      "Average test loss: 0.003837197175870339\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020933170245753394\n",
      "Average test loss: 0.003451403288377656\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02095039241015911\n",
      "Average test loss: 0.003356395227420661\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020929432347416878\n",
      "Average test loss: 0.003432689362516006\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02090630176332262\n",
      "Average test loss: 0.003449349914987882\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02090737752450837\n",
      "Average test loss: 0.00350902876837386\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02095901289416684\n",
      "Average test loss: 0.003391468240155114\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020899854035841093\n",
      "Average test loss: 0.0035195089576558936\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020911487999889587\n",
      "Average test loss: 0.0036111552392443023\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020904727197355696\n",
      "Average test loss: 0.003635872428615888\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020909550241298147\n",
      "Average test loss: 0.003512493689441019\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020883610733681255\n",
      "Average test loss: 0.0034672872233721943\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020851221644216114\n",
      "Average test loss: 0.003352995284522573\n",
      "Epoch 201/300\n",
      "Average training loss: 0.020843847195307415\n",
      "Average test loss: 0.0034147536947081486\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02084848445819484\n",
      "Average test loss: 0.003472526724760731\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020832532134321\n",
      "Average test loss: 0.0034322197526279424\n",
      "Epoch 204/300\n",
      "Average training loss: 0.020856231388118533\n",
      "Average test loss: 0.0034616013235516017\n",
      "Epoch 205/300\n",
      "Average training loss: 0.020818392568164402\n",
      "Average test loss: 0.0034587415705124537\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02083406559460693\n",
      "Average test loss: 0.0037402015251831876\n",
      "Epoch 207/300\n",
      "Average training loss: 0.020808437615633012\n",
      "Average test loss: 0.003461755358717508\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020803471573525006\n",
      "Average test loss: 0.007909146358569463\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020806543399062422\n",
      "Average test loss: 0.0034153997198575073\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020788622215390205\n",
      "Average test loss: 0.003463795537542966\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0207982303334607\n",
      "Average test loss: 0.0034396668687048885\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020781510687536663\n",
      "Average test loss: 0.0038499349397089744\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020793669887714916\n",
      "Average test loss: 0.003377474626733197\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020794704106118944\n",
      "Average test loss: 0.0033796630930155516\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02075917675263352\n",
      "Average test loss: 0.0035871303373326857\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020821292949219545\n",
      "Average test loss: 0.0034816232671340307\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020752592684494125\n",
      "Average test loss: 0.0038270940241507357\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02071338118861119\n",
      "Average test loss: 0.0034223038090599907\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020744451994697252\n",
      "Average test loss: 0.003405159092404776\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020699744587143264\n",
      "Average test loss: 0.0033815040743599336\n",
      "Epoch 221/300\n",
      "Average training loss: 0.020739210478133624\n",
      "Average test loss: 0.0034112548952301342\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020736203723483615\n",
      "Average test loss: 0.0035191917247656317\n",
      "Epoch 223/300\n",
      "Average training loss: 0.020701269038849408\n",
      "Average test loss: 0.0034884053743961785\n",
      "Epoch 224/300\n",
      "Average training loss: 0.020686821470657986\n",
      "Average test loss: 0.003488405815636118\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020717569654186568\n",
      "Average test loss: 0.0033994241687986585\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02069768090546131\n",
      "Average test loss: 0.0035307309217751026\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020705447057882943\n",
      "Average test loss: 0.003559082140525182\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020684670959909758\n",
      "Average test loss: 0.003589978700503707\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020709522684415182\n",
      "Average test loss: 0.0033927592769679095\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020659828899635208\n",
      "Average test loss: 0.003429959247716599\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020693023934960364\n",
      "Average test loss: 0.003473088838573959\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020678980014390416\n",
      "Average test loss: 0.003476035535335541\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0206627955602275\n",
      "Average test loss: 0.0034711844341622456\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02064148815141784\n",
      "Average test loss: 0.0034124148450791836\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020673577338457108\n",
      "Average test loss: 0.0035183823688162696\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02063694954249594\n",
      "Average test loss: 0.0037645700230366655\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0206382594704628\n",
      "Average test loss: 0.003476938342468606\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02059982775317298\n",
      "Average test loss: 0.0035782376997586752\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02061127728720506\n",
      "Average test loss: 0.0034378961502677862\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020649678924017482\n",
      "Average test loss: 0.003456173037075334\n",
      "Epoch 241/300\n",
      "Average training loss: 0.020629954435759122\n",
      "Average test loss: 0.003466234140512016\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020622155437866847\n",
      "Average test loss: 0.003483170932365788\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020597745754652553\n",
      "Average test loss: 0.0033744869848920237\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020627149999141694\n",
      "Average test loss: 0.003497031777890192\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02060152430501249\n",
      "Average test loss: 0.0034605689710006118\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020601699926786954\n",
      "Average test loss: 0.0034466139742483693\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02060110316508346\n",
      "Average test loss: 0.0035393798645171853\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02058196428335375\n",
      "Average test loss: 0.0035003181596597035\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020566234403186376\n",
      "Average test loss: 0.0036685950607061387\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02057790449923939\n",
      "Average test loss: 0.0034404656129578748\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02054747477173805\n",
      "Average test loss: 0.0034959814209077093\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020542611870500778\n",
      "Average test loss: 0.004126716949459579\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02052025743159983\n",
      "Average test loss: 0.003471670656568474\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020543622561626965\n",
      "Average test loss: 0.003476263722197877\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020523918852210044\n",
      "Average test loss: 0.0034455546918842526\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02055773408545388\n",
      "Average test loss: 0.0035086008223394554\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02053010845515463\n",
      "Average test loss: 0.0034244804667929807\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020523786506719058\n",
      "Average test loss: 0.003435837342300349\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020516877060963046\n",
      "Average test loss: 0.0034939407089518175\n",
      "Epoch 260/300\n",
      "Average training loss: 0.020504213967257077\n",
      "Average test loss: 0.0034222355797472927\n",
      "Epoch 261/300\n",
      "Average training loss: 0.020527896419167517\n",
      "Average test loss: 0.0036731826687852543\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020505370115240416\n",
      "Average test loss: 0.0036347478040390543\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020512933790683745\n",
      "Average test loss: 0.003559880179663499\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020465424885352454\n",
      "Average test loss: 0.0034445624384615158\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020504238814115526\n",
      "Average test loss: 0.003439862058808406\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020486911945872835\n",
      "Average test loss: 0.003445403630948729\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02046070102519459\n",
      "Average test loss: 0.0035275259398751787\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020484097424480652\n",
      "Average test loss: 0.003441939005628228\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02045032998257213\n",
      "Average test loss: 0.0034823961899512344\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020443833607766363\n",
      "Average test loss: 0.003686115561880999\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020480467811226845\n",
      "Average test loss: 0.0035280793681740762\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02046573169860575\n",
      "Average test loss: 0.0034722000931700072\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020456976877318487\n",
      "Average test loss: 0.0038520998342169654\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020413077824645572\n",
      "Average test loss: 0.003415316057494945\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02044274624188741\n",
      "Average test loss: 0.003452321130471925\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020427691885166698\n",
      "Average test loss: 0.0035486305496758887\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020450439463059106\n",
      "Average test loss: 0.0034698973863075177\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020417304363515642\n",
      "Average test loss: 0.003478547854969899\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02043084251218372\n",
      "Average test loss: 0.003570034891573919\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020398627999756072\n",
      "Average test loss: 0.003479544619512227\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020417196548647352\n",
      "Average test loss: 0.003494137579575181\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020423255518906645\n",
      "Average test loss: 0.0035503155121372806\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0204309255298641\n",
      "Average test loss: 0.004819405975027217\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020428730209668476\n",
      "Average test loss: 0.003571487867583831\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020378934818837378\n",
      "Average test loss: 0.003582943128214942\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0203860035472446\n",
      "Average test loss: 0.003425296296676\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020358724228209918\n",
      "Average test loss: 0.00477714400490125\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0204018474386798\n",
      "Average test loss: 0.003441239604312513\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020388801082968713\n",
      "Average test loss: 0.0034969749628669684\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020407697775297693\n",
      "Average test loss: 0.0037248031662570104\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02038816456662284\n",
      "Average test loss: 0.003509232401019997\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020405904392401378\n",
      "Average test loss: 0.0035667324242078594\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020343581047323017\n",
      "Average test loss: 0.0034132057384898265\n",
      "Epoch 294/300\n",
      "Average training loss: 0.020352923298875492\n",
      "Average test loss: 0.0034972404214657017\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020365039219458897\n",
      "Average test loss: 0.0037759563186102445\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02034567384256257\n",
      "Average test loss: 0.0035066823781364494\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020343669679429795\n",
      "Average test loss: 0.003623368149002393\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020335276224546964\n",
      "Average test loss: 0.003569996448026763\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02033744709359275\n",
      "Average test loss: 0.0038415792483008572\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02036639807952775\n",
      "Average test loss: 0.003463109275119172\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12205260232753223\n",
      "Average test loss: 0.0060411838615934054\n",
      "Epoch 2/300\n",
      "Average training loss: 0.042251137339406546\n",
      "Average test loss: 0.004831411658889718\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03704197375641929\n",
      "Average test loss: 0.004375542593499025\n",
      "Epoch 4/300\n",
      "Average training loss: 0.034125018747316466\n",
      "Average test loss: 0.0043000496385826\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03185451274779108\n",
      "Average test loss: 0.004534480361060964\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03014118919438786\n",
      "Average test loss: 0.0038264006281064616\n",
      "Epoch 7/300\n",
      "Average training loss: 0.028389457950989405\n",
      "Average test loss: 0.0037240448540283573\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02708935254315535\n",
      "Average test loss: 0.0036446888730343845\n",
      "Epoch 9/300\n",
      "Average training loss: 0.026131335533327527\n",
      "Average test loss: 0.003931485996064212\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02516703584127956\n",
      "Average test loss: 0.0034648653438521756\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02442886487642924\n",
      "Average test loss: 0.0035604609226187072\n",
      "Epoch 12/300\n",
      "Average training loss: 0.023820370422469244\n",
      "Average test loss: 0.0032759332994206085\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02328850522140662\n",
      "Average test loss: 0.0031651997172998055\n",
      "Epoch 14/300\n",
      "Average training loss: 0.022821250561210843\n",
      "Average test loss: 0.0030402946159657504\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022347898395525083\n",
      "Average test loss: 0.0044671485635141535\n",
      "Epoch 16/300\n",
      "Average training loss: 0.022090089034703043\n",
      "Average test loss: 0.0031853343351847595\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021669382951325842\n",
      "Average test loss: 0.0028267353396448825\n",
      "Epoch 18/300\n",
      "Average training loss: 0.021367697848214042\n",
      "Average test loss: 0.002789630976712538\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021114087131288317\n",
      "Average test loss: 0.002717680152091715\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020881814294391208\n",
      "Average test loss: 0.0027140022986051108\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02069987038274606\n",
      "Average test loss: 0.0026206850515057643\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02049791851308611\n",
      "Average test loss: 0.0027167219169851806\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020340062810315026\n",
      "Average test loss: 0.0025988632624761926\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02012907773339086\n",
      "Average test loss: 0.002536807192903426\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019969910960230562\n",
      "Average test loss: 0.002570002131784956\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019852554433875614\n",
      "Average test loss: 0.0025313434809860254\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01977455760538578\n",
      "Average test loss: 0.0024854375711745686\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019630600132875972\n",
      "Average test loss: 0.0024610816261006726\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019514462856782808\n",
      "Average test loss: 0.0025351386157174906\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01940234552986092\n",
      "Average test loss: 0.0026747843449314436\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019343355254994497\n",
      "Average test loss: 0.0024229848724272515\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01921642611341344\n",
      "Average test loss: 0.0024853144335663982\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01912830907603105\n",
      "Average test loss: 0.0024070945637714534\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019057255463467704\n",
      "Average test loss: 0.002395805274032884\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018951837718486787\n",
      "Average test loss: 0.0024088568881981904\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018880593020055028\n",
      "Average test loss: 0.002500635127019551\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018851697565780746\n",
      "Average test loss: 0.0024116816189554\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018770863256520696\n",
      "Average test loss: 0.002366308180615306\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018696869435409706\n",
      "Average test loss: 0.002335410487320688\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018656468749046327\n",
      "Average test loss: 0.002336172338678605\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018620377134945656\n",
      "Average test loss: 0.002325279160299235\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018568127047684457\n",
      "Average test loss: 0.002942534454166889\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018488982898493607\n",
      "Average test loss: 0.002328264623880386\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018462160682512656\n",
      "Average test loss: 0.002344856283730931\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018415432799607515\n",
      "Average test loss: 0.0024896080820924708\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0183530186480946\n",
      "Average test loss: 0.002346320074879461\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018311283613244692\n",
      "Average test loss: 0.0023598739080544977\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018282331022951338\n",
      "Average test loss: 0.0023684305718375577\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018242245501942103\n",
      "Average test loss: 0.002295446269938515\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018225568138890796\n",
      "Average test loss: 0.002425066594034433\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018155680757429866\n",
      "Average test loss: 0.002297641274933186\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018161061449183356\n",
      "Average test loss: 0.002322959383742677\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01809060327708721\n",
      "Average test loss: 0.0023012453452166586\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018024537068274286\n",
      "Average test loss: 0.0023289737110543584\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018032935928139423\n",
      "Average test loss: 0.0023223971443043813\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017979308486812645\n",
      "Average test loss: 0.002308079366365241\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017986417286925845\n",
      "Average test loss: 0.002385897384128637\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01790840350257026\n",
      "Average test loss: 0.002267517087360223\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01788240700049533\n",
      "Average test loss: 0.0023516481527023846\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01784404200812181\n",
      "Average test loss: 0.0023603562736469836\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017844103643463716\n",
      "Average test loss: 0.0022690224351568354\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01780099530186918\n",
      "Average test loss: 0.0023087077277402085\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017784906388984785\n",
      "Average test loss: 0.0022961195807697046\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017761672522458766\n",
      "Average test loss: 0.0023894704547193315\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017715864032506943\n",
      "Average test loss: 0.002277663099889954\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017707891614900696\n",
      "Average test loss: 0.0022587398841149276\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017685507582293616\n",
      "Average test loss: 0.0023148191941695082\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017663029005130133\n",
      "Average test loss: 0.0022659040552874404\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01763078710767958\n",
      "Average test loss: 0.0025890558364076747\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01761682803928852\n",
      "Average test loss: 0.0022589267670280406\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017599898050228754\n",
      "Average test loss: 0.002246643001006709\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017591838071743648\n",
      "Average test loss: 0.002388562772423029\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01753396457930406\n",
      "Average test loss: 0.002546127769061261\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01753996904856629\n",
      "Average test loss: 0.002261353309990631\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017503374954064687\n",
      "Average test loss: 0.0022991986609995365\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017459508271680937\n",
      "Average test loss: 0.0022867727792925305\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017478749034305414\n",
      "Average test loss: 0.0022788152173161508\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017434425072537528\n",
      "Average test loss: 0.002285935919524895\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017414213528235755\n",
      "Average test loss: 0.002382148377597332\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01742041428387165\n",
      "Average test loss: 0.002644870279977719\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017398392119341428\n",
      "Average test loss: 0.002384096422129207\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017394747378925483\n",
      "Average test loss: 0.0022585661019095117\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01734471203552352\n",
      "Average test loss: 0.0023635743637051847\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01734139535824458\n",
      "Average test loss: 0.002286173235004147\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017317903673483265\n",
      "Average test loss: 0.002304597705602646\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017294758915901185\n",
      "Average test loss: 0.0023543500300082895\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017273958346909948\n",
      "Average test loss: 0.002268839876891838\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017275163855817583\n",
      "Average test loss: 0.002269906521257427\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01727039727485842\n",
      "Average test loss: 0.0022802628953423765\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017236406784918574\n",
      "Average test loss: 0.002413408455956313\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017217586542997095\n",
      "Average test loss: 0.0023480956703424453\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017200396421055\n",
      "Average test loss: 0.002253070637376772\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017181070388191277\n",
      "Average test loss: 0.0022758286588618324\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017179074365231726\n",
      "Average test loss: 0.0023180136666115787\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01718251515014304\n",
      "Average test loss: 0.002564614028773374\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017136304462121593\n",
      "Average test loss: 0.0022712618824508456\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01711013696839412\n",
      "Average test loss: 0.0023325815724415913\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01713874335255888\n",
      "Average test loss: 0.0023154336503810355\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017090256101555294\n",
      "Average test loss: 0.0022875291404003896\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017096141043636534\n",
      "Average test loss: 0.002206960579587354\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017090275080667603\n",
      "Average test loss: 0.0023954750459848177\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017089230644206206\n",
      "Average test loss: 0.0022831416710590324\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01703039951622486\n",
      "Average test loss: 0.0023692362416121693\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01703547067940235\n",
      "Average test loss: 0.002274960620742705\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017060635909438134\n",
      "Average test loss: 0.00225345306408902\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01703423845105701\n",
      "Average test loss: 0.002378510382026434\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017004166745477254\n",
      "Average test loss: 0.0024027364138099884\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01701407201008664\n",
      "Average test loss: 0.0022657299923400085\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01697807568973965\n",
      "Average test loss: 0.0026853473018854856\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017003347252806028\n",
      "Average test loss: 0.0022364519383344383\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016925899639725687\n",
      "Average test loss: 0.002249688955437806\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01693213551574283\n",
      "Average test loss: 0.002708676519493262\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016942849553293653\n",
      "Average test loss: 0.0022723520011123685\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016925922142962613\n",
      "Average test loss: 0.014658637428035338\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016922400375207265\n",
      "Average test loss: 0.0022940261132187315\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016885123896929952\n",
      "Average test loss: 0.002263963997984926\n",
      "Epoch 117/300\n",
      "Average training loss: 0.016885090872645378\n",
      "Average test loss: 0.0022985672058744564\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016883670623103777\n",
      "Average test loss: 0.002406001294963062\n",
      "Epoch 119/300\n",
      "Average training loss: 0.016909964334633616\n",
      "Average test loss: 0.0023136502378102806\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016836907549864714\n",
      "Average test loss: 0.002308584636905127\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016840831753280428\n",
      "Average test loss: 0.0023115893988352684\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016824235885507532\n",
      "Average test loss: 0.0026266879120634664\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016838328700098725\n",
      "Average test loss: 0.0022565136550822193\n",
      "Epoch 124/300\n",
      "Average training loss: 0.016802774763769575\n",
      "Average test loss: 0.002236073418623871\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016799246138996547\n",
      "Average test loss: 0.002308534181418104\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01680915473567115\n",
      "Average test loss: 0.002227362345904112\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016760698590013716\n",
      "Average test loss: 0.002230208767371045\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01679818623016278\n",
      "Average test loss: 0.0023479327180733283\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016800342104501195\n",
      "Average test loss: 0.0022898046016279192\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016762374103069307\n",
      "Average test loss: 0.004131194473554691\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016733994505471653\n",
      "Average test loss: 0.002293432880813877\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016745245732367037\n",
      "Average test loss: 0.0022333715616001024\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016723111854659186\n",
      "Average test loss: 0.0025733944918546413\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01674133341676659\n",
      "Average test loss: 0.0022390859385745393\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01670963139500883\n",
      "Average test loss: 0.0023650456979456874\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016714783871339428\n",
      "Average test loss: 0.00233027872029278\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016701091764701737\n",
      "Average test loss: 0.0022141714429275857\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016670452926721837\n",
      "Average test loss: 0.0023020698057694567\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016684659471114475\n",
      "Average test loss: 0.00236067372891638\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01669669316543473\n",
      "Average test loss: 0.00227891226278411\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016635840642783378\n",
      "Average test loss: 0.0023057508510020046\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016655041811366875\n",
      "Average test loss: 0.0022951289891368814\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016632473293277952\n",
      "Average test loss: 0.0022821946404874326\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016624831918213102\n",
      "Average test loss: 0.0022493187584396866\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016625933109886117\n",
      "Average test loss: 0.0022486607105367713\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016645164097348848\n",
      "Average test loss: 0.002375247005994121\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016623408623867564\n",
      "Average test loss: 0.0022876082046164405\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016614777956571843\n",
      "Average test loss: 0.002333515729341242\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01659877698371808\n",
      "Average test loss: 0.0024464758802205324\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01658275217148993\n",
      "Average test loss: 0.0022968498611201844\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01657950781368547\n",
      "Average test loss: 0.0022664631434405843\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016570248117049536\n",
      "Average test loss: 0.0022984485452373824\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016570439020792645\n",
      "Average test loss: 0.0036032421009408104\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016574820222126113\n",
      "Average test loss: 0.002370226044414772\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016554664530687862\n",
      "Average test loss: 0.002290072872613867\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016544491704967287\n",
      "Average test loss: 0.002266290256029202\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016531587604019377\n",
      "Average test loss: 0.002255807764414284\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016508984592225818\n",
      "Average test loss: 0.0022785252736260493\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01654648155139552\n",
      "Average test loss: 0.002275213127127952\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016517822345925703\n",
      "Average test loss: 0.002309017197539409\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016543861120939255\n",
      "Average test loss: 0.002228368138273557\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016492268798251948\n",
      "Average test loss: 0.002268721374683082\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01647794269522031\n",
      "Average test loss: 0.0034820739320582813\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016508791390392515\n",
      "Average test loss: 0.0023110380882604255\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016511811397969724\n",
      "Average test loss: 0.002299628439048926\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016459174497259987\n",
      "Average test loss: 0.0022396564221837455\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016443645714885657\n",
      "Average test loss: 0.0023038882759089272\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016462203464574283\n",
      "Average test loss: 0.002307621252619558\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016440080346332656\n",
      "Average test loss: 0.0023155130160351593\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016439292921788164\n",
      "Average test loss: 0.0023475863355108435\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016454752456810738\n",
      "Average test loss: 0.0022432868215772842\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016426075047916835\n",
      "Average test loss: 0.0022996991703079805\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016434233827723396\n",
      "Average test loss: 0.002236575798648927\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016443853314552042\n",
      "Average test loss: 0.0023256466430094506\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016426388728121917\n",
      "Average test loss: 0.0023471041094097825\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016395497186316384\n",
      "Average test loss: 0.002250893014897075\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01638619844781028\n",
      "Average test loss: 0.0023426807510356106\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0163837817410628\n",
      "Average test loss: 0.002325080496362514\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01638614364961783\n",
      "Average test loss: 0.002310902553300063\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016376841813325882\n",
      "Average test loss: 0.002412032067258325\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016382470975319544\n",
      "Average test loss: 0.002409327466454771\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016384290445182057\n",
      "Average test loss: 0.0023281939414640266\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016342810516556103\n",
      "Average test loss: 0.0022636180706322195\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016357660157812967\n",
      "Average test loss: 0.0022891842438321976\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0163510053306818\n",
      "Average test loss: 0.0030876035425398086\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016331626388761733\n",
      "Average test loss: 0.002380306202918291\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01632723229792383\n",
      "Average test loss: 0.0022594129755679104\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01634135498272048\n",
      "Average test loss: 0.002324092027420799\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016305855312281185\n",
      "Average test loss: 0.0023192484279473623\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016340105211569204\n",
      "Average test loss: 0.00225461087272399\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016315269417232936\n",
      "Average test loss: 0.002382036365982559\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016325007461839253\n",
      "Average test loss: 0.0026078484697888296\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016300374817517067\n",
      "Average test loss: 0.002251903787255287\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01629464198152224\n",
      "Average test loss: 0.002259421521383855\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016320277573333847\n",
      "Average test loss: 0.0024018065449264315\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01628974414202902\n",
      "Average test loss: 0.002356043096114364\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01626909595810705\n",
      "Average test loss: 0.002299033816696869\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01627124566750394\n",
      "Average test loss: 0.0023475609570741654\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01627937297936943\n",
      "Average test loss: 0.0048014748526944056\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016277899580697218\n",
      "Average test loss: 0.0023328739731676047\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01627497462183237\n",
      "Average test loss: 0.0024150715641056497\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01624870429105229\n",
      "Average test loss: 0.0023907126633243426\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016262305588357978\n",
      "Average test loss: 0.0022802457794961002\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016259458386235767\n",
      "Average test loss: 0.002374090699478984\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016257250154183972\n",
      "Average test loss: 0.002326854354288015\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01620698559946484\n",
      "Average test loss: 0.002264806809835136\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016237252578139304\n",
      "Average test loss: 0.003549372169085675\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016214256033301355\n",
      "Average test loss: 0.002287326278578904\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01621692575348748\n",
      "Average test loss: 0.0023309409986767503\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016209853255086476\n",
      "Average test loss: 0.0023039357920901643\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016196798539823955\n",
      "Average test loss: 0.002379680031703578\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016205269623961713\n",
      "Average test loss: 0.002242049096359147\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016210721004340383\n",
      "Average test loss: 0.0022652030028402804\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016186382118198606\n",
      "Average test loss: 0.0025576316617015337\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01617523097495238\n",
      "Average test loss: 0.002247711921110749\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016188367907371787\n",
      "Average test loss: 0.0023059207504201266\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016196321468386385\n",
      "Average test loss: 0.0023737509952237207\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016165405697292752\n",
      "Average test loss: 0.002318488775442044\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016146171941939326\n",
      "Average test loss: 0.0024716131327052913\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016164967189232507\n",
      "Average test loss: 0.002338087341023816\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016154191936055818\n",
      "Average test loss: 0.002458100659151872\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01617442436474893\n",
      "Average test loss: 0.0024187431128488645\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016136532313293882\n",
      "Average test loss: 0.002355774586399396\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01614462913076083\n",
      "Average test loss: 0.002395475559971399\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016149469590849348\n",
      "Average test loss: 0.0022869563206202453\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016134917027420467\n",
      "Average test loss: 0.0024694608468562363\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016145492234163814\n",
      "Average test loss: 0.0022516604349431067\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016123105094664626\n",
      "Average test loss: 0.0023493639640510084\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016126021224591468\n",
      "Average test loss: 0.0024382586075613897\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016127803179952833\n",
      "Average test loss: 0.002323722098022699\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01610007138384713\n",
      "Average test loss: 0.002255443661266731\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016112436544564034\n",
      "Average test loss: 0.0022718211400012175\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016115729237596195\n",
      "Average test loss: 0.002361274968004889\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016106822184390492\n",
      "Average test loss: 0.0023429751648671097\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01611324691772461\n",
      "Average test loss: 0.002373976091750794\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016095681086182593\n",
      "Average test loss: 0.0023552363187902504\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016078072557018864\n",
      "Average test loss: 0.0023319405027561716\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016068407411376634\n",
      "Average test loss: 0.00262629259398414\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016094430655241014\n",
      "Average test loss: 0.002391504977933235\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016053495457602873\n",
      "Average test loss: 0.0025025519067421557\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016077488967114025\n",
      "Average test loss: 0.0022833569360276064\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01605673926572005\n",
      "Average test loss: 0.002428272919729352\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016090759810474184\n",
      "Average test loss: 0.0023020242475387124\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01605614651325676\n",
      "Average test loss: 0.002342738688406017\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01606387680106693\n",
      "Average test loss: 0.002308343395487302\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016052376811703045\n",
      "Average test loss: 0.0023220077922774685\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016043960574600433\n",
      "Average test loss: 0.002644094160757959\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016041994971533618\n",
      "Average test loss: 0.0024044909464816253\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016043990660872726\n",
      "Average test loss: 0.002325508459471166\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01604297079394261\n",
      "Average test loss: 0.0023980061217314668\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016033109741078484\n",
      "Average test loss: 0.0024412426754004428\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016015961457457806\n",
      "Average test loss: 0.0023219576904343234\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01602213349027766\n",
      "Average test loss: 0.0023529875786561104\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016044038706355623\n",
      "Average test loss: 0.0023000024356361892\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01601703366968367\n",
      "Average test loss: 0.0022836785276109973\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01601717190610038\n",
      "Average test loss: 0.0023340300710664856\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016007218801312978\n",
      "Average test loss: 0.0023847972402969994\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01601459979183144\n",
      "Average test loss: 0.0023731024473284683\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015995346918702125\n",
      "Average test loss: 0.0023455741993255084\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015993000578549174\n",
      "Average test loss: 0.002295874350911213\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016005508601665497\n",
      "Average test loss: 0.0023061246880226663\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015984895776543353\n",
      "Average test loss: 0.002358438440391587\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015978125799033377\n",
      "Average test loss: 0.0023103249459010032\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015972992470694912\n",
      "Average test loss: 0.0023349506355201206\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015977185236083137\n",
      "Average test loss: 0.0023653437750827933\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015994063172075485\n",
      "Average test loss: 0.0027031867255767187\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01596426042334901\n",
      "Average test loss: 0.002422497720975015\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015967312431997723\n",
      "Average test loss: 0.0023504067121280564\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015964085976282755\n",
      "Average test loss: 0.0022917586943755545\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015956995531088775\n",
      "Average test loss: 0.0023295049454189008\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015965442859464222\n",
      "Average test loss: 0.002362650975998905\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01595329523086548\n",
      "Average test loss: 0.002385217174577216\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015958838077055084\n",
      "Average test loss: 0.0023880694328496854\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015964190629621348\n",
      "Average test loss: 0.0028008184602691067\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01592852274576823\n",
      "Average test loss: 0.002420964675438073\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015926099840965536\n",
      "Average test loss: 0.002395869071802331\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01594570445186562\n",
      "Average test loss: 0.002354357470654779\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01594305754866865\n",
      "Average test loss: 0.0023910812296801145\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01593286989794837\n",
      "Average test loss: 0.0023911436729960973\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01590564130163855\n",
      "Average test loss: 0.0023140079103824165\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01593183525072204\n",
      "Average test loss: 0.002311360752830903\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01594077132559485\n",
      "Average test loss: 0.002406302188626594\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015898495823144912\n",
      "Average test loss: 0.002386887096489469\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015913816054662067\n",
      "Average test loss: 0.0023400026314581435\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015891352402667206\n",
      "Average test loss: 0.0023056064690980646\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015906095134715238\n",
      "Average test loss: 0.0024058793348570666\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015911287996504042\n",
      "Average test loss: 0.002337326192504002\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01589748621566428\n",
      "Average test loss: 0.00239253274930848\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015905257946915097\n",
      "Average test loss: 0.0023495925431036286\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015904715328580805\n",
      "Average test loss: 0.002369142219838169\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015877886977460648\n",
      "Average test loss: 0.0023102802996420196\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015864237282839085\n",
      "Average test loss: 0.0023311759647395877\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015873714097672038\n",
      "Average test loss: 0.0023194212975601357\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01586479817248053\n",
      "Average test loss: 0.0023246802180591558\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01588134315278795\n",
      "Average test loss: 0.0024055517862240474\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015861653045647673\n",
      "Average test loss: 0.0023936459320700832\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015890714635451636\n",
      "Average test loss: 0.0025213858392089607\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015859617695212363\n",
      "Average test loss: 0.002368953805934224\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015853421171506246\n",
      "Average test loss: 0.002378866515432795\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01584728162901269\n",
      "Average test loss: 0.002308474527154532\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10985713960064782\n",
      "Average test loss: 0.004998553656455543\n",
      "Epoch 2/300\n",
      "Average training loss: 0.036138779292503996\n",
      "Average test loss: 0.004009129245248106\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03123371755083402\n",
      "Average test loss: 0.0041103541666848795\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02860102040734556\n",
      "Average test loss: 0.003518701195095976\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02678421719868978\n",
      "Average test loss: 0.003002864755069216\n",
      "Epoch 6/300\n",
      "Average training loss: 0.024902906628118622\n",
      "Average test loss: 0.0030756914988160134\n",
      "Epoch 7/300\n",
      "Average training loss: 0.023732961108287175\n",
      "Average test loss: 0.0029821703028347756\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022453225817945267\n",
      "Average test loss: 0.002736349596745438\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021539693042635918\n",
      "Average test loss: 0.002718990666584836\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02079289281864961\n",
      "Average test loss: 0.002477412011474371\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0202039339211252\n",
      "Average test loss: 0.002395541507543789\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01967808535695076\n",
      "Average test loss: 0.002450227276939485\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01923612577219804\n",
      "Average test loss: 0.0022160640315463144\n",
      "Epoch 14/300\n",
      "Average training loss: 0.018898339899049866\n",
      "Average test loss: 0.002393214619615012\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01851716617577606\n",
      "Average test loss: 0.0023752395891480974\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0181878778371546\n",
      "Average test loss: 0.0021315990139005913\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01797963251339065\n",
      "Average test loss: 0.002071915529668331\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017633993809421857\n",
      "Average test loss: 0.002275543589351906\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01746400047507551\n",
      "Average test loss: 0.002077812737474839\n",
      "Epoch 20/300\n",
      "Average training loss: 0.017289217758509848\n",
      "Average test loss: 0.001975728947462307\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017075787822405496\n",
      "Average test loss: 0.0020009925957355235\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016960008192393516\n",
      "Average test loss: 0.001932423228604926\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0167776886622111\n",
      "Average test loss: 0.0018853720538318157\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01667074638687902\n",
      "Average test loss: 0.001994247450803717\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016529897881878745\n",
      "Average test loss: 0.0018680848996672364\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01641836417383618\n",
      "Average test loss: 0.0018666559300488895\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016265084938870536\n",
      "Average test loss: 0.0017976804338395596\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01624024579798182\n",
      "Average test loss: 0.0018536150089154642\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016087997636861272\n",
      "Average test loss: 0.0017911555303467646\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016028781005077892\n",
      "Average test loss: 0.0017929030725111564\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015961079004738067\n",
      "Average test loss: 0.0017168592365665568\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01585826265315215\n",
      "Average test loss: 0.0017588759247834484\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01580905697411961\n",
      "Average test loss: 0.001727444429985351\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015796029673682318\n",
      "Average test loss: 0.0017432846334866352\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015696471974253653\n",
      "Average test loss: 0.0017274548568659359\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015599556639790535\n",
      "Average test loss: 0.001723000696135892\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015568106626470884\n",
      "Average test loss: 0.001697105224761698\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015493830691609117\n",
      "Average test loss: 0.0017499767454961935\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015472611569696003\n",
      "Average test loss: 0.0017364436129315031\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01544565277132723\n",
      "Average test loss: 0.0017211883173634608\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015381547511451774\n",
      "Average test loss: 0.0016604494256898762\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015334071474770705\n",
      "Average test loss: 0.0016843209787168437\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015323750192092525\n",
      "Average test loss: 0.0016665518053082956\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015241640602548918\n",
      "Average test loss: 0.0016479643961000775\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015267734156714546\n",
      "Average test loss: 0.0018714824378904369\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015192511048581865\n",
      "Average test loss: 0.0017067110259085894\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015121123703817527\n",
      "Average test loss: 0.0016408034154834846\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015099131626387437\n",
      "Average test loss: 0.001665703138957421\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015071336053311825\n",
      "Average test loss: 0.0016827221553151806\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015064698737528588\n",
      "Average test loss: 0.001638439554721117\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015027564624945323\n",
      "Average test loss: 0.0016673889974546101\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014963796114756001\n",
      "Average test loss: 0.0016530381977144216\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014979722731643252\n",
      "Average test loss: 0.0016477763836996423\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014911242222620382\n",
      "Average test loss: 0.0017185077561686437\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01491492518948184\n",
      "Average test loss: 0.0016908604050469068\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014868883336583774\n",
      "Average test loss: 0.0016654810226625866\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014845715869632032\n",
      "Average test loss: 0.0016417199140414595\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014810352838701671\n",
      "Average test loss: 0.001634025357870592\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014819321293797759\n",
      "Average test loss: 0.0016716373167518113\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014770867422637012\n",
      "Average test loss: 0.0017225600336160925\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014753416052295102\n",
      "Average test loss: 0.0017019930651618375\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014744261809521252\n",
      "Average test loss: 0.0018503376853962739\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014707774573730098\n",
      "Average test loss: 0.0016748766498122778\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01468678591814306\n",
      "Average test loss: 0.0016645661842905812\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014646273897753822\n",
      "Average test loss: 0.0016236333586275577\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014674646933873494\n",
      "Average test loss: 0.001612444105040696\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014630703810188505\n",
      "Average test loss: 0.0021356394870413672\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014608860779967573\n",
      "Average test loss: 0.0017710629648839435\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01458482052716944\n",
      "Average test loss: 0.0016438292674720288\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014566622042821513\n",
      "Average test loss: 0.0016385060963738296\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014553385820653703\n",
      "Average test loss: 0.0016616518981754779\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014527632601559162\n",
      "Average test loss: 0.001636991973552439\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014514286960992549\n",
      "Average test loss: 0.0016196521846060124\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014532726720803314\n",
      "Average test loss: 0.001696588624889652\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014474561097721259\n",
      "Average test loss: 0.0016436560093942615\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0144672698908382\n",
      "Average test loss: 0.0016584036314032144\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014459961032701862\n",
      "Average test loss: 0.0016410949541669753\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014434897977444862\n",
      "Average test loss: 0.0016774137840709752\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014417642907136017\n",
      "Average test loss: 0.0016429332012517584\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014385408091876242\n",
      "Average test loss: 0.0016863416102197435\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014380740616884496\n",
      "Average test loss: 0.001675860258957578\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014382109916044605\n",
      "Average test loss: 0.001608061959553096\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014336477561957306\n",
      "Average test loss: 0.001628626575693488\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014372532565560606\n",
      "Average test loss: 0.0016800492432796294\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01435190146168073\n",
      "Average test loss: 0.0016339637133189373\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014293559461832046\n",
      "Average test loss: 0.0016031170941682325\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01429429586769806\n",
      "Average test loss: 0.001727061891824835\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014289230205118656\n",
      "Average test loss: 0.0016224762379295297\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014270462864802944\n",
      "Average test loss: 0.001704852083283994\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014261443997422855\n",
      "Average test loss: 0.0036708950301011403\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014264623022741741\n",
      "Average test loss: 0.0016468400201863713\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014228067461815146\n",
      "Average test loss: 0.0016383651634678245\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014211747281253339\n",
      "Average test loss: 0.001608249750609199\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014194206920762857\n",
      "Average test loss: 0.001616961133873297\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014207778407053815\n",
      "Average test loss: 0.001673135766138633\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014178566479020648\n",
      "Average test loss: 0.0017580307893868949\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014198222486509217\n",
      "Average test loss: 0.0016219983640023404\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014177296185245117\n",
      "Average test loss: 0.0016280177617445588\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014143308724794123\n",
      "Average test loss: 0.001667066569853988\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014115699494878451\n",
      "Average test loss: 0.0016153457574546338\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014125505259467496\n",
      "Average test loss: 0.001599712499934766\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014138783877094586\n",
      "Average test loss: 0.0016160298015715348\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014098742576936881\n",
      "Average test loss: 0.001597424427771734\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014095167939861616\n",
      "Average test loss: 0.0016924937746177118\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01410041928042968\n",
      "Average test loss: 0.0015973516923064987\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014090612106853062\n",
      "Average test loss: 0.0016285445387475193\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014051800837947263\n",
      "Average test loss: 0.0015930213690218\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014053443792793486\n",
      "Average test loss: 0.0016079813403387865\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01404838999112447\n",
      "Average test loss: 0.0015937959090289143\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014014031757083205\n",
      "Average test loss: 0.002011908061284986\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014045973999632729\n",
      "Average test loss: 0.0017992576309997174\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014030046617819203\n",
      "Average test loss: 0.001671024886166884\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014019890137844616\n",
      "Average test loss: 0.0016001132492803865\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013999973366657892\n",
      "Average test loss: 0.0016541985781449412\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01398510495490498\n",
      "Average test loss: 0.0016057256553322077\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01398891593557265\n",
      "Average test loss: 0.0017198192322005828\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013961242192321354\n",
      "Average test loss: 0.0017206970573299461\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013970388841297891\n",
      "Average test loss: 0.0016288044200175337\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013934542832275233\n",
      "Average test loss: 0.0016191843152046203\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013929203377829658\n",
      "Average test loss: 0.0016069420466406478\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013946497833563222\n",
      "Average test loss: 0.0016565527635005613\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013930033805469672\n",
      "Average test loss: 0.005631724635553029\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013917198101679485\n",
      "Average test loss: 0.001633004227342705\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013921588893565867\n",
      "Average test loss: 0.0015891499001946714\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01389121142278115\n",
      "Average test loss: 0.0016015618296547067\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01389325357725223\n",
      "Average test loss: 0.001655603843430678\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013879523899820116\n",
      "Average test loss: 0.0016285848911437723\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0138593424078491\n",
      "Average test loss: 0.0017169870574855143\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013872848090198306\n",
      "Average test loss: 0.0017104440153472952\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013867719793485271\n",
      "Average test loss: 0.0016173671864800983\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013820353688465225\n",
      "Average test loss: 0.0016227248002671534\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01383340133809381\n",
      "Average test loss: 0.0016942391813629203\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01382072844935788\n",
      "Average test loss: 0.0016058055114828878\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013824472343756092\n",
      "Average test loss: 0.0016912507117829388\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013821180252979199\n",
      "Average test loss: 0.0017418599807553822\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013781792011525896\n",
      "Average test loss: 0.001651503260143929\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013807208110060957\n",
      "Average test loss: 0.0016411001269395153\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01378656824512614\n",
      "Average test loss: 0.0016250789208958547\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013785115854607687\n",
      "Average test loss: 0.0018491645956204997\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013798281863331794\n",
      "Average test loss: 0.0016709054233506322\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013779202510913214\n",
      "Average test loss: 0.001655930285445518\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013778380374113719\n",
      "Average test loss: 0.0016317306923576526\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013740046250323455\n",
      "Average test loss: 0.0016017867181864049\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01375439176046186\n",
      "Average test loss: 0.0016010436239755816\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013745136886007255\n",
      "Average test loss: 0.00161158904640211\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013740780832866827\n",
      "Average test loss: 0.0016402366264826722\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013719556846552424\n",
      "Average test loss: 0.001637822845743762\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013713771339919831\n",
      "Average test loss: 0.001603957800194621\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013708055359621843\n",
      "Average test loss: 0.0016219860350506173\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013701028737756941\n",
      "Average test loss: 0.0016247754109402497\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013677469063136313\n",
      "Average test loss: 0.001644858472256197\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013699888258344598\n",
      "Average test loss: 0.0016005517569267088\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013685348979300923\n",
      "Average test loss: 0.0016161913675152592\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013673821479082108\n",
      "Average test loss: 0.001599778427535461\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013668686439593633\n",
      "Average test loss: 0.0019307430501406392\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013664860871103074\n",
      "Average test loss: 0.001646119685119225\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01366430826816294\n",
      "Average test loss: 0.0016475230447120137\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013642958767712116\n",
      "Average test loss: 0.0017213852424174548\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01364579825517204\n",
      "Average test loss: 0.0016893564690318372\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013614178053206867\n",
      "Average test loss: 0.0016583388683696588\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013636643220980961\n",
      "Average test loss: 0.001612671877257526\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01361681549747785\n",
      "Average test loss: 0.001605394017468724\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013614104870292876\n",
      "Average test loss: 0.0016655357054745157\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013623261563479901\n",
      "Average test loss: 0.0016785242115664814\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01359189851085345\n",
      "Average test loss: 0.001667092009447515\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013609119325876235\n",
      "Average test loss: 0.001703342811204493\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013600669789645407\n",
      "Average test loss: 0.0017091318784902493\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013599555817743142\n",
      "Average test loss: 0.001653601103565759\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013602509340478314\n",
      "Average test loss: 0.0016119358248801695\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013561188562876648\n",
      "Average test loss: 0.0016955590324683322\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013575180182026492\n",
      "Average test loss: 0.0017658249804129204\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01356515198118157\n",
      "Average test loss: 0.0016488127429038287\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013550123263564374\n",
      "Average test loss: 0.00167781246950229\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01355687945501672\n",
      "Average test loss: 0.0016468151598754857\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013539158360825644\n",
      "Average test loss: 0.0016766227228153083\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013546216222974989\n",
      "Average test loss: 0.0016849260674789547\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013547692250874308\n",
      "Average test loss: 0.0016831050937374432\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013525489173001714\n",
      "Average test loss: 0.0017080058184348874\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013538428368667761\n",
      "Average test loss: 0.001655545648187399\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01353467161125607\n",
      "Average test loss: 0.0016269598722768327\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013516176677412457\n",
      "Average test loss: 0.0016567847842557564\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013501713713837994\n",
      "Average test loss: 0.0016206019704954491\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013488490419255362\n",
      "Average test loss: 0.0016198344023691284\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013506150307754676\n",
      "Average test loss: 0.0016667366858778728\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013475417058500978\n",
      "Average test loss: 0.0016601705292446747\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013502843650678794\n",
      "Average test loss: 0.0016319173525811898\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013481824037929377\n",
      "Average test loss: 0.0016257540779188275\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013468199773795075\n",
      "Average test loss: 0.0018420260017220345\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013473888744082715\n",
      "Average test loss: 0.0016802678329663145\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01345524690631363\n",
      "Average test loss: 0.0017085760615559089\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013462875597178936\n",
      "Average test loss: 0.0016839075057456891\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013442676837245622\n",
      "Average test loss: 0.0017546809419161744\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013467999688453145\n",
      "Average test loss: 0.0016059797964990139\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013441873240802022\n",
      "Average test loss: 0.001648899934358067\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013439956908424696\n",
      "Average test loss: 0.001629061562836998\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013424566288789113\n",
      "Average test loss: 0.001660024585771478\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01342532891780138\n",
      "Average test loss: 0.0016362909658087625\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013420592551430067\n",
      "Average test loss: 0.0016491055568266246\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013413363178571065\n",
      "Average test loss: 0.011354580988486608\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013445024838878048\n",
      "Average test loss: 0.0017153765886194175\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0134017595011327\n",
      "Average test loss: 0.0016525709375532138\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01341793802132209\n",
      "Average test loss: 0.0016236603413191106\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013420665808849864\n",
      "Average test loss: 0.0016779873931987417\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013398624893691804\n",
      "Average test loss: 0.0016832339014444086\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013377771351072524\n",
      "Average test loss: 0.001626901659804086\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013406379589604007\n",
      "Average test loss: 0.0016744708644433155\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013402855351567268\n",
      "Average test loss: 0.0016850825140459669\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013362535196873877\n",
      "Average test loss: 0.0016923894254076812\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013406689389712281\n",
      "Average test loss: 0.001976171133418878\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013357992045581342\n",
      "Average test loss: 0.0016671225428581237\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013348276132510767\n",
      "Average test loss: 0.001722066244110465\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013351967920031812\n",
      "Average test loss: 0.001644183158564071\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01338604414711396\n",
      "Average test loss: 0.001651115094156315\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013352901329596838\n",
      "Average test loss: 0.0016169008021760319\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013352095706595315\n",
      "Average test loss: 0.0016897791210148069\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013348522103495068\n",
      "Average test loss: 0.0016539229053176112\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013338291237751643\n",
      "Average test loss: 0.001621464133469595\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013316500188575851\n",
      "Average test loss: 0.0016329826941299769\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013320760945479074\n",
      "Average test loss: 0.0016910570445987914\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013339391287002298\n",
      "Average test loss: 0.0016774459578510787\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013338600246442688\n",
      "Average test loss: 0.0017475056941103604\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013330763877265983\n",
      "Average test loss: 0.0016611229358240963\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013297860304514566\n",
      "Average test loss: 0.0017166160425792138\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013326398599478933\n",
      "Average test loss: 0.0017766823957984647\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013315309159457684\n",
      "Average test loss: 0.0016862285195125475\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013300996172345347\n",
      "Average test loss: 0.0018176566995680332\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013291824632220797\n",
      "Average test loss: 0.001727272716557814\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01331552221129338\n",
      "Average test loss: 0.001619278801398145\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013290817719366816\n",
      "Average test loss: 0.0016330980687505668\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013309238149060144\n",
      "Average test loss: 0.001706463259851767\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013280263019104798\n",
      "Average test loss: 0.0016764016189715928\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013261707919339338\n",
      "Average test loss: 0.0016737493904721405\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013289475318458345\n",
      "Average test loss: 0.0017213346266912088\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01327752590427796\n",
      "Average test loss: 0.0016210522695134083\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013282629070182642\n",
      "Average test loss: 0.001678654046315286\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01326048546946711\n",
      "Average test loss: 0.0017040984596953623\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013251904446217749\n",
      "Average test loss: 0.0017001806643481055\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01324675877392292\n",
      "Average test loss: 0.0016549480391873253\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0132410838968224\n",
      "Average test loss: 0.0017060933644986816\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013265364128682348\n",
      "Average test loss: 0.0016463136572597756\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013235975739028719\n",
      "Average test loss: 0.0017269945626871453\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013232045861581962\n",
      "Average test loss: 0.003069884370598528\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013235042710271146\n",
      "Average test loss: 0.0016888042433808247\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013229828736020459\n",
      "Average test loss: 0.0017296838017387523\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01322234244313505\n",
      "Average test loss: 0.0016571359141833253\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013230477560725477\n",
      "Average test loss: 0.0016781525069640743\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013209290410081545\n",
      "Average test loss: 0.0017082889013820225\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013217895728846392\n",
      "Average test loss: 0.001632164179864857\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013227105318672128\n",
      "Average test loss: 0.0016340083934159742\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01321050469411744\n",
      "Average test loss: 0.0017549501026256217\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013198614245487584\n",
      "Average test loss: 0.0016805680783258544\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013214423042204645\n",
      "Average test loss: 0.0016634278022166756\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013208191408051385\n",
      "Average test loss: 0.0017293509875113764\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013210244602627224\n",
      "Average test loss: 0.001644728116898073\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013185994541479482\n",
      "Average test loss: 0.0016259270168633925\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013171387429038683\n",
      "Average test loss: 0.0016616102922190395\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013185070307718382\n",
      "Average test loss: 0.0018679139980425437\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013180319569177097\n",
      "Average test loss: 0.0016924624349921942\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013199236041141881\n",
      "Average test loss: 0.0016421392157466876\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013161030798322624\n",
      "Average test loss: 0.0016847813442970316\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013171428456074661\n",
      "Average test loss: 0.0016839796545811826\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01318159601257907\n",
      "Average test loss: 0.0016346572299177449\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013169037163257598\n",
      "Average test loss: 0.0016414371786846055\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013161750352217092\n",
      "Average test loss: 0.0016672840629600816\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013142484339574973\n",
      "Average test loss: 0.0016984909171652463\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013148722980585363\n",
      "Average test loss: 0.0016545126792043448\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013167399652302266\n",
      "Average test loss: 0.0017876670682388876\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013146143820550706\n",
      "Average test loss: 0.001656570007507172\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013137084012230238\n",
      "Average test loss: 0.0016369996684499912\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01312499047567447\n",
      "Average test loss: 0.0016699352492060926\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013130217488441202\n",
      "Average test loss: 0.001659344807991551\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013122522935271263\n",
      "Average test loss: 0.001642949872960647\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013167992180420293\n",
      "Average test loss: 0.0016436690472376844\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013117606052094036\n",
      "Average test loss: 0.0017245018911651439\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013131000541150569\n",
      "Average test loss: 0.001686196819672154\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013144460853603152\n",
      "Average test loss: 0.002224662756340371\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01312725496871604\n",
      "Average test loss: 0.001703000106331375\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013114504083991051\n",
      "Average test loss: 0.0017086999242504437\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013105135016143321\n",
      "Average test loss: 0.001672892042539186\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013102946108414067\n",
      "Average test loss: 0.0016506578457645244\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01310531343271335\n",
      "Average test loss: 0.0017160190037555166\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013102963351541095\n",
      "Average test loss: 0.0017717264604030383\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013106929439637396\n",
      "Average test loss: 0.0016600754434863726\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013100412267777655\n",
      "Average test loss: 0.0016501388422523935\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013081283898817169\n",
      "Average test loss: 0.0016687632964717018\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013082482649220361\n",
      "Average test loss: 0.0016375847030431032\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013089686882992585\n",
      "Average test loss: 0.001677659111407896\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013086167360345522\n",
      "Average test loss: 0.001691407705243263\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01307759204838011\n",
      "Average test loss: 0.0016783237248245214\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013077348089880413\n",
      "Average test loss: 0.0016959398929029704\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01307058919303947\n",
      "Average test loss: 0.001618848006758425\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013073132404022747\n",
      "Average test loss: 0.0017043248209067518\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013068748016324308\n",
      "Average test loss: 0.0017179652848798369\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013058194503188134\n",
      "Average test loss: 0.0016709216009411546\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013064306596914927\n",
      "Average test loss: 0.0016693742390101155\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01306244022730324\n",
      "Average test loss: 0.0016633173194196488\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013044009891649087\n",
      "Average test loss: 0.0018434939450687833\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013063305041028393\n",
      "Average test loss: 0.001673377974786692\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013052151157624191\n",
      "Average test loss: 0.0017074785913444228\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01303966480327977\n",
      "Average test loss: 0.00166780578945246\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.29\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.45\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.06\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.01\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.51\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.8128470046255323\n",
      "Average test loss: 0.013332056494222746\n",
      "Epoch 2/300\n",
      "Average training loss: 0.6274896492958069\n",
      "Average test loss: 0.011093540892004967\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4211861271063487\n",
      "Average test loss: 0.008965187228388256\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3317610483699375\n",
      "Average test loss: 0.008080871842387649\n",
      "Epoch 5/300\n",
      "Average training loss: 0.27819782082239786\n",
      "Average test loss: 0.008764437411394384\n",
      "Epoch 6/300\n",
      "Average training loss: 0.24363117143842908\n",
      "Average test loss: 0.007708004371159607\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2193109679089652\n",
      "Average test loss: 0.008623649338881175\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1980011900001102\n",
      "Average test loss: 0.0076002062120371396\n",
      "Epoch 9/300\n",
      "Average training loss: 0.18619073836008707\n",
      "Average test loss: 0.007526874903589487\n",
      "Epoch 10/300\n",
      "Average training loss: 0.17451438939571381\n",
      "Average test loss: 0.006888441105269724\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1638106925090154\n",
      "Average test loss: 0.007146347532255782\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15503997614648607\n",
      "Average test loss: 0.00767756038531661\n",
      "Epoch 13/300\n",
      "Average training loss: 0.14883469831943513\n",
      "Average test loss: 0.008054684047069815\n",
      "Epoch 14/300\n",
      "Average training loss: 0.14500976933373344\n",
      "Average test loss: 0.006603341436634461\n",
      "Epoch 15/300\n",
      "Average training loss: 0.13902317062351438\n",
      "Average test loss: 0.01069725257737769\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13368257009983062\n",
      "Average test loss: 0.006507879512591495\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13053409043947856\n",
      "Average test loss: 0.006742696377966139\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1262245808839798\n",
      "Average test loss: 0.006230209941665331\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1223761701716317\n",
      "Average test loss: 0.0061372791702548665\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11944809810320536\n",
      "Average test loss: 0.005883873764011595\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11663518931468328\n",
      "Average test loss: 0.006034900177684095\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11460930198431014\n",
      "Average test loss: 0.006022119041946199\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11250523312886555\n",
      "Average test loss: 0.005699287083413866\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11001906024085151\n",
      "Average test loss: 0.005939401433285739\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10841476313273112\n",
      "Average test loss: 0.005659682564023469\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10672750239239799\n",
      "Average test loss: 0.005691083242081934\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10511705195241504\n",
      "Average test loss: 0.00570907912361953\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10370460611581803\n",
      "Average test loss: 0.005641645963407225\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1023174273967743\n",
      "Average test loss: 0.005573250837624073\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10111357270346748\n",
      "Average test loss: 0.005812532988066475\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09995292733775245\n",
      "Average test loss: 0.0057342789330416255\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0990669193400277\n",
      "Average test loss: 0.0055864425086312825\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09746913480758666\n",
      "Average test loss: 0.0054114664275613095\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09683204619752037\n",
      "Average test loss: 0.005512009188532829\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09590738008419673\n",
      "Average test loss: 0.0054896130474905176\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09526401447587543\n",
      "Average test loss: 0.00535846832787825\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09437228131956524\n",
      "Average test loss: 0.005386904171150592\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09368187338113784\n",
      "Average test loss: 0.005532510957784123\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09311389815145069\n",
      "Average test loss: 0.005395242664549086\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09232124642531077\n",
      "Average test loss: 0.006523760909835498\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09159261101484299\n",
      "Average test loss: 0.005320796030677027\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09116538857751423\n",
      "Average test loss: 0.005431553258250157\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09055479350354936\n",
      "Average test loss: 0.0053605530853900645\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09035647123058636\n",
      "Average test loss: 0.011493874739027686\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08931831574440002\n",
      "Average test loss: 0.005263351831171248\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08884247787793477\n",
      "Average test loss: 0.005249204558216863\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08899969235393736\n",
      "Average test loss: 0.0052675188038912085\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0880761856370502\n",
      "Average test loss: 0.0798260781665643\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08774901866912842\n",
      "Average test loss: 0.005526884158659312\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08703966420226628\n",
      "Average test loss: 0.005610709319305089\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08678653769360649\n",
      "Average test loss: 0.005386124171316624\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08656631234619352\n",
      "Average test loss: 0.006333302482962608\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0860719634393851\n",
      "Average test loss: 0.00674296149963306\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08578923360506693\n",
      "Average test loss: 0.005330361071974039\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08552613428566191\n",
      "Average test loss: 0.00623274214234617\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08520822446876102\n",
      "Average test loss: 0.010745537815822496\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08508693936136034\n",
      "Average test loss: 0.005609697143236796\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08445135033792919\n",
      "Average test loss: 0.005470288484046857\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08448838834298981\n",
      "Average test loss: 0.005336611352447006\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0839129956761996\n",
      "Average test loss: 0.005942146862339642\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08369771545463138\n",
      "Average test loss: 0.005717256021582418\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08341800059874853\n",
      "Average test loss: 0.005317488319343991\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08337326520681382\n",
      "Average test loss: 0.005294845484611061\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08566225594282151\n",
      "Average test loss: 0.005276262979126639\n",
      "Epoch 65/300\n",
      "Average training loss: 2930.4829006026785\n",
      "Average test loss: 0.26597156352467005\n",
      "Epoch 66/300\n",
      "Average training loss: 16.752379392835827\n",
      "Average test loss: 0.08390814020236333\n",
      "Epoch 67/300\n",
      "Average training loss: 15.438389114379882\n",
      "Average test loss: 0.05604206578599082\n",
      "Epoch 68/300\n",
      "Average training loss: 14.530848487854003\n",
      "Average test loss: 0.04022311862640911\n",
      "Epoch 69/300\n",
      "Average training loss: 13.745560516357422\n",
      "Average test loss: 0.031086016330454085\n",
      "Epoch 70/300\n",
      "Average training loss: 13.032252241346571\n",
      "Average test loss: 0.02470721825460593\n",
      "Epoch 71/300\n",
      "Average training loss: 12.353887524074977\n",
      "Average test loss: 0.021827858020861943\n",
      "Epoch 72/300\n",
      "Average training loss: 11.68458353000217\n",
      "Average test loss: 0.019126448483930695\n",
      "Epoch 73/300\n",
      "Average training loss: 10.992932864718966\n",
      "Average test loss: 0.01617744648704926\n",
      "Epoch 74/300\n",
      "Average training loss: 10.246233601040311\n",
      "Average test loss: 0.014427562324537171\n",
      "Epoch 75/300\n",
      "Average training loss: 9.24921243709988\n",
      "Average test loss: 0.015930297722419103\n",
      "Epoch 76/300\n",
      "Average training loss: 7.837648277282715\n",
      "Average test loss: 0.013489529007010989\n",
      "Epoch 77/300\n",
      "Average training loss: 6.670309736039903\n",
      "Average test loss: 0.01201987124648359\n",
      "Epoch 78/300\n",
      "Average training loss: 5.797051696777344\n",
      "Average test loss: 0.012985844770239459\n",
      "Epoch 79/300\n",
      "Average training loss: 5.024858857472737\n",
      "Average test loss: 0.0113670785319474\n",
      "Epoch 80/300\n",
      "Average training loss: 4.304700292163425\n",
      "Average test loss: 0.010479976243442959\n",
      "Epoch 81/300\n",
      "Average training loss: 3.6628903823428685\n",
      "Average test loss: 0.010289122863776154\n",
      "Epoch 82/300\n",
      "Average training loss: 3.124371452967326\n",
      "Average test loss: 0.009881299720870124\n",
      "Epoch 83/300\n",
      "Average training loss: 2.6836171554989283\n",
      "Average test loss: 0.008948020766178767\n",
      "Epoch 84/300\n",
      "Average training loss: 2.318640195210775\n",
      "Average test loss: 0.008268761436972354\n",
      "Epoch 85/300\n",
      "Average training loss: 2.0040279976526896\n",
      "Average test loss: 0.007456503584567044\n",
      "Epoch 86/300\n",
      "Average training loss: 1.7273154134750366\n",
      "Average test loss: 0.008000581256217427\n",
      "Epoch 87/300\n",
      "Average training loss: 1.4780749365488688\n",
      "Average test loss: 0.007052165596021546\n",
      "Epoch 88/300\n",
      "Average training loss: 1.262877197901408\n",
      "Average test loss: 0.007460201909558641\n",
      "Epoch 89/300\n",
      "Average training loss: 1.076039757569631\n",
      "Average test loss: 0.0069418406805230514\n",
      "Epoch 90/300\n",
      "Average training loss: 0.9164940649138557\n",
      "Average test loss: 0.006716811202466488\n",
      "Epoch 91/300\n",
      "Average training loss: 0.7771399266984728\n",
      "Average test loss: 0.006333083968609572\n",
      "Epoch 92/300\n",
      "Average training loss: 0.6536262106895446\n",
      "Average test loss: 0.006299695919785235\n",
      "Epoch 93/300\n",
      "Average training loss: 0.5447065833674537\n",
      "Average test loss: 0.006146952463520898\n",
      "Epoch 94/300\n",
      "Average training loss: 0.4501486583550771\n",
      "Average test loss: 0.006618927145169841\n",
      "Epoch 95/300\n",
      "Average training loss: 0.37162510490417483\n",
      "Average test loss: 0.005937990088843637\n",
      "Epoch 96/300\n",
      "Average training loss: 0.3034865066210429\n",
      "Average test loss: 0.005863819429857863\n",
      "Epoch 97/300\n",
      "Average training loss: 0.249470715019438\n",
      "Average test loss: 0.0058195262046323885\n",
      "Epoch 98/300\n",
      "Average training loss: 0.21217741066879697\n",
      "Average test loss: 0.005749904451270898\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1861777533955044\n",
      "Average test loss: 0.005614553769429525\n",
      "Epoch 100/300\n",
      "Average training loss: 0.16663046859370337\n",
      "Average test loss: 0.005571981553816133\n",
      "Epoch 101/300\n",
      "Average training loss: 0.15134133493900298\n",
      "Average test loss: 0.005495124677817027\n",
      "Epoch 102/300\n",
      "Average training loss: 0.13967073686917622\n",
      "Average test loss: 0.005445703251908223\n",
      "Epoch 103/300\n",
      "Average training loss: 0.13043417616022956\n",
      "Average test loss: 0.0073406184150113\n",
      "Epoch 104/300\n",
      "Average training loss: 0.12346295710404714\n",
      "Average test loss: 0.005546194312059217\n",
      "Epoch 105/300\n",
      "Average training loss: 0.1184517020781835\n",
      "Average test loss: 0.005590493616130617\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11390504262182448\n",
      "Average test loss: 0.005452828586515453\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11055759876966477\n",
      "Average test loss: 0.005430634544541438\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10744389632344246\n",
      "Average test loss: 0.0056016430552634925\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10470070883962843\n",
      "Average test loss: 0.005469411976221535\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10301719485388862\n",
      "Average test loss: 0.00536191480482618\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10011252611875535\n",
      "Average test loss: 0.0054600909488896525\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09817134439282947\n",
      "Average test loss: 0.0052830261439085\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09626772672600216\n",
      "Average test loss: 0.005206217858526442\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09461280784673161\n",
      "Average test loss: 0.005229268670702974\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0930530774725808\n",
      "Average test loss: 0.005223324492573738\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09159590088658863\n",
      "Average test loss: 0.005255994348890252\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09003439847628275\n",
      "Average test loss: 0.005195478448023399\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08864069237311681\n",
      "Average test loss: 0.0052244065879947605\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0874206678337521\n",
      "Average test loss: 0.011351895462307665\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08637504479620192\n",
      "Average test loss: 0.00559249623450968\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09177323212226232\n",
      "Average test loss: 0.005622325672457616\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08786090701818466\n",
      "Average test loss: 0.005221403820233212\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08667644408676359\n",
      "Average test loss: 0.0053438437581062315\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08457085389229986\n",
      "Average test loss: 0.006937647932105594\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08410807702276442\n",
      "Average test loss: 0.005224555597123173\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08368261404832204\n",
      "Average test loss: 0.006240992873907089\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08341357519229253\n",
      "Average test loss: 0.005373880283286174\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08298203785883056\n",
      "Average test loss: 0.005221082655712962\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0828231470419301\n",
      "Average test loss: 0.0056145618342691\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08240236427386602\n",
      "Average test loss: 0.006368771549728182\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08235500836372375\n",
      "Average test loss: 0.005454371848040157\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08178681142462624\n",
      "Average test loss: 0.0053685021468748645\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0817206598189142\n",
      "Average test loss: 0.005230317197326157\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08145358203517065\n",
      "Average test loss: 0.005339687316574984\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08130760036905607\n",
      "Average test loss: 0.005475027345948749\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08090633607904116\n",
      "Average test loss: 0.005659185477015045\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08074522462818358\n",
      "Average test loss: 0.005222124275234011\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08038683078686397\n",
      "Average test loss: 0.005355844278302458\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08025388116306728\n",
      "Average test loss: 0.005276468484352033\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08013847646448348\n",
      "Average test loss: 0.005252081971201632\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07973577999406391\n",
      "Average test loss: 0.00525241822997729\n",
      "Epoch 142/300\n",
      "Average training loss: 59.00051628676388\n",
      "Average test loss: 0.02970312677986092\n",
      "Epoch 143/300\n",
      "Average training loss: 5.888371849483914\n",
      "Average test loss: 0.015601435316933527\n",
      "Epoch 144/300\n",
      "Average training loss: 4.652123341878255\n",
      "Average test loss: 0.013106140403283967\n",
      "Epoch 145/300\n",
      "Average training loss: 3.894885395050049\n",
      "Average test loss: 0.011496985589464506\n",
      "Epoch 146/300\n",
      "Average training loss: 3.3429705827501084\n",
      "Average test loss: 0.010475666715866989\n",
      "Epoch 147/300\n",
      "Average training loss: 2.878576021406386\n",
      "Average test loss: 0.00984269700696071\n",
      "Epoch 148/300\n",
      "Average training loss: 2.4803038537767197\n",
      "Average test loss: 0.010123380279375448\n",
      "Epoch 149/300\n",
      "Average training loss: 2.1245266354878742\n",
      "Average test loss: 0.01244666318429841\n",
      "Epoch 150/300\n",
      "Average training loss: 1.8212672983805338\n",
      "Average test loss: 0.008417715537879202\n",
      "Epoch 151/300\n",
      "Average training loss: 1.5663137162526448\n",
      "Average test loss: 0.009105322210325135\n",
      "Epoch 152/300\n",
      "Average training loss: 1.346675176832411\n",
      "Average test loss: 0.017700858592987062\n",
      "Epoch 153/300\n",
      "Average training loss: 1.171971510887146\n",
      "Average test loss: 0.008287317910956013\n",
      "Epoch 154/300\n",
      "Average training loss: 1.0121379653082954\n",
      "Average test loss: 0.03592577381928762\n",
      "Epoch 155/300\n",
      "Average training loss: 0.8697467087109884\n",
      "Average test loss: 0.006956297096278932\n",
      "Epoch 156/300\n",
      "Average training loss: 0.7432848121854994\n",
      "Average test loss: 0.006898940156731341\n",
      "Epoch 157/300\n",
      "Average training loss: 0.6281870642768013\n",
      "Average test loss: 0.006648712026990122\n",
      "Epoch 158/300\n",
      "Average training loss: 0.528646416929033\n",
      "Average test loss: 0.006675856107638942\n",
      "Epoch 159/300\n",
      "Average training loss: 0.43825565740797257\n",
      "Average test loss: 0.00684404832455847\n",
      "Epoch 160/300\n",
      "Average training loss: 0.3585617747041914\n",
      "Average test loss: 0.0065153544371326765\n",
      "Epoch 161/300\n",
      "Average training loss: 0.28700444711579215\n",
      "Average test loss: 0.006014572259452608\n",
      "Epoch 162/300\n",
      "Average training loss: 0.23420278346538545\n",
      "Average test loss: 0.006065332814637158\n",
      "Epoch 163/300\n",
      "Average training loss: 0.20329764596621194\n",
      "Average test loss: 0.005835108155591621\n",
      "Epoch 164/300\n",
      "Average training loss: 0.17999698164727954\n",
      "Average test loss: 0.005848228257563379\n",
      "Epoch 165/300\n",
      "Average training loss: 0.16324998756249745\n",
      "Average test loss: 0.005728629637095663\n",
      "Epoch 166/300\n",
      "Average training loss: 0.15052736381689708\n",
      "Average test loss: 0.005717629447579384\n",
      "Epoch 167/300\n",
      "Average training loss: 0.14148026821348403\n",
      "Average test loss: 0.00573046946980887\n",
      "Epoch 168/300\n",
      "Average training loss: 0.134213725010554\n",
      "Average test loss: 0.00546292743500736\n",
      "Epoch 169/300\n",
      "Average training loss: 0.12745361632770963\n",
      "Average test loss: 0.005545195268674029\n",
      "Epoch 170/300\n",
      "Average training loss: 0.12160522088739607\n",
      "Average test loss: 0.0054969078186485505\n",
      "Epoch 171/300\n",
      "Average training loss: 0.11664257930384742\n",
      "Average test loss: 0.00547020208918386\n",
      "Epoch 172/300\n",
      "Average training loss: 0.11218598846594492\n",
      "Average test loss: 0.00542945813263456\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10785928285784191\n",
      "Average test loss: 0.005392904460430145\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10487276522318523\n",
      "Average test loss: 0.005428681612842613\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10199475354618497\n",
      "Average test loss: 0.005352890687477257\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09939869970083237\n",
      "Average test loss: 0.005241259019821882\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0970868097477489\n",
      "Average test loss: 0.005190256334013409\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09533033686214024\n",
      "Average test loss: 0.005277469623006052\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09317448346482383\n",
      "Average test loss: 0.005165428128507402\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09162376389238569\n",
      "Average test loss: 0.005482009228732851\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09040848029322095\n",
      "Average test loss: 0.005211119387919704\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08830257625050016\n",
      "Average test loss: 0.005174804671357076\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0865904497967826\n",
      "Average test loss: 0.005140791397955683\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08530381937821706\n",
      "Average test loss: 0.005209090971698364\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08404274594783782\n",
      "Average test loss: 0.005209861772755782\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08302554679579205\n",
      "Average test loss: 0.005191766218178802\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08206681189272139\n",
      "Average test loss: 0.005544000986135668\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08129449260234833\n",
      "Average test loss: 0.005395076362623109\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0810791461136606\n",
      "Average test loss: 0.005227481759256787\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08057512338956196\n",
      "Average test loss: 0.005474682759286629\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08027321485347218\n",
      "Average test loss: 0.005263129357662466\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07998829223712285\n",
      "Average test loss: 0.005441535371045272\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07970088889863756\n",
      "Average test loss: 0.005317537933174107\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07955819968382517\n",
      "Average test loss: 0.005215241382519404\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07946741735272937\n",
      "Average test loss: 0.005733198271029525\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0792954345047474\n",
      "Average test loss: 0.005362776402797964\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07967941840489705\n",
      "Average test loss: 0.005318484717359145\n",
      "Epoch 198/300\n",
      "Average training loss: 2.728427123652564\n",
      "Average test loss: 0.006457467022869322\n",
      "Epoch 199/300\n",
      "Average training loss: 0.786300480471717\n",
      "Average test loss: 0.027311419531289075\n",
      "Epoch 200/300\n",
      "Average training loss: 0.5298114015261333\n",
      "Average test loss: 0.005859363516585695\n",
      "Epoch 201/300\n",
      "Average training loss: 0.4354696656862895\n",
      "Average test loss: 0.20817621333731545\n",
      "Epoch 202/300\n",
      "Average training loss: 0.3718015055656433\n",
      "Average test loss: 0.005704060849216249\n",
      "Epoch 203/300\n",
      "Average training loss: 0.32702063857184516\n",
      "Average test loss: 0.005619110733684566\n",
      "Epoch 204/300\n",
      "Average training loss: 0.2955140012105306\n",
      "Average test loss: 0.005566114180617862\n",
      "Epoch 205/300\n",
      "Average training loss: 0.2687630921204885\n",
      "Average test loss: 0.005654216919508246\n",
      "Epoch 206/300\n",
      "Average training loss: 0.24159408272637262\n",
      "Average test loss: 0.0055705090512832\n",
      "Epoch 207/300\n",
      "Average training loss: 0.15499596915642422\n",
      "Average test loss: 0.005492012543810739\n",
      "Epoch 208/300\n",
      "Average training loss: 0.12447168420420752\n",
      "Average test loss: 0.005377847734424803\n",
      "Epoch 209/300\n",
      "Average training loss: 0.11287799210018581\n",
      "Average test loss: 0.005235355169822773\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10666978230741289\n",
      "Average test loss: 0.005253314912733105\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10323944524261687\n",
      "Average test loss: 0.0051744251950747435\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10053936777512232\n",
      "Average test loss: 0.005211302995267841\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09800113081932067\n",
      "Average test loss: 0.005185695752501488\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09617539149522782\n",
      "Average test loss: 0.0051639129929244515\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09426220178604126\n",
      "Average test loss: 0.0052580826100375915\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09275944709115558\n",
      "Average test loss: 0.006106746116446124\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09108009966876772\n",
      "Average test loss: 0.005286434245606263\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08960483370886908\n",
      "Average test loss: 0.007056664882434739\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0885216288500362\n",
      "Average test loss: 0.005332243358095487\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08702859448724323\n",
      "Average test loss: 0.005126643516537216\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08616031620899836\n",
      "Average test loss: 0.005207997197078334\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08497097384267382\n",
      "Average test loss: 0.005686713188886643\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08390698354774051\n",
      "Average test loss: 0.00541625777259469\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08303264049688976\n",
      "Average test loss: 0.005247927505109045\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08252436106072532\n",
      "Average test loss: 0.005491571447915501\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08169514928923713\n",
      "Average test loss: 0.0053123225110272565\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08084353665510813\n",
      "Average test loss: 0.007372615012857649\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08030808228916592\n",
      "Average test loss: 0.005423431295901537\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07998764993747075\n",
      "Average test loss: 0.005305399402976036\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07961586476696862\n",
      "Average test loss: 0.005348561035676135\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07935008432798915\n",
      "Average test loss: 0.005303531973312298\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07899527127212948\n",
      "Average test loss: 0.00570326731312606\n",
      "Epoch 233/300\n",
      "Average training loss: 0.078600443330076\n",
      "Average test loss: 0.005534390969408883\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07969787945350011\n",
      "Average test loss: 0.005396796034442054\n",
      "Epoch 235/300\n",
      "Average training loss: 477.14996672876015\n",
      "Average test loss: 0.06323591858148575\n",
      "Epoch 236/300\n",
      "Average training loss: 11.816425914340549\n",
      "Average test loss: 0.04006098924080531\n",
      "Epoch 237/300\n",
      "Average training loss: 10.29793903435601\n",
      "Average test loss: 0.03181580771505833\n",
      "Epoch 238/300\n",
      "Average training loss: 9.4130866800944\n",
      "Average test loss: 0.0195447591824664\n",
      "Epoch 239/300\n",
      "Average training loss: 8.692382790459527\n",
      "Average test loss: 0.018598199269837804\n",
      "Epoch 240/300\n",
      "Average training loss: 8.094773931715224\n",
      "Average test loss: 0.017083595029181903\n",
      "Epoch 241/300\n",
      "Average training loss: 7.569088306003147\n",
      "Average test loss: 0.013433174056311448\n",
      "Epoch 242/300\n",
      "Average training loss: 7.081184415605333\n",
      "Average test loss: 0.012618394129806094\n",
      "Epoch 243/300\n",
      "Average training loss: 6.613352183024088\n",
      "Average test loss: 0.011352813745538393\n",
      "Epoch 244/300\n",
      "Average training loss: 6.13379995557997\n",
      "Average test loss: 0.01026457308895058\n",
      "Epoch 245/300\n",
      "Average training loss: 5.64124292034573\n",
      "Average test loss: 0.010223165751331382\n",
      "Epoch 246/300\n",
      "Average training loss: 5.115736066182454\n",
      "Average test loss: 0.009460954670276907\n",
      "Epoch 247/300\n",
      "Average training loss: 4.597017946031358\n",
      "Average test loss: 0.009444382722179095\n",
      "Epoch 248/300\n",
      "Average training loss: 4.073430619981554\n",
      "Average test loss: 0.008152383598188559\n",
      "Epoch 249/300\n",
      "Average training loss: 3.560877544615004\n",
      "Average test loss: 0.008112095379994974\n",
      "Epoch 250/300\n",
      "Average training loss: 3.032429413265652\n",
      "Average test loss: 0.008362435969627565\n",
      "Epoch 251/300\n",
      "Average training loss: 2.533404967625936\n",
      "Average test loss: 0.007392032898962498\n",
      "Epoch 252/300\n",
      "Average training loss: 2.084001521428426\n",
      "Average test loss: 0.007367536930574311\n",
      "Epoch 253/300\n",
      "Average training loss: 1.706146464559767\n",
      "Average test loss: 0.007293420461730825\n",
      "Epoch 254/300\n",
      "Average training loss: 1.4016598518159655\n",
      "Average test loss: 0.006879574998385376\n",
      "Epoch 255/300\n",
      "Average training loss: 1.1634681413438586\n",
      "Average test loss: 0.006790926300817066\n",
      "Epoch 256/300\n",
      "Average training loss: 375.31230590544806\n",
      "Average test loss: 0.11034907003906039\n",
      "Epoch 257/300\n",
      "Average training loss: 17.091829610188803\n",
      "Average test loss: 0.04653682487209638\n",
      "Epoch 258/300\n",
      "Average training loss: 15.08779273308648\n",
      "Average test loss: 0.03560851303405232\n",
      "Epoch 259/300\n",
      "Average training loss: 13.505297050476074\n",
      "Average test loss: 0.031478099627627265\n",
      "Epoch 260/300\n",
      "Average training loss: 12.140106349521213\n",
      "Average test loss: 0.022275179035133787\n",
      "Epoch 261/300\n",
      "Average training loss: 10.98849333190918\n",
      "Average test loss: 0.024009207202328575\n",
      "Epoch 262/300\n",
      "Average training loss: 9.562562288072375\n",
      "Average test loss: 0.015409214506546656\n",
      "Epoch 263/300\n",
      "Average training loss: 8.157518450842963\n",
      "Average test loss: 0.013427659100956387\n",
      "Epoch 264/300\n",
      "Average training loss: 7.043299958123101\n",
      "Average test loss: 0.011787245343956683\n",
      "Epoch 265/300\n",
      "Average training loss: 6.263324086507161\n",
      "Average test loss: 0.010257281990514861\n",
      "Epoch 266/300\n",
      "Average training loss: 5.632253622690836\n",
      "Average test loss: 0.011705440166095893\n",
      "Epoch 267/300\n",
      "Average training loss: 4.98874385409885\n",
      "Average test loss: 0.008443843628797265\n",
      "Epoch 268/300\n",
      "Average training loss: 4.3539297044542105\n",
      "Average test loss: 0.02045090267972814\n",
      "Epoch 269/300\n",
      "Average training loss: 3.710713545481364\n",
      "Average test loss: 0.009180738390319878\n",
      "Epoch 270/300\n",
      "Average training loss: 3.0492175339592826\n",
      "Average test loss: 0.00959154782609807\n",
      "Epoch 271/300\n",
      "Average training loss: 2.4320384112464057\n",
      "Average test loss: 0.007513126851369937\n",
      "Epoch 272/300\n",
      "Average training loss: 1.9335370082855226\n",
      "Average test loss: 0.0629332834540142\n",
      "Epoch 273/300\n",
      "Average training loss: 1.5261898302502102\n",
      "Average test loss: 0.007076526981261042\n",
      "Epoch 274/300\n",
      "Average training loss: 1.2155549134148491\n",
      "Average test loss: 0.006911615505814552\n",
      "Epoch 275/300\n",
      "Average training loss: 0.9659152573479547\n",
      "Average test loss: 0.006620011840429571\n",
      "Epoch 276/300\n",
      "Average training loss: 0.7728185510635376\n",
      "Average test loss: 0.006367255396313137\n",
      "Epoch 277/300\n",
      "Average training loss: 0.6218209640714857\n",
      "Average test loss: 0.007101368427276611\n",
      "Epoch 278/300\n",
      "Average training loss: 0.4954692608780331\n",
      "Average test loss: 0.008055175411618418\n",
      "Epoch 279/300\n",
      "Average training loss: 0.39382888944943745\n",
      "Average test loss: 0.006576103186441792\n",
      "Epoch 280/300\n",
      "Average training loss: 0.31485783378283183\n",
      "Average test loss: 0.010830534444914924\n",
      "Epoch 281/300\n",
      "Average training loss: 0.25609828742345175\n",
      "Average test loss: 0.005881789967003796\n",
      "Epoch 282/300\n",
      "Average training loss: 0.2175394559568829\n",
      "Average test loss: 0.005946696604705519\n",
      "Epoch 283/300\n",
      "Average training loss: 0.190089080452919\n",
      "Average test loss: 0.005708027774675025\n",
      "Epoch 284/300\n",
      "Average training loss: 0.17082791613207923\n",
      "Average test loss: 0.006334917168650363\n",
      "Epoch 285/300\n",
      "Average training loss: 0.15579750434557596\n",
      "Average test loss: 0.005621332030329439\n",
      "Epoch 286/300\n",
      "Average training loss: 0.1432604349454244\n",
      "Average test loss: 0.005782190721895959\n",
      "Epoch 287/300\n",
      "Average training loss: 0.13295956020885044\n",
      "Average test loss: 0.0054858156701342925\n",
      "Epoch 288/300\n",
      "Average training loss: 0.12452600116199918\n",
      "Average test loss: 0.005856576214647955\n",
      "Epoch 289/300\n",
      "Average training loss: 0.11875652629799313\n",
      "Average test loss: 0.005948762866357963\n",
      "Epoch 290/300\n",
      "Average training loss: 0.11339590769343906\n",
      "Average test loss: 0.005506925307628181\n",
      "Epoch 291/300\n",
      "Average training loss: 0.1106584259139167\n",
      "Average test loss: 0.005688907010687722\n",
      "Epoch 292/300\n",
      "Average training loss: 0.10613124034802118\n",
      "Average test loss: 0.005464804206457403\n",
      "Epoch 293/300\n",
      "Average training loss: 0.1031993356347084\n",
      "Average test loss: 0.005638565640896559\n",
      "Epoch 294/300\n",
      "Average training loss: 0.10107676196098328\n",
      "Average test loss: 0.00521068252540297\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09873201884826024\n",
      "Average test loss: 0.005245956995834907\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0965438060760498\n",
      "Average test loss: 0.005255752245999045\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09461160470379723\n",
      "Average test loss: 0.005387058472053872\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09226409490240944\n",
      "Average test loss: 0.0054780042552285725\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0902171389857928\n",
      "Average test loss: 0.0054056545827123855\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08827069608370464\n",
      "Average test loss: 0.005246951428552468\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.7996053938335843\n",
      "Average test loss: 0.00775807783835464\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5920488657421535\n",
      "Average test loss: 0.006428040099226766\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3926238980823093\n",
      "Average test loss: 0.006089361659768555\n",
      "Epoch 4/300\n",
      "Average training loss: 0.29941733039749996\n",
      "Average test loss: 0.005533890224579308\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2425480774508582\n",
      "Average test loss: 0.005497890265037616\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2046074296368493\n",
      "Average test loss: 0.005175868851857053\n",
      "Epoch 7/300\n",
      "Average training loss: 0.17806455279721153\n",
      "Average test loss: 0.0048423564891434375\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15906588200065824\n",
      "Average test loss: 0.004709546025014586\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14447959813806746\n",
      "Average test loss: 0.004497462893525759\n",
      "Epoch 10/300\n",
      "Average training loss: 0.13349381623003218\n",
      "Average test loss: 0.0047666643915904895\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12422459071874618\n",
      "Average test loss: 0.00436638429802325\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11706281543440289\n",
      "Average test loss: 0.004939248495217827\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11069171948565378\n",
      "Average test loss: 0.004246405074165927\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10595012127028572\n",
      "Average test loss: 0.004146233347555001\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10108573661910163\n",
      "Average test loss: 0.0038595637215508355\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0978190499080552\n",
      "Average test loss: 0.004319804596404234\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09394448678361045\n",
      "Average test loss: 0.0038149309783346123\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0913887134525511\n",
      "Average test loss: 0.0034812737494293185\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08780875596735213\n",
      "Average test loss: 0.003413577725696895\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08537635668118795\n",
      "Average test loss: 0.003771676979131169\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08201360689931446\n",
      "Average test loss: 0.0034613694825934038\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07935078362292713\n",
      "Average test loss: 0.019098050413032374\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07725619816780091\n",
      "Average test loss: 0.0032634703937090104\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07498824772569869\n",
      "Average test loss: 0.0033234389171832137\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07283328014943335\n",
      "Average test loss: 0.0032545027401712206\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07066117841336463\n",
      "Average test loss: 0.0032782157903744114\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06924882392750846\n",
      "Average test loss: 0.003131903727331923\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0676006353828642\n",
      "Average test loss: 0.003242541517648432\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06601729885074828\n",
      "Average test loss: 0.003102231621535288\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06480685761239793\n",
      "Average test loss: 0.0030791436303406953\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06344538625743654\n",
      "Average test loss: 0.003064377523130841\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06298304394549793\n",
      "Average test loss: 0.004134266378978888\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06185005658202701\n",
      "Average test loss: 0.00514290902701517\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06102570928798781\n",
      "Average test loss: 0.0032851162103729116\n",
      "Epoch 35/300\n",
      "Average training loss: 0.060195677005582385\n",
      "Average test loss: 0.0030708159073773358\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05979283461968104\n",
      "Average test loss: 0.0032647563363942837\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06072965524925126\n",
      "Average test loss: 0.003933780153592427\n",
      "Epoch 38/300\n",
      "Average training loss: 0.13131959599918788\n",
      "Average test loss: 0.0037557051620549626\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08541383669111463\n",
      "Average test loss: 0.0034064827747642993\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07306003809637493\n",
      "Average test loss: 0.003212130918684933\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06930600738856528\n",
      "Average test loss: 0.0031341668808211884\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06668245444695155\n",
      "Average test loss: 0.0031412322757144767\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06492405214574602\n",
      "Average test loss: 0.003110708171294795\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06352173270781834\n",
      "Average test loss: 0.003066827784809801\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06250753574238883\n",
      "Average test loss: 0.0031834120373759006\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06151508110430506\n",
      "Average test loss: 0.003034930666908622\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06067510175042682\n",
      "Average test loss: 0.0031130199674516915\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06026500383681721\n",
      "Average test loss: 0.003045914041292336\n",
      "Epoch 49/300\n",
      "Average training loss: 0.059669785539309184\n",
      "Average test loss: 0.0030598600688907835\n",
      "Epoch 50/300\n",
      "Average training loss: 0.059287575638956494\n",
      "Average test loss: 0.0030699592212008104\n",
      "Epoch 51/300\n",
      "Average training loss: 0.058642884413401285\n",
      "Average test loss: 0.0029929198113580547\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05823316623104943\n",
      "Average test loss: 0.003030025287427836\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05808911107646095\n",
      "Average test loss: 0.003006219302200609\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05760312174426185\n",
      "Average test loss: 0.0030339729545844924\n",
      "Epoch 55/300\n",
      "Average training loss: 0.057194397962755626\n",
      "Average test loss: 0.003003915160894394\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05686443668935034\n",
      "Average test loss: 0.003022939458489418\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05701968908641073\n",
      "Average test loss: 0.0041908609637369715\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05828592351741261\n",
      "Average test loss: 0.0032824117785526644\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05685818706949552\n",
      "Average test loss: 0.003304976422339678\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05640158534381125\n",
      "Average test loss: 0.0029591167618831\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05547972735762596\n",
      "Average test loss: 0.0031062423491643536\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05531927470697297\n",
      "Average test loss: 0.0030016953214589094\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05507743532458941\n",
      "Average test loss: 0.0029557402266396416\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05501569593615002\n",
      "Average test loss: 0.0029575950124611458\n",
      "Epoch 65/300\n",
      "Average training loss: 0.054487195667293334\n",
      "Average test loss: 0.002935587940323684\n",
      "Epoch 66/300\n",
      "Average training loss: 0.055139886101086936\n",
      "Average test loss: 0.0029372754498488375\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10971533467372259\n",
      "Average test loss: 0.0031781355825563273\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06721745026111603\n",
      "Average test loss: 0.0030449919967601696\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06269677651921908\n",
      "Average test loss: 0.003128482555349668\n",
      "Epoch 70/300\n",
      "Average training loss: 0.060396442274252574\n",
      "Average test loss: 0.002970112796045012\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05881231748726633\n",
      "Average test loss: 0.004006100753115283\n",
      "Epoch 72/300\n",
      "Average training loss: 0.057618254012531706\n",
      "Average test loss: 0.00297153204265568\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05665353999700811\n",
      "Average test loss: 0.002978253535512421\n",
      "Epoch 74/300\n",
      "Average training loss: 0.055974678847524856\n",
      "Average test loss: 0.003089185005053878\n",
      "Epoch 75/300\n",
      "Average training loss: 0.056000037839015325\n",
      "Average test loss: 0.002946466266281075\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05593237713476022\n",
      "Average test loss: 0.003015395626011822\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05463774938384692\n",
      "Average test loss: 0.0029347534463223485\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05431203491489092\n",
      "Average test loss: 0.002970193047904306\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05414913276831309\n",
      "Average test loss: 0.002922929301444027\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05396949189901352\n",
      "Average test loss: 0.00430530566515194\n",
      "Epoch 81/300\n",
      "Average training loss: 0.053685615731610195\n",
      "Average test loss: 0.0030050971599088773\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0535997630821334\n",
      "Average test loss: 0.003802660414121217\n",
      "Epoch 83/300\n",
      "Average training loss: 0.054506940570142534\n",
      "Average test loss: 0.003083765386293332\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0535004988445176\n",
      "Average test loss: 0.0028954302596135273\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05312151008182102\n",
      "Average test loss: 0.0029802970844838355\n",
      "Epoch 86/300\n",
      "Average training loss: 0.052777996715572144\n",
      "Average test loss: 0.003003599432607492\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05268431583046913\n",
      "Average test loss: 0.0030165812480780815\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05247055540482203\n",
      "Average test loss: 0.0031591241334875423\n",
      "Epoch 89/300\n",
      "Average training loss: 0.052314164400100706\n",
      "Average test loss: 0.0030040246134416925\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05218992683291435\n",
      "Average test loss: 0.005209526936420136\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05199125747548209\n",
      "Average test loss: 0.0030712731041842036\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05178034313850933\n",
      "Average test loss: 0.002937995582405064\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05208468242817455\n",
      "Average test loss: 0.0029583837369249926\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05121436229348183\n",
      "Average test loss: 0.0031852718060836195\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05114582230978542\n",
      "Average test loss: 0.0029059277574221293\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0510189268456565\n",
      "Average test loss: 0.003291462110148536\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05098213892512851\n",
      "Average test loss: 0.0029842592266698677\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05071752053499222\n",
      "Average test loss: 0.002920690829762154\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05057012894252936\n",
      "Average test loss: 0.0029882553925530777\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08267188646064864\n",
      "Average test loss: 0.003159036633869012\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06196584408978621\n",
      "Average test loss: 0.003296142003602452\n",
      "Epoch 102/300\n",
      "Average training loss: 0.055898602922757466\n",
      "Average test loss: 0.0029455796252522205\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05322519603702757\n",
      "Average test loss: 0.0029020597287971114\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05191334947281414\n",
      "Average test loss: 0.002941025686553783\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05101750905977355\n",
      "Average test loss: 0.002932292750519183\n",
      "Epoch 106/300\n",
      "Average training loss: 0.050571215791834725\n",
      "Average test loss: 0.0029155351441974443\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05031252124243312\n",
      "Average test loss: 0.0028928670059475635\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05012744576070044\n",
      "Average test loss: 0.0042077042646706106\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05004750637875663\n",
      "Average test loss: 0.0030113409637577003\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0499456394844585\n",
      "Average test loss: 0.0029069823616494736\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04993303576774067\n",
      "Average test loss: 0.002913420922226376\n",
      "Epoch 112/300\n",
      "Average training loss: 0.049729993684424295\n",
      "Average test loss: 0.002911422379935781\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0497904901140266\n",
      "Average test loss: 0.0036858493263522783\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04962995586130354\n",
      "Average test loss: 0.0031263520483755403\n",
      "Epoch 115/300\n",
      "Average training loss: 0.049584081225925024\n",
      "Average test loss: 0.002987667109403345\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04939803006251653\n",
      "Average test loss: 0.0032266443740162584\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05173699215385649\n",
      "Average test loss: 0.0029542145698020854\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05075962714354197\n",
      "Average test loss: 0.027807338270876142\n",
      "Epoch 119/300\n",
      "Average training loss: 0.049446321583456465\n",
      "Average test loss: 0.0028934318489498563\n",
      "Epoch 120/300\n",
      "Average training loss: 0.049471017946799596\n",
      "Average test loss: 0.002945258605811331\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0491463177965747\n",
      "Average test loss: 0.0030170566940473185\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04890832239058283\n",
      "Average test loss: 0.4441365434328715\n",
      "Epoch 123/300\n",
      "Average training loss: 0.048757847683297266\n",
      "Average test loss: 0.002936208219577869\n",
      "Epoch 124/300\n",
      "Average training loss: 0.048742325570848254\n",
      "Average test loss: 0.002978233455783791\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04867410091890229\n",
      "Average test loss: 0.0029513194589979117\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04855779593851831\n",
      "Average test loss: 0.002988869581785467\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04850752476520009\n",
      "Average test loss: 0.0030138370477490956\n",
      "Epoch 128/300\n",
      "Average training loss: 0.048281872239377764\n",
      "Average test loss: 0.007767594189279609\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0489932925734255\n",
      "Average test loss: 0.00471930576612552\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06002142372727394\n",
      "Average test loss: 0.0029843025064716735\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05239760077661938\n",
      "Average test loss: 0.002901243163065778\n",
      "Epoch 132/300\n",
      "Average training loss: 0.049716086010138195\n",
      "Average test loss: 0.0029168693555725947\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04893135079410341\n",
      "Average test loss: 0.0029009495983935065\n",
      "Epoch 134/300\n",
      "Average training loss: 0.048449310425255035\n",
      "Average test loss: 0.002987363784056571\n",
      "Epoch 135/300\n",
      "Average training loss: 0.048178270353211296\n",
      "Average test loss: 0.0029623388846715293\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04797405495908525\n",
      "Average test loss: 0.0029485892556193804\n",
      "Epoch 137/300\n",
      "Average training loss: 0.048020668665568034\n",
      "Average test loss: 0.005985735076583094\n",
      "Epoch 138/300\n",
      "Average training loss: 0.047821345627307894\n",
      "Average test loss: 0.0030092200526139804\n",
      "Epoch 139/300\n",
      "Average training loss: 0.047769211702876624\n",
      "Average test loss: 0.0036457364898588923\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04782715460326936\n",
      "Average test loss: 0.0031365025577445825\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04766696953111225\n",
      "Average test loss: 0.003613064442243841\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04769930297798581\n",
      "Average test loss: 0.0030599732492119073\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04747654429078102\n",
      "Average test loss: 0.003373352061336239\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0473686385816998\n",
      "Average test loss: 0.0029785959250811073\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04736401647991604\n",
      "Average test loss: 0.0031594396304960052\n",
      "Epoch 146/300\n",
      "Average training loss: 0.048214183141787846\n",
      "Average test loss: 0.0039675588769217335\n",
      "Epoch 147/300\n",
      "Average training loss: 0.047728473880224755\n",
      "Average test loss: 0.004245163892292314\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04720567078060574\n",
      "Average test loss: 0.003245398553295268\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04686602135830455\n",
      "Average test loss: 0.002955261579404275\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04709943813416693\n",
      "Average test loss: 0.0031668484051608375\n",
      "Epoch 151/300\n",
      "Average training loss: 0.046874126861492796\n",
      "Average test loss: 0.002999659871889485\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04684710649980439\n",
      "Average test loss: 0.002980430309764213\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04708051079180506\n",
      "Average test loss: 0.002943504797087775\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04884917791022195\n",
      "Average test loss: 0.004394319564518002\n",
      "Epoch 155/300\n",
      "Average training loss: 0.048363369156916934\n",
      "Average test loss: 0.002932041310187843\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0464840655359957\n",
      "Average test loss: 0.00301860138359997\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0463913340833452\n",
      "Average test loss: 0.2794192467249102\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04637764781382349\n",
      "Average test loss: 526472.7508888888\n",
      "Epoch 159/300\n",
      "Average training loss: 0.056434066947963504\n",
      "Average test loss: 0.0100225161164999\n",
      "Epoch 160/300\n",
      "Average training loss: 0.046789494971434274\n",
      "Average test loss: 0.002961474368866119\n",
      "Epoch 161/300\n",
      "Average training loss: 0.046343535211351185\n",
      "Average test loss: 0.0029688107085724673\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04737155182162921\n",
      "Average test loss: 0.0032416964872843687\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04643842268983523\n",
      "Average test loss: 0.0031865095814896955\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0465323265393575\n",
      "Average test loss: 0.005452279679684175\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04665927187270588\n",
      "Average test loss: 0.0030890589865545433\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04609316892425219\n",
      "Average test loss: 0.003090952944372677\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04596903796328439\n",
      "Average test loss: 0.003038241978734732\n",
      "Epoch 168/300\n",
      "Average training loss: 0.045977708753612304\n",
      "Average test loss: 0.0031630928673677975\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04604374608728621\n",
      "Average test loss: 0.00313748267520633\n",
      "Epoch 170/300\n",
      "Average training loss: 0.046017950491772756\n",
      "Average test loss: 0.01538772965884871\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04585347683562173\n",
      "Average test loss: 0.0030299200790209905\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04642690392169688\n",
      "Average test loss: 0.0030991759492705264\n",
      "Epoch 173/300\n",
      "Average training loss: 0.046340323014391796\n",
      "Average test loss: 0.005244803251491652\n",
      "Epoch 174/300\n",
      "Average training loss: 0.045758637617031736\n",
      "Average test loss: 0.003027147504604525\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04580727777878443\n",
      "Average test loss: 0.006902892842267951\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04584893062710762\n",
      "Average test loss: 0.003011819809675217\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04582693832450443\n",
      "Average test loss: 0.0030727104232129124\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0456514427959919\n",
      "Average test loss: 0.0034663170559538735\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04566316490040885\n",
      "Average test loss: 0.003014055503739251\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04529677326149411\n",
      "Average test loss: 0.0030691392785973017\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04524296916855706\n",
      "Average test loss: 0.002990956606550349\n",
      "Epoch 182/300\n",
      "Average training loss: 0.045349786321322125\n",
      "Average test loss: 0.003092642574467593\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04582303863598241\n",
      "Average test loss: 0.003072858433549603\n",
      "Epoch 184/300\n",
      "Average training loss: 0.045336087339454226\n",
      "Average test loss: 0.013817590743717221\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04528368922074636\n",
      "Average test loss: 0.004099731802526448\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04514693172441588\n",
      "Average test loss: 0.0030379821331136757\n",
      "Epoch 187/300\n",
      "Average training loss: 0.045110232959191005\n",
      "Average test loss: 0.00305537988535232\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04499350644813643\n",
      "Average test loss: 0.006459564975566334\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04498061801658736\n",
      "Average test loss: 0.0030795198037392563\n",
      "Epoch 190/300\n",
      "Average training loss: 0.045242384314537046\n",
      "Average test loss: 0.0029838639398415885\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04525102930102083\n",
      "Average test loss: 0.0038953642901033165\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04498009330365393\n",
      "Average test loss: 0.003076855472392506\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04530374694201681\n",
      "Average test loss: 0.008299917299093471\n",
      "Epoch 194/300\n",
      "Average training loss: 0.044767068657610154\n",
      "Average test loss: 0.003283159892592165\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0447075236969524\n",
      "Average test loss: 0.0030523335722585517\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04473998323082924\n",
      "Average test loss: 0.0031091165737145475\n",
      "Epoch 197/300\n",
      "Average training loss: 0.044794272591670355\n",
      "Average test loss: 0.0032057165911214217\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04476695523990525\n",
      "Average test loss: 0.003363352257137497\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04453441008594301\n",
      "Average test loss: 0.0031090637476493916\n",
      "Epoch 200/300\n",
      "Average training loss: 0.044596011026038065\n",
      "Average test loss: 0.0030477593439734644\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04472295763923062\n",
      "Average test loss: 0.20520071092082395\n",
      "Epoch 202/300\n",
      "Average training loss: 0.044886672920650905\n",
      "Average test loss: 0.0029889556422001786\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04456537241405911\n",
      "Average test loss: 0.0031022096174872582\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04468645149138239\n",
      "Average test loss: 0.0031676479833614496\n",
      "Epoch 205/300\n",
      "Average training loss: 0.044382608774635526\n",
      "Average test loss: 0.0030819038819107744\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04449458432197571\n",
      "Average test loss: 0.006166757853908671\n",
      "Epoch 207/300\n",
      "Average training loss: 0.044380097524987325\n",
      "Average test loss: 0.0030224163834419514\n",
      "Epoch 208/300\n",
      "Average training loss: 0.044283048964209024\n",
      "Average test loss: 0.0032191136239303484\n",
      "Epoch 209/300\n",
      "Average training loss: 0.044214951624472934\n",
      "Average test loss: 0.0030400567416929538\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04411787482102712\n",
      "Average test loss: 0.0032235343009233476\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04417761704325676\n",
      "Average test loss: 0.003350508017672433\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04410289870036973\n",
      "Average test loss: 0.0031869912805656594\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04417364995015992\n",
      "Average test loss: 0.003147425054986444\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04417901475893127\n",
      "Average test loss: 0.007976252478029994\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04522405463788245\n",
      "Average test loss: 0.0030562617253098224\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04397106588880221\n",
      "Average test loss: 0.0031095825346807637\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04405216667056084\n",
      "Average test loss: 0.0031731316088181405\n",
      "Epoch 218/300\n",
      "Average training loss: 0.044107447700368035\n",
      "Average test loss: 0.004760193866988023\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04421679281526142\n",
      "Average test loss: 0.004745979991637998\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04387287802166409\n",
      "Average test loss: 0.003104940354410145\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04383798732360204\n",
      "Average test loss: 0.004524677591812279\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04377834289603763\n",
      "Average test loss: 0.0032711624757697185\n",
      "Epoch 223/300\n",
      "Average training loss: 0.043791417583823204\n",
      "Average test loss: 0.0032407507803291083\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04372913842399915\n",
      "Average test loss: 0.003183871217485931\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0439757525159253\n",
      "Average test loss: 0.003282174303713772\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04384848318828477\n",
      "Average test loss: 0.0031868697566290695\n",
      "Epoch 227/300\n",
      "Average training loss: 0.043647269527117415\n",
      "Average test loss: 0.003055178570664591\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04371614808175299\n",
      "Average test loss: 0.003206181464302871\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04369850413997968\n",
      "Average test loss: 0.0031748970593843194\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04398968870772256\n",
      "Average test loss: 0.0031717896631194486\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04361807531118393\n",
      "Average test loss: 0.003272119081682629\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04352252270446883\n",
      "Average test loss: 0.005225452876960238\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04351769271492958\n",
      "Average test loss: 0.0030863871495756838\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04347550473279423\n",
      "Average test loss: 0.0033023988240294985\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04406911394165622\n",
      "Average test loss: 0.003997440669271681\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0433389101922512\n",
      "Average test loss: 0.003060422225130929\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04338594270083639\n",
      "Average test loss: 0.0031035072503404486\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04382121960322062\n",
      "Average test loss: 0.0032388075881948073\n",
      "Epoch 239/300\n",
      "Average training loss: 0.043427838924858304\n",
      "Average test loss: 0.0031220560835467444\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0433761106133461\n",
      "Average test loss: 0.00308715589158237\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04330581089854241\n",
      "Average test loss: 0.01590113372873101\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04344471127457089\n",
      "Average test loss: 0.003087938453588221\n",
      "Epoch 243/300\n",
      "Average training loss: 0.043405726767248576\n",
      "Average test loss: 0.003135313524140252\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04336297160387039\n",
      "Average test loss: 0.0073794212647610245\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04325499853160646\n",
      "Average test loss: 0.003108594082088934\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0430911713010735\n",
      "Average test loss: 0.003086349442290763\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04316321408086353\n",
      "Average test loss: 0.0030886154944698018\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04319875658551852\n",
      "Average test loss: 0.003087209811227189\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04396242419878642\n",
      "Average test loss: 0.0038440328737099965\n",
      "Epoch 250/300\n",
      "Average training loss: 0.043030398673481414\n",
      "Average test loss: 0.003461686210292909\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04311178617013825\n",
      "Average test loss: 0.0032424244901372327\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04310665830307537\n",
      "Average test loss: 0.003196177756620778\n",
      "Epoch 253/300\n",
      "Average training loss: 0.043275508784585526\n",
      "Average test loss: 0.00308155830990937\n",
      "Epoch 254/300\n",
      "Average training loss: 0.043260036057896085\n",
      "Average test loss: 0.0031816973591016397\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04314521006743113\n",
      "Average test loss: 0.003150799187935061\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04300027710530493\n",
      "Average test loss: 0.0031477201020138133\n",
      "Epoch 257/300\n",
      "Average training loss: 0.042888261035084724\n",
      "Average test loss: 0.003993646723321743\n",
      "Epoch 258/300\n",
      "Average training loss: 0.042917235689030754\n",
      "Average test loss: 0.0033165974201013644\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0429281055563026\n",
      "Average test loss: 0.003359629746940401\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04297955938511425\n",
      "Average test loss: 0.003137324659774701\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04305344516038895\n",
      "Average test loss: 0.0034994724067962833\n",
      "Epoch 262/300\n",
      "Average training loss: 0.042848356844650375\n",
      "Average test loss: 0.0031435038219723437\n",
      "Epoch 263/300\n",
      "Average training loss: 0.042981997347540324\n",
      "Average test loss: 0.0031862076620260875\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04310182300044431\n",
      "Average test loss: 0.003424532505787081\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04290135710438093\n",
      "Average test loss: 0.00423563155449099\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04277980749309063\n",
      "Average test loss: 0.0037957560641484127\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04270347181624837\n",
      "Average test loss: 0.0033295462702711423\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04280168483323521\n",
      "Average test loss: 0.004294919976136751\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04282926474014918\n",
      "Average test loss: 0.0032444134718841978\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0427502949933211\n",
      "Average test loss: 0.0034155296751608452\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04285698158873452\n",
      "Average test loss: 0.003109341219481495\n",
      "Epoch 272/300\n",
      "Average training loss: 0.043074779457516144\n",
      "Average test loss: 0.003044955982102288\n",
      "Epoch 273/300\n",
      "Average training loss: 0.042831618285841415\n",
      "Average test loss: 0.0032550334595143793\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04258266513877445\n",
      "Average test loss: 0.003445847259834409\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04270704243911637\n",
      "Average test loss: 8.682445967356363\n",
      "Epoch 276/300\n",
      "Average training loss: 0.042548353655470744\n",
      "Average test loss: 0.005934119382252296\n",
      "Epoch 277/300\n",
      "Average training loss: 0.042502742303742305\n",
      "Average test loss: 0.24858374426762264\n",
      "Epoch 278/300\n",
      "Average training loss: 0.042521532661385\n",
      "Average test loss: 0.0031337846277488604\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04254876319070657\n",
      "Average test loss: 0.0032228354550898077\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04319284525513649\n",
      "Average test loss: 0.0032296781688928604\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0425974222322305\n",
      "Average test loss: 0.003156555314651794\n",
      "Epoch 282/300\n",
      "Average training loss: 0.042850593186087076\n",
      "Average test loss: 0.004030545939173963\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04255245787070857\n",
      "Average test loss: 0.0039271486418114765\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04234931633207533\n",
      "Average test loss: 0.0033942096854249638\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04250083835588561\n",
      "Average test loss: 0.011359783380395836\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04239068612456322\n",
      "Average test loss: 0.0031713978079044155\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04253625698884328\n",
      "Average test loss: 0.5411874196413491\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04249547450741132\n",
      "Average test loss: 0.0031593901409457128\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04243917302621735\n",
      "Average test loss: 0.003304094903998905\n",
      "Epoch 290/300\n",
      "Average training loss: 0.042447898444202214\n",
      "Average test loss: 0.0031302031518684495\n",
      "Epoch 291/300\n",
      "Average training loss: 0.042543789976172974\n",
      "Average test loss: 0.015625562140511143\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04257585139075915\n",
      "Average test loss: 0.0032394454433686205\n",
      "Epoch 293/300\n",
      "Average training loss: 0.042334012392494416\n",
      "Average test loss: 0.003317019585520029\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04222894550032086\n",
      "Average test loss: 0.003232452345184154\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04230961436364386\n",
      "Average test loss: 0.01494849674238099\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04231492118206289\n",
      "Average test loss: 0.003152231843314237\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04244469253884421\n",
      "Average test loss: 0.003177092311696874\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04231765515605609\n",
      "Average test loss: 0.003138090557936165\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04248980841371748\n",
      "Average test loss: 0.003217211428615782\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04241129346026315\n",
      "Average test loss: 0.003146494515240192\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.472015815893809\n",
      "Average test loss: 0.005636248514884048\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4978873757786221\n",
      "Average test loss: 0.004612830092095666\n",
      "Epoch 3/300\n",
      "Average training loss: 0.33595206366644964\n",
      "Average test loss: 0.004157317662404643\n",
      "Epoch 4/300\n",
      "Average training loss: 0.256271315548155\n",
      "Average test loss: 0.0038834782859517468\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2062803643014696\n",
      "Average test loss: 0.0037031915883223216\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17314529689153035\n",
      "Average test loss: 0.003506340590926508\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1484616668621699\n",
      "Average test loss: 0.0035262609699534047\n",
      "Epoch 8/300\n",
      "Average training loss: 0.13144217548767725\n",
      "Average test loss: 0.0047021910628924765\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11840875589847565\n",
      "Average test loss: 0.0033139569059842163\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10840879455539915\n",
      "Average test loss: 0.003126099651472436\n",
      "Epoch 11/300\n",
      "Average training loss: 0.10042507100105286\n",
      "Average test loss: 0.003518183344354232\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09384721246692869\n",
      "Average test loss: 0.004149146434747511\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08879474227958255\n",
      "Average test loss: 0.003756605293808712\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08367844417360094\n",
      "Average test loss: 0.003755020556350549\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07982250115606519\n",
      "Average test loss: 0.0030854738311221204\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07558769159184561\n",
      "Average test loss: 0.0025083911644501818\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0726502214703295\n",
      "Average test loss: 0.002430327457479305\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0702866689297888\n",
      "Average test loss: 0.0024213577825576067\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06670867934491899\n",
      "Average test loss: 0.002409098267348276\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06414636352989408\n",
      "Average test loss: 0.00506256949611836\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06276345490084755\n",
      "Average test loss: 0.002688530752228366\n",
      "Epoch 22/300\n",
      "Average training loss: 0.060291917539305157\n",
      "Average test loss: 0.0023996943614135184\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05774281439847417\n",
      "Average test loss: 0.0021608080955015287\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05634142985608843\n",
      "Average test loss: 0.002181548226210806\n",
      "Epoch 25/300\n",
      "Average training loss: 0.054012949619028305\n",
      "Average test loss: 0.0025020955039395227\n",
      "Epoch 26/300\n",
      "Average training loss: 0.052690043326881195\n",
      "Average test loss: 0.0021449712708385455\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05122733899950981\n",
      "Average test loss: 0.002318081194948819\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04952699740727742\n",
      "Average test loss: 0.0021192373275342913\n",
      "Epoch 29/300\n",
      "Average training loss: 0.048755192776521045\n",
      "Average test loss: 0.0022357865708569685\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05519744415084521\n",
      "Average test loss: 0.0021187689243298436\n",
      "Epoch 31/300\n",
      "Average training loss: 0.056169748955302766\n",
      "Average test loss: 0.002119015054570304\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04814668194121784\n",
      "Average test loss: 0.0021392633933573963\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04685938834481769\n",
      "Average test loss: 0.0021372302998271253\n",
      "Epoch 34/300\n",
      "Average training loss: 0.046251410868432784\n",
      "Average test loss: 0.002242844598988692\n",
      "Epoch 35/300\n",
      "Average training loss: 0.045675133138895034\n",
      "Average test loss: 0.0020793680134746765\n",
      "Epoch 36/300\n",
      "Average training loss: 0.044871112965875204\n",
      "Average test loss: 0.002015793160845836\n",
      "Epoch 37/300\n",
      "Average training loss: 0.044355285975668166\n",
      "Average test loss: 0.0020503228245716955\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04412529942393303\n",
      "Average test loss: 0.002663775238311953\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04354059860110283\n",
      "Average test loss: 0.011684279067648781\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04279094967742761\n",
      "Average test loss: 0.002012944246849252\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04261437259448899\n",
      "Average test loss: 0.0019869925359057056\n",
      "Epoch 42/300\n",
      "Average training loss: 0.042067894683943854\n",
      "Average test loss: 0.001932411302294996\n",
      "Epoch 43/300\n",
      "Average training loss: 0.041737460576825675\n",
      "Average test loss: 0.0019502601431061824\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04170760644144482\n",
      "Average test loss: 0.0019369385215557283\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04646453697813882\n",
      "Average test loss: 0.001985220559976167\n",
      "Epoch 46/300\n",
      "Average training loss: 0.042410462809933554\n",
      "Average test loss: 0.002132506994634039\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04141413457194964\n",
      "Average test loss: 0.0019307797730693387\n",
      "Epoch 48/300\n",
      "Average training loss: 0.040874685529205534\n",
      "Average test loss: 0.0023996144818762936\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04052085644337866\n",
      "Average test loss: 0.0020393924370615017\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04042615807387564\n",
      "Average test loss: 0.0019092173307306236\n",
      "Epoch 51/300\n",
      "Average training loss: 0.040259713000721405\n",
      "Average test loss: 0.002297074300237\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04002643144461844\n",
      "Average test loss: 0.0022027970504843526\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03971233367588785\n",
      "Average test loss: 0.0019786564824688766\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03965446365873019\n",
      "Average test loss: 0.002422512024641037\n",
      "Epoch 55/300\n",
      "Average training loss: 0.039104449050294025\n",
      "Average test loss: 0.002213326513974203\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03904057239161597\n",
      "Average test loss: 0.0019560037976544763\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03881417352457841\n",
      "Average test loss: 0.0019908991818212802\n",
      "Epoch 58/300\n",
      "Average training loss: 0.038699725940823555\n",
      "Average test loss: 0.0019609864145103428\n",
      "Epoch 59/300\n",
      "Average training loss: 0.038309259461032016\n",
      "Average test loss: 0.0019853069862971703\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03809926919473542\n",
      "Average test loss: 0.0018643988279832735\n",
      "Epoch 61/300\n",
      "Average training loss: 0.037909559148881174\n",
      "Average test loss: 0.0018843197020598584\n",
      "Epoch 62/300\n",
      "Average training loss: 0.037874298105637236\n",
      "Average test loss: 0.0018893790365093284\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03753980256782638\n",
      "Average test loss: 0.0026197816510167386\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0375374666750431\n",
      "Average test loss: 0.0021086861549152267\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03777772733403577\n",
      "Average test loss: 0.003614662248848213\n",
      "Epoch 66/300\n",
      "Average training loss: 0.049859896351893744\n",
      "Average test loss: 0.002090796994459298\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04215920226938195\n",
      "Average test loss: 0.00193501234261526\n",
      "Epoch 68/300\n",
      "Average training loss: 0.039674205160803264\n",
      "Average test loss: 0.0021161903076701693\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03860158956547578\n",
      "Average test loss: 0.0018954675517355401\n",
      "Epoch 70/300\n",
      "Average training loss: 0.038012643857134716\n",
      "Average test loss: 0.001884665867106782\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03770458283027013\n",
      "Average test loss: 0.0019013351895329025\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03731230885121557\n",
      "Average test loss: 0.001903633279311988\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03715012616912524\n",
      "Average test loss: 0.0029501886287083227\n",
      "Epoch 74/300\n",
      "Average training loss: 0.037026168253686695\n",
      "Average test loss: 0.0018851548925869995\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03696169709828165\n",
      "Average test loss: 0.0019076902955356572\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03667816113928954\n",
      "Average test loss: 0.001919724986474547\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03659882589181264\n",
      "Average test loss: 0.0019103632306473122\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03651328372624185\n",
      "Average test loss: 0.00479857597914007\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03640665320886506\n",
      "Average test loss: 0.0024196217695458067\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0363088436126709\n",
      "Average test loss: 0.0018966798489499423\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03625474917557504\n",
      "Average test loss: 0.0019241572295626004\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03606486763887935\n",
      "Average test loss: 0.004891652935494979\n",
      "Epoch 83/300\n",
      "Average training loss: 0.035954356128970785\n",
      "Average test loss: 0.0018722141608595848\n",
      "Epoch 84/300\n",
      "Average training loss: 0.035893706588281525\n",
      "Average test loss: 0.002266465949929423\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03572374210092757\n",
      "Average test loss: 0.0018883700869563552\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03562816228469213\n",
      "Average test loss: 0.0018806442628718084\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0363211428059472\n",
      "Average test loss: 0.0018552790459038483\n",
      "Epoch 88/300\n",
      "Average training loss: 0.036315975677635934\n",
      "Average test loss: 0.00188862015803655\n",
      "Epoch 89/300\n",
      "Average training loss: 0.035350585296750066\n",
      "Average test loss: 0.0018700747802439662\n",
      "Epoch 90/300\n",
      "Average training loss: 0.035416053305069604\n",
      "Average test loss: 0.001882822528688444\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03513434256778823\n",
      "Average test loss: 0.0019198153272478116\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03505348696642452\n",
      "Average test loss: 0.001842380326655176\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03504328048394786\n",
      "Average test loss: 0.17673955542676978\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03500870676173104\n",
      "Average test loss: 0.0024268243903708127\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03503391934269005\n",
      "Average test loss: 0.0018468226680739058\n",
      "Epoch 96/300\n",
      "Average training loss: 0.034990211576223375\n",
      "Average test loss: 0.0018954764175125294\n",
      "Epoch 97/300\n",
      "Average training loss: 0.034839051958587434\n",
      "Average test loss: 0.0020507572966938217\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03471643689274788\n",
      "Average test loss: 0.0019615835446036524\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03450696308745278\n",
      "Average test loss: 0.0018574884199640818\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03449886299504174\n",
      "Average test loss: 0.0018828311871944203\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03436824270420604\n",
      "Average test loss: 0.0019277978375968005\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03471163209031026\n",
      "Average test loss: 0.0019274699975632959\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0351320747964912\n",
      "Average test loss: 0.0018689464535046783\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03416513982249631\n",
      "Average test loss: 0.00187907282423435\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03416254368424416\n",
      "Average test loss: 0.0018630224621544282\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03414300164249208\n",
      "Average test loss: 0.0018915733448747131\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03394450253248215\n",
      "Average test loss: 0.0019197695069015025\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03396817175547282\n",
      "Average test loss: 0.010620688070853552\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03387931706508001\n",
      "Average test loss: 0.0019101122370403674\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0337597974224223\n",
      "Average test loss: 0.0020781035332216157\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0338738371067577\n",
      "Average test loss: 0.0019539288903276126\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03371774484879441\n",
      "Average test loss: 0.009250472040639984\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03362141366634104\n",
      "Average test loss: 0.0018991672640873325\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03361590391397476\n",
      "Average test loss: 0.0018812932700125707\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03353201837009854\n",
      "Average test loss: 0.002080239638375739\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03370341094666057\n",
      "Average test loss: 0.0018703405223786831\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03363797828886244\n",
      "Average test loss: 0.001917783417738974\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03340365598267979\n",
      "Average test loss: 0.0019279333539307117\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03330363991194301\n",
      "Average test loss: 0.001967443297927578\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03324552764495214\n",
      "Average test loss: 0.0018887222092598677\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03332701905899578\n",
      "Average test loss: 0.001889557077963319\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03317810096674495\n",
      "Average test loss: 0.0025759433298889133\n",
      "Epoch 123/300\n",
      "Average training loss: 0.033302421400944394\n",
      "Average test loss: 0.0018831312829214665\n",
      "Epoch 124/300\n",
      "Average training loss: 0.033461840785211985\n",
      "Average test loss: 0.001895978183278607\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03303095335099432\n",
      "Average test loss: 0.0019361812341958285\n",
      "Epoch 126/300\n",
      "Average training loss: 0.033082141704029504\n",
      "Average test loss: 0.001907899540124668\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03297146270010207\n",
      "Average test loss: 0.001906531112579008\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03302568943632973\n",
      "Average test loss: 0.005481334592940079\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03298584617177645\n",
      "Average test loss: 0.0022634344239615732\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03273689824342728\n",
      "Average test loss: 0.0018927946481853723\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03272384142213398\n",
      "Average test loss: 0.0019024764356306858\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03291455260912577\n",
      "Average test loss: 0.00204600548144016\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03292936804228359\n",
      "Average test loss: 0.0018711603649167552\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03260067121187846\n",
      "Average test loss: 0.0020655186766137677\n",
      "Epoch 135/300\n",
      "Average training loss: 0.032559126438366046\n",
      "Average test loss: 0.0023205714776284164\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03265894322925144\n",
      "Average test loss: 0.002060616606639491\n",
      "Epoch 137/300\n",
      "Average training loss: 0.032563530585832064\n",
      "Average test loss: 0.0020032495615176027\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03259876504540443\n",
      "Average test loss: 0.001981379372585151\n",
      "Epoch 139/300\n",
      "Average training loss: 0.032606111475163034\n",
      "Average test loss: 1.6484564911259545\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03260333788726065\n",
      "Average test loss: 0.006041246046208673\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03272783149447706\n",
      "Average test loss: 0.0019667158780826465\n",
      "Epoch 142/300\n",
      "Average training loss: 0.032327465330561\n",
      "Average test loss: 0.001899148091984292\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03227155375811789\n",
      "Average test loss: 0.987844894806544\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03229320997165309\n",
      "Average test loss: 0.0019487375093417034\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03225818059510655\n",
      "Average test loss: 0.008751710379289256\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03261135517557462\n",
      "Average test loss: 0.0019238636248434583\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03219612347582976\n",
      "Average test loss: 0.0019459903246412675\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03228046803341972\n",
      "Average test loss: 0.0019338151013685598\n",
      "Epoch 149/300\n",
      "Average training loss: 0.032277724471357135\n",
      "Average test loss: 0.01886016939497656\n",
      "Epoch 150/300\n",
      "Average training loss: 0.032385974668794205\n",
      "Average test loss: 0.0019551233117365177\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03203795261018806\n",
      "Average test loss: 0.0019467619663725297\n",
      "Epoch 152/300\n",
      "Average training loss: 0.032011513440145384\n",
      "Average test loss: 0.07534066991342439\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03204297332962354\n",
      "Average test loss: 0.03435020301408238\n",
      "Epoch 154/300\n",
      "Average training loss: 0.032147936092482676\n",
      "Average test loss: 0.002507183002308011\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0319615758061409\n",
      "Average test loss: 0.0020529769130258097\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03189101972679297\n",
      "Average test loss: 0.00192689275327656\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03183620117770301\n",
      "Average test loss: 0.0020540546412683197\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0319348207546605\n",
      "Average test loss: 0.00196921515568263\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03192434476150407\n",
      "Average test loss: 0.004769539019092917\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0326115717821651\n",
      "Average test loss: 0.001917253070200483\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03184003473156028\n",
      "Average test loss: 0.001996396889910102\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031623896554112435\n",
      "Average test loss: 0.001957903258295523\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031749019530084396\n",
      "Average test loss: 0.0021397606013342737\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031738557679785624\n",
      "Average test loss: 0.0020352651400284633\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031734432919157875\n",
      "Average test loss: 0.0019717249408778216\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031710012717379464\n",
      "Average test loss: 0.0019190005275110404\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03195529584255483\n",
      "Average test loss: 0.002006655460430516\n",
      "Epoch 168/300\n",
      "Average training loss: 0.031761403775877425\n",
      "Average test loss: 0.002574813185777101\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0316136090291871\n",
      "Average test loss: 0.0025317374610652524\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03161292513377137\n",
      "Average test loss: 0.007540134988932146\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03159865919748942\n",
      "Average test loss: 0.0019207790245612463\n",
      "Epoch 172/300\n",
      "Average training loss: 0.031584298551082614\n",
      "Average test loss: 0.00193701474637621\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03147765715420246\n",
      "Average test loss: 0.0019685232320593464\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0316050102379587\n",
      "Average test loss: 0.0022000246646089686\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03172386067112287\n",
      "Average test loss: 0.002348973938367433\n",
      "Epoch 176/300\n",
      "Average training loss: 0.031464002726806536\n",
      "Average test loss: 0.002082833184964127\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03138447515832053\n",
      "Average test loss: 0.0019242685503429837\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03134116075436274\n",
      "Average test loss: 0.0028251155382022263\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031424534229768646\n",
      "Average test loss: 0.0020635131189806594\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03144899520940251\n",
      "Average test loss: 0.0020187557456600998\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03135395638313558\n",
      "Average test loss: 0.0021592044127691124\n",
      "Epoch 182/300\n",
      "Average training loss: 0.031304639821251236\n",
      "Average test loss: 0.022341309247331485\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03138477910889519\n",
      "Average test loss: 0.0022319815594496\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03131631568239795\n",
      "Average test loss: 0.08411375016719103\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03144155632290575\n",
      "Average test loss: 0.0020747458181447453\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03149869222111172\n",
      "Average test loss: 0.004538822202218904\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03127784557806121\n",
      "Average test loss: 0.0021583672625323136\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03132201293110847\n",
      "Average test loss: 0.0020827161444144115\n",
      "Epoch 189/300\n",
      "Average training loss: 0.031123211312625142\n",
      "Average test loss: 0.029655962900982963\n",
      "Epoch 190/300\n",
      "Average training loss: 0.031218342542648315\n",
      "Average test loss: 0.0020455866997751097\n",
      "Epoch 191/300\n",
      "Average training loss: 0.031201976971493826\n",
      "Average test loss: 0.0020388507988924782\n",
      "Epoch 192/300\n",
      "Average training loss: 0.031178787829147445\n",
      "Average test loss: 0.009863605048766154\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03118555795815256\n",
      "Average test loss: 0.002067020723389255\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03114444575044844\n",
      "Average test loss: 0.013928469849129517\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031233097583055498\n",
      "Average test loss: 0.002004653750090963\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03107594340377384\n",
      "Average test loss: 0.0025769585406200755\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03104316194852193\n",
      "Average test loss: 0.0019720334011233513\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03104535795417097\n",
      "Average test loss: 0.00290267497777111\n",
      "Epoch 199/300\n",
      "Average training loss: 0.031067490172055033\n",
      "Average test loss: 0.0020937057739744583\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03097566948334376\n",
      "Average test loss: 0.0022585745236525932\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031028746729095776\n",
      "Average test loss: 0.0020067611263237064\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03097429517408212\n",
      "Average test loss: 0.0019840433901796737\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030922789009080992\n",
      "Average test loss: 0.00207532366613547\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0311312401427163\n",
      "Average test loss: 0.0023861406180593703\n",
      "Epoch 205/300\n",
      "Average training loss: 0.031006838315063054\n",
      "Average test loss: 0.0019913976875444253\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05778126390112771\n",
      "Average test loss: 0.00206511338841584\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04138129737062587\n",
      "Average test loss: 0.001950950137236052\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03663810594876607\n",
      "Average test loss: 0.0018640447375882004\n",
      "Epoch 209/300\n",
      "Average training loss: 0.034352697650591534\n",
      "Average test loss: 0.00229032413309647\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03291816859609551\n",
      "Average test loss: 0.0023947955367879734\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03192939048674372\n",
      "Average test loss: 0.0019356367764994502\n",
      "Epoch 212/300\n",
      "Average training loss: 0.031321687031123376\n",
      "Average test loss: 0.002005490113670627\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031008605274889203\n",
      "Average test loss: 0.0020173497307631703\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0309949924548467\n",
      "Average test loss: 0.002028971693995926\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030791506735814943\n",
      "Average test loss: 0.0026377416315178075\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03076842713687155\n",
      "Average test loss: 0.0019821047987788917\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030827979294790163\n",
      "Average test loss: 0.0024413581076595517\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03080560809208287\n",
      "Average test loss: 0.002042374364617798\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03089143018093374\n",
      "Average test loss: 0.0021323156586537757\n",
      "Epoch 220/300\n",
      "Average training loss: 0.030701189595792027\n",
      "Average test loss: 0.0025403844060169324\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030902742978599337\n",
      "Average test loss: 0.002015715146644248\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030758279234170915\n",
      "Average test loss: 0.002294974050898519\n",
      "Epoch 223/300\n",
      "Average training loss: 0.030848273689548174\n",
      "Average test loss: 0.0020524522887749802\n",
      "Epoch 224/300\n",
      "Average training loss: 0.030877929478883744\n",
      "Average test loss: 0.0020282539934333827\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030704124079810248\n",
      "Average test loss: 0.0030061147523423037\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030729547017150454\n",
      "Average test loss: 0.0020069729598859946\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030833636674616072\n",
      "Average test loss: 0.020013919747744996\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030688853532075882\n",
      "Average test loss: 0.0025585010540154244\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030672069460153578\n",
      "Average test loss: 0.002052434224428402\n",
      "Epoch 230/300\n",
      "Average training loss: 0.030635001374615563\n",
      "Average test loss: 0.0020868867478436894\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030732616605030166\n",
      "Average test loss: 0.002005829721896185\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030709319821662372\n",
      "Average test loss: 0.002380505060466627\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030733923070960574\n",
      "Average test loss: 0.0019960967475134467\n",
      "Epoch 234/300\n",
      "Average training loss: 0.030703223544690343\n",
      "Average test loss: 0.0020181799481312434\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03054280389348666\n",
      "Average test loss: 0.004834022441878915\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03069244721531868\n",
      "Average test loss: 0.002028798093657113\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030604745248953503\n",
      "Average test loss: 0.0022678129668864937\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030604279327723714\n",
      "Average test loss: 0.0028170112921959826\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030543782881564563\n",
      "Average test loss: 0.0020654406437857283\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030770045371519193\n",
      "Average test loss: 0.003192517071755396\n",
      "Epoch 241/300\n",
      "Average training loss: 0.030418894638617834\n",
      "Average test loss: 0.00200766783228351\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030518626617060767\n",
      "Average test loss: 0.002017805818778773\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03053121036787828\n",
      "Average test loss: 0.0019945070321878626\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030516848236322402\n",
      "Average test loss: 0.02820663683116436\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030460161053472094\n",
      "Average test loss: 0.002019820571773582\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0304177581568559\n",
      "Average test loss: 0.0022047209718989\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030500804997152752\n",
      "Average test loss: 0.006587018292811182\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030488098679317368\n",
      "Average test loss: 0.002069918212015182\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030601019428835974\n",
      "Average test loss: 0.002833068282653888\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030388455405831336\n",
      "Average test loss: 0.002041353978216648\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030305699156390295\n",
      "Average test loss: 0.0020034433777133625\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030414890858862134\n",
      "Average test loss: 0.0019890921719165314\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030337953478097916\n",
      "Average test loss: 0.002184984722485145\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03039657853047053\n",
      "Average test loss: 0.0021949752109746137\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030324105123678842\n",
      "Average test loss: 0.002076939451922145\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030248260933491918\n",
      "Average test loss: 0.0023210259533176817\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030942035181654823\n",
      "Average test loss: 0.22912062504225306\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03020313568247689\n",
      "Average test loss: 0.0020387535429456168\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030221186704105802\n",
      "Average test loss: 0.006152868780928354\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03030475182665719\n",
      "Average test loss: 0.17284956228194964\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030227758099635443\n",
      "Average test loss: 0.0025607730833192665\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030243567377328872\n",
      "Average test loss: 0.0021208748111708297\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030248270276519988\n",
      "Average test loss: 0.0021007796246558428\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03029826678501235\n",
      "Average test loss: 0.002020053729104499\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030174954172637727\n",
      "Average test loss: 0.001995598282458054\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030241426712936826\n",
      "Average test loss: 0.002020383073017001\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03021418957081106\n",
      "Average test loss: 0.0021943161107806698\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0301236479298936\n",
      "Average test loss: 0.002649966353343593\n",
      "Epoch 269/300\n",
      "Average training loss: 0.030256531374322042\n",
      "Average test loss: 0.002106541128518681\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03023812503947152\n",
      "Average test loss: 0.04267876885003514\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03013472299443351\n",
      "Average test loss: 0.0031113348739842572\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03010695958137512\n",
      "Average test loss: 0.006584455767646432\n",
      "Epoch 273/300\n",
      "Average training loss: 0.030107943427231578\n",
      "Average test loss: 0.0022719869361155563\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030183677312400604\n",
      "Average test loss: 0.002071947042416367\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030155087124970226\n",
      "Average test loss: 0.00210541414283216\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03018411898944113\n",
      "Average test loss: 0.0024237450930393403\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030048294057448705\n",
      "Average test loss: 0.002026534154597256\n",
      "Epoch 278/300\n",
      "Average training loss: 0.030293507119019826\n",
      "Average test loss: 0.002241422354968058\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03043211475511392\n",
      "Average test loss: 0.0019887007569066354\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030047833507259685\n",
      "Average test loss: 0.00203055048858126\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03004720112019115\n",
      "Average test loss: 0.002148062043926782\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029973153048091464\n",
      "Average test loss: 0.002250732046034601\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03004881238937378\n",
      "Average test loss: 0.002042969453872906\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0300250333994627\n",
      "Average test loss: 0.001990313141917189\n",
      "Epoch 285/300\n",
      "Average training loss: 0.029984266537759038\n",
      "Average test loss: 0.010108376770383782\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030031748783257272\n",
      "Average test loss: 0.00196434544151028\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03001551805436611\n",
      "Average test loss: 0.0020434785233810543\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0299630709124936\n",
      "Average test loss: 0.0020812324633200964\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02993720062242614\n",
      "Average test loss: 0.002063299774606195\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03017730772660838\n",
      "Average test loss: 0.0020897640472071037\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02989319813582632\n",
      "Average test loss: 0.0020337569006822175\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030038876122898527\n",
      "Average test loss: 0.002083022558854686\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0298946534064081\n",
      "Average test loss: 0.002020679123596185\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029951899074845845\n",
      "Average test loss: 0.002049535074167781\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030113851143254173\n",
      "Average test loss: 0.0020879032942983837\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03007629574669732\n",
      "Average test loss: 0.002041099498255385\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029949237849977283\n",
      "Average test loss: 0.8244220251507229\n",
      "Epoch 298/300\n",
      "Average training loss: 0.030014912004272143\n",
      "Average test loss: 0.0020422387574281957\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029845710721280838\n",
      "Average test loss: 0.002394558028111027\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02980733560025692\n",
      "Average test loss: 0.0020604076580040985\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.461366982460022\n",
      "Average test loss: 0.004886885919504695\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4683951200644175\n",
      "Average test loss: 0.0039044187578062216\n",
      "Epoch 3/300\n",
      "Average training loss: 0.30479587409231396\n",
      "Average test loss: 0.0034124409775767063\n",
      "Epoch 4/300\n",
      "Average training loss: 0.22291720118787553\n",
      "Average test loss: 0.0033775486648082733\n",
      "Epoch 5/300\n",
      "Average training loss: 0.17598508530192905\n",
      "Average test loss: 0.002920139844632811\n",
      "Epoch 6/300\n",
      "Average training loss: 0.14541489236884647\n",
      "Average test loss: 0.0030989379146032863\n",
      "Epoch 7/300\n",
      "Average training loss: 0.12471012269126044\n",
      "Average test loss: 0.004506718686885304\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10951793087853326\n",
      "Average test loss: 0.002910725848128398\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09803524655765958\n",
      "Average test loss: 0.0025758816084514064\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08965947782331044\n",
      "Average test loss: 0.004143641301741202\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08301815663443671\n",
      "Average test loss: 0.002305107559594843\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07763328940338558\n",
      "Average test loss: 0.002276370354824596\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07280224606725905\n",
      "Average test loss: 0.002503189549677902\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06861069599456257\n",
      "Average test loss: 0.002200786626380351\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06530945612986883\n",
      "Average test loss: 0.0021252024160284136\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06267071320282089\n",
      "Average test loss: 0.0023143781603624423\n",
      "Epoch 17/300\n",
      "Average training loss: 0.059595494512054654\n",
      "Average test loss: 0.0021190921129244896\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05704050644569927\n",
      "Average test loss: 0.0022587961186137464\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05487106972601678\n",
      "Average test loss: 0.001984442134698232\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05252187195420265\n",
      "Average test loss: 0.0020200192791720232\n",
      "Epoch 21/300\n",
      "Average training loss: 0.050526321745581096\n",
      "Average test loss: 0.0020373825650248263\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04880117059416241\n",
      "Average test loss: 0.0018027275188101663\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04726465778218375\n",
      "Average test loss: 0.01154713405130638\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04518897884090742\n",
      "Average test loss: 0.0016951832638846503\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04387441558970345\n",
      "Average test loss: 0.0016050707316026091\n",
      "Epoch 26/300\n",
      "Average training loss: 0.042022157073020934\n",
      "Average test loss: 0.005204264147414102\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04070856088399887\n",
      "Average test loss: 0.0016456668921228912\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039081050329738194\n",
      "Average test loss: 0.001570843889274531\n",
      "Epoch 29/300\n",
      "Average training loss: 0.038121812630030845\n",
      "Average test loss: 0.0014926020313675206\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03704149060944716\n",
      "Average test loss: 0.001619164291355345\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03633682448168596\n",
      "Average test loss: 0.0014617840245159136\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03585325741105609\n",
      "Average test loss: 0.001472500322904024\n",
      "Epoch 33/300\n",
      "Average training loss: 0.034764385686980356\n",
      "Average test loss: 0.0014818120628802313\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03398711535665724\n",
      "Average test loss: 0.0013806776656872697\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03349247345328331\n",
      "Average test loss: 0.0032200104124430152\n",
      "Epoch 36/300\n",
      "Average training loss: 0.033123019953568776\n",
      "Average test loss: 0.0013786888281918234\n",
      "Epoch 37/300\n",
      "Average training loss: 0.032632620028323595\n",
      "Average test loss: 0.001416472050257855\n",
      "Epoch 38/300\n",
      "Average training loss: 0.032202056848340564\n",
      "Average test loss: 0.0013464616523641679\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03170321315858099\n",
      "Average test loss: 0.001353486620924539\n",
      "Epoch 40/300\n",
      "Average training loss: 0.031545010728968516\n",
      "Average test loss: 0.0014265640922304656\n",
      "Epoch 41/300\n",
      "Average training loss: 0.031315043648084004\n",
      "Average test loss: 0.0015063036131776042\n",
      "Epoch 42/300\n",
      "Average training loss: 0.031107979594005478\n",
      "Average test loss: 0.001387465399896933\n",
      "Epoch 43/300\n",
      "Average training loss: 0.030558197849326663\n",
      "Average test loss: 0.0013130814435684847\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03027071943713559\n",
      "Average test loss: 0.0015935466768633988\n",
      "Epoch 45/300\n",
      "Average training loss: 0.030307435429758497\n",
      "Average test loss: 0.0013082617561643323\n",
      "Epoch 46/300\n",
      "Average training loss: 0.029766000345349312\n",
      "Average test loss: 0.001617289457263218\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02956791056527032\n",
      "Average test loss: 0.0013158325551905566\n",
      "Epoch 48/300\n",
      "Average training loss: 0.029483984510103863\n",
      "Average test loss: 0.0014653168287542132\n",
      "Epoch 49/300\n",
      "Average training loss: 0.029239021386537285\n",
      "Average test loss: 0.001413393186715742\n",
      "Epoch 50/300\n",
      "Average training loss: 0.029146073892712593\n",
      "Average test loss: 0.0014588343283782402\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0289511692341831\n",
      "Average test loss: 0.0012619034064312775\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02872307579053773\n",
      "Average test loss: 0.0013303896638875207\n",
      "Epoch 53/300\n",
      "Average training loss: 0.028670053758554987\n",
      "Average test loss: 0.0013732479957656728\n",
      "Epoch 54/300\n",
      "Average training loss: 0.028580370941095883\n",
      "Average test loss: 0.001266764048797389\n",
      "Epoch 55/300\n",
      "Average training loss: 0.029333100125193596\n",
      "Average test loss: 0.0015077163242838449\n",
      "Epoch 56/300\n",
      "Average training loss: 0.028266880808605087\n",
      "Average test loss: 0.0012757903757608599\n",
      "Epoch 57/300\n",
      "Average training loss: 0.028040374784006013\n",
      "Average test loss: 0.0012669118330296543\n",
      "Epoch 58/300\n",
      "Average training loss: 0.028176810087429154\n",
      "Average test loss: 0.001264113401994109\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02839063128332297\n",
      "Average test loss: 0.0014050211509068807\n",
      "Epoch 60/300\n",
      "Average training loss: 0.027809854924678804\n",
      "Average test loss: 0.0012491910788748\n",
      "Epoch 61/300\n",
      "Average training loss: 0.027693663444783953\n",
      "Average test loss: 0.001558387272266878\n",
      "Epoch 62/300\n",
      "Average training loss: 0.027682161077857016\n",
      "Average test loss: 0.0013937724239917264\n",
      "Epoch 63/300\n",
      "Average training loss: 0.027603657695982192\n",
      "Average test loss: 0.0012497738474566075\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02754318574236499\n",
      "Average test loss: 0.001270523096745213\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02737405469185776\n",
      "Average test loss: 0.0012588088400661945\n",
      "Epoch 66/300\n",
      "Average training loss: 0.027612722666727173\n",
      "Average test loss: 0.0012960041213987603\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02754950276348326\n",
      "Average test loss: 0.0012869324814528227\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027219348134266005\n",
      "Average test loss: 0.001237981234987577\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02709720078938537\n",
      "Average test loss: 0.0021566220875829456\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02718512837919924\n",
      "Average test loss: 0.001903891751749648\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0269891733262274\n",
      "Average test loss: 0.001621986110177305\n",
      "Epoch 72/300\n",
      "Average training loss: 0.026890227504902414\n",
      "Average test loss: 0.0013166743264430099\n",
      "Epoch 73/300\n",
      "Average training loss: 0.026957843146390385\n",
      "Average test loss: 0.0016960581757335198\n",
      "Epoch 74/300\n",
      "Average training loss: 0.026808826024333637\n",
      "Average test loss: 0.0012334310387571653\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02682676764494843\n",
      "Average test loss: 0.0030065074442989295\n",
      "Epoch 76/300\n",
      "Average training loss: 0.026690955203440454\n",
      "Average test loss: 0.0032938487858821947\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02661067904697524\n",
      "Average test loss: 0.0013651046233976053\n",
      "Epoch 78/300\n",
      "Average training loss: 0.026678868798746\n",
      "Average test loss: 0.0012927715473083985\n",
      "Epoch 79/300\n",
      "Average training loss: 0.026705371810330284\n",
      "Average test loss: 0.0012609966899940952\n",
      "Epoch 80/300\n",
      "Average training loss: 0.026452764276001187\n",
      "Average test loss: 0.0021442159303050076\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02635422496497631\n",
      "Average test loss: 0.002638146733451221\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026365051862266328\n",
      "Average test loss: 0.0012628561292464535\n",
      "Epoch 83/300\n",
      "Average training loss: 0.026385453247361714\n",
      "Average test loss: 0.0013864694541941087\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02627027663588524\n",
      "Average test loss: 0.0013071797030667464\n",
      "Epoch 85/300\n",
      "Average training loss: 0.026123635369870397\n",
      "Average test loss: 0.0012885129052317805\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02610692703558339\n",
      "Average test loss: 0.002179640616497232\n",
      "Epoch 87/300\n",
      "Average training loss: 0.026195054612225956\n",
      "Average test loss: 0.001393303205103924\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025974228345685534\n",
      "Average test loss: 0.010925552733656432\n",
      "Epoch 89/300\n",
      "Average training loss: 0.025975310837229093\n",
      "Average test loss: 0.0017736132418633335\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02590570306777954\n",
      "Average test loss: 0.0013472707546833488\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025894947060280376\n",
      "Average test loss: 0.001921744384802878\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0259960808356603\n",
      "Average test loss: 0.0014572771538255943\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02584722765452332\n",
      "Average test loss: 0.0013701982231189807\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025705588571727275\n",
      "Average test loss: 0.001438093745874034\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0256836159825325\n",
      "Average test loss: 0.0013376622795541254\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025714273436201942\n",
      "Average test loss: 0.0634945347495377\n",
      "Epoch 97/300\n",
      "Average training loss: 0.025661075618531967\n",
      "Average test loss: 0.0013836374055180285\n",
      "Epoch 98/300\n",
      "Average training loss: 0.025544079866674212\n",
      "Average test loss: 0.0012533342447131873\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02597674534552627\n",
      "Average test loss: 2.517897997624344\n",
      "Epoch 100/300\n",
      "Average training loss: 0.025546894676155514\n",
      "Average test loss: 0.0013091915210502016\n",
      "Epoch 101/300\n",
      "Average training loss: 0.025371619525882932\n",
      "Average test loss: 0.0012801304377822412\n",
      "Epoch 102/300\n",
      "Average training loss: 0.025397936293648348\n",
      "Average test loss: 0.0013238541913322276\n",
      "Epoch 103/300\n",
      "Average training loss: 0.025461000468995836\n",
      "Average test loss: 0.0014000233822605676\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02532108203238911\n",
      "Average test loss: 0.001402680706129306\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02534920205341445\n",
      "Average test loss: 0.0013310195702231592\n",
      "Epoch 106/300\n",
      "Average training loss: 0.025364495466152825\n",
      "Average test loss: 0.001275029568415549\n",
      "Epoch 107/300\n",
      "Average training loss: 0.025334048019515144\n",
      "Average test loss: 0.0012837737673479649\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02523390156030655\n",
      "Average test loss: 0.0014084108175916805\n",
      "Epoch 109/300\n",
      "Average training loss: 0.025095483034849167\n",
      "Average test loss: 0.0013044085303942363\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02506417499234279\n",
      "Average test loss: 0.001325753381010145\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02530088660783238\n",
      "Average test loss: 0.0012676433436572552\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02502526059663958\n",
      "Average test loss: 0.0012995839543226693\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02513816685643461\n",
      "Average test loss: 0.002268790182347099\n",
      "Epoch 114/300\n",
      "Average training loss: 0.025197466908229722\n",
      "Average test loss: 0.001687420210852805\n",
      "Epoch 115/300\n",
      "Average training loss: 0.024951129135158328\n",
      "Average test loss: 0.0012984336444383693\n",
      "Epoch 116/300\n",
      "Average training loss: 0.024905044002665414\n",
      "Average test loss: 0.08621658719620771\n",
      "Epoch 117/300\n",
      "Average training loss: 0.024994170856144693\n",
      "Average test loss: 0.0012641407849474085\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02493637909822994\n",
      "Average test loss: 0.012656105168577698\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02494349006149504\n",
      "Average test loss: 0.0012787890838873055\n",
      "Epoch 120/300\n",
      "Average training loss: 0.024773296430706977\n",
      "Average test loss: 0.0014219111228982607\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025115173010362517\n",
      "Average test loss: 0.001695173436568843\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02469873894088798\n",
      "Average test loss: 0.0014663953678682447\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02491647589703401\n",
      "Average test loss: 0.0013581703475986917\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024718994128207366\n",
      "Average test loss: 0.0020413137171417474\n",
      "Epoch 125/300\n",
      "Average training loss: 0.024688098424010808\n",
      "Average test loss: 0.0026926405487789047\n",
      "Epoch 126/300\n",
      "Average training loss: 0.024657317004270023\n",
      "Average test loss: 0.001392471340795358\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024707651495933534\n",
      "Average test loss: 0.001460324480301804\n",
      "Epoch 128/300\n",
      "Average training loss: 0.024661018156343036\n",
      "Average test loss: 0.001483590126141078\n",
      "Epoch 129/300\n",
      "Average training loss: 0.024680544457501834\n",
      "Average test loss: 0.0012928970190696418\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024592853122287327\n",
      "Average test loss: 0.00129263896195011\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02462987935874197\n",
      "Average test loss: 0.0013114432519715693\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024527626236279805\n",
      "Average test loss: 0.0013557556294318703\n",
      "Epoch 133/300\n",
      "Average training loss: 0.024488951308859717\n",
      "Average test loss: 0.0016318264580849145\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02440759006308185\n",
      "Average test loss: 0.0013320184814640218\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02444989711542924\n",
      "Average test loss: 0.0016455737409285373\n",
      "Epoch 136/300\n",
      "Average training loss: 0.025211040928959847\n",
      "Average test loss: 0.0014596168650831613\n",
      "Epoch 137/300\n",
      "Average training loss: 0.024439255020684666\n",
      "Average test loss: 0.001288410603017029\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02436472691098849\n",
      "Average test loss: 0.0013088278021249507\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024297616372505822\n",
      "Average test loss: 0.0013087575698478354\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02433164432975981\n",
      "Average test loss: 0.0012669510982102818\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02468485491640038\n",
      "Average test loss: 0.001708191867193414\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02425750824974643\n",
      "Average test loss: 0.0015165320498247942\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024234483346343042\n",
      "Average test loss: 0.0018485283131400743\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02418383934431606\n",
      "Average test loss: 0.00127922276686877\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02438620935380459\n",
      "Average test loss: 0.0013214432672700948\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02423697090314494\n",
      "Average test loss: 0.0012994349330870641\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02417473673986064\n",
      "Average test loss: 0.001293504831691583\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024158702769213253\n",
      "Average test loss: 0.004833769162495931\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02415242096119457\n",
      "Average test loss: 0.0020940226399236254\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02417354482329554\n",
      "Average test loss: 0.0015633738486924105\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024303671442800098\n",
      "Average test loss: 0.0022358515358840425\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02407413762311141\n",
      "Average test loss: 0.0013037662472989823\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024070406837595832\n",
      "Average test loss: 0.0015481804777971572\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024184197223848767\n",
      "Average test loss: 0.001293817467470136\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024175776922040516\n",
      "Average test loss: 0.180227205807964\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02403299084305763\n",
      "Average test loss: 0.001440075287905832\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024028626864155134\n",
      "Average test loss: 0.001374959330798851\n",
      "Epoch 158/300\n",
      "Average training loss: 0.024112130223049057\n",
      "Average test loss: 0.001309300354578429\n",
      "Epoch 159/300\n",
      "Average training loss: 0.023953492124875388\n",
      "Average test loss: 0.0030503860778278774\n",
      "Epoch 160/300\n",
      "Average training loss: 0.024870852107803026\n",
      "Average test loss: 0.0013052761685103179\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02381042290561729\n",
      "Average test loss: 0.0013536168999142117\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023909924593236712\n",
      "Average test loss: 0.0013941272202258308\n",
      "Epoch 163/300\n",
      "Average training loss: 0.023848752735389604\n",
      "Average test loss: 3236.9779895833335\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02384811783168051\n",
      "Average test loss: 0.002147766569836272\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023872004411286778\n",
      "Average test loss: 0.0013349550973830952\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02383385067019198\n",
      "Average test loss: 0.0013639276122881306\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023869352845682038\n",
      "Average test loss: 0.005535561645196543\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0239068891753753\n",
      "Average test loss: 0.0015417525360567701\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02387071703043249\n",
      "Average test loss: 0.002349163134685821\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023877173465159204\n",
      "Average test loss: 0.001610712572104401\n",
      "Epoch 171/300\n",
      "Average training loss: 0.023813818607065412\n",
      "Average test loss: 0.03710408552342819\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02381775047050582\n",
      "Average test loss: 0.001368278943416145\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023815144013199542\n",
      "Average test loss: 0.0013374408756693203\n",
      "Epoch 174/300\n",
      "Average training loss: 0.023743621226814057\n",
      "Average test loss: 0.0013736753842482964\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02376459797554546\n",
      "Average test loss: 0.001520318124236332\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023764307823446063\n",
      "Average test loss: 0.0029656867906968625\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02377481814225515\n",
      "Average test loss: 0.0013538951620252597\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02365980730785264\n",
      "Average test loss: 0.0013594773399333159\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023643100596136515\n",
      "Average test loss: 0.0013041063187023004\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023697572901844977\n",
      "Average test loss: 0.0013781596945805683\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023671189677384165\n",
      "Average test loss: 0.0020233007094098464\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023668483664592108\n",
      "Average test loss: 0.0013738117644356356\n",
      "Epoch 183/300\n",
      "Average training loss: 0.023664621429310903\n",
      "Average test loss: 0.01398431540777286\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023852813179294267\n",
      "Average test loss: 0.0013404884454276827\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023661853566765784\n",
      "Average test loss: 0.05189859243482351\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023619423453178672\n",
      "Average test loss: 0.001376808825776809\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02354085059795115\n",
      "Average test loss: 0.0014131844679100647\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023569245424535538\n",
      "Average test loss: 0.0013493096399017507\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023519227468305162\n",
      "Average test loss: 0.0013429666959887576\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023508472128046885\n",
      "Average test loss: 0.001342061472332312\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02350191033217642\n",
      "Average test loss: 0.0015341751374718215\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02348977964040306\n",
      "Average test loss: 0.012166496944096354\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023561693663398425\n",
      "Average test loss: 0.0016357308563051953\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02346309840430816\n",
      "Average test loss: 0.0014913032713035743\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023450257851017845\n",
      "Average test loss: 0.001388280520319111\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02343981594261196\n",
      "Average test loss: 0.0014233300633107622\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02359162480632464\n",
      "Average test loss: 0.0013505676622605986\n",
      "Epoch 198/300\n",
      "Average training loss: 0.023405476931068633\n",
      "Average test loss: 0.001447769799373216\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023405154085821576\n",
      "Average test loss: 0.005157644018116925\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023414994878901377\n",
      "Average test loss: 0.0014154540736021268\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023464506899317106\n",
      "Average test loss: 0.0013353972193888492\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023488956724603972\n",
      "Average test loss: 0.0013609250056453876\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02335118004348543\n",
      "Average test loss: 0.0014043863436414136\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02334997254113356\n",
      "Average test loss: 0.001361986366669751\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023502197265625\n",
      "Average test loss: 0.0013501203471484283\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023338609158992767\n",
      "Average test loss: 0.0014392600260261033\n",
      "Epoch 207/300\n",
      "Average training loss: 0.024517135818799336\n",
      "Average test loss: 0.0023944591962628894\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023260776014791595\n",
      "Average test loss: 0.0016059379171476597\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023226864008439912\n",
      "Average test loss: 0.001361976955085993\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023253824538654752\n",
      "Average test loss: 0.0013467029000942905\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023218141597178246\n",
      "Average test loss: 0.0019466901123523712\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023295351597997876\n",
      "Average test loss: 0.02684127277218633\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023225888554420735\n",
      "Average test loss: 0.0014727530068614416\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023403762300809223\n",
      "Average test loss: 0.0014094922286975716\n",
      "Epoch 215/300\n",
      "Average training loss: 0.023250930867261356\n",
      "Average test loss: 0.0014049456684539715\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023476128295063972\n",
      "Average test loss: 0.001366378114817457\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02315743040210671\n",
      "Average test loss: 0.0015783283313115439\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0231655906115969\n",
      "Average test loss: 0.0014091426546478437\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0231918234212531\n",
      "Average test loss: 0.0013665502397343517\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02318364293542173\n",
      "Average test loss: 0.001313969968093766\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02322119499577416\n",
      "Average test loss: 0.0013895520732427637\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023497720102469127\n",
      "Average test loss: 0.001332911828532815\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023142898110879793\n",
      "Average test loss: 0.0014116610442805621\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023067193269729613\n",
      "Average test loss: 0.0014128632153280907\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023174044334226183\n",
      "Average test loss: 0.0018768084516955746\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023089450715316668\n",
      "Average test loss: 0.0014292822484340932\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023107360806730058\n",
      "Average test loss: 0.001371298307294233\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02306339060763518\n",
      "Average test loss: 0.0014037420955590077\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02313114264441861\n",
      "Average test loss: 0.001479647257986168\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023083618119359016\n",
      "Average test loss: 0.03290766417152352\n",
      "Epoch 231/300\n",
      "Average training loss: 0.023012879023949304\n",
      "Average test loss: 0.002741747692330844\n",
      "Epoch 232/300\n",
      "Average training loss: 0.023243561392029127\n",
      "Average test loss: 0.0013756229309365153\n",
      "Epoch 233/300\n",
      "Average training loss: 0.023173294270204172\n",
      "Average test loss: 0.0027160292027725115\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022953404148419697\n",
      "Average test loss: 0.0014103552060615685\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022993462834093304\n",
      "Average test loss: 0.015213287757606142\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02340682509375943\n",
      "Average test loss: 0.0013977736776901615\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023005076820651692\n",
      "Average test loss: 0.0013804547799647683\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022928061420718827\n",
      "Average test loss: 0.0013237899463209842\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02323238674302896\n",
      "Average test loss: 0.0014587741080257627\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02292255088024669\n",
      "Average test loss: 0.0019435562297908796\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02291003667149279\n",
      "Average test loss: 0.0024516573775456183\n",
      "Epoch 242/300\n",
      "Average training loss: 0.023103262821833292\n",
      "Average test loss: 0.0014681686455797819\n",
      "Epoch 243/300\n",
      "Average training loss: 0.022965258717536928\n",
      "Average test loss: 0.0018216470604141553\n",
      "Epoch 244/300\n",
      "Average training loss: 0.022960554644465447\n",
      "Average test loss: 0.0014128038327520093\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02307161890467008\n",
      "Average test loss: 0.0015175387010806136\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022927120935585764\n",
      "Average test loss: 0.0013870261163036856\n",
      "Epoch 247/300\n",
      "Average training loss: 0.023213641385237377\n",
      "Average test loss: 0.001398460557580822\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022956151748696963\n",
      "Average test loss: 0.00549000134691596\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023025802390442954\n",
      "Average test loss: 0.0014532726927556925\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022842900140417947\n",
      "Average test loss: 0.001389918376588159\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02299666835864385\n",
      "Average test loss: 0.001665287746840881\n",
      "Epoch 252/300\n",
      "Average training loss: 0.022869170846210585\n",
      "Average test loss: 0.0014386486074783736\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023490933254361152\n",
      "Average test loss: 0.0013388044897259938\n",
      "Epoch 254/300\n",
      "Average training loss: 0.022782690475384393\n",
      "Average test loss: 0.0018059941277735762\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02283503583073616\n",
      "Average test loss: 0.0015418172704262866\n",
      "Epoch 256/300\n",
      "Average training loss: 0.022778811068998442\n",
      "Average test loss: 0.0014729349130971564\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022869517925712797\n",
      "Average test loss: 0.001535840092640784\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022843341413471434\n",
      "Average test loss: 0.0013546664777418806\n",
      "Epoch 259/300\n",
      "Average training loss: 0.022877012292544047\n",
      "Average test loss: 0.0015214890577933855\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022919706076383592\n",
      "Average test loss: 0.0027289279248151515\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022871133708291583\n",
      "Average test loss: 0.0014530592856721745\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022817290120654636\n",
      "Average test loss: 0.0019295012642525965\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022789225136240324\n",
      "Average test loss: 0.0013751641440515718\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022748186863130994\n",
      "Average test loss: 0.0034480800843901104\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022853623242841827\n",
      "Average test loss: 0.0014579061436363393\n",
      "Epoch 266/300\n",
      "Average training loss: 0.022916695528560214\n",
      "Average test loss: 0.002646499454975128\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02269246885014905\n",
      "Average test loss: 0.0013761243901422454\n",
      "Epoch 268/300\n",
      "Average training loss: 0.022753073601259124\n",
      "Average test loss: 0.0015118776780242722\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022961400209201708\n",
      "Average test loss: 0.0015787032366626793\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022671187899178928\n",
      "Average test loss: 0.0016616608206079238\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022739364465077717\n",
      "Average test loss: 0.03153585021011531\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022718483961290784\n",
      "Average test loss: 0.0013805406686539451\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02278550345533424\n",
      "Average test loss: 0.0014103914085361693\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02493544400566154\n",
      "Average test loss: 0.0013928576373598643\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02260993545750777\n",
      "Average test loss: 0.0013706645785520474\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02258458001083798\n",
      "Average test loss: 0.0013595296067910063\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022590090269843738\n",
      "Average test loss: 0.001391745039779279\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02263630332549413\n",
      "Average test loss: 0.0013686087171857556\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022754794523119928\n",
      "Average test loss: 0.0030884329095068907\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02263630922138691\n",
      "Average test loss: 0.0013924280713415809\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022671353023913172\n",
      "Average test loss: 0.0013473311896539397\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022684769678446983\n",
      "Average test loss: 0.0013954805034316249\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022697291501694257\n",
      "Average test loss: 0.005647121214411325\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022676118897067176\n",
      "Average test loss: 0.0013814843302178714\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022654910411271784\n",
      "Average test loss: 0.0014046229873266485\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022629713401198386\n",
      "Average test loss: 0.0014777863795558612\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02295829845137066\n",
      "Average test loss: 0.0013607941620672743\n",
      "Epoch 288/300\n",
      "Average training loss: 0.022603974036044545\n",
      "Average test loss: 0.0013792278147819969\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022625155174069933\n",
      "Average test loss: 0.001349881760465602\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02258029092517164\n",
      "Average test loss: 0.02084308474014203\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022782805777258344\n",
      "Average test loss: 0.001380811527474887\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022776781688133876\n",
      "Average test loss: 0.004631917578478654\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022548995135558976\n",
      "Average test loss: 0.0014161528427567747\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022543681846724617\n",
      "Average test loss: 0.0014054894619103934\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022513003903958533\n",
      "Average test loss: 0.0014133001498671041\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02265406773487727\n",
      "Average test loss: 0.002303217233469089\n",
      "Epoch 297/300\n",
      "Average training loss: 0.022550905055469937\n",
      "Average test loss: 0.0014079313118838602\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02264463284942839\n",
      "Average test loss: 0.0013581425735933912\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022547169296277893\n",
      "Average test loss: 0.00137637401268714\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022555149556861984\n",
      "Average test loss: 0.068012895050976\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.33\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.82\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.98\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.75\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.12\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.99\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.93\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.64\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.60\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.24\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.35\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.70\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.97\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.57\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.61\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.36\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.21\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.48\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.86\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.43\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 17.006296507941354\n",
      "Average test loss: 0.012761438671085569\n",
      "Epoch 2/300\n",
      "Average training loss: 6.854104012383355\n",
      "Average test loss: 0.011917208497722943\n",
      "Epoch 3/300\n",
      "Average training loss: 4.83390207417806\n",
      "Average test loss: 0.009277244169678954\n",
      "Epoch 4/300\n",
      "Average training loss: 3.7487004044850667\n",
      "Average test loss: 0.01248670683387253\n",
      "Epoch 5/300\n",
      "Average training loss: 3.373695351070828\n",
      "Average test loss: 0.008986344186796082\n",
      "Epoch 6/300\n",
      "Average training loss: 2.8942209294637045\n",
      "Average test loss: 0.008243061256905397\n",
      "Epoch 7/300\n",
      "Average training loss: 2.252735419591268\n",
      "Average test loss: 0.009929984371695254\n",
      "Epoch 8/300\n",
      "Average training loss: 1.8523642128838433\n",
      "Average test loss: 0.007617619958188799\n",
      "Epoch 9/300\n",
      "Average training loss: 1.4936695555580988\n",
      "Average test loss: 0.007944885877685414\n",
      "Epoch 10/300\n",
      "Average training loss: 1.2365991355048285\n",
      "Average test loss: 0.007929378184179464\n",
      "Epoch 11/300\n",
      "Average training loss: 1.0461132060156928\n",
      "Average test loss: 0.007521260187857681\n",
      "Epoch 12/300\n",
      "Average training loss: 0.898410346031189\n",
      "Average test loss: 0.0072409545071423055\n",
      "Epoch 13/300\n",
      "Average training loss: 0.7654103939798144\n",
      "Average test loss: 0.007948802948825889\n",
      "Epoch 14/300\n",
      "Average training loss: 0.6573137532869975\n",
      "Average test loss: 0.0073002361042631995\n",
      "Epoch 15/300\n",
      "Average training loss: 0.5711888289981418\n",
      "Average test loss: 0.0075607230112784436\n",
      "Epoch 16/300\n",
      "Average training loss: 0.5076781968010796\n",
      "Average test loss: 0.009511673202117285\n",
      "Epoch 17/300\n",
      "Average training loss: 0.4589856055577596\n",
      "Average test loss: 0.006626094667861859\n",
      "Epoch 18/300\n",
      "Average training loss: 0.4230025540192922\n",
      "Average test loss: 0.006947026652594408\n",
      "Epoch 19/300\n",
      "Average training loss: 0.390103794336319\n",
      "Average test loss: 0.019922594543960358\n",
      "Epoch 20/300\n",
      "Average training loss: 0.3623280244403415\n",
      "Average test loss: 0.005994293155769507\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3407627306514316\n",
      "Average test loss: 0.006110083499716388\n",
      "Epoch 22/300\n",
      "Average training loss: 0.3204869239860111\n",
      "Average test loss: 0.00678596407506201\n",
      "Epoch 23/300\n",
      "Average training loss: 0.30482615839110483\n",
      "Average test loss: 0.007404545931352509\n",
      "Epoch 24/300\n",
      "Average training loss: 0.2932968495686849\n",
      "Average test loss: 0.005826325200084183\n",
      "Epoch 25/300\n",
      "Average training loss: 0.27995420667860244\n",
      "Average test loss: 0.005760916630840964\n",
      "Epoch 26/300\n",
      "Average training loss: 0.27024549701478745\n",
      "Average test loss: 0.021216549976004494\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2602721309661865\n",
      "Average test loss: 0.005644610316389137\n",
      "Epoch 28/300\n",
      "Average training loss: 0.25260714570681253\n",
      "Average test loss: 0.0055372378056248025\n",
      "Epoch 29/300\n",
      "Average training loss: 0.24458960178163316\n",
      "Average test loss: 0.005449674086438285\n",
      "Epoch 30/300\n",
      "Average training loss: 0.23792262762122685\n",
      "Average test loss: 0.00549104883066482\n",
      "Epoch 31/300\n",
      "Average training loss: 0.23269065056906807\n",
      "Average test loss: 0.005992821274946133\n",
      "Epoch 32/300\n",
      "Average training loss: 0.22724196803569793\n",
      "Average test loss: 0.005489032526397043\n",
      "Epoch 33/300\n",
      "Average training loss: 0.2199297139644623\n",
      "Average test loss: 0.005541475639161136\n",
      "Epoch 34/300\n",
      "Average training loss: 0.21490535769197677\n",
      "Average test loss: 0.005922647489027845\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2092496630218294\n",
      "Average test loss: 0.005296939888348182\n",
      "Epoch 36/300\n",
      "Average training loss: 0.20675916978385714\n",
      "Average test loss: 0.005580459837284353\n",
      "Epoch 37/300\n",
      "Average training loss: 0.20265123686525557\n",
      "Average test loss: 0.005914338880115085\n",
      "Epoch 38/300\n",
      "Average training loss: 0.19811232342322668\n",
      "Average test loss: 0.005515736464824942\n",
      "Epoch 39/300\n",
      "Average training loss: 0.19614181707965003\n",
      "Average test loss: 0.005615980922347969\n",
      "Epoch 40/300\n",
      "Average training loss: 0.19408052469624412\n",
      "Average test loss: 0.005185626628291275\n",
      "Epoch 41/300\n",
      "Average training loss: 0.19059113681316375\n",
      "Average test loss: 0.006898343551903963\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1887922856542799\n",
      "Average test loss: 0.005329257001479467\n",
      "Epoch 43/300\n",
      "Average training loss: 0.18572876414987777\n",
      "Average test loss: 0.006685019102775388\n",
      "Epoch 44/300\n",
      "Average training loss: 0.18417909268538157\n",
      "Average test loss: 1.1533929748535157\n",
      "Epoch 45/300\n",
      "Average training loss: 0.18236697661876677\n",
      "Average test loss: 0.0082901426412993\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1802923812866211\n",
      "Average test loss: 0.005226853477044238\n",
      "Epoch 47/300\n",
      "Average training loss: 0.17952415872944727\n",
      "Average test loss: 0.005162683923211363\n",
      "Epoch 48/300\n",
      "Average training loss: 0.17710214975145128\n",
      "Average test loss: 0.005171340345508523\n",
      "Epoch 49/300\n",
      "Average training loss: 0.17574860364860959\n",
      "Average test loss: 1.1987760435740153\n",
      "Epoch 50/300\n",
      "Average training loss: 0.17453648821512857\n",
      "Average test loss: 0.005180321704596281\n",
      "Epoch 51/300\n",
      "Average training loss: 0.17409021433194477\n",
      "Average test loss: 0.005071675637943877\n",
      "Epoch 52/300\n",
      "Average training loss: 0.17096839413377973\n",
      "Average test loss: 0.005441767179303699\n",
      "Epoch 53/300\n",
      "Average training loss: 0.17025913438532086\n",
      "Average test loss: 0.006568691783895095\n",
      "Epoch 54/300\n",
      "Average training loss: 0.16980536222457887\n",
      "Average test loss: 0.005131440739664766\n",
      "Epoch 55/300\n",
      "Average training loss: 0.16836525399155086\n",
      "Average test loss: 0.21106311776571804\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1669813854628139\n",
      "Average test loss: 0.005886779744178057\n",
      "Epoch 57/300\n",
      "Average training loss: 0.16703107321924635\n",
      "Average test loss: 0.004978867013421324\n",
      "Epoch 58/300\n",
      "Average training loss: 0.16688468440373738\n",
      "Average test loss: 0.005123750895675685\n",
      "Epoch 59/300\n",
      "Average training loss: 0.16430958321359423\n",
      "Average test loss: 0.005016199728680982\n",
      "Epoch 60/300\n",
      "Average training loss: 0.1626217064526346\n",
      "Average test loss: 0.005215199525778492\n",
      "Epoch 61/300\n",
      "Average training loss: 0.162317990899086\n",
      "Average test loss: 0.0050507269476850825\n",
      "Epoch 62/300\n",
      "Average training loss: 0.1637305499712626\n",
      "Average test loss: 0.005034047943436437\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1606538814968533\n",
      "Average test loss: 0.10703613551126587\n",
      "Epoch 64/300\n",
      "Average training loss: 0.16572791700892978\n",
      "Average test loss: 0.005129687967813677\n",
      "Epoch 65/300\n",
      "Average training loss: 0.15881933665275574\n",
      "Average test loss: 0.005236883545501365\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1581438001394272\n",
      "Average test loss: 0.0050698389353023635\n",
      "Epoch 67/300\n",
      "Average training loss: 0.15847688240475125\n",
      "Average test loss: 0.005695030948147177\n",
      "Epoch 68/300\n",
      "Average training loss: 0.1580582540432612\n",
      "Average test loss: 0.006460860288391511\n",
      "Epoch 69/300\n",
      "Average training loss: 0.15655182288752661\n",
      "Average test loss: 0.02122313369313876\n",
      "Epoch 70/300\n",
      "Average training loss: 0.15522139108843275\n",
      "Average test loss: 0.0051504770773980356\n",
      "Epoch 71/300\n",
      "Average training loss: 0.15681554812855192\n",
      "Average test loss: 0.007929866241912047\n",
      "Epoch 72/300\n",
      "Average training loss: 0.15488531668980918\n",
      "Average test loss: 0.005419444241457515\n",
      "Epoch 73/300\n",
      "Average training loss: 0.15493310409122044\n",
      "Average test loss: 0.0055519602129028905\n",
      "Epoch 74/300\n",
      "Average training loss: 0.15274822453657785\n",
      "Average test loss: 0.005071692508334915\n",
      "Epoch 75/300\n",
      "Average training loss: 0.15556402242183684\n",
      "Average test loss: 0.004977029325233566\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1520923989613851\n",
      "Average test loss: 0.005388487976458338\n",
      "Epoch 77/300\n",
      "Average training loss: 0.15788341302341885\n",
      "Average test loss: 0.005138259267641439\n",
      "Epoch 78/300\n",
      "Average training loss: 0.15138769486215378\n",
      "Average test loss: 0.005064810590197643\n",
      "Epoch 79/300\n",
      "Average training loss: 0.15065953788492414\n",
      "Average test loss: 0.022129025899287728\n",
      "Epoch 80/300\n",
      "Average training loss: 0.1503907198243671\n",
      "Average test loss: 0.00516973786511355\n",
      "Epoch 81/300\n",
      "Average training loss: 0.15099896041552227\n",
      "Average test loss: 0.006106100170769626\n",
      "Epoch 82/300\n",
      "Average training loss: 273178.06894727657\n",
      "Average test loss: 1.644298534128401\n",
      "Epoch 83/300\n",
      "Average training loss: 18.999250817192927\n",
      "Average test loss: 357.15926621720524\n",
      "Epoch 84/300\n",
      "Average training loss: 15.238933096143935\n",
      "Average test loss: 0.06300955536630419\n",
      "Epoch 85/300\n",
      "Average training loss: 13.55400506761339\n",
      "Average test loss: 0.3459124780231052\n",
      "Epoch 86/300\n",
      "Average training loss: 12.214419345431857\n",
      "Average test loss: 0.1255406557801697\n",
      "Epoch 87/300\n",
      "Average training loss: 11.281283919440375\n",
      "Average test loss: 0.021445193005932704\n",
      "Epoch 88/300\n",
      "Average training loss: 10.579292822943794\n",
      "Average test loss: 0.01569413904680146\n",
      "Epoch 89/300\n",
      "Average training loss: 9.969388529459636\n",
      "Average test loss: 6.9905381832470495\n",
      "Epoch 90/300\n",
      "Average training loss: 9.409065156724719\n",
      "Average test loss: 310.0807667397393\n",
      "Epoch 91/300\n",
      "Average training loss: 8.873917062547472\n",
      "Average test loss: 0.051894598233203096\n",
      "Epoch 92/300\n",
      "Average training loss: 8.384774364471436\n",
      "Average test loss: 0.008262737500998709\n",
      "Epoch 93/300\n",
      "Average training loss: 7.911247089809842\n",
      "Average test loss: 10.713927015536775\n",
      "Epoch 94/300\n",
      "Average training loss: 7.4549660754733615\n",
      "Average test loss: 0.007429131998784012\n",
      "Epoch 95/300\n",
      "Average training loss: 7.053531177096897\n",
      "Average test loss: 39.71266723938783\n",
      "Epoch 96/300\n",
      "Average training loss: 6.693813839806451\n",
      "Average test loss: 0.05371890349520577\n",
      "Epoch 97/300\n",
      "Average training loss: 6.382604500664605\n",
      "Average test loss: 0.006873461142596272\n",
      "Epoch 98/300\n",
      "Average training loss: 6.09551801554362\n",
      "Average test loss: 0.0067713079183465905\n",
      "Epoch 99/300\n",
      "Average training loss: 5.8225049010382754\n",
      "Average test loss: 0.015781780913472175\n",
      "Epoch 100/300\n",
      "Average training loss: 5.559559010399712\n",
      "Average test loss: 0.0069337538079255155\n",
      "Epoch 101/300\n",
      "Average training loss: 5.308495832655165\n",
      "Average test loss: 0.006356998331844807\n",
      "Epoch 102/300\n",
      "Average training loss: 5.06194098493788\n",
      "Average test loss: 0.006668966160052353\n",
      "Epoch 103/300\n",
      "Average training loss: 4.794352768792047\n",
      "Average test loss: 0.3802957638502121\n",
      "Epoch 104/300\n",
      "Average training loss: 4.532572714911566\n",
      "Average test loss: 0.007548642857207192\n",
      "Epoch 105/300\n",
      "Average training loss: 4.251602529737684\n",
      "Average test loss: 0.006186054694155852\n",
      "Epoch 106/300\n",
      "Average training loss: 3.974481754514906\n",
      "Average test loss: 0.005930119409329361\n",
      "Epoch 107/300\n",
      "Average training loss: 3.68975702073839\n",
      "Average test loss: 0.006341062999433942\n",
      "Epoch 108/300\n",
      "Average training loss: 3.392316882663303\n",
      "Average test loss: 0.005985025006863806\n",
      "Epoch 109/300\n",
      "Average training loss: 3.0383042627970376\n",
      "Average test loss: 0.012158620476722717\n",
      "Epoch 110/300\n",
      "Average training loss: 2.663424296485053\n",
      "Average test loss: 0.005984507251944807\n",
      "Epoch 111/300\n",
      "Average training loss: 2.3761354514227975\n",
      "Average test loss: 0.0057012120489445\n",
      "Epoch 112/300\n",
      "Average training loss: 2.1371191601223414\n",
      "Average test loss: 0.00559580098837614\n",
      "Epoch 113/300\n",
      "Average training loss: 1.920321827782525\n",
      "Average test loss: 0.008735058163189226\n",
      "Epoch 114/300\n",
      "Average training loss: 1.736056168238322\n",
      "Average test loss: 0.4873941812250349\n",
      "Epoch 115/300\n",
      "Average training loss: 1.574895786497328\n",
      "Average test loss: 0.005398569292906258\n",
      "Epoch 116/300\n",
      "Average training loss: 1.4344649616877239\n",
      "Average test loss: 0.005406291965395212\n",
      "Epoch 117/300\n",
      "Average training loss: 1.309232746971978\n",
      "Average test loss: 0.0053208963515030015\n",
      "Epoch 118/300\n",
      "Average training loss: 1.192249583032396\n",
      "Average test loss: 0.00552790215694242\n",
      "Epoch 119/300\n",
      "Average training loss: 1.08186196147071\n",
      "Average test loss: 0.005235864632452528\n",
      "Epoch 120/300\n",
      "Average training loss: 0.9733763882848951\n",
      "Average test loss: 0.00546973570332759\n",
      "Epoch 121/300\n",
      "Average training loss: 0.8733491041395399\n",
      "Average test loss: 0.005255844850920968\n",
      "Epoch 122/300\n",
      "Average training loss: 0.7743037427796258\n",
      "Average test loss: 0.005427678983244631\n",
      "Epoch 123/300\n",
      "Average training loss: 0.6832633565266927\n",
      "Average test loss: 0.005305422688110007\n",
      "Epoch 124/300\n",
      "Average training loss: 0.5972451046837701\n",
      "Average test loss: 0.005452776987933451\n",
      "Epoch 125/300\n",
      "Average training loss: 0.5198972257243263\n",
      "Average test loss: 0.0054615076639586025\n",
      "Epoch 126/300\n",
      "Average training loss: 0.4479388199382358\n",
      "Average test loss: 0.005092879043271144\n",
      "Epoch 127/300\n",
      "Average training loss: 0.3855844178199768\n",
      "Average test loss: 0.0051683687801576325\n",
      "Epoch 128/300\n",
      "Average training loss: 0.3335651457309723\n",
      "Average test loss: 0.005046592772006989\n",
      "Epoch 129/300\n",
      "Average training loss: 0.29258030609289803\n",
      "Average test loss: 0.005186746488221818\n",
      "Epoch 130/300\n",
      "Average training loss: 0.2606080454852846\n",
      "Average test loss: 0.008192299513560203\n",
      "Epoch 131/300\n",
      "Average training loss: 0.23857746126916674\n",
      "Average test loss: 0.005011864404918419\n",
      "Epoch 132/300\n",
      "Average training loss: 0.22450147502952153\n",
      "Average test loss: 0.0056539550820986426\n",
      "Epoch 133/300\n",
      "Average training loss: 0.21332222301430173\n",
      "Average test loss: 0.005044643974138631\n",
      "Epoch 134/300\n",
      "Average training loss: 0.20977792682912613\n",
      "Average test loss: 0.00526613018868698\n",
      "Epoch 135/300\n",
      "Average training loss: 0.19906117072370316\n",
      "Average test loss: 0.005312774502982696\n",
      "Epoch 136/300\n",
      "Average training loss: 0.19305470711655087\n",
      "Average test loss: 0.004959361124990715\n",
      "Epoch 137/300\n",
      "Average training loss: 0.18913752245903015\n",
      "Average test loss: 0.006008030590911706\n",
      "Epoch 138/300\n",
      "Average training loss: 0.18579635275734796\n",
      "Average test loss: 0.005085214361134503\n",
      "Epoch 139/300\n",
      "Average training loss: 0.18151666221353743\n",
      "Average test loss: 0.005063562931285964\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1788699566324552\n",
      "Average test loss: 0.005260931438455979\n",
      "Epoch 141/300\n",
      "Average training loss: 0.17580871650907728\n",
      "Average test loss: 0.006686651474071874\n",
      "Epoch 142/300\n",
      "Average training loss: 0.17482983426253002\n",
      "Average test loss: 0.005122763415591584\n",
      "Epoch 143/300\n",
      "Average training loss: 0.1722438048919042\n",
      "Average test loss: 0.005161452096783452\n",
      "Epoch 144/300\n",
      "Average training loss: 0.16912961191601222\n",
      "Average test loss: 97273.64027083333\n",
      "Epoch 145/300\n",
      "Average training loss: 0.16864284074306488\n",
      "Average test loss: 0.005192404655946625\n",
      "Epoch 146/300\n",
      "Average training loss: 0.16569997496737374\n",
      "Average test loss: 0.005114808684628871\n",
      "Epoch 147/300\n",
      "Average training loss: 0.16440046554141574\n",
      "Average test loss: 0.005026590375436677\n",
      "Epoch 148/300\n",
      "Average training loss: 0.16245016703340742\n",
      "Average test loss: 0.005146961357444525\n",
      "Epoch 149/300\n",
      "Average training loss: 0.16141956101523505\n",
      "Average test loss: 0.004973103624251154\n",
      "Epoch 150/300\n",
      "Average training loss: 0.15936439575751624\n",
      "Average test loss: 0.005070133899648984\n",
      "Epoch 151/300\n",
      "Average training loss: 0.1580642563700676\n",
      "Average test loss: 0.00508345381087727\n",
      "Epoch 152/300\n",
      "Average training loss: 0.15791062786844043\n",
      "Average test loss: 0.005792454357362455\n",
      "Epoch 153/300\n",
      "Average training loss: 0.15537209067079755\n",
      "Average test loss: 0.005120685735510455\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1545123034318288\n",
      "Average test loss: 0.006023143215725819\n",
      "Epoch 155/300\n",
      "Average training loss: 0.1531713529030482\n",
      "Average test loss: 0.005400449848837322\n",
      "Epoch 156/300\n",
      "Average training loss: 0.15968202326032852\n",
      "Average test loss: 0.005026385922398832\n",
      "Epoch 157/300\n",
      "Average training loss: 0.15206266697910098\n",
      "Average test loss: 0.005121316638671689\n",
      "Epoch 158/300\n",
      "Average training loss: 0.15103001234928767\n",
      "Average test loss: 0.00504297163668606\n",
      "Epoch 159/300\n",
      "Average training loss: 0.1502124119732115\n",
      "Average test loss: 0.005104323907858796\n",
      "Epoch 160/300\n",
      "Average training loss: 0.14934347587161595\n",
      "Average test loss: 0.005529912265845471\n",
      "Epoch 161/300\n",
      "Average training loss: 0.14922360417577957\n",
      "Average test loss: 0.005616369509448608\n",
      "Epoch 162/300\n",
      "Average training loss: 0.14764948294560115\n",
      "Average test loss: 0.005262313835322857\n",
      "Epoch 163/300\n",
      "Average training loss: 0.1506123606496387\n",
      "Average test loss: 0.005069996085431841\n",
      "Epoch 164/300\n",
      "Average training loss: 0.14791393465465968\n",
      "Average test loss: 0.005146993061320649\n",
      "Epoch 165/300\n",
      "Average training loss: 0.1472131426466836\n",
      "Average test loss: 0.005158830542531278\n",
      "Epoch 166/300\n",
      "Average training loss: 0.14596818396780226\n",
      "Average test loss: 0.0052913790771530736\n",
      "Epoch 167/300\n",
      "Average training loss: 0.14682275924417706\n",
      "Average test loss: 0.005233795660444432\n",
      "Epoch 168/300\n",
      "Average training loss: 0.14483872950077056\n",
      "Average test loss: 0.0070723574376768536\n",
      "Epoch 169/300\n",
      "Average training loss: 0.14817820052305858\n",
      "Average test loss: 0.0052035085683067635\n",
      "Epoch 170/300\n",
      "Average training loss: 0.14456716978549958\n",
      "Average test loss: 0.005652721669524908\n",
      "Epoch 171/300\n",
      "Average training loss: 0.14322131354941262\n",
      "Average test loss: 0.005158825677302149\n",
      "Epoch 172/300\n",
      "Average training loss: 0.14609266255961523\n",
      "Average test loss: 0.005063725062542492\n",
      "Epoch 173/300\n",
      "Average training loss: 14.61836041381624\n",
      "Average test loss: 0.007672722671594885\n",
      "Epoch 174/300\n",
      "Average training loss: 4.158262345843845\n",
      "Average test loss: 0.027292546159691282\n",
      "Epoch 175/300\n",
      "Average training loss: 3.2249399984147815\n",
      "Average test loss: 0.005887950962616338\n",
      "Epoch 176/300\n",
      "Average training loss: 2.622530424118042\n",
      "Average test loss: 0.005918409310695198\n",
      "Epoch 177/300\n",
      "Average training loss: 2.145553503778246\n",
      "Average test loss: 0.016502863127324317\n",
      "Epoch 178/300\n",
      "Average training loss: 1.7646582027011448\n",
      "Average test loss: 0.005511355416642295\n",
      "Epoch 179/300\n",
      "Average training loss: 1.46414247947269\n",
      "Average test loss: 0.005601073398358292\n",
      "Epoch 180/300\n",
      "Average training loss: 1.233053007973565\n",
      "Average test loss: 0.005425870866825183\n",
      "Epoch 181/300\n",
      "Average training loss: 1.042819052696228\n",
      "Average test loss: 0.00550443409755826\n",
      "Epoch 182/300\n",
      "Average training loss: 0.8752726443608602\n",
      "Average test loss: 0.005241905878194504\n",
      "Epoch 183/300\n",
      "Average training loss: 0.7265552229881287\n",
      "Average test loss: 0.005628087624079651\n",
      "Epoch 184/300\n",
      "Average training loss: 0.6030619939698113\n",
      "Average test loss: 0.00846604556300574\n",
      "Epoch 185/300\n",
      "Average training loss: 0.49553414816326563\n",
      "Average test loss: 0.005870886854413483\n",
      "Epoch 186/300\n",
      "Average training loss: 0.4046387362480164\n",
      "Average test loss: 0.005982325752162271\n",
      "Epoch 187/300\n",
      "Average training loss: 0.3382354225582547\n",
      "Average test loss: 0.005200811408046219\n",
      "Epoch 188/300\n",
      "Average training loss: 0.29066701862547134\n",
      "Average test loss: 0.0051491152352343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.25802655397521124\n",
      "Average test loss: 378485.58335085213\n",
      "Epoch 190/300\n",
      "Average training loss: 0.23684134305848015\n",
      "Average test loss: 0.005006829868174262\n",
      "Epoch 191/300\n",
      "Average training loss: 0.2217983558707767\n",
      "Average test loss: 0.005138055863065852\n",
      "Epoch 192/300\n",
      "Average training loss: 0.2113274402750863\n",
      "Average test loss: 0.005059023242857721\n",
      "Epoch 193/300\n",
      "Average training loss: 0.20286517953872682\n",
      "Average test loss: 0.005841132406973176\n",
      "Epoch 194/300\n",
      "Average training loss: 0.19691163411405352\n",
      "Average test loss: 0.005879365891632107\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1920492624176873\n",
      "Average test loss: 0.005071543393863572\n",
      "Epoch 196/300\n",
      "Average training loss: 0.18690785819954342\n",
      "Average test loss: 0.005214746711154778\n",
      "Epoch 197/300\n",
      "Average training loss: 0.18362304721938238\n",
      "Average test loss: 0.00503304400253627\n",
      "Epoch 198/300\n",
      "Average training loss: 0.17990140659279294\n",
      "Average test loss: 0.010612490943322579\n",
      "Epoch 199/300\n",
      "Average training loss: 0.1764085569249259\n",
      "Average test loss: 0.007033102672133181\n",
      "Epoch 200/300\n",
      "Average training loss: 0.17343903520372178\n",
      "Average test loss: 0.005686980988002486\n",
      "Epoch 201/300\n",
      "Average training loss: 0.1697050174607171\n",
      "Average test loss: 0.0050525440043873255\n",
      "Epoch 202/300\n",
      "Average training loss: 0.16744162114461264\n",
      "Average test loss: 0.005154388532456425\n",
      "Epoch 203/300\n",
      "Average training loss: 0.16471285486221313\n",
      "Average test loss: 0.005043130242783162\n",
      "Epoch 204/300\n",
      "Average training loss: 0.16179054035080803\n",
      "Average test loss: 0.0056547707046071686\n",
      "Epoch 205/300\n",
      "Average training loss: 0.16047096668349373\n",
      "Average test loss: 19.79464791668786\n",
      "Epoch 206/300\n",
      "Average training loss: 0.15851983266406589\n",
      "Average test loss: 0.005398043624642823\n",
      "Epoch 207/300\n",
      "Average training loss: 0.15536724826362397\n",
      "Average test loss: 0.0331318760174844\n",
      "Epoch 208/300\n",
      "Average training loss: 0.154499115910795\n",
      "Average test loss: 0.00616775647799174\n",
      "Epoch 209/300\n",
      "Average training loss: 0.1620085528956519\n",
      "Average test loss: 0.009700746635595957\n",
      "Epoch 210/300\n",
      "Average training loss: 0.15238598752684063\n",
      "Average test loss: 0.005230923981302315\n",
      "Epoch 211/300\n",
      "Average training loss: 0.15431780740949844\n",
      "Average test loss: 1.678490464369456\n",
      "Epoch 212/300\n",
      "Average training loss: 0.14761689219209884\n",
      "Average test loss: 0.005407115341888534\n",
      "Epoch 213/300\n",
      "Average training loss: 0.1480852558215459\n",
      "Average test loss: 0.005219703011628655\n",
      "Epoch 214/300\n",
      "Average training loss: 0.14602475753095415\n",
      "Average test loss: 0.005593342260147134\n",
      "Epoch 215/300\n",
      "Average training loss: 0.14623869725730684\n",
      "Average test loss: 0.005882748231291771\n",
      "Epoch 216/300\n",
      "Average training loss: 0.144498173058033\n",
      "Average test loss: 0.0056397050445278485\n",
      "Epoch 217/300\n",
      "Average training loss: 0.14384415486123828\n",
      "Average test loss: 32.05432192113665\n",
      "Epoch 218/300\n",
      "Average training loss: 0.14367199930879804\n",
      "Average test loss: 0.0095476794069012\n",
      "Epoch 219/300\n",
      "Average training loss: 0.15404451462957594\n",
      "Average test loss: 0.005318601778811879\n",
      "Epoch 220/300\n",
      "Average training loss: 0.14607787156105043\n",
      "Average test loss: 0.005080136131081316\n",
      "Epoch 221/300\n",
      "Average training loss: 0.14264036847485437\n",
      "Average test loss: 0.278780104358991\n",
      "Epoch 222/300\n",
      "Average training loss: 0.14869155002302595\n",
      "Average test loss: 0.005527366327328815\n",
      "Epoch 223/300\n",
      "Average training loss: 0.14317182507779863\n",
      "Average test loss: 0.010379986178543833\n",
      "Epoch 224/300\n",
      "Average training loss: 0.1416255311701033\n",
      "Average test loss: 0.005291480676581462\n",
      "Epoch 225/300\n",
      "Average training loss: 0.14071405895551045\n",
      "Average test loss: 0.005292214659353097\n",
      "Epoch 226/300\n",
      "Average training loss: 0.14120810969670614\n",
      "Average test loss: 4.26578187500106\n",
      "Epoch 227/300\n",
      "Average training loss: 0.14035553887155322\n",
      "Average test loss: 0.005153306384881338\n",
      "Epoch 228/300\n",
      "Average training loss: 0.13986185973220402\n",
      "Average test loss: 0.006661553516156144\n",
      "Epoch 229/300\n",
      "Average training loss: 0.13954132131735483\n",
      "Average test loss: 0.005245318284465207\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1415176051457723\n",
      "Average test loss: 0.005300348407278458\n",
      "Epoch 231/300\n",
      "Average training loss: 1941396.9802419527\n",
      "Average test loss: 1.3739597626394695\n",
      "Epoch 232/300\n",
      "Average training loss: 14.2364472495185\n",
      "Average test loss: 1.2560846237606473\n",
      "Epoch 233/300\n",
      "Average training loss: 11.963300982157389\n",
      "Average test loss: 0.08536446103784773\n",
      "Epoch 234/300\n",
      "Average training loss: 10.819478374905056\n",
      "Average test loss: 0.04015746025244395\n",
      "Epoch 235/300\n",
      "Average training loss: 10.14740244377984\n",
      "Average test loss: 0.0314199310110675\n",
      "Epoch 236/300\n",
      "Average training loss: 9.613323884752061\n",
      "Average test loss: 0.017205038784278762\n",
      "Epoch 237/300\n",
      "Average training loss: 9.149928544786242\n",
      "Average test loss: 0.01683470280137327\n",
      "Epoch 238/300\n",
      "Average training loss: 8.715494284735785\n",
      "Average test loss: 0.02849073890182707\n",
      "Epoch 239/300\n",
      "Average training loss: 8.309344918145074\n",
      "Average test loss: 0.03154639604108201\n",
      "Epoch 240/300\n",
      "Average training loss: 7.898329370710585\n",
      "Average test loss: 0.011972081785400709\n",
      "Epoch 241/300\n",
      "Average training loss: 7.510868878682454\n",
      "Average test loss: 0.017846099646555053\n",
      "Epoch 242/300\n",
      "Average training loss: 7.159142501407199\n",
      "Average test loss: 0.012894992594089773\n",
      "Epoch 243/300\n",
      "Average training loss: 6.8459294929504395\n",
      "Average test loss: 0.010911762009892198\n",
      "Epoch 244/300\n",
      "Average training loss: 6.561390410529243\n",
      "Average test loss: 0.06523386647966173\n",
      "Epoch 245/300\n",
      "Average training loss: 6.302211660173204\n",
      "Average test loss: 0.011047845407492585\n",
      "Epoch 246/300\n",
      "Average training loss: 6.047793591393365\n",
      "Average test loss: 0.009242973709272014\n",
      "Epoch 247/300\n",
      "Average training loss: 5.790454931471083\n",
      "Average test loss: 0.028768408863080873\n",
      "Epoch 248/300\n",
      "Average training loss: 5.539771802266439\n",
      "Average test loss: 0.00891046748848425\n",
      "Epoch 249/300\n",
      "Average training loss: 5.288762775844998\n",
      "Average test loss: 0.012534935632513629\n",
      "Epoch 250/300\n",
      "Average training loss: 5.042738430447049\n",
      "Average test loss: 0.007589274577382538\n",
      "Epoch 251/300\n",
      "Average training loss: 4.803999844868978\n",
      "Average test loss: 0.007199778299364779\n",
      "Epoch 252/300\n",
      "Average training loss: 4.569122242821588\n",
      "Average test loss: 0.006721791555898057\n",
      "Epoch 253/300\n",
      "Average training loss: 4.346492685953776\n",
      "Average test loss: 0.008550298477212588\n",
      "Epoch 254/300\n",
      "Average training loss: 4.121163195716011\n",
      "Average test loss: 0.014429035858975517\n",
      "Epoch 255/300\n",
      "Average training loss: 3.865043085310194\n",
      "Average test loss: 0.022333454531100062\n",
      "Epoch 256/300\n",
      "Average training loss: 3.547166808658176\n",
      "Average test loss: 0.007878334452708562\n",
      "Epoch 257/300\n",
      "Average training loss: 3.2059511477152505\n",
      "Average test loss: 0.008941053354905712\n",
      "Epoch 258/300\n",
      "Average training loss: 2.8788933811187745\n",
      "Average test loss: 0.00609565454431706\n",
      "Epoch 259/300\n",
      "Average training loss: 2.5392285618252224\n",
      "Average test loss: 0.010021660482303964\n",
      "Epoch 260/300\n",
      "Average training loss: 2.230410709804959\n",
      "Average test loss: 0.005899193068759309\n",
      "Epoch 261/300\n",
      "Average training loss: 1.9893811661402385\n",
      "Average test loss: 0.005927444780038463\n",
      "Epoch 262/300\n",
      "Average training loss: 1.8000694372389052\n",
      "Average test loss: 0.006188181216104163\n",
      "Epoch 263/300\n",
      "Average training loss: 1.6385792173809475\n",
      "Average test loss: 0.006146999344229698\n",
      "Epoch 264/300\n",
      "Average training loss: 1.488487230512831\n",
      "Average test loss: 0.006801321246971687\n",
      "Epoch 265/300\n",
      "Average training loss: 1.3517864502800836\n",
      "Average test loss: 0.005479882522589631\n",
      "Epoch 266/300\n",
      "Average training loss: 1.225366063117981\n",
      "Average test loss: 0.0063009455017745495\n",
      "Epoch 267/300\n",
      "Average training loss: 1.1117329680124919\n",
      "Average test loss: 0.0055371120377547215\n",
      "Epoch 268/300\n",
      "Average training loss: 1.008115056461758\n",
      "Average test loss: 0.005256197693447272\n",
      "Epoch 269/300\n",
      "Average training loss: 0.9173456343015035\n",
      "Average test loss: 0.021134854356447855\n",
      "Epoch 270/300\n",
      "Average training loss: 0.8352805440690783\n",
      "Average test loss: 0.005200008793423573\n",
      "Epoch 271/300\n",
      "Average training loss: 0.7592707931730482\n",
      "Average test loss: 0.005159341652360227\n",
      "Epoch 272/300\n",
      "Average training loss: 0.6880473181406657\n",
      "Average test loss: 0.005195878962054848\n",
      "Epoch 273/300\n",
      "Average training loss: 0.6202533473968506\n",
      "Average test loss: 0.005167395600428184\n",
      "Epoch 274/300\n",
      "Average training loss: 0.5562723138332367\n",
      "Average test loss: 0.0050875218013922375\n",
      "Epoch 275/300\n",
      "Average training loss: 0.4963114442295498\n",
      "Average test loss: 0.005025813413163026\n",
      "Epoch 276/300\n",
      "Average training loss: 0.44132076814439564\n",
      "Average test loss: 0.005068579700258043\n",
      "Epoch 277/300\n",
      "Average training loss: 0.391413919766744\n",
      "Average test loss: 0.00509335387042827\n",
      "Epoch 278/300\n",
      "Average training loss: 0.3462811483542124\n",
      "Average test loss: 0.0050327183008193966\n",
      "Epoch 279/300\n",
      "Average training loss: 0.3060586745738983\n",
      "Average test loss: 0.0050264574107196595\n",
      "Epoch 280/300\n",
      "Average training loss: 0.2749248549408383\n",
      "Average test loss: 0.013832085359427664\n",
      "Epoch 281/300\n",
      "Average training loss: 0.2504862188498179\n",
      "Average test loss: 0.021008915940506592\n",
      "Epoch 282/300\n",
      "Average training loss: 0.23310597981346978\n",
      "Average test loss: 0.0053018735183609856\n",
      "Epoch 283/300\n",
      "Average training loss: 0.2182652703523636\n",
      "Average test loss: 0.005060962786277135\n",
      "Epoch 284/300\n",
      "Average training loss: 0.2081747165520986\n",
      "Average test loss: 0.005158324958549606\n",
      "Epoch 285/300\n",
      "Average training loss: 0.1963655821349886\n",
      "Average test loss: 0.005101616347829501\n",
      "Epoch 286/300\n",
      "Average training loss: 0.20313868021965026\n",
      "Average test loss: 0.005721641919679112\n",
      "Epoch 287/300\n",
      "Average training loss: 0.1859590024749438\n",
      "Average test loss: 0.005096162334498432\n",
      "Epoch 288/300\n",
      "Average training loss: 0.1786342600054211\n",
      "Average test loss: 0.004972351314706935\n",
      "Epoch 289/300\n",
      "Average training loss: 0.175142200158702\n",
      "Average test loss: 0.005041961205502351\n",
      "Epoch 290/300\n",
      "Average training loss: 0.1712572399907642\n",
      "Average test loss: 0.005234772615134716\n",
      "Epoch 291/300\n",
      "Average training loss: 0.16810738939709133\n",
      "Average test loss: 0.005451999499566026\n",
      "Epoch 292/300\n",
      "Average training loss: 0.16636293221844567\n",
      "Average test loss: 0.006273622183750073\n",
      "Epoch 293/300\n",
      "Average training loss: 0.16317143013742236\n",
      "Average test loss: 0.005491814159684711\n",
      "Epoch 294/300\n",
      "Average training loss: 0.16906166645553378\n",
      "Average test loss: 0.005124510678566165\n",
      "Epoch 295/300\n",
      "Average training loss: 0.1617736950582928\n",
      "Average test loss: 0.005168132845726278\n",
      "Epoch 296/300\n",
      "Average training loss: 0.15739486253261567\n",
      "Average test loss: 0.005051002050439517\n",
      "Epoch 297/300\n",
      "Average training loss: 0.15689833618534935\n",
      "Average test loss: 0.005154619591517581\n",
      "Epoch 298/300\n",
      "Average training loss: 0.15374304739634195\n",
      "Average test loss: 0.00596136881535252\n",
      "Epoch 299/300\n",
      "Average training loss: 0.1520604194667604\n",
      "Average test loss: 0.005741988827370935\n",
      "Epoch 300/300\n",
      "Average training loss: 0.15148981873194375\n",
      "Average test loss: 0.005472599150819911\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.756428714328342\n",
      "Average test loss: 11.34886697597305\n",
      "Epoch 2/300\n",
      "Average training loss: 5.655948671976725\n",
      "Average test loss: 0.007139000924511088\n",
      "Epoch 3/300\n",
      "Average training loss: 4.439841689639621\n",
      "Average test loss: 0.005926953498274088\n",
      "Epoch 4/300\n",
      "Average training loss: 3.5397228982713487\n",
      "Average test loss: 0.020188729103240703\n",
      "Epoch 5/300\n",
      "Average training loss: 2.9188609155019125\n",
      "Average test loss: 0.005374079774237341\n",
      "Epoch 6/300\n",
      "Average training loss: 2.3375586582819623\n",
      "Average test loss: 0.005171597729333573\n",
      "Epoch 7/300\n",
      "Average training loss: 1.8381555327309502\n",
      "Average test loss: 0.006277259162316719\n",
      "Epoch 8/300\n",
      "Average training loss: 1.512508387989468\n",
      "Average test loss: 0.004959392318088148\n",
      "Epoch 9/300\n",
      "Average training loss: 1.2682536102930704\n",
      "Average test loss: 0.0045475850906223055\n",
      "Epoch 10/300\n",
      "Average training loss: 1.0537577601538763\n",
      "Average test loss: 0.004682260839475526\n",
      "Epoch 11/300\n",
      "Average training loss: 0.8745995981958178\n",
      "Average test loss: 0.0048366102613508705\n",
      "Epoch 12/300\n",
      "Average training loss: 0.7400087325837877\n",
      "Average test loss: 0.004192095333089431\n",
      "Epoch 13/300\n",
      "Average training loss: 0.6294707354969449\n",
      "Average test loss: 0.004054627459082339\n",
      "Epoch 14/300\n",
      "Average training loss: 0.5404274168544345\n",
      "Average test loss: 0.003977438273115291\n",
      "Epoch 15/300\n",
      "Average training loss: 0.4674668853018019\n",
      "Average test loss: 0.003922018659818504\n",
      "Epoch 16/300\n",
      "Average training loss: 0.4096750536494785\n",
      "Average test loss: 0.004666502371016476\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3614327066474491\n",
      "Average test loss: 0.0038610846578247017\n",
      "Epoch 18/300\n",
      "Average training loss: 0.32194214261902704\n",
      "Average test loss: 0.004311495533006059\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2900634575155046\n",
      "Average test loss: 0.004051563076674938\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2643131718635559\n",
      "Average test loss: 0.003597945507408844\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2439744347333908\n",
      "Average test loss: 0.00376739546449648\n",
      "Epoch 22/300\n",
      "Average training loss: 0.22762651843494838\n",
      "Average test loss: 0.003318663291633129\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2120940727127923\n",
      "Average test loss: 0.006892982084718015\n",
      "Epoch 24/300\n",
      "Average training loss: 0.20349580988619062\n",
      "Average test loss: 0.0033806894183572795\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1932080161968867\n",
      "Average test loss: 0.0032316225932704076\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1848972747988171\n",
      "Average test loss: 0.0033686786962466107\n",
      "Epoch 27/300\n",
      "Average training loss: 0.17689804034762913\n",
      "Average test loss: 0.0033135009528034265\n",
      "Epoch 28/300\n",
      "Average training loss: 0.17088496980402204\n",
      "Average test loss: 0.0031635164322538507\n",
      "Epoch 29/300\n",
      "Average training loss: 0.16529963809914058\n",
      "Average test loss: 0.0032010793570015164\n",
      "Epoch 30/300\n",
      "Average training loss: 0.15949533430735272\n",
      "Average test loss: 0.0031981317684468295\n",
      "Epoch 31/300\n",
      "Average training loss: 0.15506650892893473\n",
      "Average test loss: 0.0036975873110608923\n",
      "Epoch 32/300\n",
      "Average training loss: 0.14969889758692848\n",
      "Average test loss: 0.00314278072056671\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14545369208521314\n",
      "Average test loss: 0.020924939019812478\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1421220777432124\n",
      "Average test loss: 0.0030520570468571453\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1392807234260771\n",
      "Average test loss: 0.003076051260034243\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13373202121920055\n",
      "Average test loss: 0.003033219443220231\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13162826552655962\n",
      "Average test loss: 0.0030286839823755953\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12849189862940047\n",
      "Average test loss: 0.0030870482921600342\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12461034747627046\n",
      "Average test loss: 0.0029771139576203294\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12200568991237216\n",
      "Average test loss: 0.002975465862494376\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11904757106966443\n",
      "Average test loss: 0.003391603538559543\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11803784691625172\n",
      "Average test loss: 0.003336377723349465\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11453301490015454\n",
      "Average test loss: 50.510634533352324\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11277447284592522\n",
      "Average test loss: 0.003119976159392132\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11103510265880161\n",
      "Average test loss: 0.0030986383300688532\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10966330123609967\n",
      "Average test loss: 0.003289347269468837\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10753195691770978\n",
      "Average test loss: 0.0030268762504888904\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10715516726175944\n",
      "Average test loss: 0.002929673785964648\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10859478187561035\n",
      "Average test loss: 0.0028693063679254716\n",
      "Epoch 50/300\n",
      "Average training loss: 4.628318197674221\n",
      "Average test loss: 0.004090320186689496\n",
      "Epoch 51/300\n",
      "Average training loss: 0.6375807882414923\n",
      "Average test loss: 0.0038458591939674485\n",
      "Epoch 52/300\n",
      "Average training loss: 0.3760306729210748\n",
      "Average test loss: 0.003404644613050752\n",
      "Epoch 53/300\n",
      "Average training loss: 0.2887736660904355\n",
      "Average test loss: 0.005034441218194035\n",
      "Epoch 54/300\n",
      "Average training loss: 0.24513337597582074\n",
      "Average test loss: 0.009188033500065406\n",
      "Epoch 55/300\n",
      "Average training loss: 0.21831271318594614\n",
      "Average test loss: 0.0033767026418613064\n",
      "Epoch 56/300\n",
      "Average training loss: 0.19680468360582987\n",
      "Average test loss: 0.07944786407757136\n",
      "Epoch 57/300\n",
      "Average training loss: 0.18084898622830708\n",
      "Average test loss: 0.003132806417842706\n",
      "Epoch 58/300\n",
      "Average training loss: 0.16485242529710134\n",
      "Average test loss: 0.0031092723338968222\n",
      "Epoch 59/300\n",
      "Average training loss: 0.15487670556704203\n",
      "Average test loss: 0.0030791989161322512\n",
      "Epoch 60/300\n",
      "Average training loss: 0.1476973523961173\n",
      "Average test loss: 0.0030474227896581095\n",
      "Epoch 61/300\n",
      "Average training loss: 0.14188607494036357\n",
      "Average test loss: 0.0029798027982728347\n",
      "Epoch 62/300\n",
      "Average training loss: 0.13668627332978778\n",
      "Average test loss: 0.003080454701764716\n",
      "Epoch 63/300\n",
      "Average training loss: 0.13272143408987258\n",
      "Average test loss: 0.0030942869149148465\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12900489613744948\n",
      "Average test loss: 0.002963283404707909\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12588436937332154\n",
      "Average test loss: 0.002927685898832149\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12367089737786187\n",
      "Average test loss: 0.006744848795235157\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12104426021046108\n",
      "Average test loss: 0.0028860834147781136\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11900862255030208\n",
      "Average test loss: 0.0028882937274045412\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1166563072403272\n",
      "Average test loss: 0.0028961193785071375\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11482917886310154\n",
      "Average test loss: 0.0029190122620720003\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11358838038974338\n",
      "Average test loss: 0.002931854712155958\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11187856208615833\n",
      "Average test loss: 0.0029317663808663685\n",
      "Epoch 73/300\n",
      "Average training loss: 0.110537479976813\n",
      "Average test loss: 0.0028555960684186884\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10984419410096274\n",
      "Average test loss: 0.0028449532337900666\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10838774120145374\n",
      "Average test loss: 0.0030309356150941716\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11058437670601738\n",
      "Average test loss: 0.0028946967344317173\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10655677234464221\n",
      "Average test loss: 0.002881215588293142\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1062169563041793\n",
      "Average test loss: 0.0028434770059668356\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10553675452868144\n",
      "Average test loss: 0.05691271070142587\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10403953898615308\n",
      "Average test loss: 0.008104913434013724\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10394469006856283\n",
      "Average test loss: 0.0027963081169873477\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10326316467920939\n",
      "Average test loss: 0.0028336199755883877\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10234106026755439\n",
      "Average test loss: 0.003153746416171392\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10145909531248941\n",
      "Average test loss: 0.0029608804278282654\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10115469581551022\n",
      "Average test loss: 0.0032146139062113233\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10017576813697815\n",
      "Average test loss: 0.012523889528794421\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09937915701998605\n",
      "Average test loss: 0.0028255937979039217\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09992335779137082\n",
      "Average test loss: 0.002789045142630736\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0985179119904836\n",
      "Average test loss: 0.0029113612928324275\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09808080606328116\n",
      "Average test loss: 0.0029995298226260475\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0972318723267979\n",
      "Average test loss: 0.003611013033737739\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09699826608763801\n",
      "Average test loss: 0.002796810565723313\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09672985592815611\n",
      "Average test loss: 0.0028985229829947153\n",
      "Epoch 94/300\n",
      "Average training loss: 2.424312336126963\n",
      "Average test loss: 0.03675212500078811\n",
      "Epoch 95/300\n",
      "Average training loss: 2.675572019259135\n",
      "Average test loss: 48.451526600175434\n",
      "Epoch 96/300\n",
      "Average training loss: 0.8633454790115357\n",
      "Average test loss: 170.146961724581\n",
      "Epoch 97/300\n",
      "Average training loss: 0.5117812929418352\n",
      "Average test loss: 291908.91512254917\n",
      "Epoch 98/300\n",
      "Average training loss: 0.36280991903940835\n",
      "Average test loss: 1534.0770021446876\n",
      "Epoch 99/300\n",
      "Average training loss: 0.2836355489359962\n",
      "Average test loss: 0.08166522126272321\n",
      "Epoch 100/300\n",
      "Average training loss: 0.23567394966549343\n",
      "Average test loss: 0.050256689223150415\n",
      "Epoch 101/300\n",
      "Average training loss: 0.19404202893045214\n",
      "Average test loss: 0.0033010060360862147\n",
      "Epoch 102/300\n",
      "Average training loss: 0.1711816570626365\n",
      "Average test loss: 0.0047603089428610275\n",
      "Epoch 103/300\n",
      "Average training loss: 0.1562120791806115\n",
      "Average test loss: 0.003062538955774572\n",
      "Epoch 104/300\n",
      "Average training loss: 0.14578777025143305\n",
      "Average test loss: 0.0033809362542298106\n",
      "Epoch 105/300\n",
      "Average training loss: 0.13750239792135027\n",
      "Average test loss: 0.0030301808971497745\n",
      "Epoch 106/300\n",
      "Average training loss: 0.13149229699373247\n",
      "Average test loss: 0.003545274136587977\n",
      "Epoch 107/300\n",
      "Average training loss: 0.12621141809887357\n",
      "Average test loss: 0.0030434467153002817\n",
      "Epoch 108/300\n",
      "Average training loss: 0.12182903107669618\n",
      "Average test loss: 0.005916637448180053\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11856794429487652\n",
      "Average test loss: 0.0028650399485809935\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11530496813191309\n",
      "Average test loss: 0.002892812314753731\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11276758396625519\n",
      "Average test loss: 0.0029774599629971716\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10993827083375719\n",
      "Average test loss: 0.002861205416524576\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10743579106198417\n",
      "Average test loss: 0.005925124449034532\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10542305302619934\n",
      "Average test loss: 0.0028698330066270297\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10385465820630392\n",
      "Average test loss: 0.0028374640874357687\n",
      "Epoch 116/300\n",
      "Average training loss: 0.1020474417673217\n",
      "Average test loss: 0.0032549646548512908\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10122028179301155\n",
      "Average test loss: 0.004239616706139511\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10189818992548519\n",
      "Average test loss: 0.0030383610932363404\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0989525778359837\n",
      "Average test loss: 0.0030337520639101665\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09775814681582981\n",
      "Average test loss: 0.002802760473349028\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09904415437910292\n",
      "Average test loss: 0.00280169478058815\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09693241129981146\n",
      "Average test loss: 0.003600601392487685\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09632182372278637\n",
      "Average test loss: 0.0028382078864508204\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0954957594540384\n",
      "Average test loss: 0.0028222370874136688\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09512770586543613\n",
      "Average test loss: 0.0028860943489190604\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09447467370827993\n",
      "Average test loss: 0.0028823165628645156\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0948686584631602\n",
      "Average test loss: 0.0028602194506675005\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0944672906862365\n",
      "Average test loss: 0.003454163559195068\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09344362379776107\n",
      "Average test loss: 0.002863808944200476\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09303363094727199\n",
      "Average test loss: 0.0030966661299268406\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09287640314631992\n",
      "Average test loss: 0.002966195749118924\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09228977127869924\n",
      "Average test loss: 0.008413144499063491\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09306215498182509\n",
      "Average test loss: 0.0028929504172669515\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09142562468184365\n",
      "Average test loss: 0.003260396475179328\n",
      "Epoch 135/300\n",
      "Average training loss: 79.36280490006341\n",
      "Average test loss: 1930.116209201389\n",
      "Epoch 136/300\n",
      "Average training loss: 20.666522854275172\n",
      "Average test loss: 0.012401076288686858\n",
      "Epoch 137/300\n",
      "Average training loss: 9.725554843478733\n",
      "Average test loss: 0.006317287096960677\n",
      "Epoch 138/300\n",
      "Average training loss: 6.742868963029649\n",
      "Average test loss: 0.029086933521760835\n",
      "Epoch 139/300\n",
      "Average training loss: 5.224111099667019\n",
      "Average test loss: 0.018616457728876008\n",
      "Epoch 140/300\n",
      "Average training loss: 4.228906745062934\n",
      "Average test loss: 0.21375329243143398\n",
      "Epoch 141/300\n",
      "Average training loss: 3.4556308805677625\n",
      "Average test loss: 0.014122950064225329\n",
      "Epoch 142/300\n",
      "Average training loss: 2.849111092037625\n",
      "Average test loss: 0.006578530535101891\n",
      "Epoch 143/300\n",
      "Average training loss: 2.399207760281033\n",
      "Average test loss: 0.003850402122363448\n",
      "Epoch 144/300\n",
      "Average training loss: 2.057655284245809\n",
      "Average test loss: 0.2447945370591349\n",
      "Epoch 145/300\n",
      "Average training loss: 1.766131762822469\n",
      "Average test loss: 0.054184596467763184\n",
      "Epoch 146/300\n",
      "Average training loss: 1.519824435128106\n",
      "Average test loss: 0.0036070596298409833\n",
      "Epoch 147/300\n",
      "Average training loss: 1.3137116236156887\n",
      "Average test loss: 0.035178043640529115\n",
      "Epoch 148/300\n",
      "Average training loss: 1.131255203458998\n",
      "Average test loss: 0.5345311166900727\n",
      "Epoch 149/300\n",
      "Average training loss: 0.9855533469518025\n",
      "Average test loss: 136.62122460338557\n",
      "Epoch 150/300\n",
      "Average training loss: 0.8548296918869018\n",
      "Average test loss: 0.003466548697816001\n",
      "Epoch 151/300\n",
      "Average training loss: 0.7419695401191712\n",
      "Average test loss: 0.004222772683948279\n",
      "Epoch 152/300\n",
      "Average training loss: 0.6478028411865234\n",
      "Average test loss: 1040.704588812934\n",
      "Epoch 153/300\n",
      "Average training loss: 0.5614531913863288\n",
      "Average test loss: 2.783927963272565\n",
      "Epoch 154/300\n",
      "Average training loss: 0.49218445499738056\n",
      "Average test loss: 0.015226866936725047\n",
      "Epoch 155/300\n",
      "Average training loss: 0.43174691647953456\n",
      "Average test loss: 93.87848357155256\n",
      "Epoch 156/300\n",
      "Average training loss: 0.384096450275845\n",
      "Average test loss: 66.82612084413651\n",
      "Epoch 157/300\n",
      "Average training loss: 0.3401610759894053\n",
      "Average test loss: 0.04600150344934728\n",
      "Epoch 158/300\n",
      "Average training loss: 0.30381988361146717\n",
      "Average test loss: 0.037489372019966445\n",
      "Epoch 159/300\n",
      "Average training loss: 0.27265782989395987\n",
      "Average test loss: 0.0030480986163020133\n",
      "Epoch 160/300\n",
      "Average training loss: 0.24726420715120104\n",
      "Average test loss: 472324.1055277778\n",
      "Epoch 161/300\n",
      "Average training loss: 0.2273082699113422\n",
      "Average test loss: 3.8929568856954573\n",
      "Epoch 162/300\n",
      "Average training loss: 0.20978530192375183\n",
      "Average test loss: 1.2173083532138003\n",
      "Epoch 163/300\n",
      "Average training loss: 0.19366614389419556\n",
      "Average test loss: 0.03512873040801949\n",
      "Epoch 164/300\n",
      "Average training loss: 0.18192536505063375\n",
      "Average test loss: 0.03955849621362156\n",
      "Epoch 165/300\n",
      "Average training loss: 0.17797502936257256\n",
      "Average test loss: 0.07751207976622713\n",
      "Epoch 166/300\n",
      "Average training loss: 0.16043809127807618\n",
      "Average test loss: 0.002922100570052862\n",
      "Epoch 167/300\n",
      "Average training loss: 0.16650859904289245\n",
      "Average test loss: 0.0030093418210744858\n",
      "Epoch 168/300\n",
      "Average training loss: 0.1403677945666843\n",
      "Average test loss: 0.0029190637978414695\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1314795313609971\n",
      "Average test loss: 0.004090161549548308\n",
      "Epoch 170/300\n",
      "Average training loss: 0.12537238135602738\n",
      "Average test loss: 0.0028482792561666834\n",
      "Epoch 171/300\n",
      "Average training loss: 0.11968489725059933\n",
      "Average test loss: 0.002964702295553353\n",
      "Epoch 172/300\n",
      "Average training loss: 0.11473402951161067\n",
      "Average test loss: 0.0027967273930294646\n",
      "Epoch 173/300\n",
      "Average training loss: 0.11212435615724987\n",
      "Average test loss: 0.002864018873828981\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10969893523719576\n",
      "Average test loss: 0.0028435971195706062\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10770076042413712\n",
      "Average test loss: 0.13890706872940065\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10508577028248045\n",
      "Average test loss: 0.002855034689108531\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10331331904066933\n",
      "Average test loss: 0.0028002910419470733\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10206631492906147\n",
      "Average test loss: 0.002829574259618918\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10004812458488677\n",
      "Average test loss: 0.002816857005779942\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09855843657917447\n",
      "Average test loss: 0.0027855286221537324\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09952876281076008\n",
      "Average test loss: 0.0027905777829388776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09679401452011532\n",
      "Average test loss: 0.0029615580590648784\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09558661929104063\n",
      "Average test loss: 0.0052906761099067\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09453320973449283\n",
      "Average test loss: 0.0028264865854548083\n",
      "Epoch 185/300\n",
      "Average training loss: 0.1326341608232922\n",
      "Average test loss: 0.0034970552950269644\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09912455976009368\n",
      "Average test loss: 0.003460764507452647\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0952885563340452\n",
      "Average test loss: 0.0031377403580894075\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09371561092800565\n",
      "Average test loss: 0.003169828851396839\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09290360351403554\n",
      "Average test loss: 0.0031368391371021666\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09239960690339406\n",
      "Average test loss: 0.0253930110823777\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09158419411049949\n",
      "Average test loss: 0.002803264388607608\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09149197383721669\n",
      "Average test loss: 0.018486845028069285\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09115218156576156\n",
      "Average test loss: 0.002892480712797907\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09116236648294661\n",
      "Average test loss: 0.0032764870048397118\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09081041157245635\n",
      "Average test loss: 0.0028546846022622454\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09015341342820062\n",
      "Average test loss: 0.002889555714196629\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08978863853216171\n",
      "Average test loss: 0.003174452562506\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08941508962048425\n",
      "Average test loss: 0.0034411328652252755\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08978616793288124\n",
      "Average test loss: 0.0028440892315573165\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0897674187289344\n",
      "Average test loss: 0.00979196361783478\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08809908065199852\n",
      "Average test loss: 0.01023778761468\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08809901872608397\n",
      "Average test loss: 22.71378293524848\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08779353967640136\n",
      "Average test loss: 0.0028372030899756484\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0869738704694642\n",
      "Average test loss: 0.0029967927765101195\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08700927472114563\n",
      "Average test loss: 0.0028895352418637937\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08681723270813624\n",
      "Average test loss: 0.0028099609702411624\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08642751389741897\n",
      "Average test loss: 0.002866717996075749\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08715091328488456\n",
      "Average test loss: 0.0031846008669171068\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08545039800802867\n",
      "Average test loss: 0.002867225911261307\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08561709459622702\n",
      "Average test loss: 0.00290574928600755\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08486132700575723\n",
      "Average test loss: 0.006754153291384379\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08477969099415673\n",
      "Average test loss: 0.002921322433898846\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0849324736793836\n",
      "Average test loss: 0.025379464987458453\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08445390519168641\n",
      "Average test loss: 0.0028889609879503646\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08831164751450221\n",
      "Average test loss: 0.0030291919683416687\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08359420456488927\n",
      "Average test loss: 0.002814554263941116\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0834688583082623\n",
      "Average test loss: 0.002862095263786614\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08416746723983023\n",
      "Average test loss: 0.0029541729903883405\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08331207386652628\n",
      "Average test loss: 0.0028916310490005545\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08639924495750004\n",
      "Average test loss: 0.002917721829480595\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08235484853718016\n",
      "Average test loss: 0.0031419052256064285\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08231055163674884\n",
      "Average test loss: 0.003310107337931792\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09544759009944068\n",
      "Average test loss: 0.0033566140337950652\n",
      "Epoch 224/300\n",
      "Average training loss: 0.997071032014158\n",
      "Average test loss: 8.812410713036854\n",
      "Epoch 225/300\n",
      "Average training loss: 0.3025881116920047\n",
      "Average test loss: 0.00330684523543136\n",
      "Epoch 226/300\n",
      "Average training loss: 0.19223549441496532\n",
      "Average test loss: 0.003211914929250876\n",
      "Epoch 227/300\n",
      "Average training loss: 0.15624314302868314\n",
      "Average test loss: 0.004409999094489548\n",
      "Epoch 228/300\n",
      "Average training loss: 0.13901034050517613\n",
      "Average test loss: 0.0030324290899766816\n",
      "Epoch 229/300\n",
      "Average training loss: 0.1283590167760849\n",
      "Average test loss: 0.0029202929337819416\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1197718024916119\n",
      "Average test loss: 0.0031095115008453527\n",
      "Epoch 231/300\n",
      "Average training loss: 0.11303245530525843\n",
      "Average test loss: 0.0028361742860741087\n",
      "Epoch 232/300\n",
      "Average training loss: 0.10822899509138531\n",
      "Average test loss: 0.003199435248453584\n",
      "Epoch 233/300\n",
      "Average training loss: 0.10466810611883799\n",
      "Average test loss: 0.003106548429777225\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10235023101833132\n",
      "Average test loss: 0.0028007868615289528\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09880235201120377\n",
      "Average test loss: 0.0028429416923059355\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0973101530207528\n",
      "Average test loss: 0.003514641263625688\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09415967902210023\n",
      "Average test loss: 0.005248998951580789\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0923469184703297\n",
      "Average test loss: 0.00280461813054151\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09031661428014437\n",
      "Average test loss: 0.002834756865269608\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08873988164133496\n",
      "Average test loss: 0.013991552636855179\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08816471277342902\n",
      "Average test loss: 0.0031508367103007104\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08627515621980031\n",
      "Average test loss: 0.0028946779718001686\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08551634754074944\n",
      "Average test loss: 0.002870747246882982\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08723038956191805\n",
      "Average test loss: 0.0028614421265406743\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08462003096606996\n",
      "Average test loss: 0.0028987156318293675\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08383925180302726\n",
      "Average test loss: 0.003053284919717246\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08377655873033736\n",
      "Average test loss: 0.002911820814634363\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08335152812798818\n",
      "Average test loss: 0.0030026109682189093\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08315795499417517\n",
      "Average test loss: 0.002959699258622196\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08329961846934425\n",
      "Average test loss: 0.0030014207400381565\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08189042306608624\n",
      "Average test loss: 0.003183923213225272\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08833171174923579\n",
      "Average test loss: 0.003018080675560567\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08273779582977295\n",
      "Average test loss: 0.047105066465834775\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08248743702967962\n",
      "Average test loss: 0.0031812514952487417\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08117831432157092\n",
      "Average test loss: 0.002892882933219274\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08151283969481786\n",
      "Average test loss: 0.0030383930425677036\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08101244755585989\n",
      "Average test loss: 0.0029530026929246054\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08074035629630089\n",
      "Average test loss: 0.00518989282184177\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08563396480348374\n",
      "Average test loss: 0.003180826312241455\n",
      "Epoch 260/300\n",
      "Average training loss: 0.1724426614774598\n",
      "Average test loss: 0.003372737916807334\n",
      "Epoch 261/300\n",
      "Average training loss: 0.1097208362751537\n",
      "Average test loss: 0.07242805202802022\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0982084944513109\n",
      "Average test loss: 0.00280978549271822\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09064250386423535\n",
      "Average test loss: 0.0028764799361427626\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08621163501673275\n",
      "Average test loss: 0.002936057744133804\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08345742690563202\n",
      "Average test loss: 0.0029325651920711\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08204919046163558\n",
      "Average test loss: 0.003191507060908609\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08118935503562291\n",
      "Average test loss: 0.0028687605001032354\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08163526405228509\n",
      "Average test loss: 0.00473792207116882\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07948824310302735\n",
      "Average test loss: 0.003037411592486832\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07931737081209818\n",
      "Average test loss: 0.0030942127721177207\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08027544392148654\n",
      "Average test loss: 0.002936438169123398\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07962363192770217\n",
      "Average test loss: 0.00292154805527793\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07977447592549854\n",
      "Average test loss: 0.003230091037435664\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08168944089280235\n",
      "Average test loss: 0.002864880924113095\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07886440331406064\n",
      "Average test loss: 0.0029223591859141988\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07907896801498202\n",
      "Average test loss: 0.003071536220610142\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0791647777656714\n",
      "Average test loss: 0.003008522010097901\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07946249377065234\n",
      "Average test loss: 0.002999284588628345\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07893995136684842\n",
      "Average test loss: 0.003003063430699209\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07905646477805243\n",
      "Average test loss: 0.003015017731115222\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0784554052816497\n",
      "Average test loss: 0.002961892761910955\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08016211609045665\n",
      "Average test loss: 0.003009193321276042\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07794870805740356\n",
      "Average test loss: 0.0029349769382841055\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07783098322815366\n",
      "Average test loss: 0.0030249723601672384\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07803575908475452\n",
      "Average test loss: 0.0031438067189107338\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07865818969408671\n",
      "Average test loss: 0.0029658661835516493\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07723651823732589\n",
      "Average test loss: 0.0031162772917499144\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07938320619530148\n",
      "Average test loss: 0.002959725933149457\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07732341708077324\n",
      "Average test loss: 0.00324148493198057\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07779316249158648\n",
      "Average test loss: 0.0036999791955782306\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07835090402099822\n",
      "Average test loss: 0.002903941432014108\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0777172377705574\n",
      "Average test loss: 0.0029385930572946866\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07677610509925419\n",
      "Average test loss: 0.003143846666543848\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07682740157842637\n",
      "Average test loss: 0.0034091745391488077\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0768281648490164\n",
      "Average test loss: 0.0029875758482764163\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07670151643620597\n",
      "Average test loss: 0.004921501206441058\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10525242165062162\n",
      "Average test loss: 0.0034356178912437626\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08621241736743185\n",
      "Average test loss: 0.003004676566562719\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07893085506889555\n",
      "Average test loss: 0.0030466559529304504\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07691887526379691\n",
      "Average test loss: 0.0029646538607776165\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.073963185628255\n",
      "Average test loss: 0.006677858162257406\n",
      "Epoch 2/300\n",
      "Average training loss: 6.300397864023845\n",
      "Average test loss: 0.04318599856893222\n",
      "Epoch 3/300\n",
      "Average training loss: 5.373535794576009\n",
      "Average test loss: 0.004560190318359269\n",
      "Epoch 4/300\n",
      "Average training loss: 4.522502360449897\n",
      "Average test loss: 0.004318143844190571\n",
      "Epoch 5/300\n",
      "Average training loss: 3.4092885117001\n",
      "Average test loss: 0.21304189697197742\n",
      "Epoch 6/300\n",
      "Average training loss: 3.1552512827979196\n",
      "Average test loss: 0.003914627024696933\n",
      "Epoch 7/300\n",
      "Average training loss: 2.6978076888190374\n",
      "Average test loss: 0.006598592795224653\n",
      "Epoch 8/300\n",
      "Average training loss: 2.2198192053900825\n",
      "Average test loss: 0.0037418842108713256\n",
      "Epoch 9/300\n",
      "Average training loss: 1.9392617962095473\n",
      "Average test loss: 0.004271330178611809\n",
      "Epoch 10/300\n",
      "Average training loss: 1.636287239604526\n",
      "Average test loss: 0.003275250607273645\n",
      "Epoch 11/300\n",
      "Average training loss: 1.3949351709153917\n",
      "Average test loss: 0.0033143557701259853\n",
      "Epoch 12/300\n",
      "Average training loss: 1.2127399195565118\n",
      "Average test loss: 0.003677712652625309\n",
      "Epoch 13/300\n",
      "Average training loss: 1.0585061905119155\n",
      "Average test loss: 0.006675700050261285\n",
      "Epoch 14/300\n",
      "Average training loss: 0.9154613850381639\n",
      "Average test loss: 0.003193513471219275\n",
      "Epoch 15/300\n",
      "Average training loss: 0.7784903966585796\n",
      "Average test loss: 0.0029219244637837013\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6652435088687473\n",
      "Average test loss: 0.0027002948112785814\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5663586304452685\n",
      "Average test loss: 0.0031060263593163757\n",
      "Epoch 18/300\n",
      "Average training loss: 0.48445952094925776\n",
      "Average test loss: 0.0029720207014017636\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4170234026114146\n",
      "Average test loss: 0.0025872302020175588\n",
      "Epoch 20/300\n",
      "Average training loss: 0.36284596599472896\n",
      "Average test loss: 0.0024625281747430562\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3186372258928087\n",
      "Average test loss: 0.0026966666049427455\n",
      "Epoch 22/300\n",
      "Average training loss: 0.28255492548147837\n",
      "Average test loss: 0.00252833989303973\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2526315751340654\n",
      "Average test loss: 0.002323256513517764\n",
      "Epoch 24/300\n",
      "Average training loss: 0.22814692458841535\n",
      "Average test loss: 0.0037073827406598463\n",
      "Epoch 25/300\n",
      "Average training loss: 0.20911799743440415\n",
      "Average test loss: 0.0023876840985483593\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1914046309126748\n",
      "Average test loss: 0.002359745158917374\n",
      "Epoch 27/300\n",
      "Average training loss: 0.17756553743945228\n",
      "Average test loss: 0.0023956552830835184\n",
      "Epoch 28/300\n",
      "Average training loss: 0.16432227141327327\n",
      "Average test loss: 0.0021976241150663957\n",
      "Epoch 29/300\n",
      "Average training loss: 0.15368000030517578\n",
      "Average test loss: 0.0020829433873295784\n",
      "Epoch 30/300\n",
      "Average training loss: 0.14448022482130263\n",
      "Average test loss: 0.06830884294874139\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13731973332828945\n",
      "Average test loss: 0.0023214750647958782\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13173634135723114\n",
      "Average test loss: 0.002302981898188591\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12459588588608636\n",
      "Average test loss: 0.0021074604586594635\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11914441112677256\n",
      "Average test loss: 0.002668380949542754\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11442963806788127\n",
      "Average test loss: 0.00218968638467292\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10980421908034219\n",
      "Average test loss: 0.002053850532198946\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10654965997404522\n",
      "Average test loss: 0.0020675537186778253\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10389272035492791\n",
      "Average test loss: 0.002234953236662679\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09926706510119968\n",
      "Average test loss: 0.001969762359849281\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09673594825797611\n",
      "Average test loss: 0.002023303667600784\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09355693201886282\n",
      "Average test loss: 0.0019244892507170638\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0910100766354137\n",
      "Average test loss: 0.002069786423610316\n",
      "Epoch 43/300\n",
      "Average training loss: 0.15767325007253222\n",
      "Average test loss: 0.0022211388871073725\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10475109022855758\n",
      "Average test loss: 0.005878759343591001\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09522786218590207\n",
      "Average test loss: 0.002045124390266008\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09022664660877652\n",
      "Average test loss: 0.001959844034579065\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08752737692991892\n",
      "Average test loss: 0.0018932582894340157\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08488232096698549\n",
      "Average test loss: 0.0023006522073927855\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08319823624690374\n",
      "Average test loss: 0.002096132318385773\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08180909768740337\n",
      "Average test loss: 0.001962100794021454\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08078741015990575\n",
      "Average test loss: 0.0020298403431144025\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07849676571289699\n",
      "Average test loss: 0.0018512882863481839\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07980960273080402\n",
      "Average test loss: 0.0019430305735828976\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08879456180665228\n",
      "Average test loss: 0.0022867291098874476\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07986734553509288\n",
      "Average test loss: 0.0019083523486430447\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07684255717860328\n",
      "Average test loss: 0.002689339956268668\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07853347471025254\n",
      "Average test loss: 0.001895095717575815\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0747546244263649\n",
      "Average test loss: 0.001865893778287702\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07342514989773433\n",
      "Average test loss: 0.0018746708037538661\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07318592361609141\n",
      "Average test loss: 0.0019127193092265064\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07435242937670813\n",
      "Average test loss: 0.001873188827600744\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07318852463695738\n",
      "Average test loss: 0.0019181474122322267\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07137461108631558\n",
      "Average test loss: 0.0019801201255371173\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07064755091567834\n",
      "Average test loss: 0.0018270545988861057\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0698366735080878\n",
      "Average test loss: 0.0018189723833153646\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0697436005042659\n",
      "Average test loss: 0.0018755595858933197\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06857258070508639\n",
      "Average test loss: 0.0019063602457754314\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06890364629030228\n",
      "Average test loss: 0.0018832102915686039\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06765784661306275\n",
      "Average test loss: 0.0018270001997136407\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06740098994639185\n",
      "Average test loss: 0.03210088835656643\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06706198499600093\n",
      "Average test loss: 0.0018028895390323468\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06639860916137695\n",
      "Average test loss: 0.0018484437621405555\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0658859398762385\n",
      "Average test loss: 0.0018348991413497263\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06598631216420067\n",
      "Average test loss: 0.0020256379859315025\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06532677279247177\n",
      "Average test loss: 0.001808991759808527\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06415735380848249\n",
      "Average test loss: 0.0018170001432299614\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0641436527868112\n",
      "Average test loss: 0.002660321866058641\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06362281531757778\n",
      "Average test loss: 0.00191265923767868\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0635862817896737\n",
      "Average test loss: 0.001908639130493005\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0628432369099723\n",
      "Average test loss: 0.0018403822357455888\n",
      "Epoch 81/300\n",
      "Average training loss: 0.062498218337694804\n",
      "Average test loss: 0.0017976349735011657\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06220222257905536\n",
      "Average test loss: 0.0018187622303764024\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06200099610951212\n",
      "Average test loss: 0.0018373225917004876\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06150539185934597\n",
      "Average test loss: 0.0018298395103257563\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06122514717446433\n",
      "Average test loss: 0.0018640800312989289\n",
      "Epoch 86/300\n",
      "Average training loss: 0.061099936174021825\n",
      "Average test loss: 0.0018245906924001045\n",
      "Epoch 87/300\n",
      "Average training loss: 0.060559553103314505\n",
      "Average test loss: 0.0017986484373816185\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06074633115530014\n",
      "Average test loss: 0.001862935692227135\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06013401607010099\n",
      "Average test loss: 0.0018096430227160453\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05968560616837607\n",
      "Average test loss: 0.001798150886884994\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06454203914933734\n",
      "Average test loss: 0.0023157679395129282\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06307127045591672\n",
      "Average test loss: 0.0018001433328414956\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0600094018081824\n",
      "Average test loss: 0.0018388503261117471\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05899706616335445\n",
      "Average test loss: 0.001993309647258785\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06010455834534433\n",
      "Average test loss: 0.0019697757073574595\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05881558523906602\n",
      "Average test loss: 0.0023143876511603594\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05828969003425704\n",
      "Average test loss: 0.0020170779717672204\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05839172490106689\n",
      "Average test loss: 0.001896512738842931\n",
      "Epoch 99/300\n",
      "Average training loss: 0.058328693803813725\n",
      "Average test loss: 0.0017860616445541382\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05794692807396253\n",
      "Average test loss: 0.001854159526940849\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05760072672035959\n",
      "Average test loss: 0.0023030536507980693\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05772398295005163\n",
      "Average test loss: 0.0018863832366963229\n",
      "Epoch 103/300\n",
      "Average training loss: 0.057620713041888344\n",
      "Average test loss: 0.0019177796571618981\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05709453882773717\n",
      "Average test loss: 0.001834344029530055\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05810728404919306\n",
      "Average test loss: 0.0020181008927110168\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05661549585726526\n",
      "Average test loss: 0.0022284000793264974\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0563349695470598\n",
      "Average test loss: 0.001790161885528101\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05686353270212809\n",
      "Average test loss: 0.001893937626025743\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05797506790359815\n",
      "Average test loss: 0.0018559992285445332\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0572267799311214\n",
      "Average test loss: 0.0018206127549832067\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05656840239299668\n",
      "Average test loss: 0.0018849868068678511\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05582773259613249\n",
      "Average test loss: 0.0019208249588393504\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05596651078263919\n",
      "Average test loss: 0.009543325891097387\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05546095248725679\n",
      "Average test loss: 0.0027491318756300543\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05536649902992778\n",
      "Average test loss: 0.001957066343062454\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05546702785955535\n",
      "Average test loss: 0.0018462940684209268\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05513855408959919\n",
      "Average test loss: 0.0018362469658669497\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05488596071137322\n",
      "Average test loss: 0.0019222250032342142\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05726478668053945\n",
      "Average test loss: 0.0019412393781046073\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05487134495708677\n",
      "Average test loss: 0.0019660703260451556\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05448717877931065\n",
      "Average test loss: 0.0019377629417512152\n",
      "Epoch 122/300\n",
      "Average training loss: 0.054890715999735724\n",
      "Average test loss: 0.0022871090014361674\n",
      "Epoch 123/300\n",
      "Average training loss: 0.054546862029367024\n",
      "Average test loss: 0.0019189377119764686\n",
      "Epoch 124/300\n",
      "Average training loss: 0.054282055089871085\n",
      "Average test loss: 0.002005857033862008\n",
      "Epoch 125/300\n",
      "Average training loss: 0.053879529969559775\n",
      "Average test loss: 0.0019170714478111929\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05409508329298761\n",
      "Average test loss: 0.0021791056316966813\n",
      "Epoch 127/300\n",
      "Average training loss: 0.053979963994688455\n",
      "Average test loss: 0.0018918914683163166\n",
      "Epoch 128/300\n",
      "Average training loss: 0.053645890679624345\n",
      "Average test loss: 0.0019104376613265938\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05501152367393176\n",
      "Average test loss: 0.0018556348342034551\n",
      "Epoch 130/300\n",
      "Average training loss: 0.054250801967249976\n",
      "Average test loss: 0.001962310869557162\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05328702911734581\n",
      "Average test loss: 0.002081222146956457\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05412354891167746\n",
      "Average test loss: 0.0018645841288897726\n",
      "Epoch 133/300\n",
      "Average training loss: 0.053139485584364994\n",
      "Average test loss: 0.001905180505476892\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05361126198702389\n",
      "Average test loss: 0.0019801207043023575\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05332463924752341\n",
      "Average test loss: 0.0018909430692179336\n",
      "Epoch 136/300\n",
      "Average training loss: 0.053449899795982574\n",
      "Average test loss: 0.0021273812957935865\n",
      "Epoch 137/300\n",
      "Average training loss: 0.053220249003834194\n",
      "Average test loss: 0.0018813633458274934\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05292412636677424\n",
      "Average test loss: 0.05100468598554532\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05273377928800053\n",
      "Average test loss: 0.0019522685786295267\n",
      "Epoch 140/300\n",
      "Average training loss: 0.053115703516536286\n",
      "Average test loss: 0.0020773209987415206\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05299923298756282\n",
      "Average test loss: 0.0019513474331340856\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05246570634841919\n",
      "Average test loss: 0.0019704441411627664\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05383910464909342\n",
      "Average test loss: 0.0019054226830291252\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0524800215529071\n",
      "Average test loss: 0.0019386119362898172\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05221588732136621\n",
      "Average test loss: 0.001941901191862093\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05210462107261022\n",
      "Average test loss: 0.0023017913911284674\n",
      "Epoch 147/300\n",
      "Average training loss: 0.052237902965810566\n",
      "Average test loss: 0.0020446297727111313\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05298431305421723\n",
      "Average test loss: 0.0018913922399903338\n",
      "Epoch 149/300\n",
      "Average training loss: 0.052066857705513635\n",
      "Average test loss: 0.0018979870457616118\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05196205628580517\n",
      "Average test loss: 0.0019409930385235283\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05167509650852945\n",
      "Average test loss: 0.002156023392246829\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05158799063165983\n",
      "Average test loss: 0.0020286747522445187\n",
      "Epoch 153/300\n",
      "Average training loss: 0.051890138152572846\n",
      "Average test loss: 0.005076137019528283\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05244803297519684\n",
      "Average test loss: 0.0021763447934968604\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05144654491212633\n",
      "Average test loss: 0.0019646674300440484\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05278373513950242\n",
      "Average test loss: 0.009843758029656278\n",
      "Epoch 157/300\n",
      "Average training loss: 0.051354864252938164\n",
      "Average test loss: 0.005219788588169548\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05118823414047559\n",
      "Average test loss: 0.0020469247588059966\n",
      "Epoch 159/300\n",
      "Average training loss: 0.052438214629888534\n",
      "Average test loss: 0.002690437330553929\n",
      "Epoch 160/300\n",
      "Average training loss: 0.051383652253283396\n",
      "Average test loss: 0.001885884702205658\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05089239461223284\n",
      "Average test loss: 0.001989971665975948\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05111768561932776\n",
      "Average test loss: 0.0019317476912919017\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05125039238731066\n",
      "Average test loss: 0.002010106296278536\n",
      "Epoch 164/300\n",
      "Average training loss: 0.051016287396351494\n",
      "Average test loss: 0.0048558967755072645\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05108716358244419\n",
      "Average test loss: 0.0021146397668247423\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05073951291872395\n",
      "Average test loss: 0.0022486182548519637\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05090222275257111\n",
      "Average test loss: 0.001972744644929965\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05110012192527453\n",
      "Average test loss: 0.002922883250026239\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05763516873121262\n",
      "Average test loss: 0.0019225541583986745\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05221116023924616\n",
      "Average test loss: 0.0021841741737185253\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05057999501625697\n",
      "Average test loss: 0.0018916127879379525\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05063774560226334\n",
      "Average test loss: 0.0020802023001532588\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05053002742264006\n",
      "Average test loss: 0.0022026948359691436\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05112580620911386\n",
      "Average test loss: 0.002151255765101976\n",
      "Epoch 175/300\n",
      "Average training loss: 0.050728222436375085\n",
      "Average test loss: 0.0021284776081641516\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05102746845616234\n",
      "Average test loss: 0.0026390131894085144\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05033323363131947\n",
      "Average test loss: 0.002166045484236545\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05052318080100748\n",
      "Average test loss: 0.0021281680348846648\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05016500899857945\n",
      "Average test loss: 0.0392212702938252\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05032821657591396\n",
      "Average test loss: 0.0034943393282592296\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05032166469759411\n",
      "Average test loss: 0.002839352645497355\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05008355325129297\n",
      "Average test loss: 0.0020282587252246835\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10201092624002032\n",
      "Average test loss: 0.0026494729467150236\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0869105343454414\n",
      "Average test loss: 0.002129539875831041\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06795681332217322\n",
      "Average test loss: 0.001790984782183336\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06219076580471462\n",
      "Average test loss: 0.012521643806662824\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05782206651237276\n",
      "Average test loss: 0.008167209854142533\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05468818121155103\n",
      "Average test loss: 0.9979823208699624\n",
      "Epoch 189/300\n",
      "Average training loss: 0.052599882894092134\n",
      "Average test loss: 0.01791822949382994\n",
      "Epoch 190/300\n",
      "Average training loss: 0.051524684432480074\n",
      "Average test loss: 0.0019036925061502391\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05067753849426905\n",
      "Average test loss: 0.0019282484963122342\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05030492810739411\n",
      "Average test loss: 0.0019368213810440566\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05022946725289027\n",
      "Average test loss: 0.0020720216441485616\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05015157855881585\n",
      "Average test loss: 0.001957396984514263\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04992137402627203\n",
      "Average test loss: 0.003321410559738676\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05001617921060986\n",
      "Average test loss: 0.0021006903231350913\n",
      "Epoch 197/300\n",
      "Average training loss: 0.049942760285403995\n",
      "Average test loss: 0.0020399717653377188\n",
      "Epoch 198/300\n",
      "Average training loss: 0.050004016409317655\n",
      "Average test loss: 0.00790995074229108\n",
      "Epoch 199/300\n",
      "Average training loss: 0.051105101194646625\n",
      "Average test loss: 0.001970726497264372\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04982856911420822\n",
      "Average test loss: 0.001984958381909463\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04943555279241668\n",
      "Average test loss: 0.001923737950829996\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04966896755827798\n",
      "Average test loss: 0.0020037929729248088\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04970428035987748\n",
      "Average test loss: 0.10990594574064017\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04955405142572191\n",
      "Average test loss: 0.0019572081518256\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04948907167050574\n",
      "Average test loss: 0.002160152350862821\n",
      "Epoch 206/300\n",
      "Average training loss: 0.050103008314967154\n",
      "Average test loss: 0.002227329002900256\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04946371225847138\n",
      "Average test loss: 0.0026551723923120235\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04966810457905133\n",
      "Average test loss: 0.0019282455185635223\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0493122819562753\n",
      "Average test loss: 0.0019772997140470477\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05142761193878121\n",
      "Average test loss: 0.0021382616818365124\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04908467631869846\n",
      "Average test loss: 0.0026341817275517517\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04907170743081305\n",
      "Average test loss: 0.003709496369378434\n",
      "Epoch 213/300\n",
      "Average training loss: 0.049211756313840546\n",
      "Average test loss: 0.41497330124386483\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0492996241847674\n",
      "Average test loss: 0.0019854718887557586\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05012908524274826\n",
      "Average test loss: 0.002117207000239028\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04940571274691158\n",
      "Average test loss: 0.0034317084670894677\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04888658145070076\n",
      "Average test loss: 0.0019669744179894528\n",
      "Epoch 218/300\n",
      "Average training loss: 0.049321366313430995\n",
      "Average test loss: 0.0024724061347337232\n",
      "Epoch 219/300\n",
      "Average training loss: 0.049204545117086836\n",
      "Average test loss: 0.0019839747318377097\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04886567111147774\n",
      "Average test loss: 0.049159761475192174\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04896748414304521\n",
      "Average test loss: 0.002068731931762563\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04903577059176233\n",
      "Average test loss: 0.005074372679719495\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04921200012167295\n",
      "Average test loss: 0.001924359234670798\n",
      "Epoch 224/300\n",
      "Average training loss: 0.048700651066170796\n",
      "Average test loss: 0.0025378467481997277\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04882319280174043\n",
      "Average test loss: 0.001988503346012698\n",
      "Epoch 226/300\n",
      "Average training loss: 0.048715072711308795\n",
      "Average test loss: 0.004354940951491396\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04901837142308553\n",
      "Average test loss: 0.001995602989776267\n",
      "Epoch 228/300\n",
      "Average training loss: 0.048613019999530584\n",
      "Average test loss: 0.0026389809492975472\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04895562709371249\n",
      "Average test loss: 0.03774597534206178\n",
      "Epoch 230/300\n",
      "Average training loss: 0.049470674488279555\n",
      "Average test loss: 0.0020026844666442938\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048377166983154085\n",
      "Average test loss: 0.0020566793605685234\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04846767637795872\n",
      "Average test loss: 0.0030410082280221914\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04849178151620759\n",
      "Average test loss: 0.0020000187793953552\n",
      "Epoch 234/300\n",
      "Average training loss: 0.048932116581334006\n",
      "Average test loss: 0.001974513477542334\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0483435524072912\n",
      "Average test loss: 0.0019977068609247604\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04900100624230173\n",
      "Average test loss: 0.0031122027362386387\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04817674397097694\n",
      "Average test loss: 0.0026725961172746285\n",
      "Epoch 238/300\n",
      "Average training loss: 0.048384482916858464\n",
      "Average test loss: 0.00390686947107315\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04813594150543213\n",
      "Average test loss: 0.0023075891608993213\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04857276901602745\n",
      "Average test loss: 0.01835430972278118\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0481980816208654\n",
      "Average test loss: 0.001994659255258739\n",
      "Epoch 242/300\n",
      "Average training loss: 0.048323836122949916\n",
      "Average test loss: 0.0020849480446841983\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04860709548327658\n",
      "Average test loss: 0.002291133438133531\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04803571206000116\n",
      "Average test loss: 0.001969830529402114\n",
      "Epoch 245/300\n",
      "Average training loss: 0.048159144209490884\n",
      "Average test loss: 0.0023803982633269494\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04873799108134376\n",
      "Average test loss: 0.009573892323093282\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04791628461082777\n",
      "Average test loss: 0.0019415218023997214\n",
      "Epoch 248/300\n",
      "Average training loss: 0.048770484013689885\n",
      "Average test loss: 0.0020267323792601626\n",
      "Epoch 249/300\n",
      "Average training loss: 0.047797764889068076\n",
      "Average test loss: 0.002017564172132148\n",
      "Epoch 250/300\n",
      "Average training loss: 0.048341989560259715\n",
      "Average test loss: 0.0020101797580718995\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04846232882142067\n",
      "Average test loss: 0.0035391994216996763\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04875599827037917\n",
      "Average test loss: 0.0020007962899075615\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04761713647180133\n",
      "Average test loss: 0.0020649775202489563\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04783420545856158\n",
      "Average test loss: 0.23524364715566237\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04787321166859733\n",
      "Average test loss: 0.002024822033320864\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04783734326892429\n",
      "Average test loss: 0.002057044345678555\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04783895454141829\n",
      "Average test loss: 0.0027576609585020276\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04778306551112069\n",
      "Average test loss: 0.0019585642270329925\n",
      "Epoch 259/300\n",
      "Average training loss: 0.048268346789810394\n",
      "Average test loss: 0.00239109027509888\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04774170432819261\n",
      "Average test loss: 0.05210884787307845\n",
      "Epoch 261/300\n",
      "Average training loss: 0.047798427638080385\n",
      "Average test loss: 0.0020380598915119967\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04777187380856938\n",
      "Average test loss: 0.001991685926914215\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04773447491394149\n",
      "Average test loss: 0.001983146180709203\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04770926200681262\n",
      "Average test loss: 0.0019865485187619924\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0483635438117716\n",
      "Average test loss: 0.00211612953318076\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04750966166125403\n",
      "Average test loss: 0.002572058694023225\n",
      "Epoch 267/300\n",
      "Average training loss: 0.047285487191544635\n",
      "Average test loss: 0.014872043943239583\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04782081482145521\n",
      "Average test loss: 0.001991003125905991\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04762934531437026\n",
      "Average test loss: 0.5860912783203853\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04755575269460678\n",
      "Average test loss: 0.0022663140959209866\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05548626462949647\n",
      "Average test loss: 0.002968329946613974\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04845707587732209\n",
      "Average test loss: 0.002044809492718842\n",
      "Epoch 273/300\n",
      "Average training loss: 0.047301113542583255\n",
      "Average test loss: 0.002234667872182197\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04707297650310728\n",
      "Average test loss: 0.001964413103957971\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04783538662062751\n",
      "Average test loss: 0.0020524018208185834\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04705060328377618\n",
      "Average test loss: 0.0020189675629759827\n",
      "Epoch 277/300\n",
      "Average training loss: 0.047432008230023914\n",
      "Average test loss: 0.0031471524553166497\n",
      "Epoch 278/300\n",
      "Average training loss: 0.047708534495698084\n",
      "Average test loss: 0.0020374225947178072\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0474884310497178\n",
      "Average test loss: 0.0020465107787814405\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04717373310857349\n",
      "Average test loss: 0.00195997325801808\n",
      "Epoch 281/300\n",
      "Average training loss: 0.047637986855374445\n",
      "Average test loss: 0.001970043402786056\n",
      "Epoch 282/300\n",
      "Average training loss: 0.047314153952731024\n",
      "Average test loss: 0.002779831768117017\n",
      "Epoch 283/300\n",
      "Average training loss: 0.049550663136773636\n",
      "Average test loss: 0.0020295687339579064\n",
      "Epoch 284/300\n",
      "Average training loss: 0.046822924991448724\n",
      "Average test loss: 0.00201989170939972\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04751734247141414\n",
      "Average test loss: 0.0019774225149303674\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04691334237323867\n",
      "Average test loss: 0.0020231874576873247\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04737888471285502\n",
      "Average test loss: 0.0029444345055768886\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04708566344115469\n",
      "Average test loss: 0.002236086945566866\n",
      "Epoch 289/300\n",
      "Average training loss: 0.047016672607925206\n",
      "Average test loss: 0.0021689889910113482\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04705480157666736\n",
      "Average test loss: 0.0020159044892837604\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04722152982983324\n",
      "Average test loss: 0.050094173130889735\n",
      "Epoch 292/300\n",
      "Average training loss: 0.047132035301791296\n",
      "Average test loss: 0.018127130857772296\n",
      "Epoch 293/300\n",
      "Average training loss: 0.046919435130225284\n",
      "Average test loss: 0.0021793710154791673\n",
      "Epoch 294/300\n",
      "Average training loss: 0.047437614070044626\n",
      "Average test loss: 0.01731372761891948\n",
      "Epoch 295/300\n",
      "Average training loss: 0.047153768486446805\n",
      "Average test loss: 0.21162428693059418\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04658140466941728\n",
      "Average test loss: 0.002267842028238293\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04770491763949394\n",
      "Average test loss: 0.0021852735850132173\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04678452393743727\n",
      "Average test loss: 0.0022293687332421543\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04672389262252384\n",
      "Average test loss: 0.0020502642213056483\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04726876368456417\n",
      "Average test loss: 0.002014112781112393\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 706.9687106704712\n",
      "Average test loss: 944.6256594431102\n",
      "Epoch 2/300\n",
      "Average training loss: 9.258278891669379\n",
      "Average test loss: 8.942354905246033\n",
      "Epoch 3/300\n",
      "Average training loss: 8.967399875217014\n",
      "Average test loss: 0.20312500307791762\n",
      "Epoch 4/300\n",
      "Average training loss: 7.768070662180583\n",
      "Average test loss: 0.00840084446966648\n",
      "Epoch 5/300\n",
      "Average training loss: 6.73304283396403\n",
      "Average test loss: 2.286469047135363\n",
      "Epoch 6/300\n",
      "Average training loss: 5.657460959116618\n",
      "Average test loss: 24.046283616996472\n",
      "Epoch 7/300\n",
      "Average training loss: 5.209254958258735\n",
      "Average test loss: 0.006097560164415174\n",
      "Epoch 8/300\n",
      "Average training loss: 4.523412222120497\n",
      "Average test loss: 0.007697574377059937\n",
      "Epoch 9/300\n",
      "Average training loss: 4.021839207119412\n",
      "Average test loss: 0.010952547535300255\n",
      "Epoch 10/300\n",
      "Average training loss: 3.482480525546604\n",
      "Average test loss: 0.004268129645122422\n",
      "Epoch 11/300\n",
      "Average training loss: 3.021661506652832\n",
      "Average test loss: 0.06271614574765165\n",
      "Epoch 12/300\n",
      "Average training loss: 2.623148920059204\n",
      "Average test loss: 0.004068836307773988\n",
      "Epoch 13/300\n",
      "Average training loss: 2.3051857503255206\n",
      "Average test loss: 0.004261538723483681\n",
      "Epoch 14/300\n",
      "Average training loss: 2.026727746327718\n",
      "Average test loss: 0.0035406775364859237\n",
      "Epoch 15/300\n",
      "Average training loss: 1.8113991005155776\n",
      "Average test loss: 0.003279964626663261\n",
      "Epoch 16/300\n",
      "Average training loss: 1.591236317952474\n",
      "Average test loss: 0.0035756843307365974\n",
      "Epoch 17/300\n",
      "Average training loss: 1.40056092982822\n",
      "Average test loss: 0.0030829901933256123\n",
      "Epoch 18/300\n",
      "Average training loss: 1.2334215433332656\n",
      "Average test loss: 0.002927531360131171\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0822729671266345\n",
      "Average test loss: 0.0030803811653620666\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9470944653087192\n",
      "Average test loss: 0.0028794059389167363\n",
      "Epoch 21/300\n",
      "Average training loss: 0.8265463904804654\n",
      "Average test loss: 0.0026608131954239476\n",
      "Epoch 22/300\n",
      "Average training loss: 0.7182981552547879\n",
      "Average test loss: 0.0026539442481266127\n",
      "Epoch 23/300\n",
      "Average training loss: 0.6213284429974026\n",
      "Average test loss: 0.0029216100660463176\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5370347438388401\n",
      "Average test loss: 0.002491444021049473\n",
      "Epoch 25/300\n",
      "Average training loss: 0.4649046964115567\n",
      "Average test loss: 0.0024038941270361344\n",
      "Epoch 26/300\n",
      "Average training loss: 0.40299268579483033\n",
      "Average test loss: 0.0025770937752806478\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3512060026062859\n",
      "Average test loss: 0.0021427021752008133\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3082032022078832\n",
      "Average test loss: 0.0022204349967133667\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2725103699763616\n",
      "Average test loss: 0.00221272152952022\n",
      "Epoch 30/300\n",
      "Average training loss: 0.24111328429645962\n",
      "Average test loss: 0.0021475937627255916\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2144311374425888\n",
      "Average test loss: 0.0029300100606762703\n",
      "Epoch 32/300\n",
      "Average training loss: 0.19247985826598274\n",
      "Average test loss: 0.0022730979188862774\n",
      "Epoch 33/300\n",
      "Average training loss: 0.17478024620480007\n",
      "Average test loss: 0.0018826763413639532\n",
      "Epoch 34/300\n",
      "Average training loss: 0.16175080090098912\n",
      "Average test loss: 0.0020036981228945985\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1473414188358519\n",
      "Average test loss: 0.0018674100728498565\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13903711998462678\n",
      "Average test loss: 0.0031018462371495037\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13003854860199823\n",
      "Average test loss: 0.002246940285795265\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12565330764982435\n",
      "Average test loss: 0.0019897532974266345\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11484990655051337\n",
      "Average test loss: 0.001756509179663327\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10860874917772081\n",
      "Average test loss: 0.003955695165942113\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10599993813037872\n",
      "Average test loss: 0.0022280347645282747\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10407291261355082\n",
      "Average test loss: 0.002026991098291344\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09593561834096909\n",
      "Average test loss: 0.0017069103682620659\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09354435416062673\n",
      "Average test loss: 0.0018899262417107821\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08861701032850477\n",
      "Average test loss: 0.0016778950274197592\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08575955305165715\n",
      "Average test loss: 0.0017162358346395194\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08319748955633906\n",
      "Average test loss: 0.001943144388910797\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08294000823630227\n",
      "Average test loss: 0.0018293587555074028\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1420693174931738\n",
      "Average test loss: 0.009359527309735617\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0916368770202001\n",
      "Average test loss: 0.0016871586139831278\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08223368765910466\n",
      "Average test loss: 0.001548732272297558\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0789684028658602\n",
      "Average test loss: 0.0019144825279298755\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07508108653624852\n",
      "Average test loss: 0.0016574764863277476\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07251986092329026\n",
      "Average test loss: 0.0015071978796687392\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07080265367031098\n",
      "Average test loss: 0.0022667379700578747\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06849618474311299\n",
      "Average test loss: 0.0014512937693960137\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06702815216117435\n",
      "Average test loss: 1.9108329518900977\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06532857538263002\n",
      "Average test loss: 0.001441530405336784\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06380781233641836\n",
      "Average test loss: 0.0017293029843519132\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06229789102739758\n",
      "Average test loss: 0.0013728007356015345\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0605458624098036\n",
      "Average test loss: 0.0013857267597276303\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06169547955526246\n",
      "Average test loss: 0.001476118688368135\n",
      "Epoch 63/300\n",
      "Average training loss: 0.058820159004794224\n",
      "Average test loss: 0.0013388554154791766\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05823580253455374\n",
      "Average test loss: 0.0024134272303846147\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05614541337887446\n",
      "Average test loss: 0.001378513073652155\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05556714998020066\n",
      "Average test loss: 0.002783556330845588\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05488423798481623\n",
      "Average test loss: 0.0012794193481612537\n",
      "Epoch 68/300\n",
      "Average training loss: 0.054495970553821986\n",
      "Average test loss: 0.0013393732780176732\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05439922493033939\n",
      "Average test loss: 0.0013725025203699867\n",
      "Epoch 70/300\n",
      "Average training loss: 0.052446320364872616\n",
      "Average test loss: 0.0012855309231931138\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05230719049771627\n",
      "Average test loss: 0.0012964237710047098\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05126510066125128\n",
      "Average test loss: 0.001244333019376629\n",
      "Epoch 73/300\n",
      "Average training loss: 0.050758924000793036\n",
      "Average test loss: 0.0012746841214183304\n",
      "Epoch 74/300\n",
      "Average training loss: 0.050264261331823136\n",
      "Average test loss: 0.0012588654912801252\n",
      "Epoch 75/300\n",
      "Average training loss: 0.049505757557021246\n",
      "Average test loss: 0.001238374664241241\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04933243269721667\n",
      "Average test loss: 0.0014561710177092917\n",
      "Epoch 77/300\n",
      "Average training loss: 0.049640037771728304\n",
      "Average test loss: 0.0012472834033477637\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0480430268711514\n",
      "Average test loss: 0.0013471848344844249\n",
      "Epoch 79/300\n",
      "Average training loss: 0.047882996728022896\n",
      "Average test loss: 0.0012386283319857385\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04725724419951439\n",
      "Average test loss: 0.0015105268926256233\n",
      "Epoch 81/300\n",
      "Average training loss: 0.047411950800153944\n",
      "Average test loss: 0.0017857333072978589\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04667960812979274\n",
      "Average test loss: 0.001505750037315819\n",
      "Epoch 83/300\n",
      "Average training loss: 0.046500261684258776\n",
      "Average test loss: 0.001229048639949825\n",
      "Epoch 84/300\n",
      "Average training loss: 0.046890631844600045\n",
      "Average test loss: 0.0016523840575892893\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04587755212518904\n",
      "Average test loss: 0.0012588498494070437\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04568536005086369\n",
      "Average test loss: 0.001201709129485405\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0453872696393066\n",
      "Average test loss: 0.0013995870701554748\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04514511732591523\n",
      "Average test loss: 0.001542401449340913\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04602575025624699\n",
      "Average test loss: 0.0023134109928376143\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04446565998593966\n",
      "Average test loss: 0.0011963439534107845\n",
      "Epoch 91/300\n",
      "Average training loss: 0.044918505407041974\n",
      "Average test loss: 0.0013133812288546728\n",
      "Epoch 92/300\n",
      "Average training loss: 0.047035060548120074\n",
      "Average test loss: 0.0012706036133588187\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04398684828314516\n",
      "Average test loss: 0.001212263565743342\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04674637269642618\n",
      "Average test loss: 147783.02172395834\n",
      "Epoch 95/300\n",
      "Average training loss: 0.043902393456962376\n",
      "Average test loss: 0.0012089465063893133\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0433757597969638\n",
      "Average test loss: 0.0916962411031127\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0434603366918034\n",
      "Average test loss: 0.0011775912629543907\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0431802688241005\n",
      "Average test loss: 0.0012015846345263222\n",
      "Epoch 99/300\n",
      "Average training loss: 0.043206355217430326\n",
      "Average test loss: 0.004014341638940904\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04280151754286554\n",
      "Average test loss: 0.0012460652025830415\n",
      "Epoch 101/300\n",
      "Average training loss: 0.042678493486510385\n",
      "Average test loss: 0.0019980495255440475\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04300364041659567\n",
      "Average test loss: 0.0021779020238253807\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04245529096325239\n",
      "Average test loss: 0.0011995908490692576\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04203169502483474\n",
      "Average test loss: 0.0013216671611492833\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04258901635805766\n",
      "Average test loss: 0.0018423228780221608\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04620645569430457\n",
      "Average test loss: 0.0011950620197587543\n",
      "Epoch 107/300\n",
      "Average training loss: 0.041781639291180506\n",
      "Average test loss: 0.0012776163364760578\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04163025385306941\n",
      "Average test loss: 0.0013144581959479385\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04171740211877558\n",
      "Average test loss: 0.0012276496522956423\n",
      "Epoch 110/300\n",
      "Average training loss: 0.041723448609312375\n",
      "Average test loss: 0.0012535883233779006\n",
      "Epoch 111/300\n",
      "Average training loss: 0.041574857814444435\n",
      "Average test loss: 0.0013154177493933175\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04106064260005951\n",
      "Average test loss: 0.002050150078617864\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04098154947823948\n",
      "Average test loss: 0.0012311520541293754\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04087053271465831\n",
      "Average test loss: 0.0012450250765412218\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04171865225169394\n",
      "Average test loss: 0.0018289802083745598\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04060041677620676\n",
      "Average test loss: 0.0015165395026819574\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04212791157762209\n",
      "Average test loss: 0.0017581934192114406\n",
      "Epoch 118/300\n",
      "Average training loss: 0.040405959023369686\n",
      "Average test loss: 0.0014478162163868546\n",
      "Epoch 119/300\n",
      "Average training loss: 0.040308565941121845\n",
      "Average test loss: 0.0023370352525057063\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04081562469402949\n",
      "Average test loss: 0.0014734254752078817\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0398854189068079\n",
      "Average test loss: 0.0014286197231461605\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03993750176827113\n",
      "Average test loss: 0.0012759170597419142\n",
      "Epoch 123/300\n",
      "Average training loss: 0.039898882750007844\n",
      "Average test loss: 0.0013252370956664285\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04196903888384501\n",
      "Average test loss: 0.0016364369388255809\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03992378396458096\n",
      "Average test loss: 0.0012397747275729974\n",
      "Epoch 126/300\n",
      "Average training loss: 0.039528400053580603\n",
      "Average test loss: 0.001242907092285653\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03927756261163288\n",
      "Average test loss: 0.0012635967160264652\n",
      "Epoch 128/300\n",
      "Average training loss: 0.039464846048090196\n",
      "Average test loss: 0.0012618077447534436\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0394077467487918\n",
      "Average test loss: 0.001529418438569539\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03946660212344594\n",
      "Average test loss: 0.0014086592903153764\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0395415869537327\n",
      "Average test loss: 0.001287163465180331\n",
      "Epoch 132/300\n",
      "Average training loss: 0.039047399272521335\n",
      "Average test loss: 0.0012578175237609281\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03896445366077953\n",
      "Average test loss: 0.001356371749088996\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04136673540539212\n",
      "Average test loss: 0.0013031656206585467\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03850613183776538\n",
      "Average test loss: 0.0012311747417681747\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03908497073749701\n",
      "Average test loss: 0.021053894095329775\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03905257189273834\n",
      "Average test loss: 0.0015510899054093493\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03854064319365554\n",
      "Average test loss: 0.0013155410808200637\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03869262804256545\n",
      "Average test loss: 0.0012613658679442273\n",
      "Epoch 140/300\n",
      "Average training loss: 0.038630672766102686\n",
      "Average test loss: 0.0012743630790048176\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03835272080368466\n",
      "Average test loss: 0.0012620876014439596\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03876847462356091\n",
      "Average test loss: 0.02780025321824683\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03843515351414681\n",
      "Average test loss: 0.001546120763083713\n",
      "Epoch 144/300\n",
      "Average training loss: 0.038192368292146256\n",
      "Average test loss: 0.0012647529551759362\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03837147536211544\n",
      "Average test loss: 0.006195002324879169\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0401451771888468\n",
      "Average test loss: 0.0017628968294916881\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03784929639763302\n",
      "Average test loss: 0.0020114867050821583\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03872107829319106\n",
      "Average test loss: 0.0014163456654383076\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03784728795952267\n",
      "Average test loss: 0.0013238354766120512\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03878025603294372\n",
      "Average test loss: 0.0012481983153977327\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03766675059662925\n",
      "Average test loss: 0.029833010777831077\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0383996882405546\n",
      "Average test loss: 0.0021754413988027306\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03773175248503685\n",
      "Average test loss: 0.0013544872752908202\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03828021869063378\n",
      "Average test loss: 0.0012883453774783346\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0371512067715327\n",
      "Average test loss: 0.0018237737162659566\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03762500105301539\n",
      "Average test loss: 0.0012858460336509679\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03772108154661125\n",
      "Average test loss: 0.0013332495089206432\n",
      "Epoch 158/300\n",
      "Average training loss: 0.037354277557796904\n",
      "Average test loss: 0.0016687165836079252\n",
      "Epoch 159/300\n",
      "Average training loss: 0.037410318990548454\n",
      "Average test loss: 0.001355464441029148\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03758255258864827\n",
      "Average test loss: 0.006390321102821165\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03724762981964482\n",
      "Average test loss: 0.0013099661510334246\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03693713900115755\n",
      "Average test loss: 0.02493142321705818\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03897171819872326\n",
      "Average test loss: 0.0013340174300182197\n",
      "Epoch 164/300\n",
      "Average training loss: 0.037262692587243185\n",
      "Average test loss: 0.002157234124528865\n",
      "Epoch 165/300\n",
      "Average training loss: 0.036996083443363505\n",
      "Average test loss: 0.0015528932503528065\n",
      "Epoch 166/300\n",
      "Average training loss: 0.036819777571492726\n",
      "Average test loss: 0.0013736415390546122\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04042215906911426\n",
      "Average test loss: 0.002174643038461606\n",
      "Epoch 168/300\n",
      "Average training loss: 0.037055389089716805\n",
      "Average test loss: 0.001435522489560147\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03706475614176856\n",
      "Average test loss: 0.0018710771691468028\n",
      "Epoch 170/300\n",
      "Average training loss: 0.037139473829004496\n",
      "Average test loss: 0.001304805096031891\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03658690892325507\n",
      "Average test loss: 0.0017379277188123929\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03681161220206155\n",
      "Average test loss: 0.0013169037336483599\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03696235672301716\n",
      "Average test loss: 0.003898912039740632\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03666848556200663\n",
      "Average test loss: 0.0014479191671642993\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03695827543735504\n",
      "Average test loss: 0.001274401413794193\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03663734688692623\n",
      "Average test loss: 0.001981372986506257\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03747295704483986\n",
      "Average test loss: 0.0034789390969607566\n",
      "Epoch 178/300\n",
      "Average training loss: 0.036451341178682116\n",
      "Average test loss: 0.0013146252147853373\n",
      "Epoch 179/300\n",
      "Average training loss: 0.036912640695770584\n",
      "Average test loss: 0.0013241628360831075\n",
      "Epoch 180/300\n",
      "Average training loss: 0.036141480818390845\n",
      "Average test loss: 0.0022680373313940234\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03623497464921739\n",
      "Average test loss: 0.0013646357980453307\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03675755893025133\n",
      "Average test loss: 0.0013595900955713457\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03626348688701789\n",
      "Average test loss: 0.0015658547747880221\n",
      "Epoch 184/300\n",
      "Average training loss: 0.037064990503920446\n",
      "Average test loss: 0.0013560222439053986\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03593829226493835\n",
      "Average test loss: 0.0013050838650928603\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03659142322672738\n",
      "Average test loss: 0.001384140547675391\n",
      "Epoch 187/300\n",
      "Average training loss: 0.036267004115713965\n",
      "Average test loss: 0.0015727821404321327\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03571496838496791\n",
      "Average test loss: 0.0018515160656016735\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03618157196872764\n",
      "Average test loss: 0.002704362723355492\n",
      "Epoch 190/300\n",
      "Average training loss: 0.036019325935178335\n",
      "Average test loss: 0.0017988331751484009\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03744215026994546\n",
      "Average test loss: 0.0013505584453749988\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03616984682281812\n",
      "Average test loss: 0.0014413872984134488\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03578892962468995\n",
      "Average test loss: 0.001298863764748805\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03586647883885437\n",
      "Average test loss: 0.0014195798406791355\n",
      "Epoch 195/300\n",
      "Average training loss: 0.035795839753415847\n",
      "Average test loss: 0.0017522033450918066\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03651459145545959\n",
      "Average test loss: 0.0015589593053898878\n",
      "Epoch 197/300\n",
      "Average training loss: 0.035849484619167114\n",
      "Average test loss: 0.0013629845146917634\n",
      "Epoch 198/300\n",
      "Average training loss: 0.035492783625920614\n",
      "Average test loss: 0.0021445439516877135\n",
      "Epoch 199/300\n",
      "Average training loss: 0.035769925187031426\n",
      "Average test loss: 0.004114785047144525\n",
      "Epoch 200/300\n",
      "Average training loss: 0.035981070124440725\n",
      "Average test loss: 0.0018117900987466176\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0358753454056051\n",
      "Average test loss: 0.001511531234304938\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03548988423413701\n",
      "Average test loss: 0.001682108199172136\n",
      "Epoch 203/300\n",
      "Average training loss: 0.035678963048590556\n",
      "Average test loss: 0.0013146604505471057\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03557838481002384\n",
      "Average test loss: 0.007031105575358702\n",
      "Epoch 205/300\n",
      "Average training loss: 0.035833375008569826\n",
      "Average test loss: 0.0013645568589369455\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03597444964117474\n",
      "Average test loss: 0.0015513436019213662\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03530051661862267\n",
      "Average test loss: 0.0013612765971985129\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03539412328269746\n",
      "Average test loss: 0.001338602269327061\n",
      "Epoch 209/300\n",
      "Average training loss: 0.036169908699062135\n",
      "Average test loss: 0.0013175419646625718\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03565729338179032\n",
      "Average test loss: 0.006120838842044274\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0353121216694514\n",
      "Average test loss: 0.0014258050410490897\n",
      "Epoch 212/300\n",
      "Average training loss: 0.035391860746675066\n",
      "Average test loss: 0.0013727155652725033\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0357010225372182\n",
      "Average test loss: 0.003709152254793379\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03500455218553543\n",
      "Average test loss: 0.0013277503532460994\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03635719124144978\n",
      "Average test loss: 0.0015204853338913785\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03571288679043452\n",
      "Average test loss: 0.027702169733328952\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0350724522570769\n",
      "Average test loss: 0.0013267513818314505\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03488135027885437\n",
      "Average test loss: 0.0018586300431440274\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03542154098219342\n",
      "Average test loss: 0.0013562046067996157\n",
      "Epoch 220/300\n",
      "Average training loss: 0.035301684311694566\n",
      "Average test loss: 0.001398543170135882\n",
      "Epoch 221/300\n",
      "Average training loss: 0.035877652522590425\n",
      "Average test loss: 0.0015956469194756615\n",
      "Epoch 222/300\n",
      "Average training loss: 0.035068369736274085\n",
      "Average test loss: 0.001352712262628807\n",
      "Epoch 223/300\n",
      "Average training loss: 0.034645908173587585\n",
      "Average test loss: 0.004586076913194524\n",
      "Epoch 224/300\n",
      "Average training loss: 0.043950201004743575\n",
      "Average test loss: 0.0015129621444890896\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03526872057053778\n",
      "Average test loss: 0.0037635758005910448\n",
      "Epoch 226/300\n",
      "Average training loss: 0.035236523555384744\n",
      "Average test loss: 0.0013588974515700506\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03464005844460594\n",
      "Average test loss: 0.001383577197479705\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03473655861284998\n",
      "Average test loss: 0.001975843022370504\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03570948325263129\n",
      "Average test loss: 0.0013123763889695207\n",
      "Epoch 230/300\n",
      "Average training loss: 0.034631564021110534\n",
      "Average test loss: 0.0013365175071845038\n",
      "Epoch 231/300\n",
      "Average training loss: 0.035395598987738294\n",
      "Average test loss: 0.0013503396997435226\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03488872864014573\n",
      "Average test loss: 0.001767924273903999\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03474513141645325\n",
      "Average test loss: 0.0013158168567137586\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03487463992668523\n",
      "Average test loss: 0.001354084641672671\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03570727112227016\n",
      "Average test loss: 0.0013452289805023206\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03464698745641444\n",
      "Average test loss: 0.0013694302248251106\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0343921993970871\n",
      "Average test loss: 0.0014952948537344734\n",
      "Epoch 238/300\n",
      "Average training loss: 0.035380129834016165\n",
      "Average test loss: 0.0013556398832653132\n",
      "Epoch 239/300\n",
      "Average training loss: 0.034473686206671926\n",
      "Average test loss: 0.0015017825133270687\n",
      "Epoch 240/300\n",
      "Average training loss: 0.035476590540674\n",
      "Average test loss: 0.001337024474930432\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0344619626071718\n",
      "Average test loss: 0.0014380383732625179\n",
      "Epoch 242/300\n",
      "Average training loss: 0.035165698981947366\n",
      "Average test loss: 0.001365984202362597\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03445863081680404\n",
      "Average test loss: 0.0013424136808349028\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03822685791552067\n",
      "Average test loss: 0.0013959507215250697\n",
      "Epoch 245/300\n",
      "Average training loss: 0.034675259951088166\n",
      "Average test loss: 0.0016300198627739316\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03466234907011191\n",
      "Average test loss: 0.001556153350406223\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03519554603099823\n",
      "Average test loss: 0.002294506967895561\n",
      "Epoch 248/300\n",
      "Average training loss: 0.034554499045014384\n",
      "Average test loss: 0.0017461718093189928\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03425785571667883\n",
      "Average test loss: 0.0016102245488307542\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0346585407588217\n",
      "Average test loss: 0.0013457884682963291\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03514178631371922\n",
      "Average test loss: 0.0017261179224070575\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03429470042718781\n",
      "Average test loss: 0.0013531936522987154\n",
      "Epoch 253/300\n",
      "Average training loss: 0.034149563312530516\n",
      "Average test loss: 0.0015082644580139055\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03454354680577914\n",
      "Average test loss: 0.001362221982744005\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03553690131836467\n",
      "Average test loss: 0.0019167922207464774\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03407212728261948\n",
      "Average test loss: 0.0014477154096174572\n",
      "Epoch 257/300\n",
      "Average training loss: 0.035542264107200836\n",
      "Average test loss: 0.0014030488405583634\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03436014276577367\n",
      "Average test loss: 0.0013480231242461336\n",
      "Epoch 259/300\n",
      "Average training loss: 0.034028144945700965\n",
      "Average test loss: 0.0013815000977160203\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03586089426279068\n",
      "Average test loss: 0.0014403118007919856\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0345646938946512\n",
      "Average test loss: 0.0012936286753457453\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03387279999918408\n",
      "Average test loss: 0.0013739721110711495\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034183504811591575\n",
      "Average test loss: 0.003356763295518855\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03757727240853839\n",
      "Average test loss: 0.0015345393099511662\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03383662434750133\n",
      "Average test loss: 0.0016678293074170749\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03504136020110713\n",
      "Average test loss: 0.0014071528414367801\n",
      "Epoch 267/300\n",
      "Average training loss: 0.033983864463037916\n",
      "Average test loss: 0.0015251724090841082\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03404624444577429\n",
      "Average test loss: 2.727781722333696\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03464599785539839\n",
      "Average test loss: 0.0014145454254208339\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03414586318532626\n",
      "Average test loss: 0.002305389816769295\n",
      "Epoch 271/300\n",
      "Average training loss: 0.034035119235515596\n",
      "Average test loss: 0.001359030279951791\n",
      "Epoch 272/300\n",
      "Average training loss: 0.034402644677294626\n",
      "Average test loss: 0.0013558932758039898\n",
      "Epoch 273/300\n",
      "Average training loss: 0.036667125024729304\n",
      "Average test loss: 0.0014524895692658094\n",
      "Epoch 274/300\n",
      "Average training loss: 0.033594689150651294\n",
      "Average test loss: 0.001338786083790991\n",
      "Epoch 275/300\n",
      "Average training loss: 0.034062452210320365\n",
      "Average test loss: 0.0018022698321276242\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0340428813331657\n",
      "Average test loss: 0.0014326130048268372\n",
      "Epoch 277/300\n",
      "Average training loss: 0.034749845892190934\n",
      "Average test loss: 0.0013809231536773344\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03384490731358528\n",
      "Average test loss: 0.0013458284909526508\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03392112153437402\n",
      "Average test loss: 0.0013322152921722995\n",
      "Epoch 280/300\n",
      "Average training loss: 0.033983059159583516\n",
      "Average test loss: 0.0014043600615113974\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03436443416111999\n",
      "Average test loss: 0.0014121676283991998\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03549689661463102\n",
      "Average test loss: 0.0037623107958998946\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0338010451760557\n",
      "Average test loss: 0.0014074819311499596\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03383720794651243\n",
      "Average test loss: 0.0013550274097878072\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03405040222406387\n",
      "Average test loss: 0.0014446315916462078\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03376849990089734\n",
      "Average test loss: 0.0016717633648465078\n",
      "Epoch 287/300\n",
      "Average training loss: 0.033687845018174914\n",
      "Average test loss: 0.001484227531382607\n",
      "Epoch 288/300\n",
      "Average training loss: 0.034279389114843475\n",
      "Average test loss: 0.001610389985351099\n",
      "Epoch 289/300\n",
      "Average training loss: 0.033604743199216\n",
      "Average test loss: 0.0013295768643211987\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0345720141099559\n",
      "Average test loss: 0.001365209588367078\n",
      "Epoch 291/300\n",
      "Average training loss: 0.034578241285350586\n",
      "Average test loss: 0.0018766388843456903\n",
      "Epoch 292/300\n",
      "Average training loss: 0.033354878425598145\n",
      "Average test loss: 0.0013213756494224072\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03348073025875621\n",
      "Average test loss: 0.0014228955338605575\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03494834408164024\n",
      "Average test loss: 0.0013857525686422985\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03326994456185235\n",
      "Average test loss: 0.0014213616484258739\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03373344763782289\n",
      "Average test loss: 0.0013181361697303753\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03330332025554445\n",
      "Average test loss: 0.0014769451030426555\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03380795286926958\n",
      "Average test loss: 0.0015624052409289612\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03364748028417428\n",
      "Average test loss: 0.011166061520990398\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03379739893145031\n",
      "Average test loss: 0.0013465315442946222\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 18.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 20.71\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 21.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 22.57\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.34\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.02\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.48\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.95\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.31\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.63\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.74\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.07\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.21\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.78\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.84\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.46\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.62\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.65\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.20\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.76\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.80\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.81\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.03\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.97\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.31\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.43\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.63\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.36\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.15\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.05\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.69\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.90\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.04\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.22\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.55\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.74\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.91\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 34.04\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 34.05\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 34.04\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 34.22\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 34.19\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 34.42\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 34.27\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 34.33\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 34.59\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 34.51\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 34.50\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 34.58\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 34.74\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.83\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.86\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
